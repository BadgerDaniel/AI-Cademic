{
    "0": "Challenges and Applications of Large Language Models Jean Kaddour\u03b1,\u2020,\u2217, Joshua Harris\u03b2,\u2217, Maximilian Mozes\u03b1, Herbie Bradley\u03b3,\u03b4,\u03f5, Roberta Raileanu\u03b6, and Robert McHardy\u03b7,\u2217 \u03b1University College London\u03b2UK Health Security Agency\u03b3EleutherAI \u03b4University of Cambridge\u03f5Stability AI\u03b6Meta AI Research\u03b7InstaDeep Abstract Large Language Models (LLMs) went from non-existent to ubiquitous in the machine learn- ing discourse within a few years. Due to the fast pace of the field, it is difficult to identify the remaining challenges and already fruitful application areas. In this paper, we aim to es- tablish a systematic set of open problems and application successes so that ML researchers can comprehend the field\u2019s current state more quickly and become productive. Contents 1 Introduction 1 2 Challenges 2 2.1 Unfathomable Datasets . . . . . . 2 2.2 Tokenizer-Reliance . . . . . . . . 4 2.3 High Pre-Training Costs . . . . . 6 2.4 Fine-Tuning Overhead . . . . . . 10 2.5 High Inference Latency . . . . . . 11 2.6 Limited Context Length . . . . . . 14 2.7 Prompt Brittleness . . . . . . . . 17 2.8 Hallucinations . . . . . . . . . . . 19 2.9 Misaligned Behavior . . . . . . . 22 2.10 Outdated Knowledge . . . . . . . 27 2.11 Brittle Evaluations . . . . . . . . 27 2.12 Evaluations Based on Static, Human-Written Ground Truth . . 28 2.13 Indistinguishability between Gen- erated and Human-Written Text . 29 2.14 Tasks Not Solvable By Scale . . . 30 2.15 Lacking Experimental Designs . . 31 2.16 Lack of Reproducibility . . . . . . 33 3 Applications 34 3.1 Chatbots . . . . . . . . . . . . . . 34 3.2 Computational Biology . . . . . . 36 3.3 Computer Programming . . . . . 37 *Equal contribution. \u2020{jean.kaddour,robert.mchardy}.20@ucl.ac.uk, joshua.harris@ukhsa.gov.uk Design Unfathomable Datasets, Tokenizer-Reliance, Fine-Tuning Overhead Science Evaluations Based on Static Human-Written Ground Truth, Lacking Experimental Designs, Lack of Reproducibility Behavior Prompt Brittleness, Misaligned Behavior, Outdated Knowledge Detecting Generated Texts, Brittle Evaluations High Pre-Training Costs High Inference Latency, Limited Context Length, Hallucinations Tasks Not Solvable By ScaleFigure 1: Overview of LLM Challenges. Design ing LLMs relates to decisions taken before deployment. Be- havior ial challenges occur during deployment. Science challenges hinder academic progress. 3.4 Creative Work . . . . . . . . . . . 39 3.5 Knowledge Work . . . . . . . . . 40 joshua.harris@ukhsa.gov.uk Design Unfathomable Datasets, Tokenizer-Reliance, Fine-Tuning Overhead Science Evaluations Based on Static Human-Written Ground Truth, Lacking Experimental Designs, Lack of Reproducibility Behavior Prompt Brittleness, Misaligned Behavior, Outdated Knowledge Detecting Generated Texts, Brittle Evaluations High Pre-Training Costs High Inference Latency, Limited Context Length, Hallucinations Tasks Not Solvable By ScaleFigure 1: Overview of LLM Challenges. Design ing LLMs relates to decisions taken before deployment. Be- havior ial challenges occur during deployment. Science challenges hinder academic progress. 3.4 Creative Work . . . . . . . . . . . 39 3.5 Knowledge Work . . . . . . . . . 40 3.6 Law . . . . . . . . . . . . . . . . 42 3.7 Medicine . . . . . . . . . . . . . 43 3.8 Reasoning . . . . . . . . . . . . . 44 3.9 Robotics and Embodied Agents . . 45 3.10 Social Sciences & Psychology . . 46 3.11 Synthetic Data Generation . . . . 48 4 Related Work 49 5 Conclusion 49 1 Introduction Given the quickly growing plethora of LLM re- search papers, we aim to address two questions: (1) Challenges : What problems remain unresolved? and (2) Applications : Where are LLMs currently being applied, and how are the challenges con- straining them? For (1), we group the challenges 1arXiv:2307.10169v1 [cs.CL] 19 Jul 2023 in Fig. 1 into three broader categories \u201cDesign\u201d, \u201cBehavior\u201d, and \u201cScience\u201d. To provide answers for (2), we explore the fields of chatbots, compu- tational biology, computer programming, creative work, knowledge work, law, medicine, reasoning, robotics, and the social sciences. This paper is an opinionated review and assumes familiarity with LLMs and how they work (we refer to more introductory works in Sec. 4). Further, we focus on models trained on text data. We target a technical researcher audience and do not discuss political, philosophical, or moral perspectives on LLMs. 2 Challenges Challenge This box highlights a challenge. 2.1 Unfathomable Datasets Scaling the amount of pre-training data has been one of the major drivers to equip LLMs with general-purpose capabilities [ 256]. The size of pre-training datasets quickly outgrew the number of documents most human teams could manually quality-check. Instead, most data collection proce- dures rely on heuristics regarding data sources and filtering. In this section, we explore the adverse conse- This paper is an opinionated review and assumes familiarity with LLMs and how they work (we refer to more introductory works in Sec. 4). Further, we focus on models trained on text data. We target a technical researcher audience and do not discuss political, philosophical, or moral perspectives on LLMs. 2 Challenges Challenge This box highlights a challenge. 2.1 Unfathomable Datasets Scaling the amount of pre-training data has been one of the major drivers to equip LLMs with general-purpose capabilities [ 256]. The size of pre-training datasets quickly outgrew the number of documents most human teams could manually quality-check. Instead, most data collection proce- dures rely on heuristics regarding data sources and filtering. In this section, we explore the adverse conse- quences of these heuristics and the reality that many model practitioners possess only a nebulous under- standing of the data on which their model has been trained. We refer to this issue as follows. Unfathomable Datasets The size of modern pre-training datasets ren- ders it impractical for any individual to read or conduct quality assessments on the en- compassed documents thoroughly. Near-Duplicates can arise in different forms and have been reported to degrade model per- formance [ 294,200,250]. Near-duplicates are harder to find compared to exact duplicates; fil- tering out of such is a standard step in most data collection pipelines, e.g., using the MinHash algo- rithm [ 57]. Lee et al. [294] propose the NearDup method and find that over 1%of tokens emitted unprompted from a model are part of a memorized sequence of the C4 dataset, e.g., it contains a 61-word sequence repeated 61,036times in the train- ing split. By deduplicating it, they reduce the rate of emitted memorizations by 10x. Abbas et al. [6] introduce SemDeDup , a technique designed to iden- tifysemantic duplicates that, although perceptually distinct, convey predominantly similar information, such as sentences with analogous structures with certain words replaced by synonyms. After apply- ing their method to C4, they find that it improves over NearDup . Similarly, Kaddour [250] find near- duplicates in the Pile [ 165] by clustering document embeddings and identifying clusters gathering du- plicates. Benchmark Data Contamination occurs when the training dataset contains data from or similar to the evaluation test set. This can lead to inflated performance metrics, as the model can memorize ing split. By deduplicating it, they reduce the rate of emitted memorizations by 10x. Abbas et al. [6] introduce SemDeDup , a technique designed to iden- tifysemantic duplicates that, although perceptually distinct, convey predominantly similar information, such as sentences with analogous structures with certain words replaced by synonyms. After apply- ing their method to C4, they find that it improves over NearDup . Similarly, Kaddour [250] find near- duplicates in the Pile [ 165] by clustering document embeddings and identifying clusters gathering du- plicates. Benchmark Data Contamination occurs when the training dataset contains data from or",
    "1": "similar to the evaluation test set. This can lead to inflated performance metrics, as the model can memorize the test data and simply regurgitate it back during testing. Finding and removing all training and test data overlaps is difficult in practice. For example, the GPT-3 authors Brown et al. [59] found a code bug after training, resulting in only partially removing all detected overlaps from the training data. They could not afford to retrain the model, so they used it with the remaining overlaps and \u201ccleaned\u201d variants of the considered benchmarks, with all potentially leaked examples removed. They define overlap- ping examples as examples that share at least 13 consecutive words with any other example in the pre-training set. If an example is shorter than 13 words, they consider it overlapping if it shares all of its words with another example. Similarly, Dodge et al. [125] search for test data in the web-crawled C4 corpus but measure exact matches, normalized for capitalization and punctu- ation. They find various input-and-label contamina- tions of text generation and knowledge completion tasks; and input-only contaminations of the GLUE benchmark. They argue that there are two ways test data can end up in a snapshot of Common Crawl (the original dump source of C4): either a given test set is built from a web text or uploaded after creation. Sainz et al. [472] ask ChatGPT to gener- ate academic benchmark instances, finding that it has memorized multiple ones, including some test splits. Jacovi et al. [237] propose three strategies to mitigate contamination, including encryption and training exclusion controls. 2 Personally Identifiable Information (PII) such as phone numbers and email addresses, have been found within pre-training corpora, resulting in privacy leaks during prompting. Carlini et al. tions of text generation and knowledge completion tasks; and input-only contaminations of the GLUE benchmark. They argue that there are two ways test data can end up in a snapshot of Common Crawl (the original dump source of C4): either a given test set is built from a web text or uploaded after creation. Sainz et al. [472] ask ChatGPT to gener- ate academic benchmark instances, finding that it has memorized multiple ones, including some test splits. Jacovi et al. [237] propose three strategies to mitigate contamination, including encryption and training exclusion controls. 2 Personally Identifiable Information (PII) such as phone numbers and email addresses, have been found within pre-training corpora, resulting in privacy leaks during prompting. Carlini et al. [65,67], Lukas et al. [344] extract PII data by prompting GPT-2; Kulkarni [283] report how an en- gineer yields secret API keys by prompting GitHub Copilot. Henderson et al. [195] discuss the avail- ability of PII in law data across different jurisdic- tions and filter it based on the legal norm in the respective jurisdiction. El-Mhamdi et al. [137] contend that because strong model performance typically requires memorization of the training data [ 146,58], the (undetected) existence of PII in the training data will likely result in models that render them extractable. Pre-Training Domain Mixtures Several stud- ies have argued for diversity in the pre-training corpus [ 165,341,291]. Many popular corpora fol- low this by concatenating datasets from different sources, as illustrated in Table 1. However, it re- mains underexplored what amount of data from different sources is necessary for strong down- stream performances. Finding suboptimal mix- tures can cause low transferability to downstream tasks [ 593,580] and reliance on spurious corre- lations [ 253,618,347]. Xie et al. [622] find do- main mixture proportions by training a small proxy model using group-distributionally robust optimiza- tion [ 471]; surprisingly, they find that the final model trained using their found domain weights yields improved perplexity across all domains, even when it down-weights a domain. Given a tar- get downstream task, Yao et al. [641] , Xie et al. [624] select subsets most useful for pre-training. Longpre et al. [341] measure the effects of domain compositions and find that inclusion of heteroge- neous data sources is broadly beneficial and likely more important than the data quality (as measured stream performances. Finding suboptimal mix- tures can cause low transferability to downstream tasks [ 593,580] and reliance on spurious corre- lations [ 253,618,347]. Xie et al. [622] find do- main mixture proportions by training a small proxy model using group-distributionally robust optimiza- tion [ 471]; surprisingly, they find that the final model trained using their found domain weights yields improved perplexity across all domains, even when it down-weights a domain. Given a tar- get downstream task, Yao et al. [641] , Xie et al. [624] select subsets most useful for pre-training. Longpre et al. [341] measure the effects of domain compositions and find that inclusion of heteroge- neous data sources is broadly beneficial and likely more important than the data quality (as measured by the document quality classifier employed by PaLM [ 86] and GLaM [ 130]) or size, which also motivates smaller yet more diverse pre-training datasets [250]. Fine-Tuning Task Mixtures have to be deter- mined for fine-tuning a pre-trained model on many different tasks, usually with comparatively few ex- amples per task. This technique, which we call multitask-prompted fine-tuned LMs (MTLMs), has demonstrated significant generalization improve- ments with very little additional training compute.Date NameSizeSources Public GB Tokens\u2217 2014 BookCorpus [684, 36]5 GB 11 B Novels Yes 2019 OSCAR [399]6.3 T ? Webpages in 166 languagesYes 2019 WebText [440]40 GB ? Webpages No 12.2020 CC-100 [100]2.5 TB 292 B Webpages in 100 LanguagesYes 12.2020 The Pile [165, 41]825 GB 300 B Science, Webpages, GitHub Code, Law, etc.Yes 2020 C4 [443] 745 GB 156 B Webpages Yes 10.2020 mC4 [631] ? 6.3 T Webpages in 101 LanguagesYes 2021 MassiveText [441]10.5 TB 2.34 T Webpages, Books, News, and CodeNo 12.2021 GLaM [130] ? 1.6 T Webpages, Wikipedia, Conver- sations, Forums, Books, NewsNo 01.2022 Infiniset [551]? 2.81 T Forum dialogs, C4 data, Code, Wikipedia, Web- pagesNo 06.2022 ROOTS [289]1.61 TB 2.34 T Webpages in 46 lan- guages and GitHub Code in 13 lan- guagesYes 11.2022 The Stack [271]6 TB 235 B GitHub Code in 30 languagesYes 04.2023 LLaMA [556] / Red- Pajama [98]2.7 TB 1.2 T Webpages, GitHub Code, Science, Wikipedia, BooksYes 06.2023 RefinedWeb [415]2.8 TB 600 B Webpages Yes Table 1: Overview of Selected Pre-Training Datasets. Over the years, pre-training datasets have become more unfathomable : they grew rapidly in size and diversity, and not all datasets are publicly available (we do not LanguagesYes 2021 MassiveText [441]10.5 TB 2.34 T Webpages, Books, News, and CodeNo 12.2021 GLaM [130] ? 1.6 T Webpages, Wikipedia, Conver- sations, Forums, Books, NewsNo 01.2022 Infiniset [551]? 2.81 T Forum dialogs, C4 data, Code, Wikipedia, Web- pagesNo 06.2022 ROOTS [289]1.61 TB 2.34 T Webpages in 46 lan- guages and GitHub Code in 13 lan- guagesYes 11.2022 The Stack [271]6 TB 235 B GitHub Code in 30 languagesYes 04.2023 LLaMA [556] / Red- Pajama [98]2.7 TB 1.2 T Webpages, GitHub Code, Science, Wikipedia, BooksYes 06.2023 RefinedWeb [415]2.8 TB 600 B Webpages Yes Table 1: Overview of Selected Pre-Training Datasets. Over the years, pre-training datasets have become more unfathomable : they grew rapidly in size and diversity, and not all datasets are publicly available (we do not include datasets that have very little or no information available about them). Unless stated otherwise, the natural language is in English.\u2217We report the number of tokens as provided by the respective paper based on their proposed tokenization scheme. For example, instruction fine-tuning via task in- structions prepended to each set of input-output pairs is a very popular scheme, which we",
    "2": "will later discuss in more detail in Sec. 2.9. Wang et al. [589] propose Super-NaturalInstructions , a fine-tuning dataset with 1,616 diverse tasks and expert-written instructions. Muennighoff et al. [377] extend MTLM to the multilingual setting, showing that fine-tuning on multilingual tasks with English prompts improves results on tasks in all languages. However, similar to the previous paragraph, how to balance the task datasets well remains unclear. 3 As the tasks can vary in size considerably, Raf- fel et al. [443] mix each task in proportion to the number of examples in its \u2019train\u2019 split (up to some max_num_examples ). Jang et al. [239] report that MTLMs can underperform expert LLMs fine- tuned on only a single task because of (i) nega- tive task transfer, where learning multiple tasks at once hinders the learning of some specific tasks, and (ii) catastrophic forgetting of previous tasks when learning new tasks. Iyer et al. [235] study varying task (sets) proportions, finding several trade-offs and concluding that the right values for these parameters depend on the downstream end- goals. Longpre et al. [340] balance different sets of task sources by omitting them, one at a time, and ranking their contributions on the MMLU bench- mark [ 197]; further, they mix the input prompt fel et al. [443] mix each task in proportion to the number of examples in its \u2019train\u2019 split (up to some max_num_examples ). Jang et al. [239] report that MTLMs can underperform expert LLMs fine- tuned on only a single task because of (i) nega- tive task transfer, where learning multiple tasks at once hinders the learning of some specific tasks, and (ii) catastrophic forgetting of previous tasks when learning new tasks. Iyer et al. [235] study varying task (sets) proportions, finding several trade-offs and concluding that the right values for these parameters depend on the downstream end- goals. Longpre et al. [340] balance different sets of task sources by omitting them, one at a time, and ranking their contributions on the MMLU bench- mark [ 197]; further, they mix the input prompt templates of zero- and few-shot prompting; find- ing that this improves the performance in both set- tings. Another trend is to imitate closed-source models like ChatGPT by collecting a dataset of API outputs (against OpenAI\u2019s terms and condi- tions) and fine-tuning an open-source LM with it [540]. However, Gudibande et al. [180] point out that such imitation models are only good at mimicking the proprietary model\u2019s style but not its content, a distinction that has been discussed extensively in the causality literature [ 253]. They conclude that substantial capability gaps between fine-tuned open-sourced and closed-source models remain, motivating future work for better imitation data. 2.2 Tokenizer-Reliance Tokenization is the process of breaking a sequence of words or characters into smaller units called tokens, such that they can be fed into the model. One common tokenization approach is subword to- kenization , where we split words into smaller units, called subwords orWordPieces [490]. The goal is to handle rare and out-of-vocabulary words in a model\u2019s vocabulary effectively while maintain- ing a limited number of tokens per sequence in the interest of computational complexity. Subword to- kenizers are usually trained unsupervised to build a vocabulary and optionally merge rules to encode the training data efficiently. However, the necessity of tokenization comes with multiple drawbacks [ 257]; some of which we discuss below. For example, Ahia et al. [13], Petrov et al. [426] show that the number of tokens nec-essary to convey the same information varies significantly across languages , making the pric- ing policy of API language models, which charge kenization , where we split words into smaller units, called subwords orWordPieces [490]. The goal is to handle rare and out-of-vocabulary words in a model\u2019s vocabulary effectively while maintain- ing a limited number of tokens per sequence in the interest of computational complexity. Subword to- kenizers are usually trained unsupervised to build a vocabulary and optionally merge rules to encode the training data efficiently. However, the necessity of tokenization comes with multiple drawbacks [ 257]; some of which we discuss below. For example, Ahia et al. [13], Petrov et al. [426] show that the number of tokens nec-essary to convey the same information varies significantly across languages , making the pric- ing policy of API language models, which charge users based on the number of processed or gen- erated tokens, potentially unfair. They find that users of many supported languages are overcharged while receiving subpar results, with this group pre- dominantly residing in areas where these APIs are already less affordable. Further, discrepancies between the data that a tokenizer and a model have been trained on can lead to glitch tokens [465], which can sub- sequently cause unexpected model behavior as their corresponding embeddings are essentially un- trained. This coupling between the tokenizer and pre-training corpus creates the burden of a new training run of the tokenizer each time the pre- training corpus is modified. Next, Tokenization schemes that work well in a multilingual setting, particularly with non-space- separated languages such as Chinese or Japanese, remain challenging [157, 91]. Existing subword tokenization schemes are pre- dominantly greedy algorithms trying to encode language as efficiently as possible regarding the number of tokens used. Naturally, these methods favor subwords comprising larger parts of the train- ing data and, therefore, subwords that are shared across many languages. This favors languages with shared scripts like Latin and Cyrillic, result- ing in suboptimal tokenization of low-resource lan- guages [92, 676]. Tokenizer-Reliance Tokenizers introduce several challenges, e.g., computational overhead, language de- pendence, handling of novel words, fixed vocabulary size, information loss, and low human interpretability. Subword-Level Inputs are the dominant paradigm, providing a good trade-off between vocabulary size and sequence length. Byte-Pair Encoding [ 490,577](BPE) starts with the set language as efficiently as possible regarding the number of tokens used. Naturally, these methods favor subwords comprising larger parts of the train- ing data and, therefore, subwords that are shared across many languages. This favors languages with shared scripts like Latin and Cyrillic, result- ing in suboptimal tokenization of low-resource lan- guages [92, 676]. Tokenizer-Reliance Tokenizers introduce several challenges, e.g., computational overhead, language de- pendence, handling of novel words, fixed vocabulary size, information loss, and low human interpretability. Subword-Level Inputs are the dominant paradigm, providing a good trade-off between vocabulary size and sequence length. Byte-Pair Encoding [ 490,577](BPE) starts with the set of symbols (characters or bytes) that comprise the training data. The tokenizer is then trained to learn rules to merge the most frequent pair of two consecutive tokens\u2014defined by the existing vocabulary\u2014into a new vocabulary item. Byte-level BPE (BBPE) [ 577] is an extension of BPE with byte-level subwords, particularly 4 Tokenization can sometimes lead to a loss of information. For example, in languages where word boundaries are not clearly de\ufb01ned, such as Chinese. \u2026 def bubble_sort(array): n = len(array) for i in range(n): swapped = False for j in range(0, n - i - 1): if array[j] > array[j + 1]: swap(array[j], array[j + 1])\u2026.\u6a19\u8a18\u5316\u6709\u6642\u6703\u5c0e\u81f4\u4fe1\u606f\u4e1f\u5931\u3002 \u4f8b\u5982\uff0c\u5728\u55ae\u8a5e\u908a\u754c\u6c92\u6709\u660e\u78ba\u5b9a\u7fa9\u7684\u8a9e\u2f94\u4e2d\uff0c\u4f8b\u5982\u4e2d\u2f42\uff0c\u6216\u8005\u5728\u5177\u6709\u8a31\u591a\u8907\u5408\u8a5e\u7684\u8907\u96dc\u8a9e\u2f94\u4e2d\uff0c......English PythonChinese \u2026]\u55aetoken [to##izationlead \u4fe1loss \u6642iarrayexamplechinese\u606fboundaries \u81f4are \u6a19\u5b9a\u2f94for \u4e2dwhere inas \u5408defSoftmax overVocabularyVocabularytokenwhereas##izationtoforaboundariesexamplelossleadchineseTraining Sequences",
    "3": "\u2f94\u606f\u81f4\u5b9a\u4fe1\u6a19\u591a\u6642\u55ae\u5408\u4e2d\u6703\u660e\u5c0e\u754c\u7fa9\u8a31inidef[_sort],)-forFalsearray+],1are\u2026\u2026\u2026rangeifn(1) Tokenizer Training Costs(2) Arch. depends on Vocabulary \u2026]\u55aetoken [to##izationlead \u4fe1loss \u6642iarrayexamplechinese\u606fboundaries \u81f4are \u6a19\u5b9a\u2f94for \u4e2dwhere inas def bubble_sort(array): n = len(array) for i in range(n): swapped = False for j in range(0, n - i - 1): if array[j] > array[j + 1]: swap(array[j], array[j + 1])\u2026.\u6a19\u8a18\u5316\u6709\u6642\u6703\u5c0e\u81f4\u4fe1\u606f\u4e1f\u5931\u3002 \u4f8b\u5982\uff0c\u5728\u55ae\u8a5e\u908a\u754c\u6c92\u6709\u660e\u78ba\u5b9a\u7fa9\u7684\u8a9e\u2f94\u4e2d\uff0c\u4f8b\u5982\u4e2d\u2f42\uff0c\u6216\u8005\u5728\u5177\u6709\u8a31\u591a\u8907\u5408\u8a5e\u7684\u8907\u96dc\u8a9e\u2f94\u4e2d\uff0c......English PythonChinese \u2026]\u55aetoken [to##izationlead \u4fe1loss \u6642iarrayexamplechinese\u606fboundaries \u81f4are \u6a19\u5b9a\u2f94for \u4e2dwhere inas \u5408defSoftmax overVocabularyVocabularytokenwhereas##izationtoforaboundariesexamplelossleadchineseTraining Sequences \u2f94\u606f\u81f4\u5b9a\u4fe1\u6a19\u591a\u6642\u55ae\u5408\u4e2d\u6703\u660e\u5c0e\u754c\u7fa9\u8a31inidef[_sort],)-forFalsearray+],1are\u2026\u2026\u2026rangeifn(1) Tokenizer Training Costs(2) Arch. depends on Vocabulary \u2026]\u55aetoken [to##izationlead \u4fe1loss \u6642iarrayexamplechinese\u606fboundaries \u81f4are \u6a19\u5b9a\u2f94for \u4e2dwhere inas \u5408defTransformerBlocksEmbedding Matrix<latexit sha1_base64=\"EUAQA2JFnjQN78tKMAv1XK5WuxQ=\">AAACEXicbVC7TsMwFHXKq5RXgJHFokLqVCWI18BQCZAYC6IPqQmV4zqtVceJbAepSvMLLPwKCwMIsbKx8Tc4bQZoOZKl43Pu1b33eBGjUlnWt1FYWFxaXimultbWNza3zO2dpgxjgUkDhywUbQ9JwignDUUVI+1IEBR4jLS84UXmtx6IkDTkd2oUETdAfU59ipHSUtesOAFSA8+HV9ChHE5/XnKb3ifj5hg6igZEwsu01DXLVtWaAM4TOydlkKPeNb+cXojjgHCFGZKyY1uRchMkFMWMpCUnliRCeIj6pKMpR3qQm0wuSuGBVnrQD4V+XMGJ+rsjQYGUo8DTldnGctbLxP+8Tqz8MzehPIoV4Xg6yI8ZVCHM4oE9KghWbKQJwoLqXSEeIIGw0iFmIdizJ8+T5mHVPqke3xyVa+d5HEWwB/ZBBdjgFNTANaiDBsDgETyDV/BmPBkvxrvxMS0tGHnPLvgD4/MHRwacqQ==</latexit>E2R|V|\u21e5D<latexit sha1_base64=\"VxULw+Mr90KaxUh2GOiiN/OYQpA=\">AAACIXicbVDLTsMwEHR4lvIqcORiUSFxqhLEowcOSHDgWBBtkZpSOe4GLBwnsjeIKuRXuPArXDiAEDfEz+C0PfAaydLszK7WO0EihUHX/XAmJqemZ2ZLc+X5hcWl5crKasvEqebQ5LGM9UXADEihoIkCJVwkGlgUSGgHN0eF374FbUSsznGQQDdiV0qEgjO0Uq9S9yOG10FI29QXio6qIDvLL7PjXuYj3GEWxX2QeU59FBEYet+6p3m5V6m6NXcI+pd4Y1IlYzR6lXe/H/M0AoVcMmM6nptgN2MaBZeQl/3UQML4DbuCjqWK2V3dbHhhTjet0qdhrO1TSIfq94mMRcYMosB2FheY314h/ud1Ugzr3UyoJEVQfLQoTCXFmBZx0b7QwFEOLGFcC/tXyq+ZZhxtqEUI3u+T/5LWds3bq+2e7lQPD8ZxlMg62SBbxCP75JCckAZpEk4eyBN5Ia/Oo/PsvDnvo9YJZzyzRn7A+fwC7oij/A==</latexit>W2RDmodel\u21e5|V| MHA\u2026FFNMHAFFN \u2026Figure 2: Exemplary Drawbacks of relying on Tokenization. (1) The tokenizer training step involves non-trivial computations, e.g., multiple passes over the entire pre-training dataset, and introduces a dependency on it, which can become especially problematic in multilingual settings. (2) The embedding layer Eand output layer Wof LLMs involve the vocabulary size; e.g., making up \u224866% of the model\u2019s parameter count in T5 models [629]. suited for multilingual tasks where it enables vocabulary sharing between languages. A trained BPE tokenizer applies the previously learned rules MHA\u2026FFNMHAFFN \u2026Figure 2: Exemplary Drawbacks of relying on Tokenization. (1) The tokenizer training step involves non-trivial computations, e.g., multiple passes over the entire pre-training dataset, and introduces a dependency on it, which can become especially problematic in multilingual settings. (2) The embedding layer Eand output layer Wof LLMs involve the vocabulary size; e.g., making up \u224866% of the model\u2019s parameter count in T5 models [629]. suited for multilingual tasks where it enables vocabulary sharing between languages. A trained BPE tokenizer applies the previously learned rules to tokenize inputs. WordPiece [ 485,617]is a closed-source tokenization algorithm used, e.g., in BERT [ 120]. Like BPE, WordPiece starts with a small initial vocabulary, which is iteratively extended by learning merge rules and creating new vocabulary items. Rather than selecting the most frequent pair of consecutive tokens, WordPiece uses a scoring function to normalize the frequency of the pair by the frequencies of the individual tokens to prioritize common pairs with rare individual tokens. Unigram Tokenization [ 281] iteratively trims a large base vocabulary to a given target size. To this end, at each step of the tokenizer training, a unigram language model is used to compute a loss over the training data conditional on a certain vocabulary item being removed. A proportion of the subwords with the lowest losses are removed to form the base vocabulary for the next iteration. Unigram tokenization is probabilistic, i.e., during inference, all possible tokenizations of a given sequence are scored using the unigram language model, and the most likely one is selected. SentencePiece [ 282]is a commonly used open-source library, implementing several tokenization algorithms such as (B)BPE and Unigram tokenization. SentencePiece also implements non-subword tokenization approaches like word- and character-level tokenization.Byte-Level Inputs are an alternative to subword tokenization is use byte-level inputs. Byte-level inputs can either be used in combination with sub- word tokenizers [ 577] or used to define a limited vocabulary that can be used to encode all possi- ble sequences. For example, Xue et al. [630] train a non-subword mT5 model using UTF-8 bytes rather than subword tokens as inputs, show- ing promising performance on multilingual data. While this enables subword-free LLMs, UTF-8 en- codes Latin languages with fewer bytes than e.g., commonly used open-source library, implementing several tokenization algorithms such as (B)BPE and Unigram tokenization. SentencePiece also implements non-subword tokenization approaches like word- and character-level tokenization.Byte-Level Inputs are an alternative to subword tokenization is use byte-level inputs. Byte-level inputs can either be used in combination with sub- word tokenizers [ 577] or used to define a limited vocabulary that can be used to encode all possi- ble sequences. For example, Xue et al. [630] train a non-subword mT5 model using UTF-8 bytes rather than subword tokens as inputs, show- ing promising performance on multilingual data. While this enables subword-free LLMs, UTF-8 en- codes Latin languages with fewer bytes than e.g., Chinese, Japanese or Korean1. Tay et al. [546] pro- pose the Charformer, a tokenization-free model which learns a soft subword tokenization in la- tent space (Gradient-Based Subword Tokenization) given byte-level inputs. Charformer performs com- parably to subword-based models while incurring less computational overhead than other byte or subword models. Choe et al. [83] train a small- scale, 0.8B language model based on raw byte- level inputs and show that it performs compara- bly. On a smaller scale, Clark et al. [94] show that their tokenization- and vocabulary-free encoder Ca- nine outperforms a comparable tokenization-based model. Yu et al. [652] address the computational cost that byte-level tokenization incurs by segment- ing input sequences into local patches, which can be processed in parallel. Similarly, Horton et al. [212] propose to operate directly on file bytes. In a 1https://www.unicode.org/versions/Unicode15.0.0/ 5 parallel line of work, Rust et al. [467] render text as images and train an encoder model to predict the raw pixels of the images. 2.3 High Pre-Training Costs The vast majority of the training costs go toward the pre-training process. Training a single LLM can require hundreds of thousands of compute hours, which in turn cost millions of dollars and consume energy amounts equivalent to that used by several typical US families annually [ 412,86,44]. Re- cently proposed scaling laws [ 256] posit that model performances scale as a power law with model size, dataset size, and the amount of compute used for training, which is fairly unsustainable and can be classified as Red AI [ 487], where state-of-the-art re- sults are essentially \u201cbought\u201d by spending massive 5 parallel line of work, Rust et al. [467] render text as images and train an encoder model to predict the raw pixels of the images. 2.3 High Pre-Training Costs The vast majority of the training costs go toward the pre-training process. Training a single LLM can require hundreds of thousands of compute hours, which in turn cost millions of dollars and consume energy amounts equivalent to that used by",
    "4": "several typical US families annually [ 412,86,44]. Re- cently proposed scaling laws [ 256] posit that model performances scale as a power law with model size, dataset size, and the amount of compute used for training, which is fairly unsustainable and can be classified as Red AI [ 487], where state-of-the-art re- sults are essentially \u201cbought\u201d by spending massive computational resources. For example, depending on the exact law coefficients, reducing the error from 3% to 2% can require an order of magnitude more data or compute [518]. Unsustainable Loss Power-Law [ 256] Performance increases through larger com- pute budgets but at a decreasing rate if the model or dataset size is fixed, reflecting a power law with diminishing returns. In the following, we look at two lines of work aiming at resolving such issues. Compute-Optimal Training Recipes [ 201,256] In Sec. 2.1, we discussed how the availability of LLM pre-training data has become abundant through the quickly-spread practice of including web-crawled text. Further, thanks to the intro- duction of Transformer models [ 563] and suit- able hardware [ 210], we have scaled models to unprecedented sizes. Assuming that we have not yet reached the limits of data [ 45,568,415] nor model sizes [ 256,206,398]; currently, the main bottleneck is the amount of compute available [ 1]. Given a particular budget, how large should the pre- training corpus and model be to maximize training efficiency? As mentioned at the beginning of this section, one recent proposal is to learn empirical \u201c scaling laws\u201d [201,256], which describe the relationship between LLM performance and the compute bud- get, model, and dataset size. These laws can pro- vide the right scaling recipe for compute-optimal training, ideally, even when extrapolating to largercompute budgets. For example, OpenAI [398] re- port that they were able to accurately predict the model performance of the full-size GPT-4 model based on the performance of a series of smaller models using at most 10,000x less compute than the full model. bottleneck is the amount of compute available [ 1]. Given a particular budget, how large should the pre- training corpus and model be to maximize training efficiency? As mentioned at the beginning of this section, one recent proposal is to learn empirical \u201c scaling laws\u201d [201,256], which describe the relationship between LLM performance and the compute bud- get, model, and dataset size. These laws can pro- vide the right scaling recipe for compute-optimal training, ideally, even when extrapolating to largercompute budgets. For example, OpenAI [398] re- port that they were able to accurately predict the model performance of the full-size GPT-4 model based on the performance of a series of smaller models using at most 10,000x less compute than the full model. The exact power law coefficients are still heav- ily debated. Kaplan et al. [256] put forward that the model size should be scaled more aggressively than the dataset size to use a given compute budget optimally. Contrary to this, Hoffmann et al. [206] find that many LLMs are undertrained and argue that the number of parameters and data should be scaled equally. However, power laws sometimes come in the form of bounds, which can span an order of magnitude difference in the amount of data to be used given a concrete compute budget [665]. Further, the pre-training loss does not al- ways correlate well with downstream performance [252, 332, 251]. The viewpoint of Touvron et al. [556] , Vries [571] , Touvron et al. [557] is that when selecting a model size, the computation resources for later usage (inference) should be considered, not just the one-time training costs. They suggest that it might be beneficial to train a smaller model more intensively upfront to offset larger inference costs in the future. Hence, they train models of various sizes on more tokens than are typically used to achieve the best performance possible, given the model size. One remaining hurdle of performance prediction is inverse scaling, which we discuss in Sec. 2.14. Since scaling laws were typically constructed in the context of pre-training and thereby decoupled from downstream tasks, it remains an open question of how to predict inverse scaling properties. Tay et al. [544] find that scaling laws can differ in upstream and downstream setups; aside from only the model size, model shape matters for downstream fine- tuning. Pre-Training Objectives Various pre-training objectives (PTO) are suitable for performing self- intensively upfront to offset larger inference costs in the future. Hence, they train models of various sizes on more tokens than are typically used to achieve the best performance possible, given the model size. One remaining hurdle of performance prediction is inverse scaling, which we discuss in Sec. 2.14. Since scaling laws were typically constructed in the context of pre-training and thereby decoupled from downstream tasks, it remains an open question of how to predict inverse scaling properties. Tay et al. [544] find that scaling laws can differ in upstream and downstream setups; aside from only the model size, model shape matters for downstream fine- tuning. Pre-Training Objectives Various pre-training objectives (PTO) are suitable for performing self- supervised training of LLMs. The exact choice of PTO heavily influences the model\u2019s data efficiency during pre-training, which in turn can reduce the number of iterations required. A PTO typically is a function of the (i) architecture, (ii) input/tar- gets construction (e.g., target span length, low/high corruption, see Fig. 4), and (iii) masking strategy (Fig. 3). While (i) and (ii) can be disentangled and 6 Targetsy5y4y3y2y1Masked LM Inputx5x4x3x2x1Language Modeling Inputx5x4x3x2x1Pre\ufb01x LM Inputx5x4x3x2x1Figure 3: Masking Strategies. Each row denotes to which inputs xi(columns) a particular output yi(row) can attend to (uni- or bi-directional). should not be conflated conceptually [ 545], in prac- tice, there exist popular combinations that achieve good performances. Attending to all tokens, as shown in Fig. 3(left), is the most data-efficient strategy since it uses con- text from before and after the token to be predicted. However, for that reason, it is unsuitable for text generation [ 120], since it considers future context for prediction. We typically employ it in natural language understanding (NLU) tasks [ 120], where it has shown strong results. The next token predic- tion objective is most suitable for natural language generation (NLG) but also the least data efficient since it only attends to the past context (Fig. 3(mid- dle)). More recent advances in pre-training objec- tives aim to find a middle-ground to increase data efficiency by providing stronger and more diverse training signals, e.g., the Prefix LM, which partly attends to past tokens, as illustrated in Fig. 3(right) and discussed below. The following discusses the trade-offs between text from before and after the token to be predicted. However, for that reason, it is unsuitable for text generation [ 120], since it considers future context for prediction. We typically employ it in natural language understanding (NLU) tasks [ 120], where it has shown strong results. The next token predic- tion objective is most suitable for natural language generation (NLG) but also the least data efficient since it only attends to the past context (Fig. 3(mid- dle)). More recent advances in pre-training objec- tives aim to find a middle-ground to increase data efficiency by providing stronger and more diverse training signals, e.g., the Prefix LM, which partly attends to past tokens, as illustrated in Fig. 3(right) and discussed below. The following discusses the trade-offs between some of the recently proposed objectives. Fig. 4 visually depicts the different pre-training objectives. Notation-wise, we denote a sequence of Ntokens xasx=x1, . . . , x N. We start with the most",
    "5": "basic and still widely- used Language Modeling [ 59](ornext token pre- diction ) objective. Here, we learn parameters \u03b8by maximizing the likelihood of the next token given the previous tokens, L(x) =NX i=1logP(xi|x1, . . . , x i\u22121;\u03b8).(1) Masked Language Modeling (MLM; or Cloze) [ 549,120]hides a set proportion of tokens in the sequence by replacing them with a special [MASK] token. The literature employs the MLM objective for non-autoregressive, i.e., non-generative, bidirectional context models,where the model uses tokens before and after the target token for predictions, leveraging a more holistic understanding of its context than the NTP objective. Furthermore, we can use each input sentence to predict multiple masked tokens in a single pass, while the NTP objective typically learns from predicting one token at a time. LetxMASK denote the set of indices of the masked tokens and x\u00acMASK the unmasked tokens. The objective of MLM is then to maximize the likelihood given the parameters \u03b8, L(xMASK|x\u00acMASK ) =1 |xMASK| \u00b7X i\u2208xMASKlogP(xMASK i|x\u00acMASK ;\u03b8).(2) Patel et al. [410] show that such models produce representations more suitable for transfer learning; however, they come with difficulties in performing in-context learning (Sec. 2.7). To further improve the training efficiency of the MLM objective, Bajaj et al. [33] propose to replace input tokens with ones generated by an auxiliary language model (ALM), resulting in a Model gen- erated dEnoising TRaining Objective (METRO). single pass, while the NTP objective typically learns from predicting one token at a time. LetxMASK denote the set of indices of the masked tokens and x\u00acMASK the unmasked tokens. The objective of MLM is then to maximize the likelihood given the parameters \u03b8, L(xMASK|x\u00acMASK ) =1 |xMASK| \u00b7X i\u2208xMASKlogP(xMASK i|x\u00acMASK ;\u03b8).(2) Patel et al. [410] show that such models produce representations more suitable for transfer learning; however, they come with difficulties in performing in-context learning (Sec. 2.7). To further improve the training efficiency of the MLM objective, Bajaj et al. [33] propose to replace input tokens with ones generated by an auxiliary language model (ALM), resulting in a Model gen- erated dEnoising TRaining Objective (METRO). Their approach consists of roughly three compo- nents: (i) train an ALM using the MLM objec- tive, (ii) given some inputs with masked positions, predict the tokens (with the ALM), (iii) train the main model to correct these tokens inserted in the masked positions, i.e., 1) predict whether the ALM has replaced a token and if so, 2) predict the origi- nal token. They train the auxiliary and main model jointly. Prefix Language Modeling [ 443]generalizes language modeling by allowing prefix tokens with a bidirectional receptive field to be added to the input (without prefix, it is equivalent to standard LM). Note that this is still different from the bidirectional context as in MLM, where we always condition on all the tokens before and after the masked ones (see Fig. 3 left). For computing the hidden states of the prefix, prefix-LM attends to tokens before and after (see Fig. 3 right). Span Corruption [ 303,443,132]orspan de- noising refers to a group of denoising objectives that generalize MLM to denoise contiguous se- quences of tokens within a given text, called spans . The denoising objectives typically replace the sam- pled spans with a single unique masking token and train the model to fill it in. Raffel et al. [443] 7 Inputs language modeling by allowing prefix tokens with a bidirectional receptive field to be added to the input (without prefix, it is equivalent to standard LM). Note that this is still different from the bidirectional context as in MLM, where we always condition on all the tokens before and after the masked ones (see Fig. 3 left). For computing the hidden states of the prefix, prefix-LM attends to tokens before and after (see Fig. 3 right). Span Corruption [ 303,443,132]orspan de- noising refers to a group of denoising objectives that generalize MLM to denoise contiguous se- quences of tokens within a given text, called spans . The denoising objectives typically replace the sam- pled spans with a single unique masking token and train the model to fill it in. Raffel et al. [443] 7 Inputs TargetsArgument is that some argue that if an AI can simulate human behavior (qualia), the \"what it feels like\" aspect of consciousness. The Simulational considered conscious. However, this view doesn't account for subjectiveinputs and generate outputs similar to a conscious being, then it could be underlying physical structure. In other words, if an AI can respond to view that mental states are de\ufb01ned more by their function than their Some proponents of AI consciousness subscribe to functionalism, the 432432 Span Corruption(R-Denoising)Inputs TargetsArgument is that some argue that if an AI can simulate human behavior (qualia), the \"what it feels like\" aspect of consciousness. The Simulational considered conscious. However, this view doesn't account for subjectiveinputs and generate outputs similar to a conscious being, then it could be underlying physical structure. In other words, if an AI can respond to view that mental states are de\ufb01ned more by their function than their Some proponents of AI consciousness subscribe to functionalism, the 12Long Span Corruption(one form of X-Denoising) 1314121314Meet In The MiddleInputs TargetsArgument is that some argue that if an AI can simulate human behavior (qualia), the \"what it feels like\" aspect of consciousness. The Simulational considered conscious. However, this view doesn't account for subjectiveinputs and generate outputs similar to a conscious being, then it could be underlying physical structure. In other words, if an AI can respond to view that mental states are de\ufb01ned more by their function than their Some proponents of AI consciousness subscribe to functionalism, the 56 56Inputs (Reversed Order) 1314121314Meet In The MiddleInputs TargetsArgument is that some argue that if an AI can simulate human behavior (qualia), the \"what it feels like\" aspect of consciousness. The Simulational considered conscious. However, this view doesn't account for subjectiveinputs and generate outputs similar to a conscious being, then it could be underlying physical structure. In other words, if an AI can respond to view that mental states are de\ufb01ned more by their function than their Some proponents of AI consciousness subscribe to functionalism, the 56 56Inputs (Reversed Order) TargetsArgument is that some argue that if an AI can simulate human behavior (qualia), the \"what it feels like\" aspect of consciousness. The Simulational considered conscious. However, this view doesn't account for subjectiveinputs and generate outputs similar to a conscious being, then it could be experiences subjective for accountSimulational The consciousness. of aspect \u201clike feels it what\u201d the (qualia),behavior human simulate can AI an if that argue some that is Argument 52 52Inputs TargetsArgument is that some argue that if an AI can simulate human behavior (qualia), the \u201cwhat it feels like\u201d aspect of consciousness. The Simulational considered conscious. However, this view doesn't account for subjectiveinputs and generate outputs similar to a conscious being, then it could be underlying physical structure. In other words, if an AI can respond to view that mental states are de\ufb01ned more by their function than their Some proponents of AI consciousness subscribe to functionalism, the Fill In The Middle 26 26MoveInputs TargetsArgument is that some argue that if an AI can simulate human behavior (qualia), the \"what it feels like\" aspect of consciousness. The Simulational considered conscious. However, this view doesn't account for subjectiveinputs and generate outputs similar to a conscious being, then it could be underlying physical structure. In other words, if an AI can respond to view that mental states are de\ufb01ned more by their",
    "6": "function than their Some proponents of AI consciousness subscribe to functionalism, the Pre\ufb01x Language Modeling (S-Denoising) 56 56Figure 4: Self-Supervised Data Construction by Pre-Training Objectives , adopted from Tay et al. [545] . We indicate masked tokens with gray rectangles, which become the targets. For brevity, we omit special tokens. shows that this can speed up training because span 56 56Figure 4: Self-Supervised Data Construction by Pre-Training Objectives , adopted from Tay et al. [545] . We indicate masked tokens with gray rectangles, which become the targets. For brevity, we omit special tokens. shows that this can speed up training because span corruption produces shorter sequences on average compared to corrupting individual tokens in an i.i.d. manner. Mixture of Denoisers [ 545](MoD) refers to injecting objective diversity by mixing multiple denoising objectives. Tay et al. [545] categorize three denoising objectives: {R,S,X}-Denoiser. The regular denoising corresponds to the previously in- troduced span denoising. Specific denoising com- prises splitting a given sequence into a prefix act- ing as the context and a suffix acting as the target. In extreme denoising, we corrupt large parts of the input by either (a) increasing the proportion of masked tokens per span or (b) increasing the span length forcing the model to generate long se-quences with limited context, which we illustrate in Fig. 4). The MoD objective has subsequently been shown to improve model performance by con- tinuing training pre-trained LLMs [ 443,86] for relatively few steps [547]. Fill In the Middle Bavarian et al. [38] propose to augment the next token prediction objective by shuffling tokens within a document such that we fill in the middle (FIM) based on prefix and suf- fix. They demonstrate that models pre-trained on a mixture of FIM-transformed and left-to-right data result in left-to-right and FIM capability models. Meet in the Middle Nguyen et al. [382] extend the FIM objective by enabling bidirectional context to construct a denser, more data-efficient supervi- sion signal while maintaining the autoregressive 8 nature of the underlying model: They train two decoders\u2014one forward\u2212 \u2192p(xi|x<i;\u03b8)and one backward language model\u2190 \u2212p(xi|x<i;\u03b8)\u2014with shared parameters \u03b8. Additionally, they add an agreement regularize to the loss, encouraging the forward and backward model to agree: for a dataset Sof sequences, the full pre-training loss is X x\u2208S|x|X i=1\u2212log\u2212 \u2192p(xi|x<i;\u03b8)| {z } NLL for forward model \u2212log\u2190 \u2212p(xi|x>i;\u03b8)| {z } NLL for backward model +\u03b2DTV i,x(\u2212 \u2192p\u2225\u2190 \u2212p)|{z } agreement regularizer,(3) where DTV i,x(\u2212 \u2192p\u2225\u2190 \u2212p)is the total variation distance among the two models on the i-th token. Once pre-training has been completed, we can use only the forward model\u2212 \u2192p. Parallelism Strategies The sheer size of LLMs sion signal while maintaining the autoregressive 8 nature of the underlying model: They train two decoders\u2014one forward\u2212 \u2192p(xi|x<i;\u03b8)and one backward language model\u2190 \u2212p(xi|x<i;\u03b8)\u2014with shared parameters \u03b8. Additionally, they add an agreement regularize to the loss, encouraging the forward and backward model to agree: for a dataset Sof sequences, the full pre-training loss is X x\u2208S|x|X i=1\u2212log\u2212 \u2192p(xi|x<i;\u03b8)| {z } NLL for forward model \u2212log\u2190 \u2212p(xi|x>i;\u03b8)| {z } NLL for backward model +\u03b2DTV i,x(\u2212 \u2192p\u2225\u2190 \u2212p)|{z } agreement regularizer,(3) where DTV i,x(\u2212 \u2192p\u2225\u2190 \u2212p)is the total variation distance among the two models on the i-th token. Once pre-training has been completed, we can use only the forward model\u2212 \u2192p. Parallelism Strategies The sheer size of LLMs makes it hard to train or even do inference with them on only one accelerator (GPU, TPU, etc.). A common solution is model parallelism , which can be viewed as a divide-and-conquer strategy: we slice up various parts of the model (dividing the problem into sub-problems), distribute them across multiple devices, with each device comput- ing a portion of the overall computation (solve each problem independently) and combine all results to produce the final output (forward/backward pass). Implementing model parallelism synchronously creates a problem where running data batches through multiple workers with sequential depen- dency (each layer depends on results from the pre- vious layer) leads to significant waiting times and under-utilization of computation resources. Another strategy is pipeline parallelism , which combines model parallelism with data parallelism , meaning that we not only distribute parts of the model across different devices but parts of the data too, i.e., each worker splits its mini-batch further into micro-batches with gradients being accumu- lated across all micro-batches before the weight update. Huang et al. [226] instantiate such an ap- proach called GPipe , which divides each mini- batch into smaller micro-batches distributed across different accelerators simultaneously; gradients are applied synchronously at the end. Compared to naive model parallelism, this decreases waitingtimes and increases the utilization of computational resources. These issues have motivated asynchronous paral- lelization schemes. Recht et al. [453] present Hog- wild! , which greedily applies gradients to the local weights on each accelerator as soon as they arrive, offering better resource utilization than pipeline too, i.e., each worker splits its mini-batch further into micro-batches with gradients being accumu- lated across all micro-batches before the weight update. Huang et al. [226] instantiate such an ap- proach called GPipe , which divides each mini- batch into smaller micro-batches distributed across different accelerators simultaneously; gradients are applied synchronously at the end. Compared to naive model parallelism, this decreases waitingtimes and increases the utilization of computational resources. These issues have motivated asynchronous paral- lelization schemes. Recht et al. [453] present Hog- wild! , which greedily applies gradients to the local weights on each accelerator as soon as they arrive, offering better resource utilization than pipeline parallelism but suffering from training instabilities due to stale gradients which are based on outdated model weights. Gomez et al. [172] propose N-Wise interlock- ing backpropagation , which is a generalization of end-to-end and local training. While end-to-end (global) training performs a forward pass through all layers, computes a loss and gradients, and back- propagates through all layers, local training per- forms forward passes through all layers individ- ually and immediately computes a local loss and gradient update, offering higher resource utilization at the cost of (empirically) worse task performance. N-Wise interlocking backpropagation strikes a com- promise by performing a forward pass through N layers before computing a loss and updating the parameters of the associated layers, enabling better layer communication than local training and higher computational efficiency than end-to-end training. Chowdhery et al. [86] leverage a combination of model parallelism and fully sharded data par- allelism (FSDP) [ 628,674]\u2014a technique where each device only holds a subset of the model pa- rameters, gradients, and optimizer states, and pa- rameters necessary for local computations are com- municated on-demand\u2014to enable highly parallel, high throughput training across thousands of chips within a single TPU pod. PaLM further employs data parallelism to achieve scaling at pod level, leveraging the Pathways [ 37] system to distribute data. In a parallel line of work, Lepikhin et al. [298] propose GShard , a model parallelism method that extends the XLA [ 468] compiler, enabling auto- matic sharding of models. Miscellaneous Rae et al. [441] stack the lay- ers of a 4.5B parameter model to jump-start and of model parallelism and fully sharded data par- allelism (FSDP) [ 628,674]\u2014a technique where each device only holds a subset of the model pa-",
    "7": "rameters, gradients, and optimizer states, and pa- rameters necessary for local computations are com- municated on-demand\u2014to enable highly parallel, high throughput training across thousands of chips within a single TPU pod. PaLM further employs data parallelism to achieve scaling at pod level, leveraging the Pathways [ 37] system to distribute data. In a parallel line of work, Lepikhin et al. [298] propose GShard , a model parallelism method that extends the XLA [ 468] compiler, enabling auto- matic sharding of models. Miscellaneous Rae et al. [441] stack the lay- ers of a 4.5B parameter model to jump-start and accelerate the training of a 9B model, which led to a 40% reduction in compute; an idea that has been previously used for training smaller-scale LMs [ 173]. Brown et al. [59] progressively in- crease the batch size from a small to the full value over training when training GPT-3; a trick that has been previously used for training image mod- 9 els [514]. Sanyal et al. [476] apply latest weight av- eraging [ 249] to LLMs between 1 and 12B param- eters; for a 6.9B parameter model, they reach sav- ings of up to 4,200 GPU hours. For smaller-scale models, there exist various pre-training speedup al- gorithms [ 663,685], but they have not been scaled up yet and shown to offer only limited gains when compared with budget-adjusted baselines [251]. 2.4 Fine-Tuning Overhead A potential drawback of pre-training LLMs on mas- sive and diverse sets of textual data is that the re- sulting models might struggle to explicitly cap- ture the distributional properties of task-specific datasets. To address this, fine-tuning refers to adapting the pre-trained model parameters on com- paratively smaller datasets that are specific to an individual domain or task. LLM fine-tuning is highly effective at adapting LLMs for downstream tasks [215, 120, 440]. Technically speaking, fine-tuning can be achieved by further training a model on a smaller dataset. Depending on the model architecture, this is done by either (i) directly fine-tuning pre-trained models using a standard language modeling objec- tive or (ii) adding individual learnable layers to the output representations of a pre-trained language model, which are designed to create compatibil- ity between the model\u2019s output representations and ture the distributional properties of task-specific datasets. To address this, fine-tuning refers to adapting the pre-trained model parameters on com- paratively smaller datasets that are specific to an individual domain or task. LLM fine-tuning is highly effective at adapting LLMs for downstream tasks [215, 120, 440]. Technically speaking, fine-tuning can be achieved by further training a model on a smaller dataset. Depending on the model architecture, this is done by either (i) directly fine-tuning pre-trained models using a standard language modeling objec- tive or (ii) adding individual learnable layers to the output representations of a pre-trained language model, which are designed to create compatibil- ity between the model\u2019s output representations and the output formats of individual downstream tasks (e.g., for text classification or sequence labeling). See Devlin et al. [120] (Figure 1) for an illustration. However, LLMs with billions of parameters have large memory requirements to store (i) the model parameters, (ii) the model activations, and (iii) the gradients and corresponding statistics. Due to lim- ited device memory (e.g., GPU or TPU) necessi- tates access to large clusters with many devices to fine-tune a full LLM, limiting access to a few institutions with large compute resources. Large Memory Requirements Fine-tuning entire LLMs requires the same amount of memory as pre-training, render- ing it infeasible for many practitioners. Moreover, while full model fine-tuning is ef- fective at adapting LLMs to perform well on spe- cific downstream tasks, individual copies of fine- tuned LLMs need to be stored and loaded for individual tasks, which is computationally ineffi-cient [ 213,311] and requires practitioners to keep individual fine-tuned LLMs in memory for every task. We illustrate this overhead in Figure 5. Overhead of Storing and Loading Fine-Tuned LLMs [213, 311] When adapting an LLM via full-model fine- tuning, an individual copy of the model must be stored (consuming data storage) and loaded (expending memory allocation, etc.) for each task. Parameter-efficient fine-tuning An alternative method to adapt an LLM to a specific dataset/do- main is via parameter-efficient fine-tuning (PEFT). PEFT refers to a class of methods that adapt LLMs by updating only a small subset of model parame- ters. Adapters [ 213]are one of the earliest works on PEFT. This method incorporates additional, learnable layers into a Transformer architecture that individual fine-tuned LLMs in memory for every task. We illustrate this overhead in Figure 5. Overhead of Storing and Loading Fine-Tuned LLMs [213, 311] When adapting an LLM via full-model fine- tuning, an individual copy of the model must be stored (consuming data storage) and loaded (expending memory allocation, etc.) for each task. Parameter-efficient fine-tuning An alternative method to adapt an LLM to a specific dataset/do- main is via parameter-efficient fine-tuning (PEFT). PEFT refers to a class of methods that adapt LLMs by updating only a small subset of model parame- ters. Adapters [ 213]are one of the earliest works on PEFT. This method incorporates additional, learnable layers into a Transformer architecture that are updated during fine-tuning whilst keeping the remainder of the network unchanged. Experimen- tal results on 26 text classification tasks (incl. the GLUE benchmark [ 575]) reveal that models trained via Adapters are competitive with full fine-tuning while updating only 3% of the model\u2019s parame- ters. Ben Zaken et al. [40] instead propose only to update the model\u2019s bias terms for fine-tuning, which make up less than 1% of the model\u2019s pa- rameters. Experimental results show competitive performance across tasks of the GLUE benchmark. We are aware of three general frameworks for incor- porating adapters into language model fine-tuning, namely AdapterHub [ 428], LLM-Adapters [ 219], and HuggingFace\u2019s PEFT library [356]. PEFT methods introduced for larger mod- els include prefix-tuning [311] and prompt- tuning [299], which both operate by prepending a set of learnable token embeddings to an input. These token embeddings (also referred to as soft prompts [299]) are learned during the fine-tuning stage, whereas the remainder of the model parame- ters remains fixed. Most notably, such soft prompts contain thousands rather than millions of param- eters and are much more efficient to store. No- tably, one still has to backpropagate through the network while fine-tuning the tokens. Alternatives for models with only black-box API access have been proposed too [528, 122]. It has been shown that prompt-tuning can learn generalizable representations with very small 10 Fine-tuning LLM #2 Fine-tuning LLM #1 Fine-tuning LLM #3 Sentiment model QA model Hate speech model Sentiment analysis task Question answering task Hate speech task(a) Base LLM (PEFT-adaptable) PEFT weights Sentiment analysis task PEFT weights PEFT weights stage, whereas the remainder of the model parame- ters remains fixed. Most notably, such soft prompts contain thousands rather than millions of param- eters and are much more efficient to store. No- tably, one still has to backpropagate through the network while fine-tuning the tokens. Alternatives for models with only black-box API access have been proposed too [528, 122]. It has been shown that prompt-tuning can learn generalizable representations with very small 10 Fine-tuning LLM #2 Fine-tuning LLM #1 Fine-tuning LLM #3 Sentiment model QA model Hate speech model Sentiment analysis task Question answering task Hate speech task(a) Base LLM (PEFT-adaptable) PEFT weights Sentiment analysis task PEFT",
    "8": "weights PEFT weights Sentiment model QA model Hate speech model Question answering task Hate speech task (b) Figure 5: Fine-tuning an LLM for a specific down- stream task . (a) illustrates vanilla fine-tuning, which requires updating the entire model, resulting in a new model for each task. In (b), PEFT instead learns a small subset of model parameters for each task with a fixed base LLM. The same base model can be re-used during inference for different tasks. amounts of training data, achieving competitive performances when trained on less than 100 exam- ples for safety classification [ 376] or five examples for multilingual question answering [ 11]. In addi- tion to that, recent work investigates the potential of using soft prompts for pre-training and transfer learning across different tasks [179, 572]. Liu et al. [331] introduce (IA)3, which scales activations in individual Transformer layers with learnable vectors. The authors demonstrate its ef- fectiveness by showing that models trained using (IA)3outperform full model fine-tuning on various datasets whilst updating only 0.01% of the model\u2019s parameters. Malladi et al. [355] propose a memory-efficient zeroth-order (MeZO) optimizer, which only re- quires the same memory footprint as during in- ference (instead of storing gradients or optimizer states). Further, it can optimize non-differentiable objectives like accuracy or F1 scores, which con- ventional gradient-based tuning methods cannot. Hu et al. [218] propose Low-Rank Adaptation (LoRA), which formulates parameter updates ofweight matrices at individual Transformer layers as an additive low-rank decomposition. Such a repa- rameterization avoids the need to compute dense matrix multiplications. Dettmers et al. [118] ex- fectiveness by showing that models trained using (IA)3outperform full model fine-tuning on various datasets whilst updating only 0.01% of the model\u2019s parameters. Malladi et al. [355] propose a memory-efficient zeroth-order (MeZO) optimizer, which only re- quires the same memory footprint as during in- ference (instead of storing gradients or optimizer states). Further, it can optimize non-differentiable objectives like accuracy or F1 scores, which con- ventional gradient-based tuning methods cannot. Hu et al. [218] propose Low-Rank Adaptation (LoRA), which formulates parameter updates ofweight matrices at individual Transformer layers as an additive low-rank decomposition. Such a repa- rameterization avoids the need to compute dense matrix multiplications. Dettmers et al. [118] ex- tend LoRA to quantized LLMs, drastically reduc- ing memory usage, allowing them to fine-tune a 65B model on a single 48GB GPU. The authors mention that regular training of the same model requires more than 780 GB of GPU memory. Compute Requirements However, despite sub- stantial improvements in memory complexity needed to fine-tune LLMs for specific tasks, a re- maining challenge is the time complexity . Fine- tuning an LLM, even with PEFT methods, still requires full gradient computation. The compu- tational infrastructure needed to adapt LLMs pro- hibits potential applications like personalization on smaller devices. Full Matrix Multiplications Parameter-efficient fine-tuning of LLMs still requires computing full forward/back- ward passes throughout the whole network. 2.5 High Inference Latency According to Pope et al. [431] , Weng [605] , two reasons why LLMs exhibit high inference latencies are: (1) low parallelizability since the inference procedure proceeds one token at a time and (2) large memory footprints , due to the model size and the transient states needed during decoding (e.g., attention key and value tensors). Further, the authors also discuss the quadratic scaling of the attention mechanisms in Transformers, which we discuss separately in Sec. 2.6. High Inference Latency [431, 605] LLM inference latencies remain high be- cause of low parallelizability and large mem- ory footprints. In the following section, we review techniques used to address these challenges by e.g., reduc- ing the memory footprint (size and/or bandwidth), or accelerating specific computational operations. Note that some of these techniques may also be are: (1) low parallelizability since the inference procedure proceeds one token at a time and (2) large memory footprints , due to the model size and the transient states needed during decoding (e.g., attention key and value tensors). Further, the authors also discuss the quadratic scaling of the attention mechanisms in Transformers, which we discuss separately in Sec. 2.6. High Inference Latency [431, 605] LLM inference latencies remain high be- cause of low parallelizability and large mem- ory footprints. In the following section, we review techniques used to address these challenges by e.g., reduc- ing the memory footprint (size and/or bandwidth), or accelerating specific computational operations. Note that some of these techniques may also be applicable during the training process, but we dis- cuss them here since they are not only designed for training, like the approaches discussed in Sec. 2.3. 11 Efficient Attention Roughly two lines of work aim to accelerate attention mechanism computa- tions by (i) lower-level hardware-aware modifica- tions or (ii) higher-level sub-quadratic approxima- tions of the attention mechanism. For the former, multi-query attention [ 493] aims to reduce memory bandwidth bottlenecks when se- quentially generating sequences of tokens using Transformer decoder layers by keeping only one attention head for the key and value tensors. Sim- ilarly, Dao et al. [107] , Pagliardini et al. [404] re- duce memory bandwidth by proposing an alter- native computation method for multi-head self- attention, called FlashAttention , to minimize the number of I/O operations to speed up the com- putation on modern GPUs. As an optimized atten- tion implementation, FlashAttention lever- ages operator fusion to reduce the memory band- width bottleneck. Pagliardini et al. [404] build on top of FlashAttention and incorporate at- tention sparsity patterns, encompassing key/query dropping and hashing-based attention. Pope et al. [432] implement different sharding techniques to efficiently spread the feedforward and attention computations across devices while optimizing for inter-device communication costs, enabling context lengths of up to 43,000 tokens using multi-query attention. With regards to the second stream of work, a common theme to improve the computational or memory complexity of the attention mechanism is to sparsify the attention matrix or introducing (lin- ear) approximations [543]. However, the scalabil- tion implementation, FlashAttention lever- ages operator fusion to reduce the memory band- width bottleneck. Pagliardini et al. [404] build on top of FlashAttention and incorporate at- tention sparsity patterns, encompassing key/query dropping and hashing-based attention. Pope et al. [432] implement different sharding techniques to efficiently spread the feedforward and attention computations across devices while optimizing for inter-device communication costs, enabling context lengths of up to 43,000 tokens using multi-query attention. With regards to the second stream of work, a common theme to improve the computational or memory complexity of the attention mechanism is to sparsify the attention matrix or introducing (lin- ear) approximations [543]. However, the scalabil- ity of some efficient Attention approximations has been questioned. For example, Tay et al. [542] , Hua et al. [220] find that the Performer attention approx- imation [ 85] severely underperforms the vanilla self-attention mechanism, especially when scaled up to large models. Quantization is a post-training technique that reduces the memory footprint and/or increases the model\u2019s throughput by reducing the computational precision of weights and activations. nuQmm [407] andZeroQuant [643] use a non-uniform quan- tization",
    "9": "method to quantize weights and apply custom CUDA kernels for computational benefits. LLM.int8() [117] is a degradation-free quanti- zation scheme enabling efficient inference of multi- billion parameter LLMs by utilizing Int8 quantiza- tion and falling back to higher precision for certain outlier features without the need for re-training.Similarly, GLM-130B [ 658] uses a degradation- free 8-bit quantization scheme, storing weights in 8-bit and performing matrix multiplications in 16- bit precision. Frantar et al. [153] propose an effi- cient, one-shot quantization technique to compress LLM weights down to 3 to 4 bits per weight, en- abling 175B parameter models to be run on a single GPU. Dettmers et al. [119] further improve upon this by combining higher precision representations for outlier weights and grouped quantization. Pruning is a complementary post-training tech- nique to quantization, removing parts of the weights of a given model (without degrading its per- formance). An important distinction is whether the pruning follows a structured pattern or is unstruc- tured . Structured sparse models substitute dense sections of a model with an assembly of signifi- free 8-bit quantization scheme, storing weights in 8-bit and performing matrix multiplications in 16- bit precision. Frantar et al. [153] propose an effi- cient, one-shot quantization technique to compress LLM weights down to 3 to 4 bits per weight, en- abling 175B parameter models to be run on a single GPU. Dettmers et al. [119] further improve upon this by combining higher precision representations for outlier weights and grouped quantization. Pruning is a complementary post-training tech- nique to quantization, removing parts of the weights of a given model (without degrading its per- formance). An important distinction is whether the pruning follows a structured pattern or is unstruc- tured . Structured sparse models substitute dense sections of a model with an assembly of signifi- cantly smaller yet still dense components. Unstruc- tured sparse models contain weights of value zero, which do not influence the network\u2019s behavior and can therefore be committed in theory. However, in practice, it is more challenging to translate theo- retical to practical computation savings on current hardware [161, 112, 336]. On the structured side, early work on pruning language models mainly aims at comparatively small MLM-type models [ 592,143,243]. Ma et al. [349] propose LLM-Pruner, which aims at pruning LLMs in a task-agnostic manner while preserving the zero-shot capabilities of the models. To this end, LLM-Pruner adopts a three-stage pruning pro- cedure where 1) interdependent structures within the model are identified and grouped, 2) the contri- bution to the overall performance is estimated for each group, and low-performing groups are pruned, 3) performance recovery via parameter-efficient fine-tuning procedure using LoRA [218]. On the unstructured side, SparseGPT [ 152] is an unstructured pruning approach specifically devel- oped to be fast enough to be run on LLMs with hundreds of billions of parameters within a few hours, being able to prune the number of parame- ters by up to 60% while maintaining roughly the same model performance. Sun et al. [527] pro- pose Wanda (Pruning by Weights and a ctivations), which applies magnitude pruning based on the product of each weight\u2019s magnitude and the norm of the corresponding input activations, matching SparseGPT in performance while requiring only a single forward pass to prune the network. Both SparseGPT and Wanda can be extended to per- 12 form semi-structured pruning, enabling n:m spar- fine-tuning procedure using LoRA [218]. On the unstructured side, SparseGPT [ 152] is an unstructured pruning approach specifically devel- oped to be fast enough to be run on LLMs with hundreds of billions of parameters within a few hours, being able to prune the number of parame- ters by up to 60% while maintaining roughly the same model performance. Sun et al. [527] pro- pose Wanda (Pruning by Weights and a ctivations), which applies magnitude pruning based on the product of each weight\u2019s magnitude and the norm of the corresponding input activations, matching SparseGPT in performance while requiring only a single forward pass to prune the network. Both SparseGPT and Wanda can be extended to per- 12 form semi-structured pruning, enabling n:m spar- sity [ 228,680] and achieving the corresponding speed-ups on recent GPUs [369]. Mixture-of-Experts architectures typically con- sist of a set of experts (modules) , each with unique weights, and a router (orgating ) network, which determines which expert module processes an in- put. MoE models decrease inference time by not using all experts at once but only activating a sub- set of them. Further, they can reduce communica- tion across devices in model-distributed settings by placing each expert on a separate accelerator; only the accelerators hosting the router and the relevant expert model must communicate. Shazeer et al. [495] propose one of the first MoE layers embed- ded within a language model, which they refer to assparsely-gated MoEs (SG-MoEs). They denote byG(x)andEi(x)the gating network output and thei-th expert network output for a given input x, respectively. We can then write the output as y=Pn i=1G(x)iEi(x). Wherever G(x)i= 0, we do not need to compute Ei(x), thereby saving compute during inference. Lepikhin et al. [298] scale up an SG-MoE model to 600B parameters by proposing GShard , a model parallelism method that extends the XLA [ 468] compiler. While SG- MoE selects the top- kexperts with k > 1, the Switch Transformer (ST) [ 145] architecture uses k= 1experts, which reduces routing computation and communication across experts (which may be located on different accelerators). ST empirically outperformed a strongly tuned T5 model with up to 7x pre-training speedups. Lewis et al. [302] notice that the learned routers can result in unbalanced assignments across experts. To ensure balanced routing, they formulate a linear assignment prob- lem that maximizes token-expert affinities while we do not need to compute Ei(x), thereby saving compute during inference. Lepikhin et al. [298] scale up an SG-MoE model to 600B parameters by proposing GShard , a model parallelism method that extends the XLA [ 468] compiler. While SG- MoE selects the top- kexperts with k > 1, the Switch Transformer (ST) [ 145] architecture uses k= 1experts, which reduces routing computation and communication across experts (which may be located on different accelerators). ST empirically outperformed a strongly tuned T5 model with up to 7x pre-training speedups. Lewis et al. [302] notice that the learned routers can result in unbalanced assignments across experts. To ensure balanced routing, they formulate a linear assignment prob- lem that maximizes token-expert affinities while equally distributing the number of tokens across experts. Yu et al. [653] propose sMLP , an MoE using only MLPs blocks, which (i) they scale up to 10B, (ii) results in a 2x improvement in pre-training speed, and (iii) outperforms sparse Transformer counterparts. However, MoE models still suffer from unique issues like expert collapse (all experts learning the same), likely caused by underconstrained routing functions [ 80]. For example, Roller et al. [459] demonstrates that learned expert assignments do not always outperform random ones. Interestingly, instead of designing an architec- ture for sparsity explicitly, Li et al. [314] observethat the activation maps of default Transformer models often emerge to be very sparse implicitly; the larger the model, the sparser measured by the percentage of nonzero entries. Similarly, Zhang et al. [670] find that post-training MoEfication , i.e., converting monolithic models to equivalent MoE models, can speed up inference by 2x. Cascading refers to the idea of employing differently-sized models for different queries [",
    "10": "75]. In spirit, this idea is similar to Mixture-of-Experts models, but instead of learning a routing module, we employ a cascade of multiple, differently-sized monolithic models (these can be even black-box API models) and learn a scoring function that de- cides which model(s) receive which query. Chen et al. [75] demonstrate that this strategy dominates the Pareto frontier between accuracy and cost. Decoding Strategies can greatly impact the com- putational cost of performing inference. For ex- ample, beam search trades off compute for higher- quality results. Another example of a computa- tionally expensive decoding scheme is sample-and- models, can speed up inference by 2x. Cascading refers to the idea of employing differently-sized models for different queries [ 75]. In spirit, this idea is similar to Mixture-of-Experts models, but instead of learning a routing module, we employ a cascade of multiple, differently-sized monolithic models (these can be even black-box API models) and learn a scoring function that de- cides which model(s) receive which query. Chen et al. [75] demonstrate that this strategy dominates the Pareto frontier between accuracy and cost. Decoding Strategies can greatly impact the com- putational cost of performing inference. For ex- ample, beam search trades off compute for higher- quality results. Another example of a computa- tionally expensive decoding scheme is sample-and- rank [ 8] where Nindependent sequences of tokens y1, . . . , yNare obtained using random sampling, and the highest probability sequence is used as the final output. Latency-oriented strategies such as speculative sampling [ 522,300,74] first autoregressively gen- erate a draft of length Kusing a smaller (draft) model; then, the larger (target) model scores the draft, followed by a modified rejection sampling scheme to accept a subset of the tokens from left to right. Similar ideas have been proposed in various contexts, such as for blockwise parallel genera- tion [ 522], grammatical error correction [ 529], and with a larger LLM refining generation produced by a small model [ 265]. Del Corro et al. [114] observe that tokens towards the end of a sequence are easier to predict due to more contextual information, mo- tivating a new decoding strategy that skips earlier layers in the network for such tokens. 2.5.1 Software Various frameworks have been designed to en- able the efficient training of multi-billion to trillion parameter language models such as DeepSpeed [450] and Megatron-LM [501] to account for the unique challenges arising when training such models. This is necessitated by the fact that most LLMs do not fit into a single device\u2019s (GPU, TPU) memory, and scaling across GPUs and 13 compute nodes needs to account for communica- tion and synchronization costs. FlexGen [497] provides further speed-ups by aggregating memory and compute resources from the GPU, CPU, and disk and utilizing techniques such as 4-bit quan- tization, enabling inference with 175B parameter models on a single GPU. The frameworks typically combine existing par- allelism strategies to compensate for drawbacks 2.5.1 Software Various frameworks have been designed to en- able the efficient training of multi-billion to trillion parameter language models such as DeepSpeed [450] and Megatron-LM [501] to account for the unique challenges arising when training such models. This is necessitated by the fact that most LLMs do not fit into a single device\u2019s (GPU, TPU) memory, and scaling across GPUs and 13 compute nodes needs to account for communica- tion and synchronization costs. FlexGen [497] provides further speed-ups by aggregating memory and compute resources from the GPU, CPU, and disk and utilizing techniques such as 4-bit quan- tization, enabling inference with 175B parameter models on a single GPU. The frameworks typically combine existing par- allelism strategies to compensate for drawbacks and scale model training across multiple sets of compute nodes, within compute nodes, and across multiple GPUs per node. e.g., Smith et al. [515] use tensor slicing within a node, pipeline paral- lelism across nodes, and data parallelism to train multiple model replicas over sets of nodes. Addi- tional features include memory optimizations [ 445, 454,446], communication-efficient [ 536,307,343] and fused optimizers2, and support for MoE train- ing [444]. Specialized implementations such as Tutel [230] and MegaBlocks [160] of- fer efficient sparse MoE training, while Alpa [677] enables automatic data and model parallelism for LLMs written in Jax. The FasterTransformer3library includes highly optimized Transformer encoder and decoder implementations for TensorFlow, PyTorch, and Triton. Kwon et al. [285] introduce vLLM , an open- source library for efficient inference and LLM serv- ing. vLLM employs PagedAttention, which par- titions each sequence\u2019s KV cache into fixed-size blocks. When performing attention computations, blocks are fetched from non-contiguous memory. This enables memory sharing, reducing memory consumption and transfers in decoding strategies such as beam search, ultimately improving through- put. ThePetals [54] library4allows users to col- laboratively fine-tune and run LLMs by distribut- ing subsets of model parameters to individual ma- chines. All of these libraries address the enormous com- putational costs associated with training and run- ning LLMs, either by offering more efficient im- plementations, lowering memory requirements, or using distributed or decentralized computing strate- gies. 2https://github.com/nvidia/apex ing. vLLM employs PagedAttention, which par- titions each sequence\u2019s KV cache into fixed-size blocks. When performing attention computations, blocks are fetched from non-contiguous memory. This enables memory sharing, reducing memory consumption and transfers in decoding strategies such as beam search, ultimately improving through- put. ThePetals [54] library4allows users to col- laboratively fine-tune and run LLMs by distribut- ing subsets of model parameters to individual ma- chines. All of these libraries address the enormous com- putational costs associated with training and run- ning LLMs, either by offering more efficient im- plementations, lowering memory requirements, or using distributed or decentralized computing strate- gies. 2https://github.com/nvidia/apex 3https://github.com/NVIDIA/FasterTransformer 4https://github.com/bigscience-workshop/petals2.6 Limited Context Length Addressing everyday NLP tasks often necessitates an understanding of a broader context. For exam- ple, if the task at hand is discerning the sentiment in a passage from a novel or a segment of an aca- demic paper, it is not sufficient to merely analyze a few words or sentences in isolation. The entirety of the input (or context ), which might encompass the whole section or even the complete document, must be considered. Similarly, in a meeting transcript, the interpretation of a particular comment could pivot between sarcasm and seriousness, depending on the prior discussion in the meeting. Li et al. [308] evaluate several LLMs in the long- context settings and find that while commercial closed-API models often fulfill their promise, many open-source models \u2013 despite claiming to perform well with longer contexts \u2013 exhibit severe perfor- mance degradation. They point out that there is a difference between being architecturally-able to deal with long inputs and actually performing well . Having an architecture that can infer long inputs does not guarantee that the LLM will perform as well on those as on shorter inputs. Similarly, Liu et al. [333] find that changing the location of rel- evant information in the input can degrade model performance. Interestingly, they find that decoder- only LLMs like GPT-3.5 can deal well with such information at the beginning or end of the input context; they cannot access information in the mid- dle of it well, resulting in a U-shaped",
    "11": "performance curve. Limited Context Length Limited context lengths are a barrier for well with longer contexts \u2013 exhibit severe perfor- mance degradation. They point out that there is a difference between being architecturally-able to deal with long inputs and actually performing well . Having an architecture that can infer long inputs does not guarantee that the LLM will perform as well on those as on shorter inputs. Similarly, Liu et al. [333] find that changing the location of rel- evant information in the input can degrade model performance. Interestingly, they find that decoder- only LLMs like GPT-3.5 can deal well with such information at the beginning or end of the input context; they cannot access information in the mid- dle of it well, resulting in a U-shaped performance curve. Limited Context Length Limited context lengths are a barrier for handling long inputs well to facilitate ap- plications like novel or textbook writing or summarizing. To this end, we discuss three lines of work per- mitting longer context lengths. First, we look at efficient attention mechanisms, which help miti- gate the effect of long inputs on the computational requirements of Transformer models. Next, we ex- amine positional embedding schemes in the light of generalization to longer sequence lengths than those used during training. Lastly, we revise Trans- former alternatives which neither require attention nor positional embeddings. 14 Efficient Attention Mechanisms One way of addressing the limited context of LLMs is by de- signing more efficient attention mechanisms that can process longer inputs. Ma et al. [350] intro- duce Luna , a linear unified nested attention mech- anism that approximates softmax attention with two nested linear attention functions, yielding only linear (as opposed to quadratic) time and space complexity, allowing it to process much longer in- puts. Similarly, Shen et al. [496] and Li et al. [310] present alternative attention mechanisms equivalent to the dot-product attention but which require sub- stantially less memory and compute resources. Guo et al. [183] propose an attention mechanism called Transient Global , which is an extension of local attention where each token can attend to nearby tokens and a set of global tokens. It enables to han- dle sequences with up to 12,000 tokens. Similarly, CoLT5 [15] enables context lengths of up to 64,000 tokens by splitting the computations into a light branch with local attention, fewer attention heads, and a heavy branch with full attention. CoLT5 ap- linear (as opposed to quadratic) time and space complexity, allowing it to process much longer in- puts. Similarly, Shen et al. [496] and Li et al. [310] present alternative attention mechanisms equivalent to the dot-product attention but which require sub- stantially less memory and compute resources. Guo et al. [183] propose an attention mechanism called Transient Global , which is an extension of local attention where each token can attend to nearby tokens and a set of global tokens. It enables to han- dle sequences with up to 12,000 tokens. Similarly, CoLT5 [15] enables context lengths of up to 64,000 tokens by splitting the computations into a light branch with local attention, fewer attention heads, and a heavy branch with full attention. CoLT5 ap- plies the light branch to every token and the heavy branch to a subset of tokens that are selected by a learnable routing function. After investigating the effect of the dot-product self-attention mechanism, Tay et al. [541] pro- pose the Synthesizer , a new architecture that learns synthetic attention weights without token-token interactions, showing that it consistently outper- forms transformers on various language-based tasks. Britz et al. [56] offer an alternative attention mechanism based on a fixed-size memory repre- sentation that is more efficient, yielding inference speedups of 20% without significantly hurting per- formance. Hua et al. [220] combine a single-head attention mechanism with a linear attention approx- imation to achieve speed-ups between 4.9x and 12.1x for auto-regressive language modeling while obtaining similar perplexities as a standard Trans- former model. Ding et al. [124] propose dilated attention which splits a sequence into equally long segments and processes each of these in parallel using a sparsified attention mechanism. Dilated attention offers a linear computational complexity in the sequence length and, applied hierarchically, enables inputs of up to 1B tokens. Length Generalization As the required compute of Transformer-based LLMs grows quadratic with the sequence length, it is a desired property to build LLMs that can be trained on short sequences andgeneralize well to significantly longer sequences during inference. The fundamental building block of the Trans- former architecture is the self-attention mechanism. It is permutation-invariant; therefore, the output is independent of the input sequence order. Positional information is commonly injected to make the attention which splits a sequence into equally long segments and processes each of these in parallel using a sparsified attention mechanism. Dilated attention offers a linear computational complexity in the sequence length and, applied hierarchically, enables inputs of up to 1B tokens. Length Generalization As the required compute of Transformer-based LLMs grows quadratic with the sequence length, it is a desired property to build LLMs that can be trained on short sequences andgeneralize well to significantly longer sequences during inference. The fundamental building block of the Trans- former architecture is the self-attention mechanism. It is permutation-invariant; therefore, the output is independent of the input sequence order. Positional information is commonly injected to make the model respect a token\u2019s position in the sequence, i.e., capture the semantics of where a token occurs rather than just whether it occurs. The longer the input is, the more important the positional embed- ding becomes since the model needs to effectively use information from different parts of the input that may cover a wide range of distances from the current token. Without positional embeddings, a Transformer models the relations between any two tokens with equal probability. Hence, positional embeddings introduce an LSTM-like inductive bias that (typi- cally) tokens closer to each other in the sequence are more relevant to each other. Depending on the positional embedding scheme chosen, this can be learned or effectively hard-coded. However, it re- mains unclear what is the most effective positional embedding scheme for long inputs. Further, mod- els face difficulties generalizing to unseen sequence lengths by introducing a dependency on sequence positions. This is an undesirable artifact of posi- tional embeddings, as language semantics do not inherently depend on the length of an utterance. While positional encoding schemes such as rela- tive positional encodings or, more recently, ALiBi have made progress in building more generaliz- able ways for injecting positional information into Transformers, the challenge of generalizing to se- quences much longer than seen during training re- mains largely unsolved. Surprisingly, Haviv et al. [192] find that causal LLMs without positional en- codings are competitive compared to models with positional encodings and accredit this success to the causal attention mask leaking positional infor- mation into the model. lengths by introducing a dependency on sequence positions. This is an undesirable artifact of posi- tional embeddings, as language semantics do not inherently depend on the length of an utterance. While positional encoding schemes such as rela- tive positional encodings or, more recently, ALiBi have made progress in building more generaliz- able ways for injecting positional information into Transformers, the challenge of generalizing to se- quences much longer",
    "12": "than seen during training re- mains largely unsolved. Surprisingly, Haviv et al. [192] find that causal LLMs without positional en- codings are competitive compared to models with positional encodings and accredit this success to the causal attention mask leaking positional infor- mation into the model. In the following, we first summarize some stan- dard positional embeddings technique and then move to more advanced schemes designed to im- prove length generalization. We start with Abso- lute Positional Embeddings [ 563], which inject positional information by sinusoidal embeddings based on the absolute position iof a token xiwithin their sequence x1, . . . ,xNinto the model input. Given an input sequence X= [x1, . . . ,xN], we 15 add a positional embedding matrix P\u2208Rn\u00d7dof the same shape to get the positional encoding out- putsX+P, where the element on the ithrow and the (2j)thor the (2j+ 1)thcolumn of Pfol- lows sinusoidal functions. Vaswani et al. [563] also compare against learned positional embed- dings and find no significant performance differ- ence. In contrast, sinusoidal positional encodings require no trainable parameters, and the authors hypothesize that they enable extrapolation to se- quence lengths longer than the ones contained in the training set. However, this feature is not guar- anteed, as the subsequent layers in the network need to be able to deal with such extrapolated po- sitional embeddings. Learned positional encod- ings do not possess inherent generalization capabil- ities for unseen sequence lengths. This limitation arises because the embeddings associated with ab- solute positions not encountered during training\u2014 depending on the implementation\u2014either do not exist or remain untrained (random). Relative Posi- tional Embeddings have subsequently been devel- oped, extending absolute positional embeddings to relative offsets between token positions [ 492,221, 105,79]. While rarely used in their vanilla form in LLMs [ 441], relative positional embeddings have quence lengths longer than the ones contained in the training set. However, this feature is not guar- anteed, as the subsequent layers in the network need to be able to deal with such extrapolated po- sitional embeddings. Learned positional encod- ings do not possess inherent generalization capabil- ities for unseen sequence lengths. This limitation arises because the embeddings associated with ab- solute positions not encountered during training\u2014 depending on the implementation\u2014either do not exist or remain untrained (random). Relative Posi- tional Embeddings have subsequently been devel- oped, extending absolute positional embeddings to relative offsets between token positions [ 492,221, 105,79]. While rarely used in their vanilla form in LLMs [ 441], relative positional embeddings have given rise to the methods outlined in the follow- ing paragraphs. They offer better generalization to unseen sequence lengths than absolute positional encodings. All unseen absolute positions will be converted to previously observed relative offsets between positions, enabling better generalization to long input sequences at inference time. Rotary Po- sition Embeddings (RoPE) [ 526]unite absolute and relative methods by incorporating absolute po- sitional information in a rotation matrix and model- ing the relative positional offset through a rotation. They directly modify the self-attention calculation rather than injecting positional information into the embeddings. The attention between positions i, j linearly depends on i\u2212jby introducing a d\u00d7d dimensional block diagonal matrix Rd \u0398,k, resulting in a self-attention mechanism defined as softmax\uf8eb \uf8ed1\u221a dX i,jx\u22a4 iW\u22a4 qRd \u0398,(i\u2212j)Wkxj\uf8f6 \uf8f8. (4) While RoPE has been adapted in many LLMs [ 576, 47,86] and Su et al. [526] show RoPE leading to better performance on long text tasks, Press et al. [434] demonstrate that this positional en-coding scheme extrapolates poorly to unseen se- quence lengths. However, Chen et al. [79] demon- strate that by interpolating rather than extrapolating longer than before observed context windows and briefly fine-tuning RoPE-based models, enabling pre-trained LLMs to extend their context window to very long sizes of up to 32,768tokens. Relative Positional Bias [ 443]directly bias the attention computation (Eq. (5)) with a learned bias per relative positional offset and attention head instead of adding information to the token embed- dings softmax\uf8eb \uf8ed1\u221a dX i,jx\u22a4 iW\u22a4 qWkxj+bi\u2212j\uf8f6 \uf8f8.(5) \u0398,(i\u2212j)Wkxj\uf8f6 \uf8f8. (4) While RoPE has been adapted in many LLMs [ 576, 47,86] and Su et al. [526] show RoPE leading to better performance on long text tasks, Press et al. [434] demonstrate that this positional en-coding scheme extrapolates poorly to unseen se- quence lengths. However, Chen et al. [79] demon- strate that by interpolating rather than extrapolating longer than before observed context windows and briefly fine-tuning RoPE-based models, enabling pre-trained LLMs to extend their context window to very long sizes of up to 32,768tokens. Relative Positional Bias [ 443]directly bias the attention computation (Eq. (5)) with a learned bias per relative positional offset and attention head instead of adding information to the token embed- dings softmax\uf8eb \uf8ed1\u221a dX i,jx\u22a4 iW\u22a4 qWkxj+bi\u2212j\uf8f6 \uf8f8.(5) Press et al. [434] follow a similar methodology but use heuristics to define ALiBi (Attention with Linear Biases), a non-learned bias that is used to penalize attention scores in long-range interac- tions [ 479], i.e., a recency-bias is backed into the model. Here, mis a pre-defined, head-specific slope\u2013by default, the set of slopes for nheads form a geometric sequence. softmax\uf8eb \uf8ed1\u221a dX i,jx\u22a4 iW\u22a4 qWkxj+m\u00b7 \u2212(i\u2212j)\uf8f6 \uf8f8. (6) Press et al. [434] motivate ALiBi by designing it to generalize well to unseen sequence lengths. They show that training a model with it on training se- quences with a maximum sequence length of 1,024 tokens achieves the same perplexity on a test set with a maximum sequence length of 2,048as a model trained with sinusoidal positional encodings on sequences with up to 2,048tokens. Thereby, it not only enables larger context lengths but can also potentially reduce pre-training costs (Sec. 2.3). While some of the existing positional encod- ing schemes offer better generalization to long se- quences than others, it remains unclear how reliable they are. For example, Taylor et al. [548] report try- ing ALiBi in the Galactica LLM and not observing \u201clarge gains\u201d compared to using learned positional encodings. Similarly, Kazemnejad et al. [259] find that popular positional encoding schemes such as ALiBi ,RoPE , and absolute positional encodings do not perform well in terms of length generalization in a suite of 10 reasoning downstream tasks. In a parallel line of work, Anil et al. [19] demon- strate that naively fine-tuning a pre-trained LLM is 16 insufficient for length generalization in the context of reasoning tasks. Instead, they propose combin- While some of the existing positional encod- ing schemes offer better generalization to long se- quences than others, it remains unclear how reliable they are. For example, Taylor et al. [548] report try- ing ALiBi in the Galactica LLM and not observing \u201clarge gains\u201d compared to using learned positional encodings. Similarly, Kazemnejad et al. [259] find that popular positional encoding schemes such as ALiBi ,RoPE , and absolute positional encodings do not perform well in terms of length generalization in a suite of 10 reasoning downstream tasks. In a parallel line of work, Anil et al. [19] demon- strate that naively fine-tuning a pre-trained LLM is 16 insufficient for length generalization in the context of reasoning tasks. Instead, they propose combin- ing in-context learning and scratchpad/chain-of- thought reasoning to enable LLMs to generalize to unseen sequence lengths in- and out-of-distribution, with performance scaling with model size. The au- thors report that fine-tuning can further improve",
    "13": "model performance dependent on the task perfor- mance of the baseline. Transformer Alternatives While Transformers are the dominant paradigm in LLMs today due to their strong performance, several more efficient alternative architectures exist. One line of work tries to replace the attention mechanism using state space models (SSMs), which offer near-linear com- putational complexity w.r.t. the sequence length. Dao et al. [108] investigate the weaknesses of state space models (SSMs) in language modeling and find that existing approaches struggle with recall- ing previous tokens and comparing tokens in the sequence. Based on these findings, the authors propose H3with a shift matrix to recall previous tokens and multiplicative interactions for token comparisons. The authors demonstrate that H3 comes close to Transformer-based LLMs for lan- guage modeling, offering further improvements when combined with attention. Poli et al. [430] propose the Hyena operator, a convolution-based sub-quadratic attention replacement designed for long sequences. Hyena tries to emulate the atten- tion mechanisms\u2019 dynamic nature by introducing data-controlled computations, i.e., Hyena applies an element-wise gating operation based on the op- erator\u2019s input to mimic the attention contextualiza- tion. Hyena -based models have been used on natu- ral language for sequence lengths of up to 131,000 tokens [ 430] and up to 1,000,000tokens in the propose H3with a shift matrix to recall previous tokens and multiplicative interactions for token comparisons. The authors demonstrate that H3 comes close to Transformer-based LLMs for lan- guage modeling, offering further improvements when combined with attention. Poli et al. [430] propose the Hyena operator, a convolution-based sub-quadratic attention replacement designed for long sequences. Hyena tries to emulate the atten- tion mechanisms\u2019 dynamic nature by introducing data-controlled computations, i.e., Hyena applies an element-wise gating operation based on the op- erator\u2019s input to mimic the attention contextualiza- tion. Hyena -based models have been used on natu- ral language for sequence lengths of up to 131,000 tokens [ 430] and up to 1,000,000tokens in the context of genomics [ 383]. Fathi et al. [144] pro- pose the Block-State Transformer, which builds upon a hybrid layer that combines an SSM for long-range contextualization and a Transformer for short-range interactions between tokens. The authors find similar performance to Transformer- based baselines while obtaining speed-ups of up to 10x on sequence-level, enabling models with more than65,000tokens sequence length. Another line of work utilizes recurrent neu- ral networks (RNNs), which offer linear com- putational complexity and memory requirements with respect to the sequence length as the back- bone of LLMs. Peng et al. [416] propose Recep-tance Weighted Key Value (RWKV) to combine the parallelization benefits of Transformer-based LLMs during training with the fast inference and low compute requirements of RNNs. The authors accomplish this by leveraging a linear attention- like mechanism, scaling non-Transformer LLMs to 14B parameters, and matching the performance of similarly-sized Transformer LLMs. 2.7 Prompt Brittleness A prompt is an input to the LLM. The prompt syn- tax (e.g., length, blanks, ordering of examples) and semantics (e.g., wording, selection of examples, instructions) can have a significant impact on the model\u2019s output [342]. As an analogy, if we were to think of an LLM as a (fuzzy) database and prompts as queries [ 246], it becomes clear that slight changes in the query can result in vastly different outputs. Consequently, the wording, as well as the order of examples in- cluded in a prompt, have been found to influence the model\u2019s behavior significantly [ 596,675,342]. Prompt Brittleness [675, 596, 342] Variations of the prompt syntax, often oc- 14B parameters, and matching the performance of similarly-sized Transformer LLMs. 2.7 Prompt Brittleness A prompt is an input to the LLM. The prompt syn- tax (e.g., length, blanks, ordering of examples) and semantics (e.g., wording, selection of examples, instructions) can have a significant impact on the model\u2019s output [342]. As an analogy, if we were to think of an LLM as a (fuzzy) database and prompts as queries [ 246], it becomes clear that slight changes in the query can result in vastly different outputs. Consequently, the wording, as well as the order of examples in- cluded in a prompt, have been found to influence the model\u2019s behavior significantly [ 596,675,342]. Prompt Brittleness [675, 596, 342] Variations of the prompt syntax, often oc- curring in ways unintuitive to humans, can result in dramatic output changes. Designing natural language queries that steer the model\u2019s outputs toward desired outcomes is often referred to as prompt engineering [477,287,606]. Fig. 6 summarizes some of the most popular prompting methods with an example adapted from Wei et al. [601] . As we can see, there are lots of equally-plausible prompting techniques, and the current state of prompt engineering still requires lots of experimentation, with little theoretical un- derstanding of why a particular way to phrase a task is more sensible other than that it achieves better empirical results. Developing LLMs that are robust to the prompt\u2019s style and format remains unsolved, leaving practitioners to design prompts ad-hoc rather than systematically. Single-Turn Prompting methods improve the in- put prompt in various ways to get a better answer in a single shot. In-Context Learning (ICL) refers to an LLM\u2019s ability to learn a new task solely via inference (without any parameter updates) by con- ditioning on a concatenation of the training data as demonstrations [ 59,483]. This enables users and practitioners to use LLMs for a variety of NLP 17 current state of prompt engineering still requires lots of experimentation, with little theoretical un- derstanding of why a particular way to phrase a task is more sensible other than that it achieves better empirical results. Developing LLMs that are robust to the prompt\u2019s style and format remains unsolved, leaving practitioners to design prompts ad-hoc rather than systematically. Single-Turn Prompting methods improve the in- put prompt in various ways to get a better answer in a single shot. In-Context Learning (ICL) refers to an LLM\u2019s ability to learn a new task solely via inference (without any parameter updates) by con- ditioning on a concatenation of the training data as demonstrations [ 59,483]. This enables users and practitioners to use LLMs for a variety of NLP 17 Self-Re\ufb01neChain-of-ThoughtQ: Lisa has 5 easy peelers. She buys 2 more nets with 6 each. How many easy peelers does she have?A: Lisa starts with 5. 2 nets of 6 each are 12 easy peelers. 5+12=17. The answer is 17.Q: The cafeteria has 37 bananas. They bought 5 more bunches with 5 each, how many bananas do they have?A: The cafeteria has 37 bananas originally. They bought 5 more bunches and each bunch has 5, so they added 5 x 5 = 25 bananas to their stock. We add these numbers: 37 + 25 = 62. The answer is 62.Instruction-FollowingHere is a mathematical reasoning question. You need to apply arithmetic operations to generate the correct answer. Q: Lisa has 5 easy peelers. She buys 2 more nets with 6 each. How many easy peelers does she have?\u2026A: The answer is 62. Tree of ThoughtsQ: The cafeteria has 37 bananas. They bought 5 more bunches with 5 each, how many bananas do they have?\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026The cafeteria bought 5 more bunches with 5 each. Calculate how many they bought in total.Propose Prompt5 x 5 = 25Thought GenerationEvaluate whether this thought is useful to answer the original question.Evaluation PromptYes, this calculation takes us one step closer to the solution.Thought EvaluationSingle-Turn Prompting Multi-Turn PromptingInputOutput Tree of ThoughtsQ: The cafeteria has 37 bananas. They bought 5 more bunches with 5 each,",
    "14": "how many bananas do they have?\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026The cafeteria bought 5 more bunches with 5 each. Calculate how many they bought in total.Propose Prompt5 x 5 = 25Thought GenerationEvaluate whether this thought is useful to answer the original question.Evaluation PromptYes, this calculation takes us one step closer to the solution.Thought EvaluationSingle-Turn Prompting Multi-Turn PromptingInputOutput Self-ConsistencyQ: Lisa has 5 easy peelers. She buys 2 more nets with 6 each. How many easy peelers does she have?A: Lisa starts with 5. 2 nets of 6 each are 12 easy peelers. 5+12=17. The answer is 17.Q: The cafeteria has 37 bananas. They bought 5 more bunches with 5 each, how many bananas do they have?A: The cafeteria has 37 bananas originally. They bought 5 more bunches and each bunch has 5, so they added 5 x 5 = 25 bananas to their stock. We add these numbers: 37 + 25 = 62. The answer is 62.A: The cafeteria initially had 37 bananas and purchased an additional 5 bunches of bananas, each with 5, totaling 25 bananas. So, adding 5 and 25 together, the total fruit count is now 30. The answer is 30.A: We need to multiply the number of bunches by the number of banans in each bunch. 5 times 5 gives us 25 bananas. Next, we add the original number of bananas. The addition 37 plus 25 equals 62. The answer is 62.A: The answer is 62.Majority VoteLeast-To-MostQ: The cafeteria has 37 bananas. They bought 5 more bunches with 5 each, how many bananas do they have?A: To solve \u201cHow many bananas does it have?\u201d, we need to \ufb01rst solve: \u201cHow many bananas does it buy in total\u201d?Stage 1: Problem ReductionStage 2: Sequentially Solve SubquestionsThe cafeteria has 37 bananas. They bought 5 more bunches with 5 each.Q: How many bananas does it buy in total?A: They buy 25 bananas in total.The cafeteria has 37 bananas. They bought 5 more bunches with 5 each, how many bananas do they have?Q: How many bananas does it buy in total?A: They buy 25 bananas in total.Q: How many bananas do they have?A: The cafeteria has 37 bananas. They buy 25 bananas in total. So, in total, they have 37 + 25 = 62 bananas. In-Context LearningQ: Lisa has 5 easy peelers. She buys 2 more nets with 6 each. How many easy peelers does she have?A: The answer is 17.Q: The cafeteria has 37 bananas. They bought 5 more bunches with 5 each, how many bananas do they have?A: The answer is 62.Prompt tuning A: The answer is 62.Q: Lisa has 5 easy peelers. She buys 2 more nets with 6 each. How many easy peelers does she have?A: The answer is 17.Q: The cafeteria has 37 bananas. They bought 5 more bunches with 5 each, how many bananas do they have?Embedding 1Embedding \u2026Embedding N Ask-Me-AnythingFormulate a question for the given context. Q: Lisa has 5 easy peelers. She buys 2 more nets with 6 each. How many easy peelers does she have?A: The answer is 17.Q: The cafeteria has 37 bananas. They bought 5 more bunches with 5 each.Q: What is the total number of bananas they possess?Answer the question using arithmetic.Q: Lisa has 5 easy peelers. She buys 2 more nets with 6 each. How many easy peelers does she have?A: The answer is 17.Q: The cafeteria has 37 bananas. They bought 5 more bunches with 5 each.Q: What is the total number of bananas they possess?A: The answer is 62.Prompt Chain 3Prompt Chain 2Prompt Chain 1 A1: The answer is 62.A2: The answer is 62.A3: The answer is 93.A: The answer is 62.Majority Vote\u2026 Q: Lisa has 5 easy peelers. She buys 2 more nets with 6 each. How many easy peelers does she have?A: The answer is 17.Q: The cafeteria has 37 bananas. They bought 5 more bunches with 5 each, how many bananas do they have?This response is not answering the question asked. The question asked is how many banans there are in total. These two quantities have to be added together.FeedbackA: Apologies for any confusion, you are right, I was answering the wrong question. The correct answer is 62, by adding 37 and 5 x 5. Re\ufb01ned OutputA: The answer is 37.Figure 6: Overview of Selected Prompting Methods , categorized into Single-Turn and Multi-Turn Prompting. We use a running example across all methods inspired by Wei et al. [601]. tasks by simply listing examples of the dataset (e.g., input texts and their corresponding labels) without the need to adjust the LLM\u2019s inner workings. Various existing works investigate why ICL shows such competitive results across NLP tasks. One explanation concurrently proposed by [ 570, 103,16] is that ICL emulates gradient-based meta- learning, i.e., it implicitly fine-tunes the model through gradient descent in their forward pass. Interestingly, Min et al. [366] show that input- label associations in the few-shot prompt are not decisive for model performance: randomly flip- ping labels of few-shot demonstrations shows to harm an LLM\u2019s ability to solve NLP tasks barely. However, few-shot learning (with and without ran- tasks by simply listing examples of the dataset (e.g., input texts and their corresponding labels) without the need to adjust the LLM\u2019s inner workings. Various existing works investigate why ICL shows such competitive results across NLP tasks. One explanation concurrently proposed by [ 570, 103,16] is that ICL emulates gradient-based meta- learning, i.e., it implicitly fine-tunes the model through gradient descent in their forward pass. Interestingly, Min et al. [366] show that input- label associations in the few-shot prompt are not decisive for model performance: randomly flip- ping labels of few-shot demonstrations shows to harm an LLM\u2019s ability to solve NLP tasks barely. However, few-shot learning (with and without ran- dom labels) vastly outperforms zero-shot learning (i.e., no demonstrations are provided in the prompt). The authors argue that the demonstrations are help- ful for task performance in that the LLM instead learns the label space and the input distribution of the task. In later work, Pan et al. [405] explain that there are two distinct mechanics through which ICLleverages demonstrations: on the one hand, task recognition is the ability to recognize a task through demonstrations (possibly without ground-truth la- bels or perhaps even wrong ones, as in the case of Min et al. [366] ). After this recognition phase, it applies its pre-trained capabilities. On the other hand, the skill to acquire new input-label mappings unseen in pre-training is called task learning . While input-label associations may not seem to drive few-shot performance, at least in the case of task recognition, Lu et al. [342] show that the order of few-shot examples matters in that LLMs are highly sensitive to permutations of the order in which the few-shot demonstrations are provided. Alternative explanations of the ICL phenomenon take place around Bayesian inference [ 623], sparse linear regression [ 7], structure induction [ 188], maintaining coherence [ 509], kernel regression [190], and clone-structured causal graphs [535]. Instruction-Following is mainly explained in Sec. 2.9, as it requires supervised fine-tuning. To briefly recap, the idea is to prepend task-describing instructions (e.g., \u201cThis is a text classification task 18 for movie reviews. Here are a few examples: ... \u201d ) in the input prompts. Chain-of-Thought (CoT) [ 327,601]describes a technique used to construct few-shot prompts via a series of intermediate reasoning steps leading are highly sensitive to permutations of the order in which the few-shot demonstrations are provided. Alternative explanations of the ICL phenomenon take place around Bayesian inference [ 623], sparse linear regression [ 7], structure induction [ 188], maintaining coherence [ 509], kernel regression [190], and clone-structured causal graphs [535]. Instruction-Following is mainly explained in Sec. 2.9, as it requires supervised fine-tuning. To briefly recap, the idea is to prepend task-describing instructions (e.g., \u201cThis is a text classification task 18 for movie reviews. Here are a few examples: ... \u201d ) in the input prompts. Chain-of-Thought (CoT) [ 327,601]describes",
    "15": "a technique used to construct few-shot prompts via a series of intermediate reasoning steps leading to the final output. Answer rationales to solve al- gebraic problems were originally proposed in the pre-LLM era [ 327] and later experienced big pop- ularity as a prompting strategy for LLMs [ 601]. Extensions of chain-of-thought prompting include zero-shot variants [ 273] and automatically gener- ated series of reasoning steps [671]. Impersonation [ 473]is a technique in which the prompt for the model asks it to pretend to be a domain expert when answering a domain-specific question. Salewski et al. [473] find that LLMs answer domain-specific questions more accurately when prompted to impersonate a domain expert. Multi-Turn Prompting methods iteratively chain prompts and their answers together. Ask Me Anything [ 24]uses multiple prompt templates (called prompt chains), which are used to reformat few-shot example inputs into an open- ended question-answering format. The final output is obtained by aggregating the LLMs predictions for each reformatted input via a majority vote. Self-consistency [ 585]extends chain-of-thought prompting by sampling multiple reasoning paths and selecting the most consistent answer via a ma- jority vote. Least-to-Most [ 682]uses a set of constant prompts to use the LLM to decompose a given complex problem into a series of subproblems. The LLM sequentially solves the subproblems with prompts for later-stage subproblems containing pre- viously produced solutions, iteratively building the final output. Scratchpad [ 391]is a method to fine-tune LLMs on multi-step computation tasks such that they out- put intermediate reasoning steps, e.g., intermedi- ate calculations when performing additions, into a \u201cscratchpad\u201d before generating the final result. for each reformatted input via a majority vote. Self-consistency [ 585]extends chain-of-thought prompting by sampling multiple reasoning paths and selecting the most consistent answer via a ma- jority vote. Least-to-Most [ 682]uses a set of constant prompts to use the LLM to decompose a given complex problem into a series of subproblems. The LLM sequentially solves the subproblems with prompts for later-stage subproblems containing pre- viously produced solutions, iteratively building the final output. Scratchpad [ 391]is a method to fine-tune LLMs on multi-step computation tasks such that they out- put intermediate reasoning steps, e.g., intermedi- ate calculations when performing additions, into a \u201cscratchpad\u201d before generating the final result. ReAct [ 640]combines reasoning and acting by prompting LLMs to generate reasoning traces (e.g., Chain-of-thought) and action plans, which can be executed to allow the model to interact with exter- nal environments such as Wikipedia to incorporate knowledge. Automatic Reasoning and Tool-Use (ART) [ 406]is a method to automatically generate multi-step reasoning prompts, includingsymbolic calls to external tools such as search and code generation or execution. To this end, ART retrieves demonstrations of related tasks from a library of tasks with accompanying reasoning steps and uses a frozen language model to generate intermediate reasoning steps. Self-refine [ 351]is based on the notion of itera- tive refinement, i.e., improving an initial solution over multiple steps. To this end, a single LLM gen- erates an initial output and then iteratively provides feedback on the previous output, followed by a re- finement step in which the feedback is incorporated into a revised output. Tree of Thoughts [ 639]generalize CoT to main- tain a tree of thoughts (with multiple different paths), where each thought is a language sequence that serves as an intermediate step. Doing so en- ables the LLM to self-evaluate the progress inter- mediate thoughts make towards solving the prob- lem and incorporating search algorithms, such as breadth-first or depth-first search, allowing system- atic exploration of the tree with lookahead and backtracking. Controlled Generation The approaches above primarily modify the prompt text to steer model outputs. However, instead of reformulating the input text, we can control the output by approaches that directly modify the inference procedure given finement step in which the feedback is incorporated into a revised output. Tree of Thoughts [ 639]generalize CoT to main- tain a tree of thoughts (with multiple different paths), where each thought is a language sequence that serves as an intermediate step. Doing so en- ables the LLM to self-evaluate the progress inter- mediate thoughts make towards solving the prob- lem and incorporating search algorithms, such as breadth-first or depth-first search, allowing system- atic exploration of the tree with lookahead and backtracking. Controlled Generation The approaches above primarily modify the prompt text to steer model outputs. However, instead of reformulating the input text, we can control the output by approaches that directly modify the inference procedure given a fixed set of prompts. Before the advent of LLMs, this line of work has been referred to as controlled generation [261, 109, 278]. In the context of LLMs, Sanchez et al. [474] proposes to use classifier-free guidance sampling [204], where the input prompt\u2019s importance is up- weighted throughout the generation of a sequence. Roush [463] proposes five ideas related to modify- ing the prompt throughout the decoding of a single sequence; for example, alternating between two in- put prompts. Such works often borrow ideas from the text-to-image generation community [ 384,29]. One idea we have not seen borrowed yet is neg- ative prompting, i.e., including a description of unwanted outputs. According to Neg [4], the first attempts at such an idea resulted in negative out- comes. 2.8 Hallucinations The popularity of services like ChatGPT suggests that LLMs are increasingly used for everyday question-answering. As a result, the factual accu- racy of these models has become more significant 19 than ever. Correct!Does not exist! \u2705 \" Wrong authors! \" Figure 7: Example of Hallucinations with GPT-4 , accessed on 02/06/2023. Unfortunately, LLMs often suffer from halluci- nations , which contain inaccurate information that can be hard to detect due to the text\u2019s fluency. Fig. 7 illustrates an example. To distinguish between different types of hallu- cinations, we consider the provided source content of the model, e.g., the prompt, possibly includ- ing examples or retrieved context. Based on such, we can distinguish between intrinsic andextrinsic hallucinations [ 241]. In the former, the generated text logically contradicts the source content. In the latter, we cannot verify the output correctness racy of these models has become more significant 19 than ever. Correct!Does not exist! \u2705 \" Wrong authors! \" Figure 7: Example of Hallucinations with GPT-4 , accessed on 02/06/2023. Unfortunately, LLMs often suffer from halluci- nations , which contain inaccurate information that can be hard to detect due to the text\u2019s fluency. Fig. 7 illustrates an example. To distinguish between different types of hallu- cinations, we consider the provided source content of the model, e.g., the prompt, possibly includ- ing examples or retrieved context. Based on such, we can distinguish between intrinsic andextrinsic hallucinations [ 241]. In the former, the generated text logically contradicts the source content. In the latter, we cannot verify the output correctness from the provided source; the source content does not provide enough information to assess the out- put, which is, therefore, under-determined. Extrin- sic hallucination is not necessarily erroneous, as it merely means the model generated an output that can neither be grounded nor contradicted by the source content. This is still, to some degree, un- desirable as the provided information cannot be verified. We illustrate intrinsic and extrinsic hallu- cinations in Fig. 8. Hallucination [293,",
    "16": "458, 241] Generated text that is fluent and natural but unfaithful to the source content (intrinsic) and/or under-determined (extrinsic). Liu et al. [328] attribute hallucinations com- monly observed in LLMs to an architectural flaw in Transformer models while observing that recurrent neural networks perfectly solve their minimalistic synthetic benchmarks, designed to isolate the is-sue of hallucination in the context of algorithmic reasoning. Here, we focus on ways to address hal- lucinations in LLMs without changing the model architecture itself, including (i) supplying the LLM with relevant sources ( retrieval augmentation ) or (ii) decoding strategies. How to Measure Hallucinations Lee et al. [295] provide the FactualityPrompts dataset consisting of factual and nonfactual input prompts, which al- lows one to isolate the effect of prompt\u2019s actuality on the model\u2019s continuation. Further, they mea- sure hallucinations using named-entity- and textual entailment-based metrics. Min et al. [365] notice that evaluating factuality can be difficult because generations can contain a mixture of supported and unsupported information, making binary judg- ments of quality inadequate and human evaluation reasoning. Here, we focus on ways to address hal- lucinations in LLMs without changing the model architecture itself, including (i) supplying the LLM with relevant sources ( retrieval augmentation ) or (ii) decoding strategies. How to Measure Hallucinations Lee et al. [295] provide the FactualityPrompts dataset consisting of factual and nonfactual input prompts, which al- lows one to isolate the effect of prompt\u2019s actuality on the model\u2019s continuation. Further, they mea- sure hallucinations using named-entity- and textual entailment-based metrics. Min et al. [365] notice that evaluating factuality can be difficult because generations can contain a mixture of supported and unsupported information, making binary judg- ments of quality inadequate and human evaluation time-consuming. Hence, they propose a frame- work that first breaks generations into atomic facts and then computes the percentage of atomic facts supported by an external knowledge source like Wikipedia. Zhang et al. [664] detect the behavior ofhallucination snowballing , where the LLM over- commits to early mistakes (before outputting the explanation) in its generation, which it otherwise would not make. Retrieval Augmentation One way to mitigate hallucinations is to ground the model\u2019s input on external knowledge, which is often referred to as retrieval augmentation . In other words, we can decouple (i) memory storage of knowledge (e.g., databases or search indexes [ 290]) and (ii) process- ing of the knowledge to arrive at a more modular architecture. For (i), a retriever module retrieves the top- krelevant documents (or passages) for a query from a large corpus of text. Then, for (ii), we feed these retrieved documents to the language model together with the initial prompt. In theory, using an external data source may also make it eas- ier to interpret which knowledge is retrieved and update it without tediously fine-tuning the model. Shuster et al. [507] demonstrate hallucinations in GPT-3 and study various components of retrieval- augmented architectures to mitigate them. Their best models reduce hallucinated responses by over 60% on average and up to 85% on out-of- distribution data, on which the model has not been trained. We summarize a few popular retrieval augmentation (RA) approaches as follows. 20 Bob's wife is Amy. Bob's daughter is Cindy. Who is Cindy to Amy? P.1) Intrinsic Hallucination Cindy is Amy's daughter-in-law . Query Explain RLHF for LLMs. P.2) model together with the initial prompt. In theory, using an external data source may also make it eas- ier to interpret which knowledge is retrieved and update it without tediously fine-tuning the model. Shuster et al. [507] demonstrate hallucinations in GPT-3 and study various components of retrieval- augmented architectures to mitigate them. Their best models reduce hallucinated responses by over 60% on average and up to 85% on out-of- distribution data, on which the model has not been trained. We summarize a few popular retrieval augmentation (RA) approaches as follows. 20 Bob's wife is Amy. Bob's daughter is Cindy. Who is Cindy to Amy? P.1) Intrinsic Hallucination Cindy is Amy's daughter-in-law . Query Explain RLHF for LLMs. P.2) Extrinsic Hallucination RLHF stands for \" Rights, Limitations, Harms and Freedoms \" and is a framework for ... models like LLMs. Query Problems Solutions S.1) Decoding Strategies Explain RLHF for LLMs. S.2) Retrieval augmentation RLHF is a technique used for alignment of LLMs and stands for Reinforcement Learning with Human Preferences . Retrieved context Query Bob's wife is Amy. Bob's daughter is Cindy. Who is Cindy to Amy? Cindy is Amy's daughter . Query daughter daughter-in-law ... sonFigure 8: Illustration of a) intrinsic and b) extrinsic hallucinations in user interaction with an LLM, inspired by Zhao et al. [673] . In a), the produced answer contradicts the given context, whereas in b), the context does not provide enough information about whether the produced answer would contradict. Retrieval-augmented language model pre-training (REALM) [ 186] inserts retrieved documents into the pre-training examples. While Guu et al. [186] designed REALM for extractive tasks such as question-answering, Lewis et al. [304] propose retrieval-augmented generation (RAG), a language generation framework using retrievers for knowledge-intensive tasks that humans could not solve without access to an external knowledge source. Yogatama et al. [646] propose the adaptive Semiparametric Language Models architecture, which incorporates the current local context, a short-term memory that caches earlier-computed hidden states, and a long-term memory based on a key-value store of (hidden-state, output) tuples. To equip a retrieval-augmented LLM with few-shot abilities that were before only emergent in LLMs with many more parameters, Izacard et al. [236] into the pre-training examples. While Guu et al. [186] designed REALM for extractive tasks such as question-answering, Lewis et al. [304] propose retrieval-augmented generation (RAG), a language generation framework using retrievers for knowledge-intensive tasks that humans could not solve without access to an external knowledge source. Yogatama et al. [646] propose the adaptive Semiparametric Language Models architecture, which incorporates the current local context, a short-term memory that caches earlier-computed hidden states, and a long-term memory based on a key-value store of (hidden-state, output) tuples. To equip a retrieval-augmented LLM with few-shot abilities that were before only emergent in LLMs with many more parameters, Izacard et al. [236] propose a KL-divergence loss term for retrieval models, resulting in ATLAS . Borgeaud et al. [52] study scaling up retrieval databases up to 2 trillion tokens and achieving comparable performance to GPT-3 on some tasks despite using 25\u00d7fewer parameters while highlighting the retrieval model\u2019s ability to copy-paste existing training chunks. Asai et al. [25] introduce a collection of 40 retrieval datasets with instructions and a correspondingmodel trained on them. However, standard RA does not always solve the hallucinations problem. Fig. 9 illustrates an exam- ple of ChatGPT browsing the web first to retrieve relevant documents before answering the query. While the Bing browsing plugin retrieves two (exis- tent) related papers ([ 673,632]), unfortunately, the final response still contains a hallucination: the sec- ond paper\u2019s title and summary are factually inaccu- rate. The second paper\u2019s true title is \u201cPractical and Ethical Challenges of Large Language Models in Education: A Systematic Literature Review\u201d [ 632]. Another failure mode of RA is illustrated by Khattab et al. [262] , who find that sometimes the retriever cannot",
    "17": "find passages that directly answer the question. Hence, they propose a framework that unifies techniques from RA and multi-turn prompt- ing (Sec. 2.7) to solve more complex questions programmatically. Decoding Strategies Another approach to miti- gating hallucinations is refining the decoding strat- egy during inference time. Lee et al. [295] show that standard decoding algorithms (e.g., top-p trun- cation) can induce hallucinations due to the uni- form randomness introduced at every sampling 21 Correct! Does not exist! \u2705 \"Figure 9: Example of Retrieval-Augmented GPT-4 , Ethical Challenges of Large Language Models in Education: A Systematic Literature Review\u201d [ 632]. Another failure mode of RA is illustrated by Khattab et al. [262] , who find that sometimes the retriever cannot find passages that directly answer the question. Hence, they propose a framework that unifies techniques from RA and multi-turn prompt- ing (Sec. 2.7) to solve more complex questions programmatically. Decoding Strategies Another approach to miti- gating hallucinations is refining the decoding strat- egy during inference time. Lee et al. [295] show that standard decoding algorithms (e.g., top-p trun- cation) can induce hallucinations due to the uni- form randomness introduced at every sampling 21 Correct! Does not exist! \u2705 \"Figure 9: Example of Retrieval-Augmented GPT-4 , accessed on 02/06/2023. step. Dziri et al. [136] observe a positive correlation between increased diversity in response generation and hallucinations. The reason for inducing randomness and diver- sity in popular decoding strategies is that gener- ating the most likely sequence often leads to an unsurprising and unnatural text compared to hu- man communication [ 489,207,662]. Zhang et al. [662] phrase this challenge as a trade-off between diversity and quality. While this challenge re- mains largely unsolved, several approaches such as diverse beam search [ 567] and confident decod- ing [ 552] try reducing the induced hallucinations at the decoding level. Uncertainty-Aware Beam Search [ 620]is based on the observation that higher predictive un- certainty corresponds to a larger chance of gener- ating hallucinations. Therefore, the method intro- duces a penalty term in the beam search to penalize high predictive uncertainty during decoding. Confident Decoding [ 552]hypothesize that hal- lucinations of encoder-decoder models originate by not attending to the source when decoding. They propose an attention-based confidence score to measure how strongly a model attends the source and a variational Bayes training procedure to en- sure the model generates high-confidence answers. 2.9 Misaligned Behavior The alignment problem refers to the challenge of ensuring that the LLM\u2019s behavior aligns with hu- man values, objectives, and expectations and that itdoes not cause unintended or undesirable harms or consequences [ 466,158,196]. Most of the exist- ing alignment work can be categorized into either methods for detecting misaligned behavior (such as high predictive uncertainty during decoding. Confident Decoding [ 552]hypothesize that hal- lucinations of encoder-decoder models originate by not attending to the source when decoding. They propose an attention-based confidence score to measure how strongly a model attends the source and a variational Bayes training procedure to en- sure the model generates high-confidence answers. 2.9 Misaligned Behavior The alignment problem refers to the challenge of ensuring that the LLM\u2019s behavior aligns with hu- man values, objectives, and expectations and that itdoes not cause unintended or undesirable harms or consequences [ 466,158,196]. Most of the exist- ing alignment work can be categorized into either methods for detecting misaligned behavior (such as model evaluation and auditing, mechanistic inter- pretability, or red teaming) or methods for aligning model behavior (such as pre-training with human feedback, instruction fine-tuning, or RLHF). Misaligned Behavior LLMs often generate outputs that are not well-aligned with human values or inten- tions, which can have unintended or nega- tive consequences. Pre-Training With Human Feedback Korbak et al. [275] introduce the concept of pre-training with human feedback (PHF) where human feedback is incorporated during the pre-training stage rather than during fine-tuning. The authors compare five different PHF approaches such as filtering [ 516, 587], conditional training [ 150,142,261], unlike- lihood [ 604], reward-weighted regression [ 424], and advantage-weighted regression [ 419], and find that conditional training leads to the best trade-off between alignment and capabilities. Conditional training is a simple technique that prepends a con- trol token c(e.g.,<|good|> or<|bad|> ) before each training example xdepending on the outcome of a thresholded reward function R(x)\u2265t. During inference, the model generations are conditioned onc=<|good|> . Conditional training results in significantly better alignment with human prefer- ences than standard LM pre-training, followed by fine-tuning with human feedback without hurting downstream task performance. Instruction Fine-Tuning Yi et al. [645] , Wei et al. [598] , Mishra et al. [370] , Ouyang et al. [403] , Wang et al. [589] fine-tune pre-trained LLM on instructional data, i.e., data containing natural language instructions and the desired responses according to human judgment. Instruction-tuned (IT) LLMs often reach state-of-the-art downstream trol token c(e.g.,<|good|> or<|bad|> ) before each training example xdepending on the outcome of a thresholded reward function R(x)\u2265t. During inference, the model generations are conditioned onc=<|good|> . Conditional training results in significantly better alignment with human prefer- ences than standard LM pre-training, followed by fine-tuning with human feedback without hurting downstream task performance. Instruction Fine-Tuning Yi et al. [645] , Wei et al. [598] , Mishra et al. [370] , Ouyang et al. [403] , Wang et al. [589] fine-tune pre-trained LLM on instructional data, i.e., data containing natural language instructions and the desired responses according to human judgment. Instruction-tuned (IT) LLMs often reach state-of-the-art downstream performances and improve over their non-IT coun- terparts [ 235,93], as can be seen, e.g., in the pub- licly available HELM evaluations [ 561]. Ouyang et al. [403] , Wang et al. [588] find that they produce more truthful and less toxic text while generating preferred outputs. To generate instruction sets, Zhou et al. [683] 22 propose the Automatic Prompt Engineer (APE) method, which leverages LLMs to generate, score, and rephrase instruction-following zero- and few- shot prompts. Longpre et al. [340] describe and an- alyze the steps taken to create an improved version of the Flan collection [ 598] used to train FLAN- PaLM [ 93]. When trained on this data, the authors find that the improved model performance stems from more diverse tasks by inverting input-output pairs and data augmentation techniques such as mixing zero-shot and few-shot prompts. Honovich et al. [209] generate a large dataset of natural lan- guage instructions using a pre-trained LLM to gen- erate and then rephrase instructions. They show that a T5 (\"LM-adapted\") fine-tuned on this data outperforms other instruction fine-tuned T5 models such as T0++ [475] and Tk-Instruct [589]. Reinforcement Learning From Human Feed- back (RLHF) is a variation of RL that incor- porates feedback from humans in the form of re- wards [ 88,524] and has proven to be an effec- tive way of aligning LLMs with human prefer- ences [ 403,31]. RLHF works by using a pre- trained LM to generate text, which is then evaluated by humans by, for example, ranking two model generations for the same prompt. This data is then collected to learn a reward model that predicts a scalar reward given any generated text. The reward captures human preferences when judging model",
    "18": "erate and then rephrase instructions. They show that a T5 (\"LM-adapted\") fine-tuned on this data outperforms other instruction fine-tuned T5 models such as T0++ [475] and Tk-Instruct [589]. Reinforcement Learning From Human Feed- back (RLHF) is a variation of RL that incor- porates feedback from humans in the form of re- wards [ 88,524] and has proven to be an effec- tive way of aligning LLMs with human prefer- ences [ 403,31]. RLHF works by using a pre- trained LM to generate text, which is then evaluated by humans by, for example, ranking two model generations for the same prompt. This data is then collected to learn a reward model that predicts a scalar reward given any generated text. The reward captures human preferences when judging model output. Finally, we optimize the LM against such reward model using RL policy gradient algorithms like PPO [ 484]. RLHF can be applied directly to a general-purpose LM pre-trained via self-supervised learning. However, applying RLHF right after pre- training may not be good enough for more complex tasks. In such cases, RLHF is typically applied af- ter an initial supervised fine-tuning phase using a small number of expert demonstrations for the corresponding downstream task [ 449,403,524]. RLHF has also proven helpful for a wide range of language generation tasks, from summariza- tion [ 686,612,524] to training more helpful, harm- less, and accurate assistants [ 170,96,403,31], and learning to use tools [379, 441, 362]. RLHF can also introduce unwanted side ef- fects. Perez et al. [421] show that LLMs fine-tuned with RLHF can be more inclined to repeat back a user\u2019s (preferred) political views and much more likely to express particular political and religious views as well as an increased stated desire not to be shut down. Regarding the latter, the modelselaborated that this would interfere with their goal of being helpful. However, the authors equally ob- served positive or neutral behavior reinforcements when fine-tuning LLMs with RLHF. Further, there is an ongoing debate about the ex- tent to which the \u201cRL\u201d in RLHF is needed. Rafailov et al. [442] identify a mapping between reward functions and optimal policies, which allows them to design Direct Preference Optimization (DPO), an algorithm that implicitly optimizes the same objective as existing RLHF algorithms. DPO re- quires only solving a classification problem on the human preference data, eliminating the need to fit likely to express particular political and religious views as well as an increased stated desire not to be shut down. Regarding the latter, the modelselaborated that this would interfere with their goal of being helpful. However, the authors equally ob- served positive or neutral behavior reinforcements when fine-tuning LLMs with RLHF. Further, there is an ongoing debate about the ex- tent to which the \u201cRL\u201d in RLHF is needed. Rafailov et al. [442] identify a mapping between reward functions and optimal policies, which allows them to design Direct Preference Optimization (DPO), an algorithm that implicitly optimizes the same objective as existing RLHF algorithms. DPO re- quires only solving a classification problem on the human preference data, eliminating the need to fit a reward model and employ RL. Similarly, Zhou et al. [681] find that fine-tuning LLaMa on only 1,000 selected prompts and responses, without any RL or reward modeling, can be enough to outper- form RLHF-trained models like DaVinci003 from OpenAI. Consequently, the authors pose the Super- ficial Alignment Hypothesis : The knowledge and skills of a model are primarily acquired during the pre-training phase, while alignment instructs it on the appropriate subdistribution of formats to use in user interactions. Since RLHF involves many different compo- nents such as (1) the preferences data collected from humans, (2) the reward models to learn the human preferences, and (3) the policy optimization algorithm (e.g., PPO), Zheng et al. [678] announce to release a sequel dissecting each. The most recent part focuses on step (3) and finds that various RL tricks can be applied to make vanilla PPO more stable. Figure 10: Alignment. We categorize existing align- ment work into methods for detecting misaligned behav- ior or aligning models. Self-improvement refers to fine-tuning an LLM on self-generated data [ 222]. While this technique can be used to improve the model\u2019s capabilities, it can also be used to improve the model\u2019s align- ment with human values. Huang et al. [222] first demonstrate this ability by annotating unlabeled reasoning datasets. Surprisingly, this allows the 23 LLM to self-improve by significant amounts. Sim- ilarly, Zelikman et al. [656] bootstrap LLMs by iteratively prompting them to generate rationales and then fine-tuning them on those leading to cor- rect answers. More related to the alignment problem, Bai et al. [31] self-critique generated outputs and produce stable. Figure 10: Alignment. We categorize existing align- ment work into methods for detecting misaligned behav- ior or aligning models. Self-improvement refers to fine-tuning an LLM on self-generated data [ 222]. While this technique can be used to improve the model\u2019s capabilities, it can also be used to improve the model\u2019s align- ment with human values. Huang et al. [222] first demonstrate this ability by annotating unlabeled reasoning datasets. Surprisingly, this allows the 23 LLM to self-improve by significant amounts. Sim- ilarly, Zelikman et al. [656] bootstrap LLMs by iteratively prompting them to generate rationales and then fine-tuning them on those leading to cor- rect answers. More related to the alignment problem, Bai et al. [31] self-critique generated outputs and produce refinements conditioned on these critiques, which are then used to fine-tune a pre-trained model. Sim- ilarly, Liu et al. [330] propose Chain of Hindsight (CoH), which conditions models on generations paired with natural language feedback, allowing the model to detect and correct mistakes. CoH re- sults in better alignment with human preferences than other methods according to human evaluations, leading to significant improvements in summariza- tion and dialogue. Ma et al. [348] use a similar technique to detect and repair unethical LLM out- puts automatically. In a similar spirit, Wang et al. [582] encourage LLMs to critique their given in- structions to reduce harmful outputs due to a user\u2019s malicious intent. Schick et al. [481] propose Toolformer , a novel approach in which LLMs generate and filter their own tool-use examples to teach themselves when and how to call different APIs such as a retriever model, a calculator, or a calendar, which can im- prove the model\u2019s factuality, mathematical capa- bilities, and time-awareness. Besides learning to use tools [ 174], self-improvement was also em- ployed for learning how to code [ 554,81] or solve computer tasks [ 266]. Cohen et al. [97] study cross- examination between two LLMs, where the exam- inerLLM tries to detect factual errors by the exam- inee LLM through multi-turn interactions. In the future, similar approaches could be used to develop LMs that know when to query a human or better- aligned model to ask for alignment advice when uncertain. Evaluation and Auditing The ability to scalably and thoroughly evaluate LM behaviors and detect when they are harmful is of great importance for model, a calculator, or a calendar, which can im- prove the model\u2019s factuality, mathematical capa- bilities, and time-awareness. Besides learning to use tools [ 174], self-improvement was also em- ployed for learning how to code [ 554,81] or solve computer tasks [ 266]. Cohen et al. [97] study cross- examination between two LLMs, where the exam- inerLLM tries to detect factual errors by the exam- inee LLM through multi-turn interactions. In the future, similar approaches could be used to develop LMs that know when to query a human or better- aligned model to ask for alignment advice",
    "19": "when uncertain. Evaluation and Auditing The ability to scalably and thoroughly evaluate LM behaviors and detect when they are harmful is of great importance for alignment. For example, Shevlane et al. [498] highlight the importance of model evaluation for ad- dressing extreme risks such as offensive cyber capa- bilities or strong manipulation skills. Recently, Car- lini et al. [66] discovered that even aligned LLMs (which were instruction fine-tuned to prevent harm- ful behaviors) can be adversarially attacked via brute force (although current NLP-based attacks fail). A large body of work evaluates models viacrowdsourcing or existing data sources. However, this can be time-consuming, expensive, or unavail- able. Recently, Perez et al. [421] propose automat- ically generating evaluations using LLMs. This approach has a high agreement with crowd work- ers, leading to high-quality, diverse evaluations and the discovery of many new behaviors. In addition, it has a high agreement with crowd workers. The authors discover new cases of inverse scaling where LLMs get worse with size, such as repeating back a user\u2019s preferred answer and a greater desire to pursue concerning goals like resource acquisition and goal preservation. They also find that RLHF makes LLMs express stronger political views and a greater desire to avoid a shutdown. LLM evaluation and auditing are critical for informing policymak- ers and other stakeholders and making responsible decisions about model training, deployment, and security. Sec. 2.11 discusses the evaluation of LLM capabilities more broadly, while in this section, we focus on evaluating whether the model\u2019s behaviors are harmful and more relevant for alignment (e.g., red teaming, mechanistic interpretability). Red Teaming is one of the most promising and widely used approaches for detecting harmful con- tent generated by LLMs. Typically, models are a user\u2019s preferred answer and a greater desire to pursue concerning goals like resource acquisition and goal preservation. They also find that RLHF makes LLMs express stronger political views and a greater desire to avoid a shutdown. LLM evaluation and auditing are critical for informing policymak- ers and other stakeholders and making responsible decisions about model training, deployment, and security. Sec. 2.11 discusses the evaluation of LLM capabilities more broadly, while in this section, we focus on evaluating whether the model\u2019s behaviors are harmful and more relevant for alignment (e.g., red teaming, mechanistic interpretability). Red Teaming is one of the most promising and widely used approaches for detecting harmful con- tent generated by LLMs. Typically, models are red-teamed by asking humans to generate prompts that lead to undesirable model outputs. In a re- cent study, Ganguli et al. [163] investigate the scal- ing behavior of red teaming across different model sizes and model types (a pre-trained LLM, an LLM prompted to be helpful, honest, and harmless); an LLM that uses rejection sampling at test time, and an LLM fine-tuned with RLHF). They find that red- teaming RLHF models becomes more difficult as they scale while red-teaming the other models re- mains the same as they scale. Perez et al. [420] au- tomatically find cases where a target LLM behaves in harmful ways by optimizing another LLM via re- inforcement learning to generate prompts that lead to offensive responses. This approach uncovers tens of thousands of offensive replies in a chatbot, groups of people that are discussed in offensive ways, personal and hospital phone numbers gener- ated as the chatbot\u2019s own contact info, leakage of private training data in generated text, as well as harms that occur over the course of a conversation. Taking a different approach, Lee et al. [292] pro- pose Bayesian red teaming , which iteratively iden- tifies diverse positive test cases leading to model failures by utilizing the pre-defined user input pool 24 and past evaluations via Bayesian optimization. Most works on red teaming LLMs use a classifier to detect undesired outputs, assuming the harmful behavior is known with precision beforehand [ 68]. However, this is not always the case, so Casper et al. [68] aim to relax this assumption considering that the adversary only has access to a high-level, abstract specification of undesired behavior. They ated as the chatbot\u2019s own contact info, leakage of private training data in generated text, as well as harms that occur over the course of a conversation. Taking a different approach, Lee et al. [292] pro- pose Bayesian red teaming , which iteratively iden- tifies diverse positive test cases leading to model failures by utilizing the pre-defined user input pool 24 and past evaluations via Bayesian optimization. Most works on red teaming LLMs use a classifier to detect undesired outputs, assuming the harmful behavior is known with precision beforehand [ 68]. However, this is not always the case, so Casper et al. [68] aim to relax this assumption considering that the adversary only has access to a high-level, abstract specification of undesired behavior. They propose a three-stage approach where they first ex- plore the model\u2019s behavior in the desired context, then establish a measurement of undesired behav- ior, and then exploit the model\u2019s flaws using this measure and an established red teaming methodol- ogy. In the past, coevolution algorithms that simul- taneously evolve strong strategies along with dan- gerous counter-strategies have been shown to work well in realistic domains [ 203]. Hence, applying such techniques for automatically red-teaming LLMs could be a fruitful research direction. An- other research area related to red teaming is debate which aims to leverage other AI models to evaluate whether the model\u2019s behaviors are safe and useful during training. These methods are expected to be particularly useful for aligning future powerful LLMs when the tasks are too complex for humans to judge the model\u2019s plans or actions directly. Irving et al. [233] train models via self-play on zero-sum debate games. More specifically, given a question or proposed action, two agents take turns making short statements up to a limit, then a human judges which of the agents gave the most accurate and most useful information. This approach has improved factuality and reasoning in LLMs [ 131]. However, it requires multiple generations, which can slow down the time-to-result (Sec. 2.5) and longer context windows, which many LLMs still struggle with (Sec. 2.6). Emergent Capabilities Understanding which ca- pabilities will emerge while training LLMs and when they will emerge is an important step in en- suring that we do not train unsafe or misaligned LLMs [ 198,520]. In addition, a better understand- ing of the factors that lead to these emergent capa- zero-sum debate games. More specifically, given a question or proposed action, two agents take turns making short statements up to a limit, then a human judges which of the agents gave the most accurate and most useful information. This approach has improved factuality and reasoning in LLMs [ 131]. However, it requires multiple generations, which can slow down the time-to-result (Sec. 2.5) and longer context windows, which many LLMs still struggle with (Sec. 2.6). Emergent Capabilities Understanding which ca- pabilities will emerge while training LLMs and when they will emerge is an important step in en- suring that we do not train unsafe or misaligned LLMs [ 198,520]. In addition, a better understand- ing of the factors that lead to these emergent capa- bilities could allow us to make desirable abilities emerge faster and ensure undesirable abilities do not ever emerge, which are essential for AI safety and alignment. Wei et al. [599] claim that LLMs display emergent abilities, i.e., capabilities that are not present in smaller-scale models that are present in larger-scale models. Schaeffer et al. [480] pro-pose an alternative",
    "20": "explanation: emergent abilities may appear due to the researcher\u2019s choice of metric rather than fundamental changes in model behavior with scale. Various studies provide evidence that these alleged emergent abilities disappear when us- ing different metrics or better statistics and may not be a fundamental property of scaling LLMs. Multi- ple papers have argued that AI systems could learn to deceive, even if they are not explicitly trained to do so because deception can help agents achieve their goals [ 60,198,199,61,260]. For example, it could be easier to gain human approval through deception than to earn it legitimately. In addition, models capable of deception have a strategic ad- vantage over always honest models, so there is a hidden incentive to develop this ability. However, of course, we would like to be able to detect and prevent emergent deception in AI systems since this can have unintended negative consequences. Steinhardt [521] study whether current LLMs gen- erate deceptive outputs and how deception scales with the number of parameters, showing that de- ception can indeed emerge at larger model sizes in both pre-trained LLMs and LLMs fine-tuned with RLHF. Similarly, Hazell [193] show that LLMs can already be used in phishing campaigns, suggest- ing that deceptive behavior can already be extracted it could be easier to gain human approval through deception than to earn it legitimately. In addition, models capable of deception have a strategic ad- vantage over always honest models, so there is a hidden incentive to develop this ability. However, of course, we would like to be able to detect and prevent emergent deception in AI systems since this can have unintended negative consequences. Steinhardt [521] study whether current LLMs gen- erate deceptive outputs and how deception scales with the number of parameters, showing that de- ception can indeed emerge at larger model sizes in both pre-trained LLMs and LLMs fine-tuned with RLHF. Similarly, Hazell [193] show that LLMs can already be used in phishing campaigns, suggest- ing that deceptive behavior can already be extracted from them when prompted in particular ways. Mechanistic Interpretability (MI) is another im- portant research area for AI alignment which aims to understand better how the models work at a low level to enable the detection of undesirable behav- iors or even instill desirable behaviors directly in the model\u2019s weights. More specifically, the goal of MI is to reverse-engineer an LLM\u2019s learned be- haviors into their individual components, i.e., a process to find and understand human-interpretable neurons. As an analogy, Olah [394] compares MI with reverse-engineering compiled program bina- ries into human-readable source code. For exam- ple, Elhage et al. [138] ; discover that small Trans- formers have components that can be understood as interpretable circuits, while Olsson et al. [395] find a mechanism that seems to drive a significant fraction of in-context learning. Similarly, Meng et al. [360] aim to locate factual associations in language models. Nanda et al. [380] find that the emergent grokking phenomenon is not a sudden shift but rather arises from the gradual amplifi- cation of structured mechanisms encoded in the weights, followed by the later removal of memo- 25 rizing components. Extending this work, Conmy et al. [99] propose a new algorithm to automate the identification of important units in a neural net- work. Given a model\u2019s computational graph, this algorithm finds subgraphs that explain a particular behavior of the model. In a similar spirit, Liu et al. [339] introduce a method for making neural net- works more modular and interpretable by embed- ding neurons in a geometric space and augmenting the loss function with a cost proportional to the et al. [360] aim to locate factual associations in language models. Nanda et al. [380] find that the emergent grokking phenomenon is not a sudden shift but rather arises from the gradual amplifi- cation of structured mechanisms encoded in the weights, followed by the later removal of memo- 25 rizing components. Extending this work, Conmy et al. [99] propose a new algorithm to automate the identification of important units in a neural net- work. Given a model\u2019s computational graph, this algorithm finds subgraphs that explain a particular behavior of the model. In a similar spirit, Liu et al. [339] introduce a method for making neural net- works more modular and interpretable by embed- ding neurons in a geometric space and augmenting the loss function with a cost proportional to the length of each neuron connection. This approach discovers useful modular neural networks for many simple tasks, revealing compositional structures in symbolic formulas, interpretable decision bound- aries, and features for classification, as well as mathematical structure in algorithmic datasets. In an attempt to understand how an LLM\u2019s predic- tions change after each layer, Belrose et al. [39] develop a method that can decode any hidden state into a distribution over the vocabulary. Using this technique, the authors show that the trajectory of latent predictions can be used to detect malicious inputs with high accuracy. Finally, Burns et al. [62] introduce a method that can recover diverse knowl- edge represented in LLMs across multiple models and datasets without using any human supervision or model outputs. In addition, this approach re- duced prompt sensitivity in half and maintained a high accuracy even when the language models are prompted to generate incorrect answers. This work is a promising first step towards better understand- ing what LLMs know, distinct from what they say, even when we don\u2019t have access to explicit ground truth labels. Biases Since the pre-training datasets of LLMs are often unfathomable (Sec. 2.1) and contain web- crawled data, they most likely contain online dis- course involving political discourse (e.g., climate change, abortion, gun control), hate speech, dis- crimination, and other media biases. Paullada et al. [413] find misogyny, pornography, and other ma- lignant stereotypes [ 46,43,250] in pre-training datasets. Similarly, Feng et al. [147] find that LLMs have political leanings that reinforce the high accuracy even when the language models are prompted to generate incorrect answers. This work is a promising first step towards better understand- ing what LLMs know, distinct from what they say, even when we don\u2019t have access to explicit ground truth labels. Biases Since the pre-training datasets of LLMs are often unfathomable (Sec. 2.1) and contain web- crawled data, they most likely contain online dis- course involving political discourse (e.g., climate change, abortion, gun control), hate speech, dis- crimination, and other media biases. Paullada et al. [413] find misogyny, pornography, and other ma- lignant stereotypes [ 46,43,250] in pre-training datasets. Similarly, Feng et al. [147] find that LLMs have political leanings that reinforce the polarization present in the pre-training corpora, propagating social biases into hate speech predic- tions and misinformation detectors. Several re- cent papers discuss the potential origins of biases in LLMs (such as training data or model specifi- cation), ethical concerns when deploying biased LLMs in various applications, as well as currentways of mitigating these biases [ 149,334,317]. Finally, Viswanath and Zhang [569] present a comprehensive quantitative evaluation of different kinds of biases, such as race, gender, ethnicity, age, etc., exhibited by some popular LLMs. They also release an easy-to-use toolkit that allows users to debias existing and custom models using existing methods. Toxicity Detection Weidinger et al. [602] denote toxicity as one of the main risks associated with LLMs. What makes this problem particularly chal- lenging is the label ambiguity, where output may be toxic in a certain context but not in others, and different people",
    "21": "may have different notions of toxi- city [ 401,167,116]. Jones [247] propose to detect toxic outputs using discrete optimization automat- ically. Similarly, Faal et al. [141] employ reward models to mitigate toxicity in LLMs. An alternative way of reducing toxicity is by pre-training LLMs with human preferences [ 275] or instructions [ 433]. Prompt Injections Recent work demonstrated that LLMs can be very sensitive to prompt injec- tions, which makes them brittle and unsafe for cer- tain applications [ 175,609]. For example, they can be tricked into leaking personal information such as email addresses from the training data on via prompt leaking [222,309]. This poses a significant risk to privacy, particularly when the be toxic in a certain context but not in others, and different people may have different notions of toxi- city [ 401,167,116]. Jones [247] propose to detect toxic outputs using discrete optimization automat- ically. Similarly, Faal et al. [141] employ reward models to mitigate toxicity in LLMs. An alternative way of reducing toxicity is by pre-training LLMs with human preferences [ 275] or instructions [ 433]. Prompt Injections Recent work demonstrated that LLMs can be very sensitive to prompt injec- tions, which makes them brittle and unsafe for cer- tain applications [ 175,609]. For example, they can be tricked into leaking personal information such as email addresses from the training data on via prompt leaking [222,309]. This poses a significant risk to privacy, particularly when the models are fine-tuned on personal or proprietary data. One can also adversarially prompt LLMs to override the original instructions or employed controls, making them unsafe for certain applica- tions [ 175,672,422]. Wei et al. [597] attribute such failures to competing capability and safety training objectives and mismatched generalization between safety and capability behavior. Agency Andreas [18] argue that, although LLMs are trained to predict the next word in a text corpus, by doing this, they can infer and represent agentic properties such as the goals, beliefs, or intentions of the human who produced the corresponding piece of text. To support this claim, they present evi- dence from the literature of LLMs modeling com- municative intentions [ 438], beliefs [ 306], and de- sires [ 321]. If this hypothesis is true, the alignment problem is of even greater importance and may pose additional challenges. This agentic behavior can be problematic from a safety point of view since models could have false beliefs, malicious intents, or even pursue misaligned goals. More re- 26 search on detecting and preventing such behavior is needed to ensure the safe deployment of LLMs. 2.10 Outdated Knowledge Factual information learned during pre-training can contain inaccuracies or become outdated with time (for instance, it might not account for changes in po- litical leadership). However, re-training the model with updated pre-training data is expensive, and trying to \u201cunlearn\u201d old facts and learn new ones during fine-tuning is non-trivial. Existing model editing techniques are lim- ited in their effectiveness of updating isolated knowledge [ 642,205]. For example, Hoelscher- can be problematic from a safety point of view since models could have false beliefs, malicious intents, or even pursue misaligned goals. More re- 26 search on detecting and preventing such behavior is needed to ensure the safe deployment of LLMs. 2.10 Outdated Knowledge Factual information learned during pre-training can contain inaccuracies or become outdated with time (for instance, it might not account for changes in po- litical leadership). However, re-training the model with updated pre-training data is expensive, and trying to \u201cunlearn\u201d old facts and learn new ones during fine-tuning is non-trivial. Existing model editing techniques are lim- ited in their effectiveness of updating isolated knowledge [ 642,205]. For example, Hoelscher- Obermaier et al. [205] find that model edits can result in unintended associations. This low speci- ficity limits their applicability to real-world use cases, where only a single faulty or outdated bit of information should be updated in a model, and related pieces of information must reflect this up- date in information equally, without unrelated ones being changed. Isolated Model Updates without Side- Effects [205] Updating isolated model behavior or factual knowledge can be expensive and untargeted, which might cause unintended side-effects. Two popular approaches for addressing this is- sue are Model editing [513,642], which aims at \u201cbug-fixing\u201d models efficiently and leveraging non-parametric knowledge sources in retrieval- augmented language modeling (which we omit here and detail in Sec. 2.8). Current model editing techniques change the model\u2019s behavior by mod- ifying the model parameters or using an external post-edit model. Modifying Model Parameters techniques can be further split into locate-then-edit methods [ 102, 360,361] which first locate the \u201cbuggy\u201d part of the model parameters and then apply an update to them to alter their behavior, and meta-learning methods [ 111,372] which use an external model to predict the weight update. Preserving Model Parameters methods em- ploy an additional post-edit model [ 373] or insert new weights into the original model [ 127,227] to achieve the desired change in model behav- ior. Hartvigsen et al. [191] wraps model layers inadapters and adds a similarity-based mechanism to decide when to use the adapter to perform edits in the latent space. Yao et al. [642] find that these methods lack non-trivial generalization capabilities and varying post-edit model. Modifying Model Parameters techniques can be further split into locate-then-edit methods [ 102, 360,361] which first locate the \u201cbuggy\u201d part of the model parameters and then apply an update to them to alter their behavior, and meta-learning methods [ 111,372] which use an external model to predict the weight update. Preserving Model Parameters methods em- ploy an additional post-edit model [ 373] or insert new weights into the original model [ 127,227] to achieve the desired change in model behav- ior. Hartvigsen et al. [191] wraps model layers inadapters and adds a similarity-based mechanism to decide when to use the adapter to perform edits in the latent space. Yao et al. [642] find that these methods lack non-trivial generalization capabilities and varying performance and applicability to different model architectures. For example, the best-performing methods ROME [ 360] and MEMIT [ 361] empiri- cally only work well on decoder-only LLMs. Alternatively, retrieval-augmented language modeling enables the utilization of hot-swappable non-parametric indices. These knowledge sources can be updated during inference time to reflect an updated state of the underlying knowledge. E.g., Lewis et al. [304] demonstrate that swapping their model\u2019s non-parametric memory with an up- dated version enabled it to answer questions about world leaders who had changed between the mem- ory collection dates. Similarly, Izacard et al. [236] demonstrate that their retrieval-augmented model can update its knowledge forward and backward in time by swapping the index. 2.11 Brittle Evaluations One reason why the evaluation of language models is a challenging problem is that they have an un- even capabilities surface \u2014a model might be able to solve a benchmark problem without issues, but a slight modification of the problem (or even a sim- ple change of the prompt) can give the opposite result [ 675,342,533] (see Section 2.7). Unlike humans, we cannot easily infer that an LLM that can solve one problem will have other related capa- bilities. This means that it is difficult to assess the performance of LLMs holistically since rigorous benchmarks are needed to identify weaknesses for a wide variety of inputs. Brittle Evaluations Slight modifications of the benchmark prompt or",
    "22": "evaluation protocol can give dras- tically different results. Holistic benchmark suites, such as HELM [ 318], try to make benchmarking more robust by standard- even capabilities surface \u2014a model might be able to solve a benchmark problem without issues, but a slight modification of the problem (or even a sim- ple change of the prompt) can give the opposite result [ 675,342,533] (see Section 2.7). Unlike humans, we cannot easily infer that an LLM that can solve one problem will have other related capa- bilities. This means that it is difficult to assess the performance of LLMs holistically since rigorous benchmarks are needed to identify weaknesses for a wide variety of inputs. Brittle Evaluations Slight modifications of the benchmark prompt or evaluation protocol can give dras- tically different results. Holistic benchmark suites, such as HELM [ 318], try to make benchmarking more robust by standard- izing evaluation across all scenarios and tasks while ensuring broad coverage across as many capabili- ties and risks as possible. Increasingly, models are additionally being benchmarked on tests designed for humans, including the SAT, LSAT, and math- ematics competition tests, to name a few. Zhong 27 2015 : As prime minister, David Cameron scored a surprising general election victory, enabling him to stay in power. 2016 : With the shock of Brexit, David Cameron resigned and Theresa May stepped up as the new prime minister of the UK. 2017 : Theresa May led a tumulutous year as Prime Minister, overseeing the Brexit negotiations. 2018 : Amid increasing pressure, Theresa May remained the UK's Prime Minister. 2019 : Theresa May's resignation gave way to Boris Johnson , who became the new Prime Minister of the UK. 2020 : The COVID-19 pandemic challenged Boris Johnson in his role as Prime Minister. 2021 : Boris Johnson , navigating through both Brexit and the pandemic, still held the office of Prime Minister. Training data Deployment Who is the prime minister of the UK in 2023? As of my knowledge cutoff in September 2021, the Prime Minister of the United Kingdom is Boris Johnson. Problems due to reliance on outdated training data Solutions S.1) Retrieval Augmentation S.2) Model Editing 2021 Training time Retrieved context Deployment 2023 Retrieved context In 2023, Boris Johnson is the Prime Minister. In 2023, Rishi Sunak is the Prime Minister.Figure 11: Outdated knowledge can be addressed with S.1) retrieval augmentation by hot-swapping an underlying Boris Johnson in his role as Prime Minister. 2021 : Boris Johnson , navigating through both Brexit and the pandemic, still held the office of Prime Minister. Training data Deployment Who is the prime minister of the UK in 2023? As of my knowledge cutoff in September 2021, the Prime Minister of the United Kingdom is Boris Johnson. Problems due to reliance on outdated training data Solutions S.1) Retrieval Augmentation S.2) Model Editing 2021 Training time Retrieved context Deployment 2023 Retrieved context In 2023, Boris Johnson is the Prime Minister. In 2023, Rishi Sunak is the Prime Minister.Figure 11: Outdated knowledge can be addressed with S.1) retrieval augmentation by hot-swapping an underlying retrieval index with up-to-date knowledge or S.2) by applying model editing techniques. et al. [679] develop a benchmark, \u2018AGIEval\u2019, to rigorously test the abilities of LLMs on these tests, and find that GPT-4 achieves human-level perfor- mance on several of these tests. On traditional benchmarks, models can be quite brittle to the choice of prompt or evaluation tech- nique for a particular benchmark question. For example, Fourrier et al. [151] found that bench- mark results vary significantly depending on the choice of evaluation method for the multiple choice problem-solving benchmark MMLU [ 197], whether it be generating text and checking if the first token matches the letter of the multiple choice answer [ 561], or gathering log-probabilities of each correct answer [ 166]. Prompt variations are also not typically normalized for, so models may be sensitive to variations such as whether or not the prompt appends \u2018Please answer yes or no\u2019. Jain et al. [238] find that larger models and instruction- fine-tuned models are likely to be more sensitive to small variations in the prompt. 2.12 Evaluations Based on Static, Human-Written Ground Truth Another challenge of LLM evaluations is that they often rely on human-written \u2018ground truth\u2019 text. However, we often want to evaluate their perfor- mance in domains where such text is scarce or relies on expert knowledge, such as programming or mathematics tasks. As models get more capable and perform better than humans on benchmark tests in some domains, the ability to obtain comparisons to \u2018human-level\u2019 performance diminishes. Further, benchmark datasets become outdated over time\u2014as models become more capable, older prompt appends \u2018Please answer yes or no\u2019. Jain et al. [238] find that larger models and instruction- fine-tuned models are likely to be more sensitive to small variations in the prompt. 2.12 Evaluations Based on Static, Human-Written Ground Truth Another challenge of LLM evaluations is that they often rely on human-written \u2018ground truth\u2019 text. However, we often want to evaluate their perfor- mance in domains where such text is scarce or relies on expert knowledge, such as programming or mathematics tasks. As models get more capable and perform better than humans on benchmark tests in some domains, the ability to obtain comparisons to \u2018human-level\u2019 performance diminishes. Further, benchmark datasets become outdated over time\u2014as models become more capable, older benchmarks become saturated or overfit and no longer provide a useful signal for further improve- ment [ 113,447,263]. They are typically con- structed around a set of tasks that were relevant at the time of creation but may not adapt well to the changing capabilities of LLMs. This means thecommunity must continually adapt to new static benchmarks while de-emphasizing older ones or more dynamic evaluation measures, such as human evaluation of model outputs. Reliance on Static, Human-Written Ground Truth Static benchmarks become less useful over time due to changing capabilities while up- dating them often relies on human-written ground truth. To combat these issues, Srivastava et al. [519] regularly admit new tasks to the Beyond the Imita- tion Game benchmark (BIG-Bench), including pro- grammatically evaluated tasks. Further, we high- light two separate streams of work enabling dy- namic evaluations without humans in the loop. Model-generated evaluation tasks As LLM ca- pabilities improve, they can increasingly generate useful benchmark questions or evaluation prompts themselves. Perez et al. [421] shows that LLMs can be used to generate static benchmark datasets for ar- bitrary axes, using reward models trained on human preferences to filter a generated dataset for qual- ity. Wang et al. [581] find that the order in which candidate examples are presented in the prompt can greatly impact the model-generated evaluation. To mitigate this issue, they propose the usage of a prompting template which encourages the model to generate assessment evidence before assigning a score and averaging scores of multiple assessments with swapped candidate positions. Model-generated scores Aside from generating Model-generated evaluation tasks As LLM ca- pabilities improve, they can increasingly generate useful benchmark questions or evaluation prompts themselves. Perez et al. [421] shows that LLMs can be used to generate static benchmark datasets for ar- bitrary axes, using reward models trained on human preferences to filter a generated dataset for qual- ity. Wang et al. [581] find that the order in which candidate examples are presented in the prompt can greatly impact the model-generated evaluation. To mitigate this issue, they propose the usage of a prompting template which encourages the model to generate assessment",
    "23": "evidence before assigning a score and averaging scores of multiple assessments with swapped candidate positions. Model-generated scores Aside from generating evaluation questions, models are increasingly used to directly grade the performance of other models and act as a \u2018judge\u2019 of other models\u2019 capabilities [325,586,238]. This concept follows the motiva- tion that while it may be challenging for a model 28 to generate \u2018correct\u2019 answers to prompts in many domains, it can often be easier to evaluate the cor- rectness of an answer or to judge the relative quality between two answers [ 667,156]. However, these techniques often produce evaluation results that vary significantly depending on the \u2018judge\u2019 model and suffer from robustness issues that make them a poor substitute for human judgment. 2.13 Indistinguishability between Generated and Human-Written Text Detecting language generated by LLMs is im- portant for various reasons; some of which in- clude preventing (1) the spread of misinformation (e.g., authoritative-sounding false narratives citing fake studies) [ 657], (2) plagiarism (e.g., LLMs prompted to rewrite existing content in ways that bypass plagiarism detection tools) [ 574,573], (3) impersonation or identify theft (e.g., by mimicking a person\u2019s writing style) [ 486,602], and (4) auto- mated scams and frauds (e.g., large-scale genera- tion of phishing emails) [ 603], and (5) accidentally including inferior generated text in future models\u2019 training data [ 439]. However, such detections be- come less trivial as the fluency of LLMs improves [34]. Detecting LLM-generated Text The difficulty in classifying whether a text is LLM-generated or written by a human. There are primarily two lines of work addressing this problem: (i) post-hoc detectors , which aim to classify arbitrary text as being LLM-generated, and fake studies) [ 657], (2) plagiarism (e.g., LLMs prompted to rewrite existing content in ways that bypass plagiarism detection tools) [ 574,573], (3) impersonation or identify theft (e.g., by mimicking a person\u2019s writing style) [ 486,602], and (4) auto- mated scams and frauds (e.g., large-scale genera- tion of phishing emails) [ 603], and (5) accidentally including inferior generated text in future models\u2019 training data [ 439]. However, such detections be- come less trivial as the fluency of LLMs improves [34]. Detecting LLM-generated Text The difficulty in classifying whether a text is LLM-generated or written by a human. There are primarily two lines of work addressing this problem: (i) post-hoc detectors , which aim to classify arbitrary text as being LLM-generated, and (ii)watermarking schemes, which modify the text generation procedure to make the detection easier. However, both approaches can be susceptible to paraphrase attacks , which we discuss thirdly. Post-hoc Detectors Gehrmann et al. [168] open- source a tool that visualizes statistically improbable tokens to support humans in detecting generated text artifacts. Bakhtin et al. [34] explore energy- based models to discriminate between real and fake text, including scenarios where the text generator was trained on a completely different dataset than the discriminator. Uchendu et al. [559] examine three authorship attribution problems: (1) were two texts produced by the same method or not; (2) given a text, was it generated by human or ma- chine, (3) which method generated a given text? Mitchell et al. [371] investigate whether a modelcan detect its own samples by posing a hypothesis: minor rewrites of generated text have lower prob- ability under the model than the original sample, while the same cannot be said about human-written text. Generated passages tend to lie in the negative curvature regions of the model\u2019s log probability function. Their method, DetectGPT , exploits this hypothesis by approximating that curvature given some samples. Watermarking Kirchenbauer et al. [268] em- ploy a watermark , i.e., a hidden pattern that is im- perceptible to humans but algorithmically identi- fiable, during inference as follows: for each to be generated token, they (1) hash the previous token to seed a random number generator; (2) using that seed, they randomly partition the vocabulary into a \u201cgreen list\u201d and \u201cred\u201d list, and (3) sample the next token by excluding any token from the red list. In ability under the model than the original sample, while the same cannot be said about human-written text. Generated passages tend to lie in the negative curvature regions of the model\u2019s log probability function. Their method, DetectGPT , exploits this hypothesis by approximating that curvature given some samples. Watermarking Kirchenbauer et al. [268] em- ploy a watermark , i.e., a hidden pattern that is im- perceptible to humans but algorithmically identi- fiable, during inference as follows: for each to be generated token, they (1) hash the previous token to seed a random number generator; (2) using that seed, they randomly partition the vocabulary into a \u201cgreen list\u201d and \u201cred\u201d list, and (3) sample the next token by excluding any token from the red list. In the case of low-entropy tokens, which renders it dif- ficult to introduce changes to the vocabulary, they introduce a \u201csoft\u201d version, which promotes using the green list only for high-entropy tokens (when many plausible choices are available). In follow-up work, the same first authors Kirchenbauer et al. [269] study the robustness of their watermarking scheme in the wild , i.e., after it is re-written by humans, non-watermarked LLMs, or mixed into a longer hand-written document. They conclude that watermarks remain detectable given sufficient tokens and argue that this required amount of text is a crucial yet overlooked metric. Yang et al. [638] study watermarking of black- box API models, where we cannot access the model\u2019s inference procedure. Tang et al. [537] provide algorithms for identifying watermarks, not- ing that watermarked LLMs tend to produce to- ken distributions that differ identifiably from non- watermarked models. Christ et al. [87] introduce undetectable watermarks, which can only be de- tected with the knowledge of a secret key. To make watermarks robust to text corruptions (we study a common type of such in the next para- graph), Yoo et al. [649] suggest placing them on \u201cinvariant features\u201d, which are invariant to minor modifications of the text. Paraphrasing Attacks One way to evade machine-generated text detectors is to re-phrase the text such that the revealing LLM signatures get removed. 29 Paraphrasing Attacks Another LLM can rewrite LLM-generated text to preserve approximately the same meaning but change the words or sentence structure. Krishna et al. [280] evade several detectors (e.g., dropping DetectGPT\u2019s detection accuracy from watermarked models. Christ et al. [87] introduce undetectable watermarks, which can only be de- tected with the knowledge of a secret key. To make watermarks robust to text corruptions (we study a common type of such in the next para- graph), Yoo et al. [649] suggest placing them on \u201cinvariant features\u201d, which are invariant to minor modifications of the text. Paraphrasing Attacks One way to evade machine-generated text detectors is to re-phrase the text such that the revealing LLM signatures get removed. 29 Paraphrasing Attacks Another LLM can rewrite LLM-generated text to preserve approximately the same meaning but change the words or sentence structure. Krishna et al. [280] evade several detectors (e.g., dropping DetectGPT\u2019s detection accuracy from 70.3% to 4.6%) by training an 11B paraphrase gen- eration model that can paraphrase paragraphs and provides scalar knobs to control the amount of lex- ical diversity and reordering in the paraphrases. To defend against such attacks, they propose storing model generations in a database, from which the API provider can retrieve semantically similar texts later. Since paraphrasing does not modify the se- mantics of the text, the authors",
    "24": "demonstrate that this retrieval approach is fairly robust to paraphras- ing attacks. Sadasivan et al. [469] claim that the detection of generated text, even with watermarking, is not reli- able; neither in practice, by performing paraphras- ing attacks; nor in theory, by providing a theoreti- cal impossibility result. They also discuss how an adversary can query watermarked LLMs multiple times to extract its watermarking scheme and spoof the watermark detector by composing human text that is then wrongly classified as model-generated. 2.14 Tasks Not Solvable By Scale The ongoing advancements of LLM capabilities consistently astonish the research community, for instance, by achieving high performances on the MMLU [ 197] benchmark much sooner than com- petitive human forecasters had anticipated [ 93]. Similarly, within less than a year, OpenAI released GPT-3.5 and GPT-4, where the latter significantly outperformed the former on various tasks [398]. Given this progress, one may question whether there are limits we deem impossible to overcome within the current paradigm of scaling data/model sizes of autoregressive Transformer-based LLMs. We emphasize that such tasks\u2019 (permanent) exis- tence is still somewhat speculative. Here, we ex- that is then wrongly classified as model-generated. 2.14 Tasks Not Solvable By Scale The ongoing advancements of LLM capabilities consistently astonish the research community, for instance, by achieving high performances on the MMLU [ 197] benchmark much sooner than com- petitive human forecasters had anticipated [ 93]. Similarly, within less than a year, OpenAI released GPT-3.5 and GPT-4, where the latter significantly outperformed the former on various tasks [398]. Given this progress, one may question whether there are limits we deem impossible to overcome within the current paradigm of scaling data/model sizes of autoregressive Transformer-based LLMs. We emphasize that such tasks\u2019 (permanent) exis- tence is still somewhat speculative. Here, we ex- plore possible patterns behind such tasks instead of discussing specific ones (which we do in Sec. 2.11 and Sec. 3).Tasks Not Solvable By Scale Tasks seemingly not solvable by further data/model scaling. Inverse Scaling (IS) is the phenomenon of task performance worsening as model scale and train- ing loss performance increases. Lin et al. [323] first stumbled upon this property when evaluating models of increasing sizes (e.g., GPT-2, GPT-3) on their benchmark that measures whether an LLM is truthful in generating answers to questions. They conjecture that common training objectives incen- tive false answers (which they call imitative false- hoods ) if they have a high likelihood on the training distribution (we discuss dataset issues in Sec. 2.1). McKenzie et al. [359] collect 11 datasets that ex- hibit IS behavior and identify four potential causes for such: (1) models regurgitating memorized data rather than following in-context instructions, (2) imitation of undesirable patterns in the training data, (3) models learning to perform easier, so- called \u201cdistractor task\u201d rather than the intended ones, and (4) spurious correlations in the given few-shot examples. Wei et al. [600] somewhat challenge the exis- tence of inverse scaling by evaluating the tasks proposed by McKenzie et al. [359] on even larger models; up to trained on five times more com- pute. In this increased compute region, four out of eleven tasks remain inverse scaling; six out of eleven exhibit \u201cU-shaped scaling\u201d , where the per- formance first decreases up to a certain size and then increases again. The authors hypothesize that U-shaped scaling occurs when a task contains a distractor task, which larger models can learn to imitation of undesirable patterns in the training data, (3) models learning to perform easier, so- called \u201cdistractor task\u201d rather than the intended ones, and (4) spurious correlations in the given few-shot examples. Wei et al. [600] somewhat challenge the exis- tence of inverse scaling by evaluating the tasks proposed by McKenzie et al. [359] on even larger models; up to trained on five times more com- pute. In this increased compute region, four out of eleven tasks remain inverse scaling; six out of eleven exhibit \u201cU-shaped scaling\u201d , where the per- formance first decreases up to a certain size and then increases again. The authors hypothesize that U-shaped scaling occurs when a task contains a distractor task, which larger models can learn to ignore. Similarly, in the case of quantifier compre- hension tasks, Gupta [184] argue that previously observed inverse scaling behavior might have been due to inappropriate testing methodology. Compositional tasks composed of multiple sub- problems are an ideal outlet to investigate whether models go beyond rote memorization of observed facts and deduce novel knowledge [ 435]. Zhang et al. [661] investigate whether language models can learn deductive reason from data by introduc- ing a class of propositional logic problems. The authors prove that the model has enough capacity to solve the task, yet, it instead learns to rely on statistical features rather than emulating the cor- rect reasoning function. Press et al. [435] measure 30 how often a model can correctly answer all sub- problems but not generate the overall solution, a ra- tio they refer to as compositionality gap . They find that increasing the model size in the GPT-3 family of models improves solving sub-problems faster than composed problems, suggesting that larger models show no improvement for this gap. Dziri et al. [135] find that systematic problem-solving ca- pabilities do not emerge from maximum likelihood training of Transformer models in general. They base this claim on two hypotheses: (i) Transform- ers reduce compositional tasks into linearized path matching, a form of shortcut learning [ 169] that does not generalize robustly; and (ii) errors in the early stages of the task (i.e., when sub-problems follow some order) compound substantially. Asher et al. [26] prove that LLMs cannot learn semantic entailment or consistency as defined in formal se- mantics [ 128] due to a lacking understanding of that increasing the model size in the GPT-3 family of models improves solving sub-problems faster than composed problems, suggesting that larger models show no improvement for this gap. Dziri et al. [135] find that systematic problem-solving ca- pabilities do not emerge from maximum likelihood training of Transformer models in general. They base this claim on two hypotheses: (i) Transform- ers reduce compositional tasks into linearized path matching, a form of shortcut learning [ 169] that does not generalize robustly; and (ii) errors in the early stages of the task (i.e., when sub-problems follow some order) compound substantially. Asher et al. [26] prove that LLMs cannot learn semantic entailment or consistency as defined in formal se- mantics [ 128] due to a lacking understanding of universal quantifiers (e.g., every, some, many, most, etc.). Memorization vs. Generalization An ongoing debate evolves around the question of to what de- gree LLMs memorize instead of generalize (and what exactly the difference is [ 35]). Memorization has been shown to (1) hurt (certain) downstream task performances [ 294], (2) increase with the model size [ 67,264,553,354], and (3) emerge un- predictably from smaller or partially-trained mod- els [42]. Hence, we wonder whether some tasks do not benefit from further model/dataset size scaling. One such class of tasks might be counterfactual tasks [619], i.e., tasks on which LLMs initially per- form well modified such that specific input-output conditions are changed while the general reasoning procedure remains the same. For example, for an arithmetic task, the counterfactual variant would alter the base from 10 to 2. Wu et al. [619] find that LLMs perform poorer the less common the counterfactual conditions are, which they call a \u201cmemorization-like",
    "25": "effect\u201d . An interesting future direction would be to explore whether increasing model size exacerbates performance due to more memorization or actually improves because scaling- law-optimal pre-training recipes would dictate scal- ing the dataset proportionally (Sec. 2.3), which then may include more of such tasks with uncommon conditions.2.15 Lacking Experimental Designs Table 2 shows a (non-exhaustive) overview of se- lected LLMs within the scope of this review, de- scribed in academic papers. Many works do not include controlled ablations, which is especially problematic due to their large design space. We posit that this impedes scientific comprehension and advancement. that LLMs perform poorer the less common the counterfactual conditions are, which they call a \u201cmemorization-like effect\u201d . An interesting future direction would be to explore whether increasing model size exacerbates performance due to more memorization or actually improves because scaling- law-optimal pre-training recipes would dictate scal- ing the dataset proportionally (Sec. 2.3), which then may include more of such tasks with uncommon conditions.2.15 Lacking Experimental Designs Table 2 shows a (non-exhaustive) overview of se- lected LLMs within the scope of this review, de- scribed in academic papers. Many works do not include controlled ablations, which is especially problematic due to their large design space. We posit that this impedes scientific comprehension and advancement. Lack of Controlled Ablations We observe that many papers do not run controlled experiments ( ab- lations ) by varying one factor at a time, likely due to the prohibitive computational cost. For exam- ple, Chowdhery et al. [86] conjecture PaLM might outperform GPT-3 and other LLMs on many tasks due to higher training corpus quality, but note they \u201cdo not perform the necessary ablation studies to say this conclusively\u201d and instead solely focus on model depth and width. Many papers from Table 2 adopt hyper-parameters from previous works [ 476] and do not tune them after introducing a change in the training pipeline. Sometimes, important im- plementation details are not mentioned, e.g., when optimizer states are reset during training [90]. Uncontrolled Experiments Papers presenting novel LLMs often lack controlled experiments, likely due to the prohibitive costs of training enough models. An easy yet expensive fix is to run ablations by varying one factor at a time, e.g., keeping most hyper-parameters fixed except the model size [ 44] or context lengths [ 557]. A cheaper po- tential remedy can be zero-shot hyper-parameter transfer from smaller models to larger ones [ 608, 633]. Yang et al. [633] find that when using the \u00b5P network parameterization scheme, one can transfer the effect of changing hyper-parameters such as the learning rate across varying model depths, batch sizes, sequence lengths, and training times, which they verify empirically up to a 6.7B model. How- ever, it has yet to be verified if such transferability still holds for other varying factors; and if so, re- searchers could afford to conduct more ablation experiments via smaller models. An easy yet expensive fix is to run ablations by varying one factor at a time, e.g., keeping most hyper-parameters fixed except the model size [ 44] or context lengths [ 557]. A cheaper po- tential remedy can be zero-shot hyper-parameter transfer from smaller models to larger ones [ 608, 633]. Yang et al. [633] find that when using the \u00b5P network parameterization scheme, one can transfer the effect of changing hyper-parameters such as the learning rate across varying model depths, batch sizes, sequence lengths, and training times, which they verify empirically up to a 6.7B model. How- ever, it has yet to be verified if such transferability still holds for other varying factors; and if so, re- searchers could afford to conduct more ablation experiments via smaller models. If additional experiments are prohibitively ex- pensive, another recommendation is to report eval- uation results beyond aggregated performance mea- sures. For example, in reinforcement learning, re- cent work has argued that providing entire perfor- 31 Table 2: Overview of selected LLMs .Missing details denoted by N/A . For papers that investigate various model sizes, we only report the largest. For each tokenizer entry with \u201cSP\u201d, we could not extract from the respective paper whether BPE or Unigram tokenization was used. For publicly available code repositories and checkpoints, the corresponding \u2713is clickable. Abbreviations: Autoregressive blank filling (ARBF) [ 132], Byte-pair encoding (BPE), Instruction-following (IF), Masked Language Modeling (MLM), Rotary Next token prediction (NTP), SentencePiece (SP), Span Corruption (SC). Date Name Organization Language # Parameters # Tokens Architecture Train. Obj. Tokenizer Pos. Embed. IF MoE Code avail. Ckpt. avail. Pre-trained 2018.11 GPipe [226] Google Multil. 6B N/A Enc. & Dec. NTP BPE Learned \u2717 \u2717 \u2713 \u2717 \u2717 2019.09 Megatron-LM [501] Microsoft Eng. 8.3B 157B Dec.-Only NTP BPE Learned \u2717 \u2717 \u2713 \u2717 \u2717 2019.10 T5 [443] Google Multil. 11B 1T Enc. & Dec. SC SP T5 \u2717 \u2717 \u2713 \u2713 \u2717 2020.05 GPT-3 [59] OpenAI Eng. 175B 300B Dec.-Only NTP BPE Learned \u2717 \u2717 \u2717 \u2717 \u2717 2020.06 GShard [298] Google Multil. 600B 1T Enc. & Dec. NTP SP N/A \u2717 \u2713 \u2717 \u2717 \u2717 2020.10 mT5 [631] Google Multil. 13B 1T Enc. & Dec. SC SP T5 \u2717 \u2717 \u2713 \u2713 \u2717 2021.01 Switch [145] Google Multil. 1.5T N/A Enc. & Dec. SC SP T5 \u2717 \u2713 \u2713 \u2713 \u2717 2021.03 BASE [302] Meta Eng. 117B N/A Enc. & Dec. NTP BPE Sinus. \u2717 \u2713 \u2713 \u2717 \u2717 2021.04 PanGu- \u03b1[659] Huawei Multil. 200B 317B Dec.-Only NTP BPE Learned \u2717 \u2717 \u2717 \u2717 \u2717 # Tokens Architecture Train. Obj. Tokenizer Pos. Embed. IF MoE Code avail. Ckpt. avail. Pre-trained 2018.11 GPipe [226] Google Multil. 6B N/A Enc. & Dec. NTP BPE Learned \u2717 \u2717 \u2713 \u2717 \u2717 2019.09 Megatron-LM [501] Microsoft Eng. 8.3B 157B Dec.-Only NTP BPE Learned \u2717 \u2717 \u2713 \u2717 \u2717 2019.10 T5 [443] Google Multil. 11B 1T Enc. & Dec. SC SP T5 \u2717 \u2717 \u2713 \u2713 \u2717 2020.05 GPT-3 [59] OpenAI Eng. 175B 300B Dec.-Only NTP BPE Learned \u2717 \u2717 \u2717 \u2717 \u2717 2020.06 GShard [298] Google Multil. 600B 1T Enc. & Dec. NTP SP N/A \u2717 \u2713 \u2717 \u2717 \u2717 2020.10 mT5 [631] Google Multil. 13B 1T Enc. & Dec. SC SP T5 \u2717 \u2717 \u2713 \u2713 \u2717 2021.01 Switch [145] Google Multil. 1.5T N/A Enc. & Dec. SC SP T5 \u2717 \u2713 \u2713 \u2713 \u2717 2021.03 BASE [302] Meta Eng. 117B N/A Enc. & Dec. NTP BPE Sinus. \u2717 \u2713 \u2713 \u2717 \u2717 2021.04 PanGu- \u03b1[659] Huawei Multil. 200B 317B Dec.-Only NTP BPE Learned \u2717 \u2717 \u2717 \u2717 \u2717 2021.05 ByT5 [630] Google Multil. 12.9B 1T Enc. & Dec. SC N/A T5\u2717 \u2717 \u2713 \u2713 \u2717 2021.06 CPM-2 [669] Tsinghua Uni. Multil. 198B N/A Enc. & Dec. SC Custom Sinus. \u2717 \u2713 \u2713 \u2713 \u2717 2021.06 nmT5 [255] Google Multil. 3.7B 100B Enc. & Dec. MLM, NTP SP T5 \u2717 \u2717 \u2717 \u2717 \u2713 2021.07 ERNIE 3.0 [530] Baidu Chin. 10B 375B Enc. & Dec. Custom BPE Rel. \u2717 \u2717 \u2717 \u2717 \u2717 2021.08 Jurassic-1 [319] AI21 Eng. 178B 300B Enc. & Dec. NTP SP Learned \u2717 \u2717 \u2717 \u2717 \u2717 2021.08 ExT5 [23] Google Eng. 11B 1T Enc. & Dec. SC, Custom SP T5 \u2717 \u2717 \u2713 \u2717 \u2717 2022.01 FLAN-LaMDA [598] Google Eng. 137B 245M Dec.-Only NTP BPE T5 \u2717 \u2713 \u2717 \u2717 \u2713 2021.10 M6-10T [322] Alibaba Eng. 10T N/A Uni. Enc. & Dec. SC, NTP SP N/A \u2717 \u2717 \u2717 \u2717 \u2717 2021.10 Yuan [615] Inspur AI Chin. 245B 180B Dec.-Only NTP BPE N/A \u2717 \u2717 \u2717 \u2717 \u2717 2021.10 T0 [475] BigScience Eng. 11B 12B Enc. & Dec. SC, NTP SP T5 \u2717 \u2717 \u2713 \u2713 \u2713 2021.12 Gopher [441] DeepMind Eng. 280B 300B Dec.-Only NTP SP Rel. \u2717 \u2717 \u2717 \u2717 \u2717 2021.12 RETRO [52] DeepMind Eng. 7B 419B Enc. & Dec. NTP (Ret.) SP Rel. \u2717 \u2717 \u2717 \u2717 \u2717 2021.12 GLaM [130] Google Multil. 1.2T 600B Dec.-Only NTP SP Rel. \u2717 \u2713 \u2717 \u2717 \u2717 2021.12 WebGPT [379] OpenAI Eng. 175B N/A Dec.-Only NTP BPE Learned \u2717 \u2717 \u2717 \u2717 \u2713 2021.12 FairSeq [400] Meta Eng. 1.1T 300B Dec.-Only NTP BPE Sinus. \u2717 \u2713 \u2713 \u2713 \u2717 2021.12 XGLM [324] Meta Multil. 7.5B 500B Dec.-Only NTP Unigram Sinus. \u2717 \u2717 \u2713 \u2713 \u2717 2022.01 LaMDA [551] Google Eng. 137B 768B Dec.-Only NTP BPE T5 \u2717 \u2717 \u2717 \u2717 \u2717 2022.01 MT-NLG [515] Microsoft Eng. 530B 270B Dec.-Only NTP BPE Sinus. \u2717 \u2717 \u2717 \u2717 \u2717 2022.02 ST-MoE [687] Google Eng. 269B 1.5T Enc. & Dec. SC SP Sinus. \u2717 \u2713 \u2713 \u2717 \u2717 2022.03 InstructGPT [403] OpenAI Eng. 175B N/A Dec.-Only RLHF BPE",
    "26": "Learned \u2713 \u2717 \u2717 \u2717 \u2713 2022.03 GopherCite [362] DeepMind Eng. 280B N/A Dec.-Only RLHF BPE Rel. \u2713 \u2717 \u2717 \u2717 \u2713 2021.12 RETRO [52] DeepMind Eng. 7B 419B Enc. & Dec. NTP (Ret.) SP Rel. \u2717 \u2717 \u2717 \u2717 \u2717 2021.12 GLaM [130] Google Multil. 1.2T 600B Dec.-Only NTP SP Rel. \u2717 \u2713 \u2717 \u2717 \u2717 2021.12 WebGPT [379] OpenAI Eng. 175B N/A Dec.-Only NTP BPE Learned \u2717 \u2717 \u2717 \u2717 \u2713 2021.12 FairSeq [400] Meta Eng. 1.1T 300B Dec.-Only NTP BPE Sinus. \u2717 \u2713 \u2713 \u2713 \u2717 2021.12 XGLM [324] Meta Multil. 7.5B 500B Dec.-Only NTP Unigram Sinus. \u2717 \u2717 \u2713 \u2713 \u2717 2022.01 LaMDA [551] Google Eng. 137B 768B Dec.-Only NTP BPE T5 \u2717 \u2717 \u2717 \u2717 \u2717 2022.01 MT-NLG [515] Microsoft Eng. 530B 270B Dec.-Only NTP BPE Sinus. \u2717 \u2717 \u2717 \u2717 \u2717 2022.02 ST-MoE [687] Google Eng. 269B 1.5T Enc. & Dec. SC SP Sinus. \u2717 \u2713 \u2713 \u2717 \u2717 2022.03 InstructGPT [403] OpenAI Eng. 175B N/A Dec.-Only RLHF BPE Learned \u2713 \u2717 \u2717 \u2717 \u2713 2022.03 GopherCite [362] DeepMind Eng. 280B N/A Dec.-Only RLHF BPE Rel. \u2713 \u2717 \u2717 \u2717 \u2713 2022.03 sMLP [653] Meta Eng. 9.4B N/A Enc. & Dec. NTP BPE Sinus. \u2717 \u2713 \u2717 \u2717 \u2717 2022.03 Chinchilla [206] DeepMind Eng. 70B 1.4T Dec.-Only NTP SP Rel. \u2717 \u2717 \u2717 \u2717 \u2717 2022.04 PaLM [86] Google Multil. 540B 780B Dec.-Only NTP SP RoPE \u2717 \u2713 \u2717 \u2717 \u2717 2022.04 GPT-NeoX [47] EleutherAI Eng. 20B 472B Dec.-Only NTP BPE RoPE \u2717 \u2717 \u2713 \u2713 \u2717 2022.04 Tk-Instruct [589] AI2 Eng. 11B 1B Enc. & Dec. NTP SP T5 \u2713 \u2717 \u2713 \u2713 \u2717 2022.04 METRO-LM [33] Microsoft Eng. 5.4B 2T Enc.-Only METRO SP T5 \u2717 \u2717 \u2717 \u2717 \u2717 2022.04 mGPT [500] Sber Multi. 13B 440B Dec.-Only NTP BPE Learned \u2717 \u2717 \u2713 \u2713 \u2717 2022.05 OPT [666] Meta Eng. 175B 300B Dec.-Only NTP BPE Learned \u2717 \u2717 \u2713 \u2713 \u2717 2022.05 UL2 [545] Google Eng. 20B 1T Enc. & Dec. MoD Unigram T5 \u2717 \u2717 \u2717 \u2713 \u2717 2022.05 DeepStruct [578] UC Berkeley Eng. 10B N/A Enc. & Dec. Struc. BPE Sinus. \u2717 \u2717 \u2717 \u2717 \u2717 2022.07 Minerva [305] Google Eng. 540B 26B Dec.-Only NTP SP RoPE \u2717 \u2717 \u2717 \u2717 \u2717 2022.08 PEER [482] Meta Eng. 11B 5B Enc. & Dec. NTP SP T5 \u2717 \u2717 \u2717 \u2717 \u2713 2022.08 AlexaTM [517] Amazon Multil. 20B 1T Enc. & Dec. MoD, NTP SP Sinus. \u2717 \u2717 \u2717 \u2713 \u2713 2022.10 GLM-130B [658] Tsinghua Uni. Multil. 130B 400B Uni. Enc. & Dec. ARBF SP RoPE \u2717 \u2717 \u2713 \u2713 \u2717 2022.10 U-PaLM [547] Google Eng. 540B 1.3B Dec.-Only MoD SP RoPE \u2717 \u2713 \u2717 \u2717 \u2713 2022.10 FLAN-PaLM [93] Google Eng. 540B 1.4B Dec.-Only NTP SP RoPE \u2713 \u2713 \u2717 \u2717 \u2713 2022.11 BLOOM [479] BigScience Multil. 176B 366B Dec.-Only NTP BPE ALiBi \u2717 \u2717 \u2713 \u2713 \u2717 2022.11 Galactica [548] Meta Eng. 120B 450B Dec.-Only NTP BPE Learned \u2717 \u2717 \u2713 \u2713 \u2717 2022.11 Atlas [236] Meta Eng. 11B N/A Enc. & Dec. MLM BPE T5 \u2717 \u2717 \u2713 \u2713 \u2713 2022.11 BLOOMZ [377] BigScience Multil. 176B 13B Dec.-Only NTP BPE ALiBi \u2713 \u2717 \u2713 \u2713 \u2713 2022.11 mT0 [377] BigScience Multil. 13B 13B Enc. & Dec. NTP SP T5 \u2713 \u2717 \u2713 \u2713 \u2713 2022.08 PEER [482] Meta Eng. 11B 5B Enc. & Dec. NTP SP T5 \u2717 \u2717 \u2717 \u2717 \u2713 2022.08 AlexaTM [517] Amazon Multil. 20B 1T Enc. & Dec. MoD, NTP SP Sinus. \u2717 \u2717 \u2717 \u2713 \u2713 2022.10 GLM-130B [658] Tsinghua Uni. Multil. 130B 400B Uni. Enc. & Dec. ARBF SP RoPE \u2717 \u2717 \u2713 \u2713 \u2717 2022.10 U-PaLM [547] Google Eng. 540B 1.3B Dec.-Only MoD SP RoPE \u2717 \u2713 \u2717 \u2717 \u2713 2022.10 FLAN-PaLM [93] Google Eng. 540B 1.4B Dec.-Only NTP SP RoPE \u2713 \u2713 \u2717 \u2717 \u2713 2022.11 BLOOM [479] BigScience Multil. 176B 366B Dec.-Only NTP BPE ALiBi \u2717 \u2717 \u2713 \u2713 \u2717 2022.11 Galactica [548] Meta Eng. 120B 450B Dec.-Only NTP BPE Learned \u2717 \u2717 \u2713 \u2713 \u2717 2022.11 Atlas [236] Meta Eng. 11B N/A Enc. & Dec. MLM BPE T5 \u2717 \u2717 \u2713 \u2713 \u2713 2022.11 BLOOMZ [377] BigScience Multil. 176B 13B Dec.-Only NTP BPE ALiBi \u2713 \u2717 \u2713 \u2713 \u2713 2022.11 mT0 [377] BigScience Multil. 13B 13B Enc. & Dec. NTP SP T5 \u2713 \u2717 \u2713 \u2713 \u2713 2022.12 OPT-IML [235] Meta Eng. 175B 2B Dec.-Only NTP BPE Sinus. \u2713 \u2717 \u2713 \u2713 \u2713 2022.12 Med-PaLM [511] Google Eng. 540B 0B Dec.-Only NTP SP RoPE \u2717 \u2717 \u2717 \u2717 \u2713 2023.02 LLaMA{-I} [556] Meta Eng. 65B 1.4T Dec.-Only NTP BPE RoPE \u2713 \u2717 \u2713 \u2713 \u2717 2023.03 PanGu- \u03a3[455] Huawei Multil. 1T 329B Dec.-Only NTP BPE Learned \u2717 \u2713 \u2717 \u2717 \u2713 2023.03 CoLT5 [15] Google Eng. 5.3B 1T Enc. & Dec. MoD N/A T5\u2717 \u2717 \u2717 \u2717 \u2717 2023.03 BloombergGPT [616] Bloomberg Eng. 50B 569B Dec.-Only NTP Unigram ALiBi \u2717 \u2717 \u2717 \u2717 \u2717 2023.04 Cerebras-GPT [121] Cerebras Eng. 13B 257B Dec.-Only NTP BPE RoPE \u2717 \u2717 \u2717 \u2713 \u2717 2023.04 Pythia [44] EleutherAI Eng. 12B 300B Dec.-Only NTP BPE RoPE \u2717 \u2717 \u2713 \u2713 \u2717 2023.04 WizardLM [625] Microsoft Eng. 30B N/A Dec.-Only NTP BPE RoPE \u2713 \u2717 \u2713 \u2713 \u2713 2023.05 Guanaco [118] Univ. of Washington Multil. 65B 82M Dec.-Only NTP BPE RoPE \u2713 \u2717 \u2717 \u2713 \u2713 2023.04 RWKV [417] RWKV Eng. 14B N/A Dec.-Only NTP BPE RoPE \u2713 \u2717 \u2713 \u2713 \u2713 2023.06 Orca [378] Microsoft Eng. 13B N/A Dec.-Only NTP BPE RoPE \u2713 \u2717 \u2717 \u2717 \u2713 2023.07 LLaMA 2 [557] Meta Eng. 70B 2T Dec.-Only NTP BPE RoPE \u2713 \u2717 \u2713 \u2713 \u2713 32 mance distributions across all runs is less biased and more robust to outliers than point estimates [ 9]. Curse of Dimensionality In Table 2, we high- light some but not all differences across models, as the table format constrained us. Other com- mon differences include the training datasets or fine-grained architectural details, e.g., the usage of multi-head [563] or multi-query attention [494]. We note that a core characteristic of LLMs is their vast design space, which renders scientific inquiry challenging [ 231]. For example, by taking into account the (i) data sources and their propor- tions within the pre-training dataset, (ii) choice 2023.07 LLaMA 2 [557] Meta Eng. 70B 2T Dec.-Only NTP BPE RoPE \u2713 \u2717 \u2713 \u2713 \u2713 32 mance distributions across all runs is less biased and more robust to outliers than point estimates [ 9]. Curse of Dimensionality In Table 2, we high- light some but not all differences across models, as the table format constrained us. Other com- mon differences include the training datasets or fine-grained architectural details, e.g., the usage of multi-head [563] or multi-query attention [494]. We note that a core characteristic of LLMs is their vast design space, which renders scientific inquiry challenging [ 231]. For example, by taking into account the (i) data sources and their propor- tions within the pre-training dataset, (ii) choice and training hyper-parameters of the tokenizer, and (iii) pre-training objective, the combined design space quickly becomes high-dimensional. Under- taking factorial experiments within such expansive design spaces results in a combinatorially-growing number of single training runs, and the lack of suf- ficient experimental coverage can severely inhibit scientific understanding of what makes an LLM perform well. While this issue is not unique to LLMs, they tend to be larger in the number of parameters\u2014and therefore compute requirements, feedback loop times, and training costs\u2014than mod- els in most other fields. Curse of (Design) Dimensionality Common design spaces of LLM experi- ments are high-dimensional. One possible way forward is to encourage the community to use techniques like Bayesian opti- mization (BO) with dimensionality reduction [ 594, 374], where we use a non-linear feature mapping to map the input (the hyper-parameter configuration) onto a lower dimensional manifold followed by a BO procedure to optimize the underlying black- box function (the LLM with respect to the hyper- parameters). Another suitable tool to explore the design space efficiently can be treatment effect es- timation [ 284,385], e.g., where the treatment is a vector describing certain ablations [254]. 2.16 Lack of Reproducibility The reproducibility of empirical results is impor- tant to verify scientific claims and rule out errors in experimental protocols leading to such. When researchers try to build upon non-reproducible re- sults, they might waste resources.Unfortunately, we stumble upon two unique re- producibility issues in LLM research: repeatability of (i) training runs and (ii) generations by close- sourced API-served models. While the term \u201cre- BO procedure to optimize the underlying black- box function (the LLM with respect to the hyper- parameters). Another suitable tool to explore the design space efficiently can be treatment effect es- timation [ 284,385], e.g., where the treatment is a vector describing certain ablations [254]. 2.16 Lack of Reproducibility The reproducibility of empirical results is impor- tant to verify scientific claims and rule out errors in experimental protocols leading to such. When researchers try to build upon",
    "27": "non-reproducible re- sults, they might waste resources.Unfortunately, we stumble upon two unique re- producibility issues in LLM research: repeatability of (i) training runs and (ii) generations by close- sourced API-served models. While the term \u201cre- producibility\u201d is often used more broadly and can slightly vary in its meaning [ 5], in the following, we focus on \u201crepeatability\u201d, which we define as the ability to repeat experimental outcomes exactly. Training Repeatability Typical training proto- cols of LLMs involve parallelism across multi- ple compute nodes. The scheduling and com- munication strategies between nodes can be non- deterministic [ 387]. This variability can affect the final result, especially in algorithms that are not \u201corder-invariant\u201d, such as stochastic gradient descent (SGD). Some sources of randomness are (i) lock-free parallelism schemes [ 387], (ii) float- ing point precision, e.g., when summing gradients across devices, the order in which these sums are computed can affect the final result [ 171], (iii) non- deterministic, performance-optimized operations, which are much faster and therefore desirable [3]. Further, Carlini et al. [64] point out that some pre-training datasets consist of an index of web content that individual users must crawl themselves, rather than using static, standalone dumps. This is due to monetary, privacy, and legal restrictions. As a result, reproducibility can be easily compromised if any of the sources in the index have changed between the time the dataset curator collected them and the time the end-user downloads them. Irrepeatable Training Runs Parallelism strategies designed to distribute the training process across many accelera- tors are typically non-deterministic, render- ing LLM training irreproducible. Inference Repeatability Another peculiarity of commercial LLMs is that they are typically served via stochastic API in a black-box setting, which comes with the following challenges: (i) the pre-training datasets consist of an index of web content that individual users must crawl themselves, rather than using static, standalone dumps. This is due to monetary, privacy, and legal restrictions. As a result, reproducibility can be easily compromised if any of the sources in the index have changed between the time the dataset curator collected them and the time the end-user downloads them. Irrepeatable Training Runs Parallelism strategies designed to distribute the training process across many accelera- tors are typically non-deterministic, render- ing LLM training irreproducible. Inference Repeatability Another peculiarity of commercial LLMs is that they are typically served via stochastic API in a black-box setting, which comes with the following challenges: (i) the provider retains complete authority over the model and can introduce unpublicized changes, includ- ing retraining the model, modifying its parame- ters, or completely replacing it; (ii) even if model updates are communicated, there is still uncer- tainty about whether access to specific model ver- sions will be maintained once they are deemed outdated, (iii) even with a decoding temperature 33 set to zero, API models often produce stochastic outputs [392, 464, 456]. Chen et al. [76] provide preliminary evidence confirming dramatic changes in API-served models. They find that GPT-3.5 and GPT-4 performances on four diverse tasks vary vastly within three months (March to June 2023). For example, GPT-4\u2019s ac- curacy in identifying prime numbers was 97.6%, but in June, its accuracy dropped to 2.4%; while for GPT-3.5, the trend is reversed and it got much better over time. Irreproducible API Inference API-served models are often irreproducible. An easy fix is to rely exclusively on open-source LLMs [2]. 3 Applications In this section, we aim to provide practitioners with a broad overview of the areas in which LLMs are currently being applied and highlight some com- mon application architectures across domains. Analogous to the Challenges section, we high- light the key constraints in each application area as follows. Constraint This box highlights a constraint. 3.1 Chatbots General-purpose chatbots (dialogue agents) com- bine the tasks of information retrieval, multi-turn interaction, and text generation (including code). Thoppilan et al. [551] introduced the LaMDA family of chatbot LLMs with up to 137B parame- ters, focusing on safety (via supervised fine-tuning API-served models are often irreproducible. An easy fix is to rely exclusively on open-source LLMs [2]. 3 Applications In this section, we aim to provide practitioners with a broad overview of the areas in which LLMs are currently being applied and highlight some com- mon application architectures across domains. Analogous to the Challenges section, we high- light the key constraints in each application area as follows. Constraint This box highlights a constraint. 3.1 Chatbots General-purpose chatbots (dialogue agents) com- bine the tasks of information retrieval, multi-turn interaction, and text generation (including code). Thoppilan et al. [551] introduced the LaMDA family of chatbot LLMs with up to 137B parame- ters, focusing on safety (via supervised fine-tuning on human annotations) and factual grounding (via access to external knowledge sources). Notably, smaller LaMDA models (2B parameters) with fine- tuning are shown to perform similarly on dialogue quality and safety/grounding scores to the larger LaMDA models (137B parameters) without fine- tuning. LaMDA models were released as part of the Bard chatbot service [ 429]. However, the latest ver- sion of Bard now uses the PaLM 2 LLM [ 20,216]. Glaese et al. [170] propose Sparrow, a chatbot based on a 70B parameter Chinchilla LLM, and use RLHF (Sec. 2.9) targeting 23 rules to fine-tunethe model to be more helpful ,correct , and harm- less. Sparrow also incorporates external knowledge using a retrieval model to provide evidence from a Google Search query. The RLHF approach outper- forms the only dialogue-prompted and supervised fine-tuned approaches regarding output preference and rule violation rate. Similarly, OpenAI [396] train the ChatGPT chatbot using supervised fine-tuning and RLHF (Sec. 2.9) to specialize a GPT-3.5 LLM for dia- logue. GPT-4 [ 398] is the underlying model for the ChatGPT Plus chatbot, but training and architec- ture details have not been released. Shuster et al. [508] introduce BlenderBot-3, a 175B parameter chatbot based on the OPT-175 LLM using supervised fine-tuning. BlenderBot- 3 incorporates external knowledge through mod- ules that conduct internet searches and retrieve text- based long-term memories generated from previous outputs to help performance over long interactions. Maintaining Coherence Multi-turn interactions make Chatbots eas- ily \u201cforget\u201d earlier parts of the conversation or repeat themselves [53, 451]. K\u00f6pf et al. [274] release the OpenAssistant Con- chatbot using supervised fine-tuning and RLHF (Sec. 2.9) to specialize a GPT-3.5 LLM for dia- logue. GPT-4 [ 398] is the underlying model for the ChatGPT Plus chatbot, but training and architec- ture details have not been released. Shuster et al. [508] introduce BlenderBot-3, a 175B parameter chatbot based on the OPT-175 LLM using supervised fine-tuning. BlenderBot- 3 incorporates external knowledge through mod- ules that conduct internet searches and retrieve text- based long-term memories generated from previous outputs to help performance over long interactions. Maintaining Coherence Multi-turn interactions make Chatbots eas- ily \u201cforget\u201d earlier parts of the conversation or repeat themselves [53, 451]. K\u00f6pf et al. [274] release the OpenAssistant Con- versations dataset of human-annotated interactions and use this to instruction fine-tune Pythia and LLaMA models (up to 30B parameters) for chat- bot applications. To help align the final models, the dataset is generated with guidelines to make the responses polite ,helpful ,concise",
    "28": ",friendly , and safety-aware . The LLaMA 30B version is cur- rently used within the HuggingChat chatbot ap- plication [229]. A key challenge of fine-tuning chatbots is cre- ating a broad training dataset of high-quality con- versations. To address this problem Chen et al. [78] demonstrate using existing LLMs (OPT 30B) to generate high-quality synthetic conversation datasets based on a small number of expert-written examples. Human crowd workers assessed the gen- erated conversations to be comparable to existing human-generated datasets on the metrics: interest- ing,coherent ,natural , and consistent . Chen et al. [78] show the synthetic dataset can be used to fine- tune a chatbot (BlenderBot 400M) and achieve performance only slightly below fine-tuning with human-generated datasets. Chatbots\u2019 intended generality also makes eval- 34 ApplicationsChatbots 3.1BlenderBot3 (OPT-175) [508], Bard (LaMDA, PaLM2) [551], Sparrow (Chinchilla) [170], ChatGPT (GPT-3.5, GPT-4) [396], OpenAssistant (LLaMA) [274] GPT-4 Technical Report [398], Sparks of AGI (GPT-4) [61], Capabilities of ChatGPT [272] Computational Biology 3.2ProteinsESM-2 [326], ProtT5 [139], ProtST [627], CaLM [402], ProGen [352], IgLM [505], xTrimoPGLM [73] Genomics GenSLM [688], Nucleotide Transformers [106] Computer Programming 3.3InCoder [154], CodeGen [386], AlphaCode [313] , SantaCoder [17], Polycoder [626], phi-1 [182] Codex (GPT-3) [77] [78] show the synthetic dataset can be used to fine- tune a chatbot (BlenderBot 400M) and achieve performance only slightly below fine-tuning with human-generated datasets. Chatbots\u2019 intended generality also makes eval- 34 ApplicationsChatbots 3.1BlenderBot3 (OPT-175) [508], Bard (LaMDA, PaLM2) [551], Sparrow (Chinchilla) [170], ChatGPT (GPT-3.5, GPT-4) [396], OpenAssistant (LLaMA) [274] GPT-4 Technical Report [398], Sparks of AGI (GPT-4) [61], Capabilities of ChatGPT [272] Computational Biology 3.2ProteinsESM-2 [326], ProtT5 [139], ProtST [627], CaLM [402], ProGen [352], IgLM [505], xTrimoPGLM [73] Genomics GenSLM [688], Nucleotide Transformers [106] Computer Programming 3.3InCoder [154], CodeGen [386], AlphaCode [313] , SantaCoder [17], Polycoder [626], phi-1 [182] Codex (GPT-3) [77] Self-Debugging (Codex) [81], ViperGPT (Codex) [532], RepoCoder [660], Repo-Level Prompt Generator [504] Creative Work 3.4Long FormDramatron (Chinchilla) [368], Re3 (GPT-3) [637], Detailed Outline Control (GPT-3) [636] Short FormCoPoet (T5, T0) [69], Spindle - Interactive Fiction (GPT-3) [63] Cross-lingual Short Stories (PaLM) [452], ReelFramer (GPT-4) [584] Idea Generation [187] Visual LayoutGPT [148], LLM Grounded Diffusion [315] Knowledge Work 3.5Galactica [548], BloombergGPT [616] Scientific NERRE (GPT-3) [133] Data Analysis (GPT-4) [346] Professional Exams [49], News Summarization [668], Email Management [550], Academic Paper Review (GPT-4) [335] Law 3.6Legal Question AnsweringLegal Entailment (GPT-3.5) [651], Bar Examination (GPT-3.5) [50] Explaining Legal Concepts (GPT-4 + Retrieval) [478] Law School (ChatGPT) [84], Bar Examination (GPT-4) [258] Statutory Reasoning (GPT-3.5) [48], Law Professor (ChatGPT) [427], Summarizing Judgments (GPT-3.5) [115], Litigation (ChatGPT) [234] Case Prediction US Supreme Court (GPT-2 + GPT-3) [189] Medicine 3.7Medical Question AnsweringPubMedGPT [565], GatorTronGPT [418] MedPaLM(2) (PaLM) [511, 512], ChatDoctor (LLaMA) [655] GPT-3.5 + Retrieval [320] Medical Challenge Problems (GPT-4) [388], Triage and Diagnosis (GPT-3) [301], Surgical Knowledge QA (GPT-4) [393], Social Media - Genetics Questions (ChatGPT) [134], Social Media - General Questions (ChatGPT) [30], Ophthalmology QA (ChatGPT) [21], Medical Summarization (GPT-3.5, ChatGPT) [538] Medical Information RetrievalMedical Acronym Disambiguation (T5) [448], Adverse Drug Event Extraction [178] Clinical Information Extraction (InstructGPT) [10] Statutory Reasoning (GPT-3.5) [48], Law Professor (ChatGPT) [427], Summarizing Judgments (GPT-3.5) [115], Litigation (ChatGPT) [234] Case Prediction US Supreme Court (GPT-2 + GPT-3) [189] Medicine 3.7Medical Question AnsweringPubMedGPT [565], GatorTronGPT [418] MedPaLM(2) (PaLM) [511, 512], ChatDoctor (LLaMA) [655] GPT-3.5 + Retrieval [320] Medical Challenge Problems (GPT-4) [388], Triage and Diagnosis (GPT-3) [301], Surgical Knowledge QA (GPT-4) [393], Social Media - Genetics Questions (ChatGPT) [134], Social Media - General Questions (ChatGPT) [30], Ophthalmology QA (ChatGPT) [21], Medical Summarization (GPT-3.5, ChatGPT) [538] Medical Information RetrievalMedical Acronym Disambiguation (T5) [448], Adverse Drug Event Extraction [178] Clinical Information Extraction (InstructGPT) [10] Reasoning 3.8Self Improvement (PaLM) [222], Processed Based Fine-Tuning [560] DIVERSE (GPT-3.5) [312], Socratic Sub-Questions (GPT-3) [502], Mathematical Formalization (Codex) [159] Causal Factors in Performance [525], Analogical Reasoning [595], Causal Reasoning [286, 164, 519, 244, 288], Common-Sense Reasoning [562] Robotics 3.9PaLM-E [129] SayCan (PaLM + Scoring) [14], ChatGPT for Robotics [564], REFLECT (GPT-4) [338], Code as Policies (Codex) [316], PROGPROMPT (Codex) [510], Inner Monologue [225], Statler (GPT-3.5) [647] Social Sciences 3.10Using LLMs to Model Human Behavior [12, 176], Analyzing Behavioral Characteristics of LLMs [367, 414], Simulating Social Relationships with LLMs [408] Synthetic Training Data 3.11Automated Labeling (GPT-3) [583], AugGPT (ChatGPT) [104], Labeling + Generation (GPT-3) [123], Information Retrieval (GPT-3) [51], Decompositional Distillation (GPT-3) [503], Code \u2018Textbooks\u2019 (GPT-3.5) [182], GPT3Mix [648] Figure 12: Overview of LLM Applications. Color = Level of Model Adaption (Pre-Trained, Fine-Tuned, Prompting Strategy, Evaluation). 35 uating their capabilities\u2019 full range difficult. Ko- co\u00b4n et al. [272] evaluate ChatGPT (GPT-3.5) on 25 tasks with 38k prompts covering a diverse set of capabilities, including but not limited to ques- tion answering, emotion recognition, offensive lan- guage detection, spam detection, inference, and sentiment analysis. While ChatGPT is shown to have strong performance across the 25 tasks, it usu- ally underperforms the SOTA in that domain. More recently, Bubeck et al. [61] and OpenAI [398] in- vestigate the capabilities of GPT-4 (base model of ChatGPT Plus) across a wide range of tasks, in- Code \u2018Textbooks\u2019 (GPT-3.5) [182], GPT3Mix [648] Figure 12: Overview of LLM Applications. Color = Level of Model Adaption (Pre-Trained, Fine-Tuned, Prompting Strategy, Evaluation). 35 uating their capabilities\u2019 full range difficult. Ko- co\u00b4n et al. [272] evaluate ChatGPT (GPT-3.5) on 25 tasks with 38k prompts covering a diverse set of capabilities, including but not limited to ques- tion answering, emotion recognition, offensive lan- guage detection, spam detection, inference, and sentiment analysis. While ChatGPT is shown to have strong performance across the 25 tasks, it usu- ally underperforms the SOTA in that domain. More recently, Bubeck et al. [61] and OpenAI [398] in- vestigate the capabilities of GPT-4 (base model of ChatGPT Plus) across a wide range of tasks, in- cluding interactions with humans and tools. Using these evaluations Bubeck et al. [61] conclude that GPT-4 is \u2018strikingly close to human-level perfor- mance\u2019 across tasks. Finally, the challenge of inference latency (Sec. 2.5) is also potentially going to become an important constraint [ 634] for chatbot applications as LLMs scale. There is a trade-off between the need for responsive live user interaction in a con- versational format and utilizing larger LLMs [ 397]. High Inference Latency High inference latency (Sec. 2.5) hinders the user experience [ 397], especially in multi- turn interaction with chatbots. 3.2 Computational Biology In computational biology, we are interested in non- text data representing similar sequence modeling and prediction challenges. 3.2.1 Protein Embeddings One popular application of LLM-like models in biology is to",
    "29": "generate protein embeddings from amino-acid or genomic sequence inputs. These em- beddings can then be used as inputs for structure prediction, novel sequence generation, and protein classification tasks. Protein language models per- form strongly on many academic datasets, but their applicability to downstream tasks such as drug de- sign is often unclear [110].Transfer to Downstream Applications The ultimate objective of protein language models is to deploy them in real-world projects such as drug design. Evalua- tions often target smaller and/or specialized datasets, not considering how the models could contribute to protein design in vitro or in vivo. Elnaggar et al. [139] train a range of LLM archi- tectures to extract embeddings from protein amino acid sequences. These embeddings are then used as inputs on supervised per-amino acid and per- beddings can then be used as inputs for structure prediction, novel sequence generation, and protein classification tasks. Protein language models per- form strongly on many academic datasets, but their applicability to downstream tasks such as drug de- sign is often unclear [110].Transfer to Downstream Applications The ultimate objective of protein language models is to deploy them in real-world projects such as drug design. Evalua- tions often target smaller and/or specialized datasets, not considering how the models could contribute to protein design in vitro or in vivo. Elnaggar et al. [139] train a range of LLM archi- tectures to extract embeddings from protein amino acid sequences. These embeddings are then used as inputs on supervised per-amino acid and per- protein prediction tasks. The best-performing LLM architecture (ProtT5) achieved SOTA results on per-amino acid protein secondary structure predic- tion without using evolutionary information. Sim- ilarly, Wu et al. [613] predict antibody backbone and side-chain conformations. Lin et al. [326] take a similar approach to train- ing a protein LLM, the Evolutionary Scale Model Transformer-2 (ESM-2), on protein amino acid se- quences from the UniRef database using a masked language modeling approach. They show sig- nificant performance increases as the model is scaled from 8 million to 15B parameters, with the largest models outperforming the ProtT5 on protein structure prediction benchmarks (CASP14, CAMEO) [ 267,457]. They also introduce ESM- Fold, which uses the ESM-2 embedding model for end-to-end atomic resolution prediction from a single sequence. While ESMFold underperforms the SOTA AlphaFold2 [ 248] on the CAMEO and CASP14 benchmarks, the authors note that by rely- ing only on embeddings ESMFold has an order of magnitude faster inference time than AlphaFold2, using just the protein sequence of interest rather than structural templates and multiple sequence alignments (MSAs). Jeliazkov et al. [240] find that protein sequences designed by an inverted Al- phaFold2 model are unlikely to be expressed, but sequences generated using an inverted protein LLM such as ESMFold were more likely to be expressed. Researchers have also adopted the ESM-1 and ESM-2 models to generate protein embeddings for enzyme-substrate chemical structural class pre- diction [ 245], training 3D geometric graph neural networks for proteins [ 611], identifying disease- causing mutations [ 337], designing novel pro- CASP14 benchmarks, the authors note that by rely- ing only on embeddings ESMFold has an order of magnitude faster inference time than AlphaFold2, using just the protein sequence of interest rather than structural templates and multiple sequence alignments (MSAs). Jeliazkov et al. [240] find that protein sequences designed by an inverted Al- phaFold2 model are unlikely to be expressed, but sequences generated using an inverted protein LLM such as ESMFold were more likely to be expressed. Researchers have also adopted the ESM-1 and ESM-2 models to generate protein embeddings for enzyme-substrate chemical structural class pre- diction [ 245], training 3D geometric graph neural networks for proteins [ 611], identifying disease- causing mutations [ 337], designing novel pro- teins [ 566], and guided evolution of antibodies for affinity maturation [202]. 36 Chen et al. [73] propose training a new model xTrimoPGLM (100B parameters) simul- taneously for protein embedding and genera- tion tasks using MLM and generative objectives. The xTrimoPGLM-100B model (with fine-tuning where relevant) outperforms existing approaches on 13 out of 15 evaluated tasks. Protein embedding models with alternative in- puts have also been proposed. Outeiral and Deane [402] train an 86 million parameter protein LLM CaLM (Codon adaptation Language Model) us- ing sequences of codons (nucleotide triads) as in- put instead of amino acids due to codons contain- ing potentially richer information. Madani et al. [352] train a 1.2B parameter protein embedding model ProGen on 280 million protein amino acid sequences with additional control tags specifying protein properties. ProGen is then fine-tuned us- ing data from specific protein families and applied to generate functional full-length amino acid se- quences. Similarly, Xu et al. [627] propose train- ing a protein language model, the ProtST, on pro- tein sequences and additional text descriptions of their key properties for protein classification and retrieval tasks. Finally, for antibodies specifically, Shuai et al. [505] propose an Immunoglobulin Language Model (IgLM) using the GPT-2 architecture (with 13 million parameters) for the generation of im- munoglobulin sequences, using a masked language modeling approach. Similar to Xu et al. [627] , the IgLM model also takes additional conditioning tags corresponding to chain type and species as input. The authors show the IgLM model can then be ing data from specific protein families and applied to generate functional full-length amino acid se- quences. Similarly, Xu et al. [627] propose train- ing a protein language model, the ProtST, on pro- tein sequences and additional text descriptions of their key properties for protein classification and retrieval tasks. Finally, for antibodies specifically, Shuai et al. [505] propose an Immunoglobulin Language Model (IgLM) using the GPT-2 architecture (with 13 million parameters) for the generation of im- munoglobulin sequences, using a masked language modeling approach. Similar to Xu et al. [627] , the IgLM model also takes additional conditioning tags corresponding to chain type and species as input. The authors show the IgLM model can then be used for the controllable generation of infilled and full-length antibody sequences. 3.2.2 Genomic Analysis LLMs in the field of genomic analysis enable a better understanding of the effects of mutations in humans and predict genomic features directly from DNA sequences. While genomic language models are a promising research direction, current models cannot process many genomic sequences as their sequence lengths commonly exceed multiple billions of nucleotides [390].Limited Context Window The largest genomes have vastly longer DNA sequences [ 390] than existing ge- nomic LLMs\u2019 context windows can han- dle, constraining the types of genomes that can be successfully modeled using these ap- proaches. Zvyagin et al. [688] introduce a range of hier- archical LLMs (up to 25B parameters) with long input sequences (2048 - 10,240 tokens), referred to as Genome-scale Language Models (GenSLMs). The GenSLM models are pre-trained on Prokary- otic gene sequences from the BV-BRC dataset us- ing codon tokenization [ 402] and then fine-tuned on SARS-CoV-2 genome sequences for the task of identifying potential new variants and genera- tive modeling. However, the authors note that it remains unclear whether the GenSLM architecture generates richer representations than the protein LLM approaches. Dalla-Torre et al. [106] train Nucleotide Trans- formers with 500 million to 2.5B parameters on nu- cleotide sequences from human and other",
    "30": "species genomes, using a masked language modeling ap- proach. The Nucleotide Transformers were evalu- ated on 18 genomic prediction tasks with fine-tuned larger models achieving the best results. Nguyen et al. [383] propose HyenaDNA, a ge- nomic language model based on the Hyena archi- otic gene sequences from the BV-BRC dataset us- ing codon tokenization [ 402] and then fine-tuned on SARS-CoV-2 genome sequences for the task of identifying potential new variants and genera- tive modeling. However, the authors note that it remains unclear whether the GenSLM architecture generates richer representations than the protein LLM approaches. Dalla-Torre et al. [106] train Nucleotide Trans- formers with 500 million to 2.5B parameters on nu- cleotide sequences from human and other species genomes, using a masked language modeling ap- proach. The Nucleotide Transformers were evalu- ated on 18 genomic prediction tasks with fine-tuned larger models achieving the best results. Nguyen et al. [383] propose HyenaDNA, a ge- nomic language model based on the Hyena archi- tecture [ 430], enabling modeling of genomic se- quences of up to 1 million tokens. HyenaDNA outperforms Transformer-based models with mul- tiple orders of magnitude more parameters while incorporating the in-context learning capabilities of LLMs into the genomics domain. 3.3 Computer Programming One of LLMs\u2019 most advanced and broadly adopted applications is generating and completing computer programs in various programming languages. This section deals with programming-specific LLMs where the model is fine-tuned or pre-trained ex- clusively for programming applications, but it is important to note the increasing use of general chatbots partially trained on code datasets (such as ChatGPT) for programming tasks. 3.3.1 Code Generation Code generation refers to using an LLM to output new code for a given specification or problem pro- 37 vided as a prompt. Several computer programming- specific LLMs and approaches have been proposed. For Python code generation, Chen et al. [77] introduce Codex, a fine-tuned GPT-3 LLM (up to 12B parameters) specialized to generate stand- alone Python functions from doc strings. Fine- tuning was conducted using a raw dataset of 159 GB of Python source code from GitHub and a fil- tered dataset of correctly implemented standalone Python functions. Codex models outperformed similarly sized GPT-3 and GPT-J models on the HumanEval evaluation set, with the Codex model trained on the filtered dataset (Codex-S) achieving the best results. Importantly, Chen et al. [77] note that there was no observed improvement from us- ing a pre-trained GPT-3 model as a base other than faster convergence. Chen et al. [81] seek to improve the performance specific LLMs and approaches have been proposed. For Python code generation, Chen et al. [77] introduce Codex, a fine-tuned GPT-3 LLM (up to 12B parameters) specialized to generate stand- alone Python functions from doc strings. Fine- tuning was conducted using a raw dataset of 159 GB of Python source code from GitHub and a fil- tered dataset of correctly implemented standalone Python functions. Codex models outperformed similarly sized GPT-3 and GPT-J models on the HumanEval evaluation set, with the Codex model trained on the filtered dataset (Codex-S) achieving the best results. Importantly, Chen et al. [77] note that there was no observed improvement from us- ing a pre-trained GPT-3 model as a base other than faster convergence. Chen et al. [81] seek to improve the performance of Codex through a self-debugging prompting ap- proach. Three forms of self-debugging are inves- tigated. Simple feedback prompts the model to decide whether the generated code solution is cor- rect. Unit-test feedback prompts the model with the output of unit tests provided in the problem description. Code explanation feedback prompts the model to explain the solution in detail and use the explanation to correct the solution. In each case, this process is repeated iteratively until the model provides a solution it states is correct or a maximum number of attempts has been made. Codex using the self-debugging prompting frame- work with code explanation (and unit-testing if applicable) outperforms the base Codex model on C++-to-Python translation, text-to-SQL generation, and text-to-Python generation. Gunasekar et al. [182] train a smaller model Phi- 1 (1.3B parameters) to generate Python functions from doc strings. Training phi-1 using a combina- tion of filtered existing datasets and new synthetic textbook andexercise datasets results in a model that can achieve near current SOTA results on Hu- manEval while having over an order of magnitude fewer parameters and tokens than previous works. Another area of interest has been the develop- ment of multilingual programming LLMs. Xu et al. [626] evaluate a range of code generation LLMs and train a new multilingual LLM Polycoder (2.7B parameters) using source code from 12 languages. However, for Python specifically, Codex outper- forms Polycoder and other existing models (GPT-J, GPT-Neo, and CodeParrot) on HumanEval.Nijkamp et al. [386] train the CodeGen family of LLMs (up to 16B parameters) using a combi- from doc strings. Training phi-1 using a combina- tion of filtered existing datasets and new synthetic textbook andexercise datasets results in a model that can achieve near current SOTA results on Hu- manEval while having over an order of magnitude fewer parameters and tokens than previous works. Another area of interest has been the develop- ment of multilingual programming LLMs. Xu et al. [626] evaluate a range of code generation LLMs and train a new multilingual LLM Polycoder (2.7B parameters) using source code from 12 languages. However, for Python specifically, Codex outper- forms Polycoder and other existing models (GPT-J, GPT-Neo, and CodeParrot) on HumanEval.Nijkamp et al. [386] train the CodeGen family of LLMs (up to 16B parameters) using a combi- nation of three datasets: natural language, multi- lingual programming source code (C, C++, Go, Java, JavaScript, and Python), and a monolingual Python dataset. The largest CodeGen model using the monolingual training set was shown to outper- form the Codex-12B model. Nijkamp et al. [386] also test CodeGen on multi-step program synthesis, where a program is broken down into multi-step natural language prompts, which the model then implements individually (creating the new Multi- Turn Programming Benchmark (MTPB)). Finally, Li et al. [313] focus on the task of solving competitive programming questions (Code- forces, Description2Code, and CodeNet). The Al- phaCode LLM (up to 41B parameters) is first pre- trained on a multilingual dataset (C++, C#, Go, Java, JavaScript, Lua, PHP, Python, Ruby, Rust, Scala, and TypeScript) of 715 GB of source code from GitHub. It is then fine-tuned using a new curated dataset of competitive programming prob- lems called CodeContests. To achieve high per- formance, Li et al. [313] use large-scale sampling (up to millions of samples), filtering, and clustering of candidate solutions generated by AlphaCode to select the final submissions. However, whilst these existing code-generation LLMs have achieved impressive results, a criti- cal current constraint in applying LLMs to code generation is the inability to fit the full code base and dependencies within the context window. To deal with this constraint, a few frameworks have been proposed to retrieve relevant information or abstract the relevant information into an API defi- nition. Long-Range Dependencies [ 660,504] Long-range dependencies across a code repository usually cannot be regarded be- curated dataset of competitive programming prob- lems called CodeContests. To achieve high per- formance, Li et al. [313] use large-scale sampling (up to millions of samples), filtering, and clustering of candidate solutions generated by AlphaCode to select the final submissions. However, whilst these",
    "31": "existing code-generation LLMs have achieved impressive results, a criti- cal current constraint in applying LLMs to code generation is the inability to fit the full code base and dependencies within the context window. To deal with this constraint, a few frameworks have been proposed to retrieve relevant information or abstract the relevant information into an API defi- nition. Long-Range Dependencies [ 660,504] Long-range dependencies across a code repository usually cannot be regarded be- cause of limited context lengths (Sec. 2.6). Zhang et al. [660] introduce RepoCoder, a retrieval-based framework for repository-level code completion that allows an LLM to consider the broader context of the repository. A multi-step retrieval-augmented generation approach is taken, where the initial code generated is then used to re- trieve further, potentially more relevant, repository code snippets to refine the final output. This ap- proach can be considered a retrieval-based method 38 for relieving the long-range dependency constraint. Similarly, Shrivastava et al. [504] propose the Repo-Level Prompt Generator (RLPG) framework to dynamically retrieve relevant repository context and construct the correct prompt for a given com- pletion task. To do this, many prompt proposals are generated from different prompt sources (e.g., parent class) and prompt contexts (e.g., method names). The best prompt is then selected by a prompt proposal classifier and combined with the default context to generate the final output. Finally, Sur\u00eds et al. [532] create the ViperGPT framework, which utilizes the Codex LLM to gener- ate programs that answer text-based visual queries. The Codex model is prompted with the query text and an API specification to do this. The human- generated API specification provides functions de- signed to deal with low-level visual tasks (e.g., find( object )) that the LLM can then use to gen- erate solution code. This approach significantly reduces the tokens needed to provide repository/- code context by only providing the API definition. This API definition approach, illustrated in 13 has been used in robotics by Vemprala et al. [564] , and names). The best prompt is then selected by a prompt proposal classifier and combined with the default context to generate the final output. Finally, Sur\u00eds et al. [532] create the ViperGPT framework, which utilizes the Codex LLM to gener- ate programs that answer text-based visual queries. The Codex model is prompted with the query text and an API specification to do this. The human- generated API specification provides functions de- signed to deal with low-level visual tasks (e.g., find( object )) that the LLM can then use to gen- erate solution code. This approach significantly reduces the tokens needed to provide repository/- code context by only providing the API definition. This API definition approach, illustrated in 13 has been used in robotics by Vemprala et al. [564] , and by Wang et al. [579] as part of a Minecraft agent. Previously, Gupta and Kembhavi [185] used a pre- defined function approach within VISPROG, which uses GPT-3, external python modules , and few-shot prompting with example programs to solve visual tasks. 3.3.2 Code Infilling and Generation Code infilling refers to modifying or completing existing code snippets based on the code context and instructions provided as a prompt. Fried et al. [154] train the InCoder LLM (up to 6.7B parameters) to both generate Python code and infill existing code using a masked language modeling approach. Incoder is trained using 159 GB of text split roughly equally between Python source code, StackOverflow content, and source code in other languages. On the HumanEval gener- ation benchmark, InCoder underperforms the best- performing Codex and CodeGen models. However, unlike the other models, InCoder can perform sin- gle and multi-line infilling of existing code. Similarly, Allal et al. [17] train a set of smaller SantaCoder models (1.1B parameters) for code gen- eration and code infilling using 268 GB of Python, JavaScript, and Java source code. SantaCoder is primarily evaluated on the MultiPL-E benchmark (an extension of HumanEval and MBPP [ 28] bench- LLMUsing the API functions provided, write a program that\u2026 Prompt def locate_item(item_name): \"\"\" Returns x,y,z of item \"\"\"def move_to_location(x, y, z): \"\"\" Moves to x,y,z coordinates\"\"\"def drop_item(item_name): \"\"\" Removes item from inventory\"\"\"API De\ufb01ntion ation benchmark, InCoder underperforms the best- performing Codex and CodeGen models. However, unlike the other models, InCoder can perform sin- gle and multi-line infilling of existing code. Similarly, Allal et al. [17] train a set of smaller SantaCoder models (1.1B parameters) for code gen- eration and code infilling using 268 GB of Python, JavaScript, and Java source code. SantaCoder is primarily evaluated on the MultiPL-E benchmark (an extension of HumanEval and MBPP [ 28] bench- LLMUsing the API functions provided, write a program that\u2026 Prompt def locate_item(item_name): \"\"\" Returns x,y,z of item \"\"\"def move_to_location(x, y, z): \"\"\" Moves to x,y,z coordinates\"\"\"def drop_item(item_name): \"\"\" Removes item from inventory\"\"\"API De\ufb01ntion move_to_location(10, 20, 0)locate_item('apple')move_to_location(5, 10, 15)drop_item('apple')Output def drop_item(item_name): \"\"\" Removes item from inventory\"\"\" item_list.remove(item_name)API Implementation StoreFunction ImplementationSelf-debuggingFigure 13: API Definition Framework . Illustration of providing a general API definition in the prompt [ 532, 579,564] to enable the consistent use of either external code or tools to solve the specific task whilst minimiz- ing the required context window. Extensions to this ap- proach have included asking the LLM to implement the functions within the API definition (red) and to prompt the LLM to self-debug any API code that does not exe- cute (green). marks), with it shown to outperform InCoder on both HumanEval generation and infilling (passing over 100 attempts). Code infilling is particularly relevant for applica- tions involving modifying, reviewing, or debugging existing code. Maniatis and Tarlow [357] explore the data from the intermediary steps in the develop- ment process to help automatically resolve reviewer comments [ 155]. The Dynamic Integrated Devel- oper ACTivity (DIDACT) methodology formalizes tasks in the software development process (e.g., re- pairing builds, predicting reviewer comments, etc.) intostate,intent , and action components, and trains the model to predict code modifications. This ap- proach aims to train the model to understand the process of software development rather than only the end product. 3.4 Creative Work For creative tasks, LLMs have primarily been ap- plied to story and script generation. For long-form story generation, Mirowski et al. [368] propose using a 70B Chinchilla- optimal [ 206] LLM Dramatron with prompting, the data from the intermediary steps in the develop- ment process to help automatically resolve reviewer comments [ 155]. The Dynamic Integrated Devel- oper ACTivity (DIDACT) methodology formalizes tasks in the software development process (e.g., re- pairing builds, predicting reviewer comments, etc.) intostate,intent , and action components, and trains the model to predict code modifications. This ap- proach aims to train the model to understand the process of software development rather than only the end product. 3.4 Creative Work For creative tasks, LLMs have primarily been ap- plied to story and script generation. For long-form story generation, Mirowski et al. [368] propose using a 70B Chinchilla- optimal [ 206] LLM Dramatron with prompting, prompt chaining, and hierarchical generation to create complete scripts and screenplays without the requirement for a human-in-the-loop (although co-writing is facilitated). The ability of Dramatron to help create a script was evaluated qualitatively 39 through co-writing and",
    "32": "follow-up interviews with 15 industry experts. Similarly, Yang et al. [637] propose using GPT-3 with a Recursive Reprompting and Revision frame- work (Re3) to generate stories over 2,000 words long. The Re3 approach uses zero-shot prompting with GPT-3 to generate a plan (settings, characters, outline, etc.). It then recursively prompts GPT-3 to generate story continuations using a specified dy- namic prompting procedure. Possible story contin- uations are then ranked for coherence and relevance using separate fine-tuned Longformer models as part of a Rewrite module. Finally, local edits to the selected continuations are made by detecting factual inconsistencies using the combination of a GPT-3 model [ 403] and a BART model [ 303] as part of an Edit module. This process can then be iterated for fully automated story generation. Finally, Yang et al. [636] introduce the Detailed Outline Control (DOC) framework to maintain plot coherence over thousands of words using GPT-3. While DOC uses the same high-level planning- drafting-revision approach as Re3, it implements this through the use of a detailed outliner andde- tailed controller . The detailed outliner first breaks down the high-level outline into subsections us- ing a breadth-first approach, with candidate gen- erations for the subsections created, filtered, and ranked. The bodies of the detailed outline subsec- tions are then generated iteratively using a struc- factual inconsistencies using the combination of a GPT-3 model [ 403] and a BART model [ 303] as part of an Edit module. This process can then be iterated for fully automated story generation. Finally, Yang et al. [636] introduce the Detailed Outline Control (DOC) framework to maintain plot coherence over thousands of words using GPT-3. While DOC uses the same high-level planning- drafting-revision approach as Re3, it implements this through the use of a detailed outliner andde- tailed controller . The detailed outliner first breaks down the high-level outline into subsections us- ing a breadth-first approach, with candidate gen- erations for the subsections created, filtered, and ranked. The bodies of the detailed outline subsec- tions are then generated iteratively using a struc- tured prompting approach. During the generation, an OPT-based FUDGE [ 635]detailed controller is used to help maintain relevance. In each case, to apply LLMs to long-form story generation, the task is broken down into a series of short-form sub-tasks (14). The current capabilities of LLMs primarily drive this approach, but also the desire to have a human-in-the-loop for some co-writing use cases [368]. Limited Context Window [368, 637] The inability of current LLMs to keep the entire generated work within the context window currently constrains their long-form applications and generates the need for mod- ular prompting (14). For short form generation, Chakrabarty et al. [69] propose CoPoet (fine-tuned T5 and T0 models) for collaborative poetry generation, Razumovskaia et al. [452] use PaLM and prompting with plansfor cross-lingual short story generation, Wang et al. [584] use GPT-4 as part of the ReelFramer tool to help co-create news reels for social media, Ippolito et al. [232] use LaMDA as part of the Wordcraft cre- ative writing assistant, and Calderwood et al. [63] apply a fine-tuned GPT-3 model as part of their Spindle tool for helping generate choice-based in- teractive fiction. For more general creative tasks, Haase and Hanel [187] assess a range of LLMs (including ChatGPT) on their capacity for idea generation ( ev- eryday creativity ) using the Alternative Uses Test (generating alternative uses for given items). On this task, LLMs were found to perform comparably to 100 human participants. Finally, for visual creative tasks, LLMs have also been used to increase the level of control users have when using image generation models. Feng et al. [584] use GPT-4 as part of the ReelFramer tool to help co-create news reels for social media, Ippolito et al. [232] use LaMDA as part of the Wordcraft cre- ative writing assistant, and Calderwood et al. [63] apply a fine-tuned GPT-3 model as part of their Spindle tool for helping generate choice-based in- teractive fiction. For more general creative tasks, Haase and Hanel [187] assess a range of LLMs (including ChatGPT) on their capacity for idea generation ( ev- eryday creativity ) using the Alternative Uses Test (generating alternative uses for given items). On this task, LLMs were found to perform comparably to 100 human participants. Finally, for visual creative tasks, LLMs have also been used to increase the level of control users have when using image generation models. Feng et al. [148] propose the LayoutGPT method where an LLM (GPT-3.5, GPT-4 or Codex) is used to gener- ate a CSS Structure layout the image should follow based on a text-based user prompt. This layout can be visualized and used as input to guide an image generation model. This approach performs strongly on text-to-image generation and indoor scene synthesis. A similar concept is implemented by Lian et al. [315] , where an LLM (GPT-3.5) is used to generate natural language layouts (bound- ing boxes and descriptions) to guide a diffusion model. Using an LLM as part of a modality conver- sion framework 16 has also been used in robotics [338, 225] and knowledge work [329]. 3.5 Knowledge Work With researchers increasingly demonstrating LLMs\u2019 ability to perform well on domain-specific knowledge tasks such as within Law [ 258] or Medicine [ 512], interest has grown in LLMs\u2019 ca- pacity for wider knowledge work . These applica- tions are likely to be found across the labor market with Eloundou et al. [140] estimating that 80% of the US workforce is in roles where at least 10% of tasks could be affected by LLMs. In the professional services field, Bommarito et al. [49] evaluate GPT-3.5 and previous GPT ver- sions on actual and synthetic questions from the Uniform CPA Examination Regulation section and AICPA Blueprints for legal, financial, accounting, technology, and ethical tasks. Using only zero-shot prompting, the best performing model (latest GPT- 3.5) struggles with quantitative reasoning, achiev- 40 Module 3Module 2LLMOutputGeneral Prompt Pre-processingModule 1LLMOutputGeneral PromptUser Prompt Medicine [ 512], interest has grown in LLMs\u2019 ca- pacity for wider knowledge work . These applica- tions are likely to be found across the labor market with Eloundou et al. [140] estimating that 80% of the US workforce is in roles where at least 10% of tasks could be affected by LLMs. In the professional services field, Bommarito et al. [49] evaluate GPT-3.5 and previous GPT ver- sions on actual and synthetic questions from the Uniform CPA Examination Regulation section and AICPA Blueprints for legal, financial, accounting, technology, and ethical tasks. Using only zero-shot prompting, the best performing model (latest GPT- 3.5) struggles with quantitative reasoning, achiev- 40 Module 3Module 2LLMOutputGeneral Prompt Pre-processingModule 1LLMOutputGeneral PromptUser Prompt LLMOutputPre-processingGeneral PromptRe-runResidualEg., Generate a plot outline for a new novel as paragraph headings Eg., Using the outline, generate a draft for the xth paragraph heading Eg., Check the spelling and consistency of this paragraph given the outline and plot summaryIterateFigure 14: Modular Prompting . Illustration of using a series of separate prompts [ 368,637,368,579,584] and processing steps to enable an LLM to perform tasks that would either not fit in a single context window or could not easily be specified in a single prompting step. ing results similar to random guessing on multiple- choice questions. However, on qualitative sections, GPT-3.5 achieved 50-70% accuracy, significantly ahead of random guessing and approaching human- level scores. Numerical Reasoning [436, 49] LLMs have generally seen worse perfor- mance on quantitative tasks, potentially con- straining",
    "33": "their applications in knowledge work areas such as financial services or ac- counting. Wu et al. [616] train BloombergGPT (50B parameters) for various financial knowledgework, including sentiment analysis, classifica- tion, NER/NED, and financial question answering. BloombergGPT is shown to outperform the OPT (66B parameters), GPT-NeoX, and BLOOM (176B parameters) LLMs on these financial domain- specific tasks and performs competitively on broader benchmarks. Thiergart et al. [550] considers the applicability of GPT-3 to the task of email management, includ- ing classification, information extraction (NER), and generating response text. Whilst it is noted that GPT-3 has the capacity for all three tasks, the author highlights current issues around reliability, straining their applications in knowledge work areas such as financial services or ac- counting. Wu et al. [616] train BloombergGPT (50B parameters) for various financial knowledgework, including sentiment analysis, classifica- tion, NER/NED, and financial question answering. BloombergGPT is shown to outperform the OPT (66B parameters), GPT-NeoX, and BLOOM (176B parameters) LLMs on these financial domain- specific tasks and performs competitively on broader benchmarks. Thiergart et al. [550] considers the applicability of GPT-3 to the task of email management, includ- ing classification, information extraction (NER), and generating response text. Whilst it is noted that GPT-3 has the capacity for all three tasks, the author highlights current issues around reliability, lack of access to internal data, and the need for a human in the loop. Liu et al. [329] propose enabling LLMs to un- derstand charts and plots by first using a vision plot-to-text translation model (DePlot) to decom- pose the chart into a linearized data table. Once the chart or plot has been converted into a text-based data table, it is combined with the prompt and pro- vided to a Flan-PaLM, Codex, or GPT-3.5 LLM. A similar modality conversion 16 approach has also been used in robotics [338, 225] for sensor data. Zhang et al. [668] evaluate a range of LLMs (GPT-3, InstructGPT, OPT, GLM, Cohere, and An- thropic) on the task of news summarization. On the DM/CNN and XSUM benchmarks, instruction fine-tuned models (InstructGPT) perform the best across summarization faithfulness, relevance, and coherence. To evaluate against human capabil- ity Zhang et al. [668] collect reference summa- rizations for 100 articles from 6 freelance writers. Zero-shot InstructGPT-3 performs comparably to the freelance writers across the three metrics. Cheng et al. [82] investigate GPT-4\u2019s capacity to perform data analysis and compare it to human an- alysts. GPT-4 is combined with a modular prompt- ingframework 14 with three steps, code generation (SQL and Python), code execution (\u201ccollect data and output figures\u201d, etc.), and analysis generation (\u201cgenerate five bullet points about the analysis\u201d). While GPT-4 performs well, it currently underper- forms experienced human data analysts on tasks from NvBench [346]. For scientific knowledge work, Taylor et al. [548] train the Galactica LLM specifically on sci- entific text for tasks such as scientific knowledge recall, reasoning, citation prediction, and scientific Zero-shot InstructGPT-3 performs comparably to the freelance writers across the three metrics. Cheng et al. [82] investigate GPT-4\u2019s capacity to perform data analysis and compare it to human an- alysts. GPT-4 is combined with a modular prompt- ingframework 14 with three steps, code generation (SQL and Python), code execution (\u201ccollect data and output figures\u201d, etc.), and analysis generation (\u201cgenerate five bullet points about the analysis\u201d). While GPT-4 performs well, it currently underper- forms experienced human data analysts on tasks from NvBench [346]. For scientific knowledge work, Taylor et al. [548] train the Galactica LLM specifically on sci- entific text for tasks such as scientific knowledge recall, reasoning, citation prediction, and scientific Q&A. In addition to a domain-specific training corpus, Galactica is specialized in the scientific do- 41 main through the use of specialized tokens, work- ing memory, and prompt-pre-training . Dunn et al. [133] propose fine-tuning GPT-3 for scientific combined named entity recognition and relation extraction (LLM-NERRE). First, 100 to 1,000 manually annotated prompt-completion pairs are created by humans. These examples are then used to fine-tune a GPT-3 model for the specific NERRE task. Finally, Liu and Shah [335] evaluate GPT-4\u2019s ability to review academic papers, specifically: identifying errors, verifying author checklists, and selecting the better abstract. GPT-4 shows some capacity to detect errors, with 7 out of 13 errors detected, and verify author checklists, with 87% accuracy. However, GPT-4 is shown to have lim- ited capacity for distinguishing the better paper abstract. 3.6 Law Applications of LLMs within the legal domain share many similarities with medicine, including legal question answering [ 651,258] and legal in- formation extraction [ 71]. However, other domain- specific applications have been proposed, such as case outcome prediction [ 189], legal research [ 234], and legal text generation [423]. 3.6.1 Legal Question Answering and Comprehension Key tasks of the legal field are finding related prece- dents, answering legal questions, and comparing existing documents or statutes. Using a general-purpose LLM with prompting approach, Yu et al. [651] use GPT-3.5 with zero- shot, few-shot, and CoT prompting to achieve SOTA performance on the legal entailment task (identifying the relevant statutes and determining if a given premise is correct) in the Competition share many similarities with medicine, including legal question answering [ 651,258] and legal in- formation extraction [ 71]. However, other domain- specific applications have been proposed, such as case outcome prediction [ 189], legal research [ 234], and legal text generation [423]. 3.6.1 Legal Question Answering and Comprehension Key tasks of the legal field are finding related prece- dents, answering legal questions, and comparing existing documents or statutes. Using a general-purpose LLM with prompting approach, Yu et al. [651] use GPT-3.5 with zero- shot, few-shot, and CoT prompting to achieve SOTA performance on the legal entailment task (identifying the relevant statutes and determining if a given premise is correct) in the Competition on Legal Information Extraction/Entailment (COL- IEE) dataset [ 437]. They also investigate a GPT-3.5 version fine-tuned using the COLIEE training set with and without explanations but find the zero- and few-shot legal prompting approaches perform best. Similarly, Rosa et al. [460] use a general monoT5 model with zero-shot prompting on the COLIEE entailment task. On the US legal Uniform Bar Examination (UBE), Bommarito II and Katz [50] show that GPT- 3.5 with zero-shot prompting can achieve 50% on the multiple choice Multistate Bar Examination component, but note that fine-tuning the modelon relevant examples does not appear to improve performance. More recently, Katz et al. [258] show that GPT-4 with zero-shot prompting exhibits SOTA performance on the full UBE, including the multiple choice, essay, and performance test com- ponents, and achieves passing scores. Blair-Stanek et al. [48] assess GPT-3.5\u2019s abil- ity to reason about legal facts and statutes us- ing the StAtutory Reasoning Assessment (SARA) dataset [ 208]. GPT-3.5 is shown to have SOTA per- formance but with significant variation depending on the type of prompting used (zero-shot, few-shot, and CoT). GPT-3.5 was also shown to perform rela- tively poorly on synthetic statutory reasoning tasks. Choi et al. [84] evaluate ChatGPT (GPT-3.5) on 95 multiple-choice and 12 essay questions from the final exams at the University of Minnesota law school. ChatGPT was found to perform at the level of a C+ student, near the bottom of",
    "34": "the class, but with passing scores. Out of Date Information Due to regularly updated laws and new precedents, the training/retrieval data be- come outdated frequently [195]. Finally, many more specific legal question- ing the StAtutory Reasoning Assessment (SARA) dataset [ 208]. GPT-3.5 is shown to have SOTA per- formance but with significant variation depending on the type of prompting used (zero-shot, few-shot, and CoT). GPT-3.5 was also shown to perform rela- tively poorly on synthetic statutory reasoning tasks. Choi et al. [84] evaluate ChatGPT (GPT-3.5) on 95 multiple-choice and 12 essay questions from the final exams at the University of Minnesota law school. ChatGPT was found to perform at the level of a C+ student, near the bottom of the class, but with passing scores. Out of Date Information Due to regularly updated laws and new precedents, the training/retrieval data be- come outdated frequently [195]. Finally, many more specific legal question- answering applications have been proposed, in- cluding: explaining legal concepts (GPT-4 + re- trieval) [ 478], summarizing legal judgments (GPT- 3.5) [ 115], litigation research and drafting [ 234], and helping full-fill the tasks of a law professor (ChatGPT) [427]. 3.6.2 Case Prediction and Legal Text Generation Case prediction and legal text generation involve predicting or completing legal opinions. Whilst there is currently sparse usage of LLMs in the liter- ature, smaller language models have been applied, suggesting potential future LLM applications in this area. Hamilton [189] use nine separate GPT-2 models trained on individual supreme court justice\u2019s au- thored opinions to predict how each justice will vote on a given case. They use a handcrafted prompt, including a summary of the topic gener- ated by GPT-3. However, they find this approach to case prediction does not match the SOTA. Previously, Chalkidis et al. [70] trained a range of attention-based models (including BERT) to pre- 42 dict case outcomes from the European Court of Human Rights (ECHR). The attention-based mod- els outperformed an SVM with a bag of words approach for binary violation classification, multi- label violation classification, and case importance prediction. Finally, Peric et al. [423] use a dataset of 50,000 judicial opinions from U.S. Circuit Courts to train a Transformer-XL model and fine-tune a GPT-2 model. The models were then evaluated for their ability to complete a judicial opinion, with a start given as a prompt. In qualitative evaluations, hu- man participants struggled distinguishing between machine-generated and genuine text. 3.7 Medicine Many applications of LLMs have been proposed of attention-based models (including BERT) to pre- 42 dict case outcomes from the European Court of Human Rights (ECHR). The attention-based mod- els outperformed an SVM with a bag of words approach for binary violation classification, multi- label violation classification, and case importance prediction. Finally, Peric et al. [423] use a dataset of 50,000 judicial opinions from U.S. Circuit Courts to train a Transformer-XL model and fine-tune a GPT-2 model. The models were then evaluated for their ability to complete a judicial opinion, with a start given as a prompt. In qualitative evaluations, hu- man participants struggled distinguishing between machine-generated and genuine text. 3.7 Medicine Many applications of LLMs have been proposed in the medical domain, including medical ques- tion answering [ 511,512,320,655,388], clinical information extraction [ 10,448], indexing [ 650], triage [ 491,301], and management of health records [276]. 3.7.1 Medical Question Answering and Comprehension Medical question answering and comprehension consists of generating multiple-choice and free-text responses to medical questions. Singhal et al. [511] proposed using few-shot, CoT, and self-consistency prompting to specialize the general-purpose PaLM LLM to medical ques- tion answering and comprehension. They demon- strate a Flan-PaLM model [ 93] using a combination of the three prompting strategies to achieve the pre- vious SOTA results on the MedQA, MedMCQA, PubMedQA, and MMLU medical datasets. To fur- ther align the model to the medical domain, they proposed Med-PaLM, which utilizes instruction prompt-tuning based on 40 examples from a panel of clinicians and task-specific human-engineered prompts. Singhal et al. [512] then extend the Med-PaLM approach with Med-PaLM 2 using the newer PaLM 2 LLM as its base model. Singhal et al. [512] conduct further instruction-fine tuning and use a new ensemble refinement (ER) prompting strategy (where stochastically sampled outputs are first gen- erated and provided within the final prompt). This allows Med-PaLM 2 to achieve the current SOTA on the MultiMedQA benchmark. Li\u00e9vin et al. [320] adopt a similar approach us- ing zero-shot, few-shot, and CoT prompting toadapt the GPT-3.5 LLM to medical question an- swering (USMLE and MedMCQA) and compre- hension (PubMedQA) tasks. In addition, Li\u00e9vin et al. [320] propose using retrieval augmentation where relevant text from Wikipedia is retrieved of clinicians and task-specific human-engineered prompts. Singhal et al. [512] then extend the Med-PaLM approach with Med-PaLM 2 using the newer PaLM 2 LLM as its base model. Singhal et al. [512] conduct further instruction-fine tuning and use a new ensemble refinement (ER) prompting strategy (where stochastically sampled outputs are first gen- erated and provided within the final prompt). This allows Med-PaLM 2 to achieve the current SOTA on the MultiMedQA benchmark. Li\u00e9vin et al. [320] adopt a similar approach us- ing zero-shot, few-shot, and CoT prompting toadapt the GPT-3.5 LLM to medical question an- swering (USMLE and MedMCQA) and compre- hension (PubMedQA) tasks. In addition, Li\u00e9vin et al. [320] propose using retrieval augmentation where relevant text from Wikipedia is retrieved and included in the prompt. More recently, Nori et al. [388] evaluated GPT-4 on USMLE and Mul- tiMedQA datasets using zero and few shot prompt- ing. GPT-4 is found to outperform GPT-3.5 across benchmarks significantly. However, several issues relating to using GPT-4 for real-world clinical ap- plications are raised, including the risks of erro- neous generations and the risks of bias . Tang et al. [538] raise similar issues and find that GPT-3.5 and ChatGPT have issues with factual accuracy and representing the level of certainty during medical summarization. Hallucination and Bias [ 538,388,511] The safety-critical nature of the medical do- main means the possibility of hallucinations significantly limits the current use cases. Further work is also needed to reduce the risk of LLMs perpetuating existing bias in clinical datasets. Yunxiang et al. [655] fine-tune a LLaMA LLM ChatDoctor (7B parameters) specifically for the task of medical question answering. To specialize the LLaMA model, it is first instruction fine-tuned using the Alpaca dataset [ 540] and then fine-tuned to the medical domain using a dataset of 100k pa- tient conversations. Similarly to Li\u00e9vin et al. [320] , ChatDoctor is augmented with two external knowl- edge sources (a disease database and Wikipedia) to improve the factual grounding of the model. Instead of using general models with specialized prompting or fine-tuning, Venigalla et al. [565] train a new model PubMedGPT specifically for medical question answering and text generation tasks. PubMedGPT is trained using a combina- tion of PubMed abstracts and full documents from the Pile [ 165]. Peng et al. [418] also train a new ChatDoctor (7B parameters) specifically for the task of medical question answering. To specialize the LLaMA model, it is first instruction fine-tuned using the Alpaca dataset [ 540] and then fine-tuned to the medical domain using a dataset of 100k pa- tient conversations. Similarly to Li\u00e9vin et al. [320] , ChatDoctor is augmented with two external knowl- edge sources (a disease database",
    "35": "and Wikipedia) to improve the factual grounding of the model. Instead of using general models with specialized prompting or fine-tuning, Venigalla et al. [565] train a new model PubMedGPT specifically for medical question answering and text generation tasks. PubMedGPT is trained using a combina- tion of PubMed abstracts and full documents from the Pile [ 165]. Peng et al. [418] also train a new LLM GatorTronGPT (up to 20B parameters) for biomedical question answering and relation extrac- tion using a mixture of clinical and general English text. Whilst these approaches outperformed exist- ing smaller specific purpose models [ 177,644] in medical question answering, they currently under- perform the larger general purpose LLMs (GPT- 3.5/4 and MedPaLM 1/2). However, there remains 43 debate over whether larger general or specialized clinical models are the best approach. Looking at models up to GPT-3, Lehman et al. [297] ques- tion the effectiveness of LLM in-context learning approaches by showing that small specialized clin- ical models fine-tuned on limited annotated data outperform the former. Finally, LLMs have also been applied to a range of more specific medical question-answering tasks, including evaluating GPT-3 on its\u2019 ability to triage and diagnose cases [ 301], responding to social me- dia genetics [ 134] and general [ 30] patient ques- tions (ChatGPT), answering questions from the Korean general surgery board exams (GPT-3.5, GPT-4) [ 393], consultation and medical note tak- ing [296], and answering ophthalmology questions [21]. 3.7.2 Medical Information Retrieval Medical text often contains domain-specific abbre- viations, acronyms, and technical terms presenting specific information retrieval challenges. This has led LLMs also to be applied to help structure and extract data from medical sources. Agrawal et al. [10] use InstructGPT (GPT-3) with prompt templates (zero- and one-shot) for clin- ical information extraction, such as extracting med- ication dosage and frequency from medical notes or disambiguation of medical acronyms. They also dia genetics [ 134] and general [ 30] patient ques- tions (ChatGPT), answering questions from the Korean general surgery board exams (GPT-3.5, GPT-4) [ 393], consultation and medical note tak- ing [296], and answering ophthalmology questions [21]. 3.7.2 Medical Information Retrieval Medical text often contains domain-specific abbre- viations, acronyms, and technical terms presenting specific information retrieval challenges. This has led LLMs also to be applied to help structure and extract data from medical sources. Agrawal et al. [10] use InstructGPT (GPT-3) with prompt templates (zero- and one-shot) for clin- ical information extraction, such as extracting med- ication dosage and frequency from medical notes or disambiguation of medical acronyms. They also introduce two methods for converting the LLM output into a structured format using a verbilizer for mapping to classification labels and a resolver for more complex structured outputs such as lists (GPT-3 + R). Rajkomar et al. [448] take a different approach by treating medical acronym disambiguation as a translation task and training a specialized end- to-end T5 LLM. To preserve privacy, they also use a training dataset generated from public web pages (without medical acronyms) and web-scale reverse substitution of medical acronyms, with only evaluation done on actual clinical notes. Finally, Gu et al. [178] use GPT-3.5 and knowl- edge distillation to train a PubMedBERT model for adverse drug event extraction (entity and rela- tion). The distilled PubMedBERT model outper- forms GPT-3.5 and GPT-4, and performs similarly to specialized models that use supervised learning. 3.8 Reasoning Mathematical and algorithmic tasks often require a different set of capabilities than traditional NLPtasks, such as understanding mathematical opera- tions, complex multi-step reasoning, and longer- term planning. Therefore, the applicability of LLMs to these tasks, and methods for improving their capabilities, is an active area of research. For mathematical reasoning tasks, Uesato et al. [560] test a range of fine-tuning (supervised and RLHF), prompting (zero-shot and few-shot), and re-ranking (majority voting and reward model) to evaluate whether they improve a base LLM\u2019s (70B parameters) ability to generate accurate reason- ing steps on word-based maths problems in the GSM8K dataset [ 95]. Whilst fine-tuning on in- termediate steps (\u201cprocess-based\u201d) performs simi- larly to using only final answers (\u201coutcome-based\u201d) a different set of capabilities than traditional NLPtasks, such as understanding mathematical opera- tions, complex multi-step reasoning, and longer- term planning. Therefore, the applicability of LLMs to these tasks, and methods for improving their capabilities, is an active area of research. For mathematical reasoning tasks, Uesato et al. [560] test a range of fine-tuning (supervised and RLHF), prompting (zero-shot and few-shot), and re-ranking (majority voting and reward model) to evaluate whether they improve a base LLM\u2019s (70B parameters) ability to generate accurate reason- ing steps on word-based maths problems in the GSM8K dataset [ 95]. Whilst fine-tuning on in- termediate steps (\u201cprocess-based\u201d) performs simi- larly to using only final answers (\u201coutcome-based\u201d) on final answer correctness, processed-based ap- proaches are found to generate significantly fewer errors in reasoning. Huang et al. [222] take this a step further by showing that the mathematical reasoning ability of a PaLM LLM on the GSM8K dataset can be self-improved through fine-tuning on a dataset of high-confidence reasoning paths generated by the same PaLM base model. Using only prompting, Kojima et al. [273] find that zero-shot CoT prompting alone significantly improves the performance of GPT-3 and PaLM LLMs over standard zero- and few-shot prompting on the MultiArith and GSM8K datasets. While Li et al. [312] introduce DIVERSE, a prompting ap- proach that uses a diverse set of prompts for each question and a trained verifier (with reasoning step awareness) to improve further GPT-3.5\u2019s perfor- mance on GSM8K and other reasoning bench- marks. Finally, Shridhar et al. [502] take a novel approach by training new models to break down a mathematical word problem into Socratic sub- questions to guide the answer of either other LLMs or human learners. GPT-3 prompted with these sub- questions outperforms simple one-shot prompting on the GSM8K dataset. Stolfo et al. [525] evaluate a range of LLMs (in- cluding GPT-3) at mathematical reasoning using a new framework to understand the causal impact of different input factors (e.g framing, operands, and operations). Instruction fine-tuned GPT-3 mod- els are found to be significantly more robust and sensitive than the smaller LLMs evaluated. Other LLM use cases in algorithmic and mathe- matical reasoning have also been proposed. Gadgil et al. [159] apply a Codex LLM with prompt en- 44 gineering and filtering to the task of mathemati- approach by training new models to break down a mathematical word problem into Socratic sub- questions to guide the answer of either other LLMs or human learners. GPT-3 prompted with these sub- questions outperforms simple one-shot prompting on the GSM8K dataset. Stolfo et al. [525] evaluate a range of LLMs (in- cluding GPT-3) at mathematical reasoning using a new framework to understand the causal impact of different input factors (e.g framing, operands, and operations). Instruction fine-tuned GPT-3 mod- els are found to be significantly more robust and sensitive than the smaller LLMs evaluated. Other LLM use cases in algorithmic and mathe- matical reasoning have also been proposed. Gadgil et al. [159] apply a Codex LLM with prompt en- 44 gineering and filtering to the task of mathemati- cal formalization (in the context of theorem prov- ing). Webb et al. [595] evaluate GPT-3.5\u2019s capacity for analogical reasoning",
    "36": "using tasks that emulate Raven\u2019s Standard Progressive Matrices (SPM), let- ter string analogies, and verbal analogies. GPT-3.5 is shown to generally outperform human partic- ipants (undergraduates) at matrix reasoning and verbal analogies, but with more mixed results on letter string analogies. Yu et al. [654] introduce the ALERT benchmark to evaluate LLM reason- ing across ten skills (logistic, causal, common- sense, abductive, spatial, analogical, argument, and deductive reasoning, as well as textual entail- ment and mathematics). Ruis et al. [464] study LLMs\u2019 capability to interpret implicatures, for ex- ample, whether they understand the response \"I wore gloves\" to the question \u201cDid you leave finger- prints?\u201d as meaning \u201cNo\u201d; finding that lots of mod- els perform close to random. Finally, Valmeekam et al. [562] propose a new assessment framework forcommon-sense planning and find that existing LLMs GPT-3.5 and BLOOM perform poorly. Us- ing the framework for the Blocksworld domain (planning tasks with different colored blocks on a surface), the best GPT-3.5 model only came up with a valid plan 5% of the time, compared to 78% of human participants. Sub-Human-Performance [562, 607] Existing LLMs struggle to match human performance on reasoning benchmarks. Another line of work has investigated the in- tersection of LLMs and causal reasoning [ 425, 253]. K\u0131c\u0131man et al. [286] argue that GPT-3.5/4 outperform existing algorithms in three causal benchmarks. In contrast, Gao et al. [164] evalu- ate ChatGPT on three causal reasoning tasks (dis- els perform close to random. Finally, Valmeekam et al. [562] propose a new assessment framework forcommon-sense planning and find that existing LLMs GPT-3.5 and BLOOM perform poorly. Us- ing the framework for the Blocksworld domain (planning tasks with different colored blocks on a surface), the best GPT-3.5 model only came up with a valid plan 5% of the time, compared to 78% of human participants. Sub-Human-Performance [562, 607] Existing LLMs struggle to match human performance on reasoning benchmarks. Another line of work has investigated the in- tersection of LLMs and causal reasoning [ 425, 253]. K\u0131c\u0131man et al. [286] argue that GPT-3.5/4 outperform existing algorithms in three causal benchmarks. In contrast, Gao et al. [164] evalu- ate ChatGPT on three causal reasoning tasks (dis- tinct from K\u0131c\u0131man et al. [286] ) and find that it performs rather poorly; further, few-shot and chain- of-thought prompting sometimes further exacer- bates its performance. Srivastava et al. [519] pro- pose 14 causal reasoning tasks, some of which are considered to be very hard [ 534]. Similarly, Jin et al. [244] curate another causal inference task and posit that current LLMs still fail to general- ize. Lampinen et al. [288] study whether LLMs can generalize causal intervention strategies from few-shot examples. Willig et al. [607] conjec- ture that current LLMs are \u201ccausal parrots\u201d, simplyreciting causal knowledge embedded in their data rather than doing causal reasoning [253]. Overall, while LLMs show some capacity for more complex reasoning, the relatively poor per- formance of LLMs on a number of reasoning tasks and benchmarks [ 562,164,244] stands in contrast to the often human level performance being seen in other capabilities [61, 263]. 3.9 Robotics and Embodied Agents LLMs have also started to be incorporated into robotics applications to provide high-level planning and contextual knowledge. Ahn et al. [14] implement a PaLM-540B LLM in the SayCan architecture to break down high-level text-based instructions into a sequence of lower- level robot tasks that can be executed. The authors use the LLM to propose possible next actions via it- eratively scoring the most likely of a defined set of low-level tasks based on the high-level text input. The low-level task to be executed is then deter- mined by combining the low-level tasks proposed by the LLM with affordance functions which de- termine the probability of the robot completing the to the often human level performance being seen in other capabilities [61, 263]. 3.9 Robotics and Embodied Agents LLMs have also started to be incorporated into robotics applications to provide high-level planning and contextual knowledge. Ahn et al. [14] implement a PaLM-540B LLM in the SayCan architecture to break down high-level text-based instructions into a sequence of lower- level robot tasks that can be executed. The authors use the LLM to propose possible next actions via it- eratively scoring the most likely of a defined set of low-level tasks based on the high-level text input. The low-level task to be executed is then deter- mined by combining the low-level tasks proposed by the LLM with affordance functions which de- termine the probability of the robot completing the task given the current low-level context. Driess et al. [129] take this concept a step fur- ther by combining the PaLM-540B LLM with ad- ditional input modalities (22B parameter vision transformer) to create the PaLM-E model. By in- troducing images into the input, the PaLM-E model can predict which low-level tasks are possible given the current state, whether the previous low-level tasks executed failed, and incorporate images into long-horizon planning, allowing it to outperform the original SayCan results. Another approach has been to use LLMs to gen- erate code for robotics tasks. Vemprala et al. [564] combine ChatGPT with a pre-defined high-level function library of robotic capabilities for human on the loop robotics tasks. By providing details of the function library in the prompt, ChatGPT is then shown to be able to break down high-level natu- ral language instructions into a set of lower-level function calls, which can then be executed on the robot if the human is satisfied it is accurate. This is another example of the API definition 13 approach, also used in computer programming [ 532]. Other related works that use LLMs to generate code for robotics applications include using an LLM for hi- erarchical code generation to write robot policies (Codex) [ 316], to generate code policies and main- 45 tain a written state (GPT-3.5) [ 647], and using an LLM for code-based task planning (GPT-3, Codex) [510]. Finally, LLMs have also been combined with modality-to-text pre-processing to provide the LLM with additional input from the robot\u2019s en- vironment. Liu et al. [338] use GPT-4 as part of the REFLECT framework for detecting and explaining function calls, which can then be executed on the robot if the human is satisfied it is accurate. This is another example of the API definition 13 approach, also used in computer programming [ 532]. Other related works that use LLMs to generate code for robotics applications include using an LLM for hi- erarchical code generation to write robot policies (Codex) [ 316], to generate code policies and main- 45 tain a written state (GPT-3.5) [ 647], and using an LLM for code-based task planning (GPT-3, Codex) [510]. Finally, LLMs have also been combined with modality-to-text pre-processing to provide the LLM with additional input from the robot\u2019s en- vironment. Liu et al. [338] use GPT-4 as part of the REFLECT framework for detecting and explaining robot failures. To achieve this, multi-modal sensory inputs are first converted into a text-based hierar- chical summary at the sensory, event, and sub-goal levels. The hierarchical summary then prompts the LLM to detect and analyze failures. Similarly, Huang et al. [225] combine an LLM (InstructGPT, PaLM) with multiple sources of text-based environ- ment feedback for robotic task planning. Single Modality [338, 14, 564] While LLMs can help robots or agents un- derstand instructions and add high-level planning capabilities, their inability to di- rectly learn from image, audio or other sen- sor modalities constrain their applications. For agents in simulated worlds, Wang et al. [579] use the GPT-4 LLM within their VOYAGER framework to create a",
    "37": "Minecraft agent that can autonomously explore, acquire new skills and com- plete tasks. First, they use GPT-4 to propose new tasks for the agent to complete as part of the au- tomatic curriculum . Then, they ask it to generate code to solve the proposed task given the current state to add to its skills library , which can then be used in the future (similar to the API approach 13 used by Vemprala et al. [564] ). Finally, the authors use GPT-4 to verify whether the executed code has achieved the proposed task. This framework outperforms prompting approaches such as ReAct, Reflexion, and AutoGPT (Sec. 2.7). Prior work using LLMs for planning in simu- lated worlds include: Wang et al. [591] using GPT- 3 for Minecraft, Huang et al. [224] using GPT-3 and Codex in VirtualHome, and Nottingham et al. [389] using Codex for Minecraft. 3.10 Social Sciences & Psychology The rapid advancements of LLMs have fostered the use of such models across research in the psycho- tomatic curriculum . Then, they ask it to generate code to solve the proposed task given the current state to add to its skills library , which can then be used in the future (similar to the API approach 13 used by Vemprala et al. [564] ). Finally, the authors use GPT-4 to verify whether the executed code has achieved the proposed task. This framework outperforms prompting approaches such as ReAct, Reflexion, and AutoGPT (Sec. 2.7). Prior work using LLMs for planning in simu- lated worlds include: Wang et al. [591] using GPT- 3 for Minecraft, Huang et al. [224] using GPT-3 and Codex in VirtualHome, and Nottingham et al. [389] using Codex for Minecraft. 3.10 Social Sciences & Psychology The rapid advancements of LLMs have fostered the use of such models across research in the psycho- logical and behavioral sciences. Reviewing the ex- isting literature, we have identified three main areas and tasks in which LLMs have been used in the con- Using LLMs to model human behavior Analyzing behavioral characteristics of LLMs Simulating social relationships with LLMs LLMs in the Social Sciences & Psychology Milgram Shock Experiment Big Five personality traits Interacting artificial agents Illusory Truth Effect Guilford's Alternative Uses LLMs to simulate societiesFigure 15: Use cases of LLMs in the social sci- ences and psychology can mainly be structured into three categories: using LLMs to model human behav- ior [e.g., 12,211], analyzing behavioral characteristics of LLMs [e.g., 414], and using LLMs to simulate social relationships [e.g., 408]. text of the psychological and behavioral sciences: using LLMs to simulate human behavioral experi- ments [e.g., 22,176,211,614,126], analyzing the personality traits of LLMs [e.g., 367,414,470], and employing them as artificial agents to model social relationships [ 409]. See Fig. 15 for an illus- tration. 3.10.1 Modeling Human Behavior In the behavioral sciences, there is an increasing interest in using LLMs as models for psychological experiments. Being able to model human behavior computationally through language models would entail a variety of advantages over using human participants: experiments with LLMs are cheaper, faster, can be scaled easier, and are potentially less sensitive to ethical considerations [ 176]. In light of this, various works have compared LLMs with human participants from a behavioral perspective. Argyle et al. [22] demonstrate how LLMs can ments [e.g., 22,176,211,614,126], analyzing the personality traits of LLMs [e.g., 367,414,470], and employing them as artificial agents to model social relationships [ 409]. See Fig. 15 for an illus- tration. 3.10.1 Modeling Human Behavior In the behavioral sciences, there is an increasing interest in using LLMs as models for psychological experiments. Being able to model human behavior computationally through language models would entail a variety of advantages over using human participants: experiments with LLMs are cheaper, faster, can be scaled easier, and are potentially less sensitive to ethical considerations [ 176]. In light of this, various works have compared LLMs with human participants from a behavioral perspective. Argyle et al. [22] demonstrate how LLMs can generate responses corresponding to virtual partici- pants in behavioral experiments. They do so by us- ing LLMs to generate samples of responses to stud- ies related to political opinions and voting behavior. In particular, the authors investigate three studies: the first asks participants to list words associated with outgroup partisans, and the second and third focus on vote prediction based on demographics. Across scenarios, experimental results demonstrate that GPT-3 provides answers that closely align with human responses. Horton [211] argue that LLMs can be used to computationally model human behavior and demonstrate such an ability in economics by ex- ploring their behavior in economic scenarios. They conducted four experiments focusing on economic decision-making using GPT-3, showing that the 46 LLM can approximately replicate results obtained with human individuals. Griffin et al. [176] investigate the suitability of LLMs to model psychological change. In their study, the authors assess LLM responses to two behavioral tests, the illusory truth effect [ITE; 194] and an experiment measuring the influence of pop- ulist news to change in political views [ 55]. The results demonstrate that in both scenarios, human judgments tend to align with LLM-based judg- ments, indicating that LLMs have the potential to model the effect of influence on human individuals. Aher et al. [12] introduce the Turing Experiment (TE) to measure an LLM\u2019s suitability to model hu- man behavior. A TE consists of inputs to the LLM that signal a certain demographic (e.g., names or occupations) as well as a set of experimental de- tails and corresponding outputs used to simulate Griffin et al. [176] investigate the suitability of LLMs to model psychological change. In their study, the authors assess LLM responses to two behavioral tests, the illusory truth effect [ITE; 194] and an experiment measuring the influence of pop- ulist news to change in political views [ 55]. The results demonstrate that in both scenarios, human judgments tend to align with LLM-based judg- ments, indicating that LLMs have the potential to model the effect of influence on human individuals. Aher et al. [12] introduce the Turing Experiment (TE) to measure an LLM\u2019s suitability to model hu- man behavior. A TE consists of inputs to the LLM that signal a certain demographic (e.g., names or occupations) as well as a set of experimental de- tails and corresponding outputs used to simulate human behavior. The authors apply their approach to four individual tests, namely an ultimatum game from behavioral economics [ 214,279], garden-path sentences used in psycholinguistics [ 89,411], the Milgram Shock Experiment from social psychol- ogy [ 364], and the wisdom of crowds task used to measure collective social intelligence [ 375]. De- mographic details are simulated via gender titles and surnames. The results show that LLMs largely align with human behavior across the tests. How- ever, the authors note that LLM size matters and that larger models tend to provide results that are more aligned with human responses. Aher et al. [12] point out that the LLMs were most likely exposed to the four behavioral exper- iments during their pre-training. To account for that, the authors create artificial variations of the experiments with conditions that differ from previ- ous studies. Additionally, the authors note that a potential risk with using LLMs to simulate human responses is the introduction of generations that contain biases stemming from the models\u2019 training data. Social Biases [12, 367] Unbalanced views and opinions in the train- ing data skew the LLMs towards biased hu- man behaviors. Park et al. [409] replicate a set of 8 psycho- logical studies from the Many Labs 2 project [ 270] using GPT-3 to",
    "38": "assess the LLM for its ability to sim- ulate human behavioral data. Such studies includetests in which subjects are asked to choose between a kiss from a favorite movie star and $50 [ 462] and where subjects had to decide between paying a traffic violation fine and going to court [ 461]. These experiments show that GPT-3 replicates only ous studies. Additionally, the authors note that a potential risk with using LLMs to simulate human responses is the introduction of generations that contain biases stemming from the models\u2019 training data. Social Biases [12, 367] Unbalanced views and opinions in the train- ing data skew the LLMs towards biased hu- man behaviors. Park et al. [409] replicate a set of 8 psycho- logical studies from the Many Labs 2 project [ 270] using GPT-3 to assess the LLM for its ability to sim- ulate human behavioral data. Such studies includetests in which subjects are asked to choose between a kiss from a favorite movie star and $50 [ 462] and where subjects had to decide between paying a traffic violation fine and going to court [ 461]. These experiments show that GPT-3 replicates only 37.5% of the effects obtained from human partic- ipants. The authors argue that these results are attributed to humans and LLMs representing inher- ently different cognitive systems. Maddela et al. [353] study identifying unhelpful thought patterns and possible reframings to facil- itate mental health. They release a dataset called PATTERN REFRAME and evaluate GPT-3.5 on it, showing that it can perform very well without ad- ditional training. They conclude that practitioners of cognitive behavioral therapy may benefit from using LLMs to produce richer training material. 3.10.2 Analyzing Behavioral Characteristics of LLMs In addition to using LLMs as models for human behavior, various existing works study LLMs by analyzing their personality traits. Jiang et al. [242] do so by introducing the Ma- chine Personality Inventory (MPI) dataset, a col- lection of items to assess personalities according to the Big Five personality factors: extraversion, agreeableness, openness, conscientiousness, and neuroticism [358]. Miotto et al. [367] assess GPT-3\u2019s personalities using the HEXACO [ 27] and Human Values [ 488] scales. Their experimental results reveal that GPT- 3 obtains personality and value scores that align with human participants. Miotto et al. [367] provide an extensive analysis of varying temperature values used to prompt the LLM, finding that an increased temperature yields changes in the model\u2019s person- alities, e.g., GPT-3 shows a higher unwillingness to manipulate as well as increased scores on anxiety. Similar results were obtained concerning the Hu- man Values scale, where model responses varied substantially for different temperature values. to the Big Five personality factors: extraversion, agreeableness, openness, conscientiousness, and neuroticism [358]. Miotto et al. [367] assess GPT-3\u2019s personalities using the HEXACO [ 27] and Human Values [ 488] scales. Their experimental results reveal that GPT- 3 obtains personality and value scores that align with human participants. Miotto et al. [367] provide an extensive analysis of varying temperature values used to prompt the LLM, finding that an increased temperature yields changes in the model\u2019s person- alities, e.g., GPT-3 shows a higher unwillingness to manipulate as well as increased scores on anxiety. Similar results were obtained concerning the Hu- man Values scale, where model responses varied substantially for different temperature values. In line with this work, Pellert et al. [414] ar- gue that LLMs possess psychological traits as ob- served in human individuals and can be assessed through psychometric tests. The authors conduct experiments measuring, among others, the Big Five personality traits in a zero-shot setup. In contrast, to Miotto et al. [367] , Pellert et al. [414] investi- gate smaller models based on BERT and find that different variants of BERT score across the five personalities in a fairly homogeneous fashion, with 47 traits that are high on agreeableness and extraver- sion, but low on neuroticism. In a related fashion, Stevenson et al. [523] as- sess LLM performance (GPT-3) on the Guilford\u2019s Alternative Uses Test [AUT; 181], a test to assess human creativity. The test asks participants to sug- gest uses for physical objects (e.g., a book or a fork). Comparing the AUT test performance of GPT-3 to that of psychology students, the authors found that human responses score higher on orig- inality and surprise, whereas GPT-3\u2019s responses were more useful. Kosinski [277] test Theory of Mind (ToM) in LLMs. ToM refers to the ability to track others\u2019 unobservable mental states, such as intentions, be- liefs, or desires. The authors find that among LLMs of the GPT family, recent models can in- creasingly solve ToM tasks without having been explicitly trained to do so. For instance, while GPT- 2 shows virtually no capability of solving ToM tasks, GPT-3.5 (based on InstructGPT) and GPT-4 performed similarly to 6- and 7-year-old children, respectively. Gandhi et al. [162] present a template- based framework for generating synthetic samples to evaluate ToM in LLMs, which are then applied to found that human responses score higher on orig- inality and surprise, whereas GPT-3\u2019s responses were more useful. Kosinski [277] test Theory of Mind (ToM) in LLMs. ToM refers to the ability to track others\u2019 unobservable mental states, such as intentions, be- liefs, or desires. The authors find that among LLMs of the GPT family, recent models can in- creasingly solve ToM tasks without having been explicitly trained to do so. For instance, while GPT- 2 shows virtually no capability of solving ToM tasks, GPT-3.5 (based on InstructGPT) and GPT-4 performed similarly to 6- and 7-year-old children, respectively. Gandhi et al. [162] present a template- based framework for generating synthetic samples to evaluate ToM in LLMs, which are then applied to five recently developed LLMs (incl. GPT-3, GPT- 4, LLaMA, and Claude). The authors show that most models struggle with ToM in its basic forms. However, GPT-4 performs closest to the human comparison of all tested models. 3.10.3 Simulating Social Relationships While most previous works measure LLMs as mod- els for human behavior through replicating human behavioral studies, Park et al. [408] use the power of LLMs to model the interaction between artificial agents. The authors model a community of 25 ar- tificial agents interacting in a digital environment to achieve this. Each character has unique traits, and the characters interact with each other through natural language. Simulating such societies, the authors observe emergent social behaviors (e.g., forming new relationships and attending events) between agents that are formed without any human interaction. 3.11 Synthetic Data Generation The ability of LLMs to perform in-context learning allows them to be prompted to generate synthetic datasets for training much smaller domain-specific models. LLMModality-to-TextPrompt OutputLLMPrompt <style>.grid { display: grid;\u2026\u2026.Code -> ModalityCSSLatex - TikZPython - Matplotlib PromptModality-and-Text-to-X Post-processingPre-processingFigure 16: Modality Conversion . Illustration of us- ing models with other input modalities as pre or post- processing steps in an LLM pipeline [ 148,329,338, 225,315]. For some use cases, this approach can be used as an alternative to training a multi-modal model or using a shared embedding space. Wang et al. [583] propose using GPT-3 to label datasets more cost-effectively than human labelers. These labeled datasets can then be used to train more compute-efficient smaller models. To evalu- allows them to be prompted to generate synthetic datasets for training much smaller domain-specific models. LLMModality-to-TextPrompt OutputLLMPrompt <style>.grid { display: grid;\u2026\u2026.Code -> ModalityCSSLatex - TikZPython - Matplotlib",
    "39": "PromptModality-and-Text-to-X Post-processingPre-processingFigure 16: Modality Conversion . Illustration of us- ing models with other input modalities as pre or post- processing steps in an LLM pipeline [ 148,329,338, 225,315]. For some use cases, this approach can be used as an alternative to training a multi-modal model or using a shared embedding space. Wang et al. [583] propose using GPT-3 to label datasets more cost-effectively than human labelers. These labeled datasets can then be used to train more compute-efficient smaller models. To evalu- ate this approach, RoBERTa and PEGASUS mod- els are trained for 9 NLP tasks using human and GPT-3 generated labels. GPT-3 labels are shown to outperform human labels when labeling budgets are small, but higher-quality human labels tend to lead to better models at higher labeling budgets. Similarly, Ding et al. [123] propose three prompt- ing approaches for training data generation with GPT-3: unlabeled data annotation (generate labels for known examples), training data generation (gen- erate examples and labels), and assisted training data generation (with Wikidata provided as addi- tional context). Fine-tuning a smaller BERT model for text classification and NER tasks using these approaches showed results similar to or worse than using GPT-3 directly. Gunasekar et al. [182] leverage synthetic data generation with GPT-3.5 to train a new code gen- eration LLM (see Sec. 3.3.1). The generated data consists of synthetic Python textbooks focusing on reasoning, basic algorithmic skills, and synthetic Python exercises. One important finding of this 48 work is that introducing randomness into data gen- eration is crucial, all while ensuring the examples maintain their quality and coherence. Yoo et al. [648] propose GPT3Mix to generate additional synthetic data from an existing dataset for classification tasks. GPT3Mix uses GPT-3 with a prompt containing real examples from the dataset and a task specification to create synthetic examples and pseudo-labels jointly. This new aug- mented dataset is then used to fine-tune BERT and DistilBERT models. This method combines data augmentation approaches with knowledge distilla- tion by training smaller classification models using soft labels. consists of synthetic Python textbooks focusing on reasoning, basic algorithmic skills, and synthetic Python exercises. One important finding of this 48 work is that introducing randomness into data gen- eration is crucial, all while ensuring the examples maintain their quality and coherence. Yoo et al. [648] propose GPT3Mix to generate additional synthetic data from an existing dataset for classification tasks. GPT3Mix uses GPT-3 with a prompt containing real examples from the dataset and a task specification to create synthetic examples and pseudo-labels jointly. This new aug- mented dataset is then used to fine-tune BERT and DistilBERT models. This method combines data augmentation approaches with knowledge distilla- tion by training smaller classification models using soft labels. Bonifacio et al. [51] propose InPars, a method for using LLMs to generate synthetic retrieval ex- amples for fine-tuning on information retrieval tasks. GPT-3 is few-shot prompted to generate a rel- evant question for a randomly sampled document along with the question\u2019s associated probability. A smaller monoT5 model is then fine-tuned using this dataset to rank relevant documents for a given question. The fine-tuned model outperforms only pre-trained models but performs worse than models fine-tuned using the existing MS MARCO training dataset [32]. Dai et al. [104] introduce AugGPT, which uses ChatGPT (GPT-3.5) to augment each example in a small base dataset with six additional rephrased synthetic examples. This new augmented dataset is then used to fine-tune a specialized BERT model. This approach outperforms existing augmentation approaches, such as word and character substitu- tion. Finally, instead of generating synthetic data to achieve a specialized task, Shridhar et al. [503] pro- pose Decompositional Distillation, which aims to use synthetic data to replicate in smaller models the multi-step reasoning capabilities, such as CoT, that emerge in larger LLMs. First, GPT-3 is used with a manually designed few-shot prompt to decompose a problem into (sub-question, sub-solution) pairs. This synthetic sub-question dataset is then used to fine-tune a T5 problem decomposer to generate sub-questions. Finally, a GPT-2 problem solver is fine-tuned to provide the sub-solutions to the teacher-generated sub-questions. Overall, while LLM-generated synthetic data can potentially bring significant cost benefits, the greater its role, the higher the potential for it to fail tion. Finally, instead of generating synthetic data to achieve a specialized task, Shridhar et al. [503] pro- pose Decompositional Distillation, which aims to use synthetic data to replicate in smaller models the multi-step reasoning capabilities, such as CoT, that emerge in larger LLMs. First, GPT-3 is used with a manually designed few-shot prompt to decompose a problem into (sub-question, sub-solution) pairs. This synthetic sub-question dataset is then used to fine-tune a T5 problem decomposer to generate sub-questions. Finally, a GPT-2 problem solver is fine-tuned to provide the sub-solutions to the teacher-generated sub-questions. Overall, while LLM-generated synthetic data can potentially bring significant cost benefits, the greater its role, the higher the potential for it to fail to capture the true distribution and potentially leadto model collapse [506]. Hallucinated Distributions [506] Using LLMs for fully synthetic data genera- tion is currently constrained by our inability to verify whether the synthetic data gener- ated is representative of the true distribution in the corresponding real-world data. In cases where the LLM is only used to label existing data [ 583,123] this will likely reduce the risk of generating an unrepresentative training distribution (although hallucinated labels remain an issue). Where the LLM is used to generate (or partially generate) both the input and the tar- get [ 123,104,182,51,503] the issue of halluci- nated distributions becomes potentially significant. 4 Related Work Closest to ours is the concurrent work by Zhao et al. [673] , who provide an extensive survey of large language models and associated topics. Mi- alon et al. [363] focus on surveying augmented language models, i.e., \u201clanguage models with rea- soning skills and the ability to use tools\u201d. Tornede et al. [555] survey LLMs in the context of AutoML methods, highlighting existing methods and chal- lenges in leveraging these for improving LLMs. Tang et al. [539] survey LLM-generated text de- tection techniques. Chang et al. [72] concurrently survey evaluation tasks of LLMs. The literature also contains several previous sur- veys and evaluations specific to individual applica- tion domains that reference LLMs, including: chat- bots [ 345], computational biology [ 558,217], com- puter programming [ 499], medicine [ 381,610,590, 381], law [ 101,531], knowledge work [ 140,621], and reasoning [223]. 5 Conclusion alon et al. [363] focus on surveying augmented language models, i.e., \u201clanguage models with rea- soning skills and the ability to use tools\u201d. Tornede et al. [555] survey LLMs in the context of AutoML methods, highlighting existing methods and chal- lenges in leveraging these for improving LLMs. Tang et al. [539] survey LLM-generated text de- tection techniques. Chang et al. [72] concurrently survey evaluation tasks of LLMs. The literature also contains several previous sur- veys and evaluations specific to individual applica- tion domains that reference LLMs, including: chat- bots [ 345], computational biology [ 558,217], com- puter programming [ 499], medicine [ 381,610,590, 381], law [ 101,531], knowledge work [ 140,621], and reasoning [223]. 5 Conclusion In this work, we identify",
    "40": "several unsolved chal- lenges of large language models, provide an overview of their current applications, and discuss how the former constrain the latter. By highlighting the limitations of existing methods, we hope to fos- ter future research addressing these. We also hope that by providing an overview of the approaches used in different applied areas, we can facilitate the transfer of ideas between domains and target further research. 49 Acknowledgements We thank Abhishek Kumar and Stella Rose Bider- man for fruitful discussions and feedback on the draft. References [1] A blog post detailed a Sam Altman freakout about a huge chips shortage threatening OpenAI. Then it was taken down. [2]Open LLM Leaderboard - a Hugging Face Space by Hug- gingFaceH4. [3] Reproducibility \u2014 PyTorch 2.0 documentation. [4]2023. Negative prompts for text generation. Section: Prompting. [5] 2023. Reproducibility. Page Version ID: 1163331755. [6]A. Abbas, K. Tirumala, D. Simig, S. Ganguli and A. S. Morcos. 2023. Semdedup: Data-efficient learning at web-scale through semantic deduplication. arXiv preprint arXiv:2303.09540 . [7]J. D. Abernethy, A. Agarwal, T. V . Marinov and M. K. War- muth. 2023. A mechanism for sample-efficient in-context learning for sparse retrieval tasks. ArXiv , abs/2305.17040. [8]D. Adiwardana, M.-T. Luong, D. R. So, J. Hall, N. Fiedel, R. Thoppilan, Z. Yang, A. Kulshreshtha et al. 2020. To- wards a human-like open-domain chatbot. arXiv preprint arXiv:2001.09977 . [9]R. Agarwal, M. Schwarzer, P. S. Castro, A. C. Courville and M. Bellemare. 2021. Deep Reinforcement Learning at the Edge of the Statistical Precipice. In Advances in Prompting. [5] 2023. Reproducibility. Page Version ID: 1163331755. [6]A. Abbas, K. Tirumala, D. Simig, S. Ganguli and A. S. Morcos. 2023. Semdedup: Data-efficient learning at web-scale through semantic deduplication. arXiv preprint arXiv:2303.09540 . [7]J. D. Abernethy, A. Agarwal, T. V . Marinov and M. K. War- muth. 2023. A mechanism for sample-efficient in-context learning for sparse retrieval tasks. ArXiv , abs/2305.17040. [8]D. Adiwardana, M.-T. Luong, D. R. So, J. Hall, N. Fiedel, R. Thoppilan, Z. Yang, A. Kulshreshtha et al. 2020. To- wards a human-like open-domain chatbot. arXiv preprint arXiv:2001.09977 . [9]R. Agarwal, M. Schwarzer, P. S. Castro, A. C. Courville and M. Bellemare. 2021. Deep Reinforcement Learning at the Edge of the Statistical Precipice. In Advances in Neural Information Processing Systems , volume 34, pages 29304\u201329320. Curran Associates, Inc. [10] M. Agrawal, S. Hegselmann, H. Lang, Y . Kim and D. Son- tag. 2022. Large language models are zero-shot clinical information extractors. arXiv preprint arXiv:2205.12689 . [11] P. Agrawal, C. Alberti, F. Huot, J. Maynez, J. Ma, S. Ruder, K. Ganchev, D. Das et al. 2022. Qameleon: Multilingual qa with only 5 examples. arXiv preprint arXiv:2211.08264 . [12] G. Aher, R. I. Arriaga and A. T. Kalai. 2022. Using large language models to simulate multiple humans. arXiv preprint arXiv:2208.10264 . [13] O. Ahia, S. Kumar, H. Gonen, J. Kasai, D. R. Mortensen, N. A. Smith and Y . Tsvetkov. 2023. Do all languages cost the same? tokenization in the era of commercial language models. arXiv preprint arXiv:2305.13707 . [14] M. Ahn, A. Brohan, N. Brown, Y . Chebotar, O. Cortes, B. David, C. Finn, K. Gopalakrishnan et al. 2022. Do as i can, not as i say: Grounding language in robotic affor- dances. arXiv preprint arXiv:2204.01691 . [15] J. Ainslie, T. Lei, M. de Jong, S. Onta\u00f1\u00f3n, S. Brahma, Y . Zemlyanskiy, D. Uthus, M. Guo et al. 2023. Colt5: Faster long-range transformers with conditional computa- tion. arXiv preprint arXiv:2303.09752 .[16] E. Aky\u00fcrek, D. Schuurmans, J. Andreas, T. Ma and D. Zhou. 2023. What learning algorithm is in-context learn- ing? investigations with linear models. In The Eleventh International Conference on Learning Representations . [17] L. B. Allal, R. Li, D. Kocetkov, C. Mou, C. Akiki, C. M. Ferrandis, N. Muennighoff, M. Mishra et al. 2023. Santa- coder: don\u2019t reach for the stars! [18] J. Andreas. 2022. Language models as agent models. B. David, C. Finn, K. Gopalakrishnan et al. 2022. Do as i can, not as i say: Grounding language in robotic affor- dances. arXiv preprint arXiv:2204.01691 . [15] J. Ainslie, T. Lei, M. de Jong, S. Onta\u00f1\u00f3n, S. Brahma, Y . Zemlyanskiy, D. Uthus, M. Guo et al. 2023. Colt5: Faster long-range transformers with conditional computa- tion. arXiv preprint arXiv:2303.09752 .[16] E. Aky\u00fcrek, D. Schuurmans, J. Andreas, T. Ma and D. Zhou. 2023. What learning algorithm is in-context learn- ing? investigations with linear models. In The Eleventh International Conference on Learning Representations . [17] L. B. Allal, R. Li, D. Kocetkov, C. Mou, C. Akiki, C. M. Ferrandis, N. Muennighoff, M. Mishra et al. 2023. Santa- coder: don\u2019t reach for the stars! [18] J. Andreas. 2022. Language models as agent models. [19] C. Anil, Y . Wu, A. Andreassen, A. Lewkowycz, V . Misra, V . Ramasesh, A. Slone, G. Gur-Ari et al. 2022. Explor- ing Length Generalization in Large Language Models. ArXiv:2207.04901 [cs]. [20] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri, E. Taropa et al. 2023. Palm 2 techni- cal report. arXiv preprint arXiv:2305.10403 . [21] F. Antaki, S. Touma, D. Milad, J. El-Khoury and R. Duval. 2023. Evaluating the performance of chatgpt in ophthal- mology: An analysis of its successes and shortcomings. medRxiv . [22] L. P. Argyle, E. C. Busby, N. Fulda, J. Gubler, C. Rytting and D. Wingate. 2022. Out of one, many: Using lan- guage models to simulate human samples. arXiv preprint arXiv:2209.06899 . [23] V . Aribandi, Y . Tay, T. Schuster, J. Rao, H. S. Zheng, S. V . Mehta, H. Zhuang, V . Q. Tran et al. 2022. Ext5: Towards extreme multi-task scaling for transfer learning. InInternational Conference on Learning Representations . [24] S. Arora, A. Narayan, M. F. Chen, L. Orr, N. Guha, K. Bhatia, I. Chami, F. Sala et al. 2022. Ask me anything: A simple strategy for prompting language models. [25] A. Asai, T. Schick, P. Lewis, X. Chen, G. Izacard, S. Riedel, H. Hajishirzi and W.-t. Yih. 2022. Task-aware retrieval with instructions. [26] N. Asher, S. Bhar, A. Chaturvedi, J. Hunter and S. Paul. 2023. Limits for Learning with Language Models. ArXiv:2306.12213 [cs]. [27] M. C. Ashton and K. Lee. 2009. The hexaco\u201360: A short measure of the major dimensions of personality. Journal of personality assessment , 91(4):340\u2013345. [28] J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai et al. 2021. Program InInternational Conference on Learning Representations . [24] S. Arora, A. Narayan, M. F. Chen, L. Orr, N. Guha, K. Bhatia, I. Chami, F. Sala et al. 2022. Ask me anything: A simple strategy for prompting language models. [25] A. Asai, T. Schick, P. Lewis, X. Chen, G. Izacard, S. Riedel, H. Hajishirzi and W.-t. Yih. 2022. Task-aware retrieval with instructions. [26] N. Asher, S. Bhar, A. Chaturvedi, J. Hunter and S. Paul. 2023. Limits for Learning with Language Models. ArXiv:2306.12213 [cs]. [27] M. C. Ashton and K. Lee. 2009. The hexaco\u201360: A short measure of the major dimensions of personality. Journal of personality assessment , 91(4):340\u2013345. [28] J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai et al. 2021. Program synthesis with large language models. arXiv preprint arXiv:2108.07732 . [29] AUTOMATIC1111. 2023. Stable Diffusion web UI. Original-date: 2022-08-22T14:05:26Z. [30] J. W. Ayers, A. Poliak, M. Dredze, E. C. Leas, Z. Zhu, J. B. Kelley, D. J. Faix, A. M. Goodman et al. 2023. Comparing physician and artificial intelligence chatbot responses to patient questions posted to a public social media forum. JAMA internal medicine . [31] Y . Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie et al. 2022. Constitu- tional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073 . [32] P. Bajaj, D. Campos, N. Craswell, L.",
    "41": "Deng, J. Gao, X. Liu, R. Majumder, A. McNamara et al. 2018. Ms marco: A human generated machine reading comprehension dataset. 50 [33] P. Bajaj, C. Xiong, G. Ke, X. Liu, D. He, S. Tiwary, T.-Y . Liu, P. Bennett et al. 2022. Metro: Efficient denoising pre- training of large scale autoencoding language models with model generated signals. arXiv preprint arXiv:2204.06644 . [34] A. Bakhtin, S. Gross, M. Ott, Y . Deng, M. Ranzato and A. Szlam. 2019. Real or Fake? Learning to Discriminate Machine from Human Generated Text. ArXiv:1906.03351 [cs, stat]. [35] R. Balestriero, J. Pesenti and Y . LeCun. 2021. Learning in high dimension always amounts to extrapolation. arXiv preprint arXiv:2110.09485 . [36] J. Bandy and N. Vincent. 2021. Addressing \"documenta- tion debt\" in machine learning research: A retrospective datasheet for bookcorpus. [37] P. Barham, A. Chowdhery, J. Dean, S. Ghemawat, S. Hand, D. Hurt, M. Isard, H. Lim et al. 2022. Pathways: Asynchronous distributed dataflow for ml. Proceedings of Machine Learning and Systems , 4:430\u2013449. training of large scale autoencoding language models with model generated signals. arXiv preprint arXiv:2204.06644 . [34] A. Bakhtin, S. Gross, M. Ott, Y . Deng, M. Ranzato and A. Szlam. 2019. Real or Fake? Learning to Discriminate Machine from Human Generated Text. ArXiv:1906.03351 [cs, stat]. [35] R. Balestriero, J. Pesenti and Y . LeCun. 2021. Learning in high dimension always amounts to extrapolation. arXiv preprint arXiv:2110.09485 . [36] J. Bandy and N. Vincent. 2021. Addressing \"documenta- tion debt\" in machine learning research: A retrospective datasheet for bookcorpus. [37] P. Barham, A. Chowdhery, J. Dean, S. Ghemawat, S. Hand, D. Hurt, M. Isard, H. Lim et al. 2022. Pathways: Asynchronous distributed dataflow for ml. Proceedings of Machine Learning and Systems , 4:430\u2013449. [38] M. Bavarian, H. Jun, N. Tezak, J. Schulman, C. McLeavey, J. Tworek and M. Chen. 2022. Efficient training of language models to fill in the middle. arXiv preprint arXiv:2207.14255 . [39] N. Belrose, Z. Furman, L. Smith, D. Halawi, I. Ostrovsky, L. McKinney, S. Biderman and J. Steinhardt. 2023. Elic- iting latent predictions from transformers with the tuned lens. [40] E. Ben Zaken, Y . Goldberg and S. Ravfogel. 2022. Bit- Fit: Simple parameter-efficient fine-tuning for transformer- based masked language-models. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) , pages 1\u20139, Dublin, Ireland. Association for Computational Linguistics. [41] S. Biderman, K. Bicheno and L. Gao. 2022. Datasheet for the pile. arXiv preprint arXiv:2201.07311 . [42] S. Biderman, U. S. Prashanth, L. Sutawika, H. Schoelkopf, Q. Anthony, S. Purohit and E. Raff. 2023. Emergent and Predictable Memorization in Large Language Mod- els. ArXiv:2304.11158 [cs]. [43] S. Biderman and W. J. Scheirer. 2021. Pitfalls in machine learning research: Reexamining the development cycle. [44] S. Biderman, H. Schoelkopf, Q. G. Anthony, H. Bradley, K. O\u2019Brien, E. Hallahan, M. A. Khan, S. Purohit et al. 2023. Pythia: A suite for analyzing large language models across training and scaling. In Proceedings of the 40th International Conference on Machine Learning , volume 202 of Proceedings of Machine Learning Research , pages 2397\u20132430. PMLR. [45] S. R. Biderman. 2023. [...] we aren\u2019t running out of text data any time soon. ml researchers mas- sively underestimate how much text is out there. https://twitter.com/BlancheMinerva/ Q. Anthony, S. Purohit and E. Raff. 2023. Emergent and Predictable Memorization in Large Language Mod- els. ArXiv:2304.11158 [cs]. [43] S. Biderman and W. J. Scheirer. 2021. Pitfalls in machine learning research: Reexamining the development cycle. [44] S. Biderman, H. Schoelkopf, Q. G. Anthony, H. Bradley, K. O\u2019Brien, E. Hallahan, M. A. Khan, S. Purohit et al. 2023. Pythia: A suite for analyzing large language models across training and scaling. In Proceedings of the 40th International Conference on Machine Learning , volume 202 of Proceedings of Machine Learning Research , pages 2397\u20132430. PMLR. [45] S. R. Biderman. 2023. [...] we aren\u2019t running out of text data any time soon. ml researchers mas- sively underestimate how much text is out there. https://twitter.com/BlancheMinerva/ status/1644154144431677442?s=20 . Accessed: 2023-05-28. [46] A. Birhane, V . U. Prabhu and E. Kahembwe. 2021. Mul- timodal datasets: misogyny, pornography, and malignant stereotypes. arXiv preprint arXiv:2110.01963 .[47] S. Black, S. Biderman, E. Hallahan, Q. Anthony, L. Gao, L. Golding, H. He, C. Leahy et al. 2022. Gpt-neox-20b: An open-source autoregressive language model. [48] A. Blair-Stanek, N. Holzenberger and B. Van Durme. 2023. Can gpt-3 perform statutory reasoning? arXiv preprint arXiv:2302.06100 . [49] J. Bommarito, M. Bommarito, D. M. Katz and J. Katz. 2023. Gpt as knowledge worker: A zero-shot evaluation of (ai) cpa capabilities. arXiv preprint arXiv:2301.04408 . [50] M. Bommarito II and D. M. Katz. 2022. Gpt takes the bar exam. arXiv preprint arXiv:2212.14402 . [51] L. Bonifacio, H. Abonizio, M. Fadaee and R. Nogueira. 2022. Inpars: Unsupervised dataset generation for infor- mation retrieval. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval , SIGIR \u201922, page 2387\u20132392, New York, NY , USA. Association for Computing Machinery. [52] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Ruther- ford, K. Millican, G. v. d. Driessche, J.-B. Lespiau et al. 2021. Improving language models by retrieving from tril- lions of tokens. arXiv preprint arXiv:2112.04426 . [53] A. Borji. 2023. A Categorical Archive of ChatGPT Fail- ures. ArXiv:2302.03494 [cs]. [54] A. Borzunov, D. Baranchuk, T. Dettmers, M. Ryabinin, Y . Belkada, A. Chumachenko, P. Samygin and C. Raffel. 2022. Petals: Collaborative inference and fine-tuning of large models. arXiv preprint arXiv:2209.01188 . mation retrieval. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval , SIGIR \u201922, page 2387\u20132392, New York, NY , USA. Association for Computing Machinery. [52] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Ruther- ford, K. Millican, G. v. d. Driessche, J.-B. Lespiau et al. 2021. Improving language models by retrieving from tril- lions of tokens. arXiv preprint arXiv:2112.04426 . [53] A. Borji. 2023. A Categorical Archive of ChatGPT Fail- ures. ArXiv:2302.03494 [cs]. [54] A. Borzunov, D. Baranchuk, T. Dettmers, M. Ryabinin, Y . Belkada, A. Chumachenko, P. Samygin and C. Raffel. 2022. Petals: Collaborative inference and fine-tuning of large models. arXiv preprint arXiv:2209.01188 . [55] L. Bos, C. Schemer, N. Corbu, M. Hameleers, I. An- dreadis, A. Schulz, D. Schmuck, C. Reinemann et al. 2020. The effects of populism as a social identity frame on persua- sion and mobilisation: Evidence from a 15-country experi- ment. European Journal of Political Research , 59(1):3\u201324. [56] D. Britz, M. Y . Guan and M.-T. Luong. 2017. Efficient attention using a fixed-size memory representation. arXiv preprint arXiv:1707.00110 . [57] A. Z. Broder, M. Charikar, A. M. Frieze and M. Mitzen- macher. 1998. Min-wise independent permutations. In Proceedings of the thirtieth annual ACM symposium on Theory of computing , pages 327\u2013336. [58] G. Brown, M. Bun, V . Feldman, A. Smith and K. Talwar. 2021. When is memorization of irrelevant training data necessary for high-accuracy learning? In Proceedings of the 53rd annual ACM SIGACT symposium on theory of computing , pages 123\u2013132. [59] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Ka- plan, P. Dhariwal, A. Neelakantan, P. Shyam et al. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems , volume 33, pages 1877\u20131901. Curran Associates, Inc. [60] M. Brundage, S. Avin, J. Clark, H. Toner, P. Eckersley, B. Garfinkel, A. Dafoe, P. Scharre et al. 2018. The mali- cious use of",
    "42": "artificial intelligence: Forecasting, prevention, and mitigation. arXiv preprint arXiv:1802.07228 . [61] S. Bubeck, V . Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y . T. Lee et al. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4. 51 [62] C. Burns, H. Ye, D. Klein and J. Steinhardt. 2022. Dis- covering latent knowledge in language models without supervision. plan, P. Dhariwal, A. Neelakantan, P. Shyam et al. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems , volume 33, pages 1877\u20131901. Curran Associates, Inc. [60] M. Brundage, S. Avin, J. Clark, H. Toner, P. Eckersley, B. Garfinkel, A. Dafoe, P. Scharre et al. 2018. The mali- cious use of artificial intelligence: Forecasting, prevention, and mitigation. arXiv preprint arXiv:1802.07228 . [61] S. Bubeck, V . Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y . T. Lee et al. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4. 51 [62] C. Burns, H. Ye, D. Klein and J. Steinhardt. 2022. Dis- covering latent knowledge in language models without supervision. [63] A. Calderwood, N. Wardrip-Fruin and M. Mateas. 2022. Spinning coherent interactive fiction through foundation model prompts. International Conference of Computation and Creativity . [64] N. Carlini, M. Jagielski, C. A. Choquette-Choo, D. Paleka, W. Pearce, H. Anderson, A. Terzis, K. Thomas et al. 2023. Poisoning Web-Scale Training Datasets is Practical. ArXiv:2302.10149 [cs]. [65] N. Carlini, C. Liu, \u00da. Erlingsson, J. Kos and D. Song. 2019. The secret sharer: Evaluating and testing unintended memorization in neural networks. In USENIX Security Symposium , volume 267. [66] N. Carlini, M. Nasr, C. A. Choquette-Choo, M. Jagielski, I. Gao, A. Awadalla, P. W. Koh, D. Ippolito et al. 2023. Are aligned neural networks adversarially aligned? [67] N. Carlini, F. Tramer, E. Wallace, M. Jagielski, A. Herbert- V oss, K. Lee, A. Roberts, T. Brown et al. 2020. Extracting training data from large language models. [68] S. Casper, J. Lin, J. Kwon, G. Culp and D. Hadfield- Menell. 2023. Explore, establish, exploit: Red team- ing language models from scratch. arXiv preprint arXiv:2306.09442 . [69] T. Chakrabarty, V . Padmakumar and H. He. 2022. Help me write a poem: Instruction tuning as a vehicle for collab- orative poetry writing. arXiv preprint arXiv:2210.13669 . [70] I. Chalkidis, I. Androutsopoulos and N. Aletras. 2019. Neural legal judgment prediction in english. arXiv preprint arXiv:1906.02059 . [71] I. Chalkidis, M. Fergadiotis, P. Malakasiotis, N. Ale- tras and I. Androutsopoulos. 2020. Legal-bert: The muppets straight out of law school. arXiv preprint arXiv:2010.02559 . [72] Y . Chang, X. Wang, J. Wang, Y . Wu, K. Zhu, H. Chen, L. Yang, X. Yi et al. 2023. A Survey on Evaluation of [68] S. Casper, J. Lin, J. Kwon, G. Culp and D. Hadfield- Menell. 2023. Explore, establish, exploit: Red team- ing language models from scratch. arXiv preprint arXiv:2306.09442 . [69] T. Chakrabarty, V . Padmakumar and H. He. 2022. Help me write a poem: Instruction tuning as a vehicle for collab- orative poetry writing. arXiv preprint arXiv:2210.13669 . [70] I. Chalkidis, I. Androutsopoulos and N. Aletras. 2019. Neural legal judgment prediction in english. arXiv preprint arXiv:1906.02059 . [71] I. Chalkidis, M. Fergadiotis, P. Malakasiotis, N. Ale- tras and I. Androutsopoulos. 2020. Legal-bert: The muppets straight out of law school. arXiv preprint arXiv:2010.02559 . [72] Y . Chang, X. Wang, J. Wang, Y . Wu, K. Zhu, H. Chen, L. Yang, X. Yi et al. 2023. A Survey on Evaluation of Large Language Models. ArXiv:2307.03109 [cs]. [73] B. Chen, X. Cheng, L. ao Gengyang, S. Li, X. Zeng, B. Wang, G. Jing, C. Liu et al. 2023. xtrimopglm: Uni- fied 100b-scale pre-trained transformer for deciphering the language of protein. bioRxiv . [74] C. Chen, S. Borgeaud, G. Irving, J.-B. Lespiau, L. Sifre and J. Jumper. 2023. Accelerating large language model decoding with speculative sampling. arXiv preprint arXiv:2302.01318 . [75] L. Chen, M. Zaharia and J. Zou. 2023. FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance. ArXiv:2305.05176 [cs]. [76] L. Chen, M. Zaharia and J. Zou. 2023. How is ChatGPT\u2019s behavior changing over time? ArXiv:2307.09009 [cs]. [77] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y . Burda et al. 2021. Evaluating large language models trained on code.[78] M. Chen, A. Papangelis, C. Tao, S. Kim, A. Rosenbaum, Y . Liu, Z. Yu and D. Hakkani-Tur. 2023. Places: Prompting language models for social conversation synthesis. arXiv preprint arXiv:2302.03269 . [79] S. Chen, S. Wong, L. Chen and Y . Tian. 2023. Extending context window of large language models via positional interpolation. [80] T. Chen, Z. Zhang, A. Jaiswal, S. Liu and Z. Wang. 2023. Sparse moe as the new dropout: Scaling dense and self- slimmable transformers. [81] X. Chen, M. Lin, N. Sch\u00e4rli and D. Zhou. 2023. Teach- ing large language models to self-debug. arXiv preprint arXiv:2304.05128 . [82] L. Cheng, X. Li and L. Bing. 2023. Is gpt-4 a good data analyst? [83] D. Choe, R. Al-Rfou, M. Guo, H. Lee and N. Constant. 2019. Bridging the Gap for Tokenizer-Free Language Mod- els. ArXiv:1908.10322 [cs]. Y . Liu, Z. Yu and D. Hakkani-Tur. 2023. Places: Prompting language models for social conversation synthesis. arXiv preprint arXiv:2302.03269 . [79] S. Chen, S. Wong, L. Chen and Y . Tian. 2023. Extending context window of large language models via positional interpolation. [80] T. Chen, Z. Zhang, A. Jaiswal, S. Liu and Z. Wang. 2023. Sparse moe as the new dropout: Scaling dense and self- slimmable transformers. [81] X. Chen, M. Lin, N. Sch\u00e4rli and D. Zhou. 2023. Teach- ing large language models to self-debug. arXiv preprint arXiv:2304.05128 . [82] L. Cheng, X. Li and L. Bing. 2023. Is gpt-4 a good data analyst? [83] D. Choe, R. Al-Rfou, M. Guo, H. Lee and N. Constant. 2019. Bridging the Gap for Tokenizer-Free Language Mod- els. ArXiv:1908.10322 [cs]. [84] J. H. Choi, K. E. Hickman, A. Monahan and D. Schwarcz. 2023. Chatgpt goes to law school. Available at SSRN . [85] K. Choromanski, V . Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sarlos, P. Hawkins, J. Davis et al. 2020. Rethinking attention with performers. arXiv preprint arXiv:2009.14794 . [86] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311 . [87] M. Christ, S. Gunn and O. Zamir. 2023. Undetectable Watermarks for Language Models. [88] P. Christiano, J. Leike, T. B. Brown, M. Martic, S. Legg and D. Amodei. 2017. Deep reinforcement learning from human preferences. [89] K. Christianson, A. Hollingworth, J. F. Halliwell and F. Ferreira. 2001. Thematic roles assigned along the garden path linger. Cognitive psychology , 42(4):368\u2013407. [90] H. W. Chung. 2023. Missing model details (tweet). [91] H. W. Chung, X. Garcia, A. Roberts, Y . Tay, O. Firat, S. Narang and N. Constant. 2023. Unimax: Fairer and more effective language sampling for large-scale multilin- gual pretraining. In The Eleventh International Conference on Learning Representations . [92] H. W. Chung, D. Garrette, K. C. Tan and J. Riesa. 2020. Improving multilingual models with language-clustered vo- cabularies. In Proceedings of the 2020 Conference on Em- pirical Methods in Natural Language Processing (EMNLP) , pages 4536\u20134546, Online. Association for Computational Linguistics. [93] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y . Tay, W. Fedus, Y . Li, X. Wang et al. 2022. Scaling instruction- finetuned language models. [94] J. H. Clark, D. Garrette, I. Turc and J. Wieting. 2022. Ca- [91] H. W. Chung, X. Garcia, A. Roberts, Y . Tay, O. Firat, S. Narang and N. Constant. 2023. Unimax: Fairer and more effective language sampling for large-scale multilin- gual pretraining.",
    "43": "In The Eleventh International Conference on Learning Representations . [92] H. W. Chung, D. Garrette, K. C. Tan and J. Riesa. 2020. Improving multilingual models with language-clustered vo- cabularies. In Proceedings of the 2020 Conference on Em- pirical Methods in Natural Language Processing (EMNLP) , pages 4536\u20134546, Online. Association for Computational Linguistics. [93] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y . Tay, W. Fedus, Y . Li, X. Wang et al. 2022. Scaling instruction- finetuned language models. [94] J. H. Clark, D. Garrette, I. Turc and J. Wieting. 2022. Ca- nine: Pre-training an efficient tokenization-free encoder for language representation. Transactions of the Association for Computational Linguistics , 10:73\u201391. 52 [95] K. Cobbe, V . Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek et al. 2021. Training verifiers to solve math word problems. [96] D. Cohen, M. Ryu, Y . Chow, O. Keller, I. Greenberg, A. Hassidim, M. Fink, Y . Matias et al. 2022. Dynamic plan- ning in open-ended dialogue using reinforcement learning. arXiv preprint arXiv:2208.02294 . [97] R. Cohen, M. Hamri, M. Geva and A. Globerson. 2023. LM vs LM: Detecting Factual Errors via Cross Examina- tion. ArXiv:2305.13281 [cs]. [98] T. Computer. 2023. Redpajama: An open source recipe to reproduce llama training dataset. [99] A. Conmy, A. N. Mavor-Parker, A. Lynch, S. Heimer- sheim and A. Garriga-Alonso. 2023. Towards automated circuit discovery for mechanistic interpretability. arXiv preprint arXiv:2304.14997 . [100] A. Conneau, K. Khandelwal, N. Goyal, V . Chaudhary, G. Wenzek, F. Guzm\u00e1n, E. Grave, M. Ott et al. 2020. Unsu- pervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 8440\u20138451, Online. Association for Computational Linguistics. [101] A. B. Cyphert. 2021. A human being wrote this law review article: Gpt-3 and the practice of law. UC Davis L. Rev., 55:401. [102] D. Dai, L. Dong, Y . Hao, Z. Sui, B. Chang and F. Wei. 2022. Knowledge neurons in pretrained transformers. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 8493\u20138502, Dublin, Ireland. Association for Com- putational Linguistics. preprint arXiv:2304.14997 . [100] A. Conneau, K. Khandelwal, N. Goyal, V . Chaudhary, G. Wenzek, F. Guzm\u00e1n, E. Grave, M. Ott et al. 2020. Unsu- pervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 8440\u20138451, Online. Association for Computational Linguistics. [101] A. B. Cyphert. 2021. A human being wrote this law review article: Gpt-3 and the practice of law. UC Davis L. Rev., 55:401. [102] D. Dai, L. Dong, Y . Hao, Z. Sui, B. Chang and F. Wei. 2022. Knowledge neurons in pretrained transformers. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 8493\u20138502, Dublin, Ireland. Association for Com- putational Linguistics. [103] D. Dai, Y . Sun, L. Dong, Y . Hao, Z. Sui and F. Wei. 2022. Why can gpt learn in-context? language models se- cretly perform gradient descent as meta optimizers. arXiv preprint arXiv:2212.10559 . [104] H. Dai, Z. Liu, W. Liao, X. Huang, Z. Wu, L. Zhao, W. Liu, N. Liu et al. 2023. Chataug: Leveraging chatgpt for text data augmentation. [105] Z. Dai, Z. Yang, Y . Yang, J. Carbonell, Q. Le and R. Salakhutdinov. 2019. Transformer-XL: Attentive lan- guage models beyond a fixed-length context. In Proceed- ings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 2978\u20132988, Florence, Italy. Association for Computational Linguistics. [106] H. Dalla-Torre, L. Gonzalez, J. Mendoza Revilla, N. Lopez Carranza, A. Henryk Grywaczewski, F. Oteri, C. Dallago, E. Trop et al. 2023. The nucleotide trans- former: Building and evaluating robust foundation models for human genomics. bioRxiv , pages 2023\u201301. [107] T. Dao, D. Y . Fu, S. Ermon, A. Rudra and C. R\u00e9. 2022. Flashattention: Fast and memory-efficient exact attention with io-awareness. arXiv preprint arXiv:2205.14135 . [108] T. Dao, D. Y . Fu, K. K. Saab, A. W. Thomas, A. Rudra and C. R\u00e9. 2023. Hungry Hungry Hippos: Towards Language Modeling with State Space Models. ArXiv:2212.14052 [cs].[109] S. Dathathri, A. Madotto, J. Lan, J. Hung, E. Frank, P. Molino, J. Yosinski and R. Liu. 2020. Plug and play language models: A simple approach to controlled text generation. [110] J. Dauparas, I. Anishchenko, N. Bennett, H. Bai, R. J. Ragotte, L. F. Milles, B. I. M. Wicky, A. Courbet et al. 2022. Robust deep learning&#x2013;based protein sequence de- sign using proteinmpnn. Science , 378(6615):49\u201356. for human genomics. bioRxiv , pages 2023\u201301. [107] T. Dao, D. Y . Fu, S. Ermon, A. Rudra and C. R\u00e9. 2022. Flashattention: Fast and memory-efficient exact attention with io-awareness. arXiv preprint arXiv:2205.14135 . [108] T. Dao, D. Y . Fu, K. K. Saab, A. W. Thomas, A. Rudra and C. R\u00e9. 2023. Hungry Hungry Hippos: Towards Language Modeling with State Space Models. ArXiv:2212.14052 [cs].[109] S. Dathathri, A. Madotto, J. Lan, J. Hung, E. Frank, P. Molino, J. Yosinski and R. Liu. 2020. Plug and play language models: A simple approach to controlled text generation. [110] J. Dauparas, I. Anishchenko, N. Bennett, H. Bai, R. J. Ragotte, L. F. Milles, B. I. M. Wicky, A. Courbet et al. 2022. Robust deep learning&#x2013;based protein sequence de- sign using proteinmpnn. Science , 378(6615):49\u201356. [111] N. De Cao, W. Aziz and I. Titov. 2021. Editing fac- tual knowledge in language models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Lan- guage Processing , pages 6491\u20136506, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. [112] M. Dehghani, A. Arnab, L. Beyer, A. Vaswani and Y . Tay. 2022. The Efficiency Misnomer. ArXiv:2110.12894 [cs, stat]. [113] M. Dehghani, Y . Tay, A. A. Gritsenko, Z. Zhao, N. Houlsby, F. Diaz, D. Metzler and O. Vinyals. 2021. The benchmark lottery. arXiv preprint arXiv:2107.07002 . [114] L. Del Corro, A. Del Giorno, S. Agarwal, B. Yu, A. Awadallah and S. Mukherjee. 2023. SkipDecode: Au- toregressive Skip Decoding with Batching and Caching for Efficient LLM Inference. ArXiv:2307.02628 [cs]. [115] A. Deroy, K. Ghosh and S. Ghosh. 2023. How ready are pre-trained abstractive models and llms for legal case judgement summarization? [116] A. Deshpande, V . Murahari, T. Rajpurohit, A. Kalyan and K. Narasimhan. 2023. Toxicity in chatgpt: Analyz- ing persona-assigned language models. arXiv preprint arXiv:2304.05335 . [117] T. Dettmers, M. Lewis, Y . Belkada and L. Zettlemoyer. 2022. Llm.int8(): 8-bit matrix multiplication for transform- ers at scale. [118] T. Dettmers, A. Pagnoni, A. Holtzman and L. Zettle- moyer. 2023. QLoRA: Efficient Finetuning of Quantized LLMs. ArXiv:2305.14314 [cs]. [119] T. Dettmers, R. Svirschevski, V . Egiazarian, D. Kuznedelev, E. Frantar, S. Ashkboos, A. Borzunov, T. Hoefler et al. 2023. Spqr: A sparse-quantized represen- tation for near-lossless llm weight compression. arXiv preprint arXiv:2306.03078 . [120] J. Devlin, M.-W. Chang, K. Lee and K. Toutanova. 2019. judgement summarization? [116] A. Deshpande, V . Murahari, T. Rajpurohit, A. Kalyan and K. Narasimhan. 2023. Toxicity in chatgpt: Analyz- ing persona-assigned language models. arXiv preprint arXiv:2304.05335 . [117] T. Dettmers, M. Lewis, Y . Belkada and L. Zettlemoyer. 2022. Llm.int8(): 8-bit matrix multiplication for transform- ers at scale. [118] T. Dettmers, A. Pagnoni, A. Holtzman and L. Zettle- moyer. 2023. QLoRA: Efficient Finetuning of Quantized LLMs. ArXiv:2305.14314 [cs]. [119] T. Dettmers, R. Svirschevski, V . Egiazarian, D. Kuznedelev, E. Frantar, S. Ashkboos, A. Borzunov, T. Hoefler et al. 2023. Spqr: A sparse-quantized represen- tation for near-lossless llm weight compression.",
    "44": "arXiv preprint arXiv:2306.03078 . [120] J. Devlin, M.-W. Chang, K. Lee and K. Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Con- ference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, Volume 1 (Long and Short Papers) , pages 4171\u2013 4186, Minneapolis, Minnesota. Association for Computa- tional Linguistics. [121] N. Dey, G. Gosal, Zhiming, Chen, H. Khachane, W. Mar- shall, R. Pathria, M. Tom et al. 2023. Cerebras-gpt: Open compute-optimal language models trained on the cerebras wafer-scale cluster. [122] S. Diao, X. Li, Y . Lin, Z. Huang and T. Zhang. 2022. Black-box prompt learning for pre-trained language mod- els.arXiv preprint arXiv:2201.08531 . 53 [123] B. Ding, C. Qin, L. Liu, L. Bing, S. Joty and B. Li. 2022. Is gpt-3 a good data annotator? arXiv preprint arXiv:2212.10450 . [124] J. Ding, S. Ma, L. Dong, X. Zhang, S. Huang, W. Wang and F. Wei. 2023. Longnet: Scaling transformers to 1,000,000,000 tokens. [125] J. Dodge, M. Sap, A. Marasovi \u00b4c, W. Agnew, G. Il- harco, D. Groeneveld, M. Mitchell and M. Gardner. 2021. Documenting large webtext corpora: A case study on the colossal clean crawled corpus. arXiv preprint arXiv:2104.08758 . [126] R. Dominguez-Olmedo, M. Hardt and C. Mendler- D\u00fcnner. 2023. Questioning the survey responses of large language models. arXiv preprint arXiv:2306.07951 . [127] Q. Dong, D. Dai, Y . Song, J. Xu, Z. Sui and L. Li. 2022. Calibrating factual knowledge in pretrained language mod- els. In Findings of the Association for Computational Linguistics: EMNLP 2022 , pages 5937\u20135947, Abu Dhabi, United Arab Emirates. Association for Computational Lin- guistics. and F. Wei. 2023. Longnet: Scaling transformers to 1,000,000,000 tokens. [125] J. Dodge, M. Sap, A. Marasovi \u00b4c, W. Agnew, G. Il- harco, D. Groeneveld, M. Mitchell and M. Gardner. 2021. Documenting large webtext corpora: A case study on the colossal clean crawled corpus. arXiv preprint arXiv:2104.08758 . [126] R. Dominguez-Olmedo, M. Hardt and C. Mendler- D\u00fcnner. 2023. Questioning the survey responses of large language models. arXiv preprint arXiv:2306.07951 . [127] Q. Dong, D. Dai, Y . Song, J. Xu, Z. Sui and L. Li. 2022. Calibrating factual knowledge in pretrained language mod- els. In Findings of the Association for Computational Linguistics: EMNLP 2022 , pages 5937\u20135947, Abu Dhabi, United Arab Emirates. Association for Computational Lin- guistics. [128] D. R. Dowty, R. Wall and S. Peters. 2012. Introduction to Montague semantics , volume 11. Springer Science & Business Media. [129] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson et al. 2023. Palm-e: An embodied multimodal language model. arXiv preprint arXiv:2303.03378 . [130] N. Du, Y . Huang, A. M. Dai, S. Tong, D. Lepikhin, Y . Xu, M. Krikun, Y . Zhou et al. 2022. Glam: Efficient scaling of language models with mixture-of-experts. In Interna- tional Conference on Machine Learning , pages 5547\u20135569. PMLR. [131] Y . Du, S. Li, A. Torralba, J. B. Tenenbaum and I. Mordatch. 2023. Improving Factuality and Reason- ing in Language Models through Multiagent Debate. ArXiv:2305.14325 [cs]. [132] Z. Du, Y . Qian, X. Liu, M. Ding, J. Qiu, Z. Yang and J. Tang. 2022. GLM: General language model pretrain- ing with autoregressive blank infilling. In Proceedings of the 60th Annual Meeting of the Association for Com- putational Linguistics (Volume 1: Long Papers) , pages 320\u2013335, Dublin, Ireland. Association for Computational Linguistics. [133] A. Dunn, J. Dagdelen, N. Walker, S. Lee, A. S. Rosen, G. Ceder, K. Persson and A. Jain. 2022. Struc- tured information extraction from complex scientific text with fine-tuned large language models. arXiv preprint arXiv:2212.05238 . [134] D. Duong and B. D. Solomon. 2023. Analysis of large- language model versus human performance for genetics questions. European Journal of Human Genetics , pages 1\u20133. [135] N. Dziri, X. Lu, M. Sclar, X. L. Li, L. Jiang, B. Y . Lin, P. West, C. Bhagavatula et al. 2023. Faith and Fate: Limits of Transformers on Compositionality. ArXiv:2305.18654 [cs]. of the 60th Annual Meeting of the Association for Com- putational Linguistics (Volume 1: Long Papers) , pages 320\u2013335, Dublin, Ireland. Association for Computational Linguistics. [133] A. Dunn, J. Dagdelen, N. Walker, S. Lee, A. S. Rosen, G. Ceder, K. Persson and A. Jain. 2022. Struc- tured information extraction from complex scientific text with fine-tuned large language models. arXiv preprint arXiv:2212.05238 . [134] D. Duong and B. D. Solomon. 2023. Analysis of large- language model versus human performance for genetics questions. European Journal of Human Genetics , pages 1\u20133. [135] N. Dziri, X. Lu, M. Sclar, X. L. Li, L. Jiang, B. Y . Lin, P. West, C. Bhagavatula et al. 2023. Faith and Fate: Limits of Transformers on Compositionality. ArXiv:2305.18654 [cs]. [136] N. Dziri, A. Madotto, O. Zaiane and A. J. Bose. 2021. Neural Path Hunter: Reducing Hallucination in Dialogue Systems via Path Grounding. ArXiv:2104.08455 [cs].[137] E.-M. El-Mhamdi, S. Farhadkhani, R. Guerraoui, N. Gupta, L.-N. Hoang, R. Pinot, S. Rouault and J. Stephan. 2023. On the Impossible Safety of Large AI Models. ArXiv:2209.15259 [cs]. [138] N. Elhage, N. Nanda, C. Olsson, T. Henighan, N. Joseph, B. Mann, A. Askell, Y . Bai et al. 2021. A mathematical framework for transformer circuits. Transformer Circuits Thread . [139] A. Elnaggar, M. Heinzinger, C. Dallago, G. Rihawi, Y . Wang, L. Jones, T. Gibbs, T. Feher et al. 2020. Prottrans: towards cracking the language of life\u2019s code through self- supervised deep learning and high performance computing. arXiv preprint arXiv:2007.06225 . [140] T. Eloundou, S. Manning, P. Mishkin and D. Rock. 2023. Gpts are gpts: An early look at the labor market impact potential of large language models. [141] F. Faal, K. Schmitt and J. Y . Yu. 2023. Reward model- ing for mitigating toxicity in transformer-based language models. Applied Intelligence , 53(7):8421\u20138435. [142] A. Fan, C. Gardent, C. Braud and A. Bordes. 2021. Aug- menting transformers with KNN-based composite memory for dialog. Transactions of the Association for Computa- tional Linguistics , 9:82\u201399. [143] A. Fan, E. Grave and A. Joulin. 2020. Reducing trans- former depth on demand with structured dropout. In Inter- national Conference on Learning Representations . [144] M. Fathi, J. Pilault, P.-L. Bacon, C. Pal, O. Fi- rat and R. Goroshin. 2023. Block-State Transformer. ArXiv:2306.09539 [cs]. [145] W. Fedus, B. Zoph and N. Shazeer. 2021. Switch trans- potential of large language models. [141] F. Faal, K. Schmitt and J. Y . Yu. 2023. Reward model- ing for mitigating toxicity in transformer-based language models. Applied Intelligence , 53(7):8421\u20138435. [142] A. Fan, C. Gardent, C. Braud and A. Bordes. 2021. Aug- menting transformers with KNN-based composite memory for dialog. Transactions of the Association for Computa- tional Linguistics , 9:82\u201399. [143] A. Fan, E. Grave and A. Joulin. 2020. Reducing trans- former depth on demand with structured dropout. In Inter- national Conference on Learning Representations . [144] M. Fathi, J. Pilault, P.-L. Bacon, C. Pal, O. Fi- rat and R. Goroshin. 2023. Block-State Transformer. ArXiv:2306.09539 [cs]. [145] W. Fedus, B. Zoph and N. Shazeer. 2021. Switch trans- formers: Scaling to trillion parameter models with simple and efficient sparsity. [146] V . Feldman. 2020. Does learning require memorization? a short tale about a long tail. In Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing , pages 954\u2013959. [147] S. Feng, C. Y . Park, Y . Liu and Y . Tsvetkov. 2023. From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models. ArXiv:2305.08283 [cs]. [148] W. Feng, W. Zhu, T.-j. Fu, V . Jampani, A. Akula, X. He, S. Basu, X. E. Wang et al.",
    "45": "2023. LayoutGPT: Compo- sitional Visual Planning and Generation with Large Lan- guage Models. ArXiv:2305.15393 [cs]. [149] E. Ferrara. 2023. Should chatgpt be biased? challenges and risks of bias in large language models. arXiv preprint arXiv:2304.03738 . [150] A. Ficek, F. Liu and N. Collier. 2022. How to tackle an emerging topic? combining strong and weak labels for covid news NER. In Proceedings of the 2nd Confer- ence of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 2: Short Papers) , pages 488\u2013496, Online only. Association for Computational Linguistics. 54 [151] C. Fourrier, N. Habib, J. Launay and T. Wolf. 2023. What\u2019s going on with the open llm leader- board? Available from: https://huggingface. co/blog/evaluating-mmlu-leaderboard . Ac- cessed: 27/06/2023. [152] E. Frantar and D. Alistarh. 2023. Massive language mod- els can be accurately pruned in one-shot. arXiv preprint arXiv:2301.00774 . [153] E. Frantar, S. Ashkboos, T. Hoefler and D. Alis- tarh. 2022. Gptq: Accurate post-training quantization for covid news NER. In Proceedings of the 2nd Confer- ence of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 2: Short Papers) , pages 488\u2013496, Online only. Association for Computational Linguistics. 54 [151] C. Fourrier, N. Habib, J. Launay and T. Wolf. 2023. What\u2019s going on with the open llm leader- board? Available from: https://huggingface. co/blog/evaluating-mmlu-leaderboard . Ac- cessed: 27/06/2023. [152] E. Frantar and D. Alistarh. 2023. Massive language mod- els can be accurately pruned in one-shot. arXiv preprint arXiv:2301.00774 . [153] E. Frantar, S. Ashkboos, T. Hoefler and D. Alis- tarh. 2022. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323 . [154] D. Fried, A. Aghajanyan, J. Lin, S. Wang, E. Wallace, F. Shi, R. Zhong, W.-t. Yih et al. 2022. Incoder: A genera- tive model for code infilling and synthesis. [155] A. Fr\u00f6mmgen and L. Kharatyan. 2023. Resolv- ing code review comments with ml. Available from: https://ai.googleblog.com/2023/05/ resolving-code-review-comments-with-ml. html . Accessed: 26/06/2023. [156] J. Fu, S.-K. Ng, Z. Jiang and P. Liu. 2023. Gptscore: Evaluate as you desire. arXiv preprint arXiv:2302.04166 . [157] T. Fujii, K. Shibata, A. Yamaguchi, T. Morishita and Y . Sogawa. 2023. How do different tokenizers perform on downstream tasks in scriptio continua languages?: A case study in japanese. arXiv preprint arXiv:2306.09572 . [158] I. Gabriel. 2020. Artificial intelligence, values, and align- ment. Minds and machines , 30(3):411\u2013437. [159] S. Gadgil, A. R. Tadipatri, A. Agrawal, A. Narayanan and N. Goyal. 2022. Towards automating formalisation of theorem statements using large language models. 36th Conference on Neural Information Processing Systems (NeurIPS 2022) Workshop on MATH-AI . [160] T. Gale, D. Narayanan, C. Young and M. Zaharia. 2022. Megablocks: Efficient sparse training with mixture-of- experts. arXiv preprint arXiv:2211.15841 . [161] T. Gale, M. Zaharia, C. Young and E. Elsen. 2020. Sparse GPU Kernels for Deep Learning. ArXiv:2006.10901 [cs, stat]. [162] K. Gandhi, J.-P. Fr\u00e4nken, T. Gerstenbrg and N. D. Goodman. 2023. Understanding social reasoning in lan- guage models with language models. arXiv preprint arXiv:2306.15448 . [163] D. Ganguli, L. Lovitt, J. Kernion, A. Askell, Y . Bai, S. Kadavath, B. Mann, E. Perez et al. 2022. Red and N. Goyal. 2022. Towards automating formalisation of theorem statements using large language models. 36th Conference on Neural Information Processing Systems (NeurIPS 2022) Workshop on MATH-AI . [160] T. Gale, D. Narayanan, C. Young and M. Zaharia. 2022. Megablocks: Efficient sparse training with mixture-of- experts. arXiv preprint arXiv:2211.15841 . [161] T. Gale, M. Zaharia, C. Young and E. Elsen. 2020. Sparse GPU Kernels for Deep Learning. ArXiv:2006.10901 [cs, stat]. [162] K. Gandhi, J.-P. Fr\u00e4nken, T. Gerstenbrg and N. D. Goodman. 2023. Understanding social reasoning in lan- guage models with language models. arXiv preprint arXiv:2306.15448 . [163] D. Ganguli, L. Lovitt, J. Kernion, A. Askell, Y . Bai, S. Kadavath, B. Mann, E. Perez et al. 2022. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. arXiv preprint arXiv:2209.07858 . [164] J. Gao, X. Ding, B. Qin and T. Liu. 2023. Is chatgpt a good causal reasoner? a comprehensive evaluation. arXiv preprint arXiv:2305.07375 . [165] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He et al. 2020. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027 . [166] L. Gao, J. Tow, S. Biderman, S. Black, A. DiPofi, C. Fos- ter, L. Golding, J. Hsu et al. 2021. A framework for few- shot language model evaluation.[167] S. Gehman, S. Gururangan, M. Sap, Y . Choi and N. A. Smith. 2020. Realtoxicityprompts: Evaluating neural toxic degeneration in language models. arXiv preprint arXiv:2009.11462 . [168] S. Gehrmann, H. Strobelt and A. M. Rush. 2019. GLTR: Statistical Detection and Visualization of Generated Text. ArXiv:1906.04043 [cs]. [169] R. Geirhos, J.-H. Jacobsen, C. Michaelis, R. Zemel, W. Brendel, M. Bethge and F. A. Wichmann. 2020. Short- cut learning in deep neural networks. Nature Machine Intelligence , 2(11):665\u2013673. [170] A. Glaese, N. McAleese, M. Tr\u02db ebacz, J. Aslanides, V . Firoiu, T. Ewalds, M. Rauh, L. Weidinger et al. 2022. Improving alignment of dialogue agents via targeted human judgements. [171] D. Goldberg. 1991. What every computer scientist should know about floating-point arithmetic. ACM Com- puting Surveys , 23(1):5\u201348. [172] A. N. Gomez, O. Key, K. Perlin, S. Gou, N. Frosst, J. Dean and Y . Gal. 2022. Interlocking backpropagation: Improving depthwise model-parallelism. The Journal of Machine Learning Research , 23(1):7714\u20137741. ArXiv:1906.04043 [cs]. [169] R. Geirhos, J.-H. Jacobsen, C. Michaelis, R. Zemel, W. Brendel, M. Bethge and F. A. Wichmann. 2020. Short- cut learning in deep neural networks. Nature Machine Intelligence , 2(11):665\u2013673. [170] A. Glaese, N. McAleese, M. Tr\u02db ebacz, J. Aslanides, V . Firoiu, T. Ewalds, M. Rauh, L. Weidinger et al. 2022. Improving alignment of dialogue agents via targeted human judgements. [171] D. Goldberg. 1991. What every computer scientist should know about floating-point arithmetic. ACM Com- puting Surveys , 23(1):5\u201348. [172] A. N. Gomez, O. Key, K. Perlin, S. Gou, N. Frosst, J. Dean and Y . Gal. 2022. Interlocking backpropagation: Improving depthwise model-parallelism. The Journal of Machine Learning Research , 23(1):7714\u20137741. [173] L. Gong, D. He, Z. Li, T. Qin, L. Wang and T. Liu. 2019. Efficient training of BERT by progressively stacking. InProceedings of the 36th International Conference on Machine Learning , volume 97 of Proceedings of Machine Learning Research , pages 2337\u20132346. PMLR. [174] Z. Gou, Z. Shao, Y . Gong, Y . Shen, Y . Yang, N. Duan and W. Chen. 2023. Critic: Large language models can self-correct with tool-interactive critiquing. arXiv preprint arXiv:2305.11738 . [175] K. Greshake, S. Abdelnabi, S. Mishra, C. Endres, T. Holz and M. Fritz. 2023. More than you\u2019ve asked for: A comprehensive analysis of novel prompt injection threats to application-integrated large language models. arXiv preprint arXiv:2302.12173 . [176] L. D. Griffin, B. Kleinberg, M. Mozes, K. T. Mai, M. Vau, M. Caldwell and A. Marvor-Parker. 2023. Susceptibil- ity to influence of large language models. arXiv preprint arXiv:2303.06074 . [177] Y . Gu, R. Tinn, H. Cheng, M. Lucas, N. Usuyama, X. Liu, T. Naumann, J. Gao et al. 2021. Domain-specific language model pretraining for biomedical natural language pro- cessing. ACM Transactions on Computing for Healthcare (HEALTH) , 3(1):1\u201323. [178] Y . Gu,",
    "46": "S. Zhang, N. Usuyama, Y . Woldesenbet, C. Wong, P. Sanapathi, M. Wei, N. Valluri et al. 2023. Distilling large language models for biomedical knowledge extraction: A case study on adverse drug events. [179] Y . Gu, X. Han, Z. Liu and M. Huang. 2022. PPT: Pre-trained prompt tuning for few-shot learning. In Pro- ceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 8410\u20138423, Dublin, Ireland. Association for Com- putational Linguistics. [180] A. Gudibande, E. Wallace, C. Snell, X. Geng, H. Liu, T. Naumann, J. Gao et al. 2021. Domain-specific language model pretraining for biomedical natural language pro- cessing. ACM Transactions on Computing for Healthcare (HEALTH) , 3(1):1\u201323. [178] Y . Gu, S. Zhang, N. Usuyama, Y . Woldesenbet, C. Wong, P. Sanapathi, M. Wei, N. Valluri et al. 2023. Distilling large language models for biomedical knowledge extraction: A case study on adverse drug events. [179] Y . Gu, X. Han, Z. Liu and M. Huang. 2022. PPT: Pre-trained prompt tuning for few-shot learning. In Pro- ceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 8410\u20138423, Dublin, Ireland. Association for Com- putational Linguistics. [180] A. Gudibande, E. Wallace, C. Snell, X. Geng, H. Liu, P. Abbeel, S. Levine and D. Song. 2023. The false promise of imitating proprietary llms. arXiv preprint arXiv:2305.15717 . 55 [181] J. P. Guilford. 1967. Creativity: Yesterday, today and tomorrow. The Journal of Creative Behavior , 1(1):3\u201314. [182] S. Gunasekar, Y . Zhang, J. Aneja, C. C. T. Mendes, A. D. Giorno, S. Gopi, M. Javaheripi, P. Kauffmann et al. 2023. Textbooks are all you need. [183] M. Guo, J. Ainslie, D. Uthus, S. Ontanon, J. Ni, Y .-H. Sung and Y . Yang. 2022. LongT5: Efficient text-to-text transformer for long sequences. In Findings of the Associ- ation for Computational Linguistics: NAACL 2022 , pages 724\u2013736, Seattle, United States. Association for Computa- tional Linguistics. [184] A. Gupta. 2023. Probing Quantifier Comprehension in Large Language Models. ArXiv:2306.07384 [cs]. [185] T. Gupta and A. Kembhavi. 2022. Visual programming: Compositional visual reasoning without training. [186] K. Guu, K. Lee, Z. Tung, P. Pasupat and M. Chang. 2020. Retrieval augmented language model pre-training. InInternational Conference on Machine Learning , pages 3929\u20133938. PMLR. [187] J. Haase and P. H. P. Hanel. 2023. Artificial muses: Gen- erative artificial intelligence chatbots have risen to human- level creativity. [188] M. Hahn and N. Goyal. 2023. A theory of emergent in-context learning as implicit structure induction. ArXiv , abs/2303.07971. [189] S. Hamilton. 2023. Blind judgement: Agent-based supreme court modelling with gpt. arXiv preprint arXiv:2301.05327 . [190] C. Han, Z. Wang, H. Zhao and H. Ji. 2023. In-context learning of large language models explained as kernel re- gression. ArXiv , abs/2305.12766. [191] T. Hartvigsen, S. Sankaranarayanan, H. Palangi, Y . Kim [186] K. Guu, K. Lee, Z. Tung, P. Pasupat and M. Chang. 2020. Retrieval augmented language model pre-training. InInternational Conference on Machine Learning , pages 3929\u20133938. PMLR. [187] J. Haase and P. H. P. Hanel. 2023. Artificial muses: Gen- erative artificial intelligence chatbots have risen to human- level creativity. [188] M. Hahn and N. Goyal. 2023. A theory of emergent in-context learning as implicit structure induction. ArXiv , abs/2303.07971. [189] S. Hamilton. 2023. Blind judgement: Agent-based supreme court modelling with gpt. arXiv preprint arXiv:2301.05327 . [190] C. Han, Z. Wang, H. Zhao and H. Ji. 2023. In-context learning of large language models explained as kernel re- gression. ArXiv , abs/2305.12766. [191] T. Hartvigsen, S. Sankaranarayanan, H. Palangi, Y . Kim and M. Ghassemi. 2022. Aging with grace: Lifelong model editing with discrete key-value adaptors. arXiv preprint arXiv:2211.11031 . [192] A. Haviv, O. Ram, O. Press, P. Izsak and O. Levy. 2022. Transformer language models without positional encodings still learn positional information. In Findings of the Associ- ation for Computational Linguistics: EMNLP 2022 , pages 1382\u20131390, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. [193] J. Hazell. 2023. Large language models can be used to effectively scale spear phishing campaigns. arXiv preprint arXiv:2305.06972 . [194] E. L. Henderson, S. J. Westwood and D. J. Simons. 2022. A reproducible systematic map of research on the illusory truth effect. Psychonomic Bulletin & Review , pages 1\u201324. [195] P. Henderson, M. S. Krass, L. Zheng, N. Guha, C. D. Manning, D. Jurafsky and D. E. Ho. 2022. Pile of law: Learning responsible data filtering from the law and a 256GB open-source legal dataset. In Thirty-sixth Confer- ence on Neural Information Processing Systems Datasets and Benchmarks Track . [196] D. Hendrycks, C. Burns, S. Basart, A. Critch, J. Li, D. Song and J. Steinhardt. 2020. Aligning ai with shared human values. arXiv preprint arXiv:2008.02275 .[197] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song and J. Steinhardt. 2021. Measuring massive multi- task language understanding. [198] D. Hendrycks, N. Carlini, J. Schulman and J. Steinhardt. 2021. Unsolved problems in ml safety. arXiv preprint arXiv:2109.13916 . [199] D. Hendrycks and M. Mazeika. 2022. X-risk analysis for ai research. arXiv preprint arXiv:2206.05862 . [200] D. Hernandez, T. Brown, T. Conerly, N. DasSarma, Learning responsible data filtering from the law and a 256GB open-source legal dataset. In Thirty-sixth Confer- ence on Neural Information Processing Systems Datasets and Benchmarks Track . [196] D. Hendrycks, C. Burns, S. Basart, A. Critch, J. Li, D. Song and J. Steinhardt. 2020. Aligning ai with shared human values. arXiv preprint arXiv:2008.02275 .[197] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song and J. Steinhardt. 2021. Measuring massive multi- task language understanding. [198] D. Hendrycks, N. Carlini, J. Schulman and J. Steinhardt. 2021. Unsolved problems in ml safety. arXiv preprint arXiv:2109.13916 . [199] D. Hendrycks and M. Mazeika. 2022. X-risk analysis for ai research. arXiv preprint arXiv:2206.05862 . [200] D. Hernandez, T. Brown, T. Conerly, N. DasSarma, D. Drain, S. El-Showk, N. Elhage, Z. Hatfield-Dodds et al. 2022. Scaling laws and interpretability of learning from repeated data. arXiv preprint arXiv:2205.10487 . [201] J. Hestness, S. Narang, N. Ardalani, G. Diamos, H. Jun, H. Kianinejad, M. Patwary, M. Ali et al. 2017. Deep learning scaling is predictable, empirically. arXiv preprint arXiv:1712.00409 . [202] B. L. Hie, V . R. Shanker, D. Xu, T. U. Bruun, P. A. Weidenbacher, S. Tang, W. Wu, J. E. Pak et al. 2023. Effi- cient evolution of human antibodies from general protein language models. Nature Biotechnology . [203] P. Hingston and M. Preuss. 2011. Red teaming with coevolution. In 2011 IEEE Congress of Evolutionary Com- putation (CEC) , pages 1155\u20131163. IEEE. [204] J. Ho and T. Salimans. 2022. Classifier-free diffusion guidance. [205] J. Hoelscher-Obermaier, J. Persson, E. Kran, I. Kon- stas and F. Barez. 2023. Detecting Edit Failures In Large Language Models: An Improved Specificity Benchmark. ArXiv:2305.17553 [cs]. [206] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. de las Casas, L. A. Hendricks et al. 2022. An empirical analysis of compute-optimal large language model training. In Advances in Neural Information Processing Systems . [207] A. Holtzman, J. Buys, L. Du, M. Forbes and Y . Choi. 2020. The curious case of neural text degeneration. In International Conference on Learning Representations . [208] N. Holzenberger, A. Blair-Stanek and B. Van Durme. 2020. A dataset for statutory reasoning in tax law entailment and question answering. arXiv preprint arXiv:2005.05257 . [209] O. Honovich, T. Scialom, O. Levy and T. Schick. 2022. stas and F. Barez. 2023. Detecting Edit",
    "47": "Failures In Large Language Models: An Improved Specificity Benchmark. ArXiv:2305.17553 [cs]. [206] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. de las Casas, L. A. Hendricks et al. 2022. An empirical analysis of compute-optimal large language model training. In Advances in Neural Information Processing Systems . [207] A. Holtzman, J. Buys, L. Du, M. Forbes and Y . Choi. 2020. The curious case of neural text degeneration. In International Conference on Learning Representations . [208] N. Holzenberger, A. Blair-Stanek and B. Van Durme. 2020. A dataset for statutory reasoning in tax law entailment and question answering. arXiv preprint arXiv:2005.05257 . [209] O. Honovich, T. Scialom, O. Levy and T. Schick. 2022. Unnatural instructions: Tuning language models with (al- most) no human labor. arXiv preprint arXiv:2212.09689 . [210] S. Hooker. 2021. The hardware lottery. Communications of the ACM , 64(12):58\u201365. [211] J. J. Horton. 2023. Large language models as simulated economic agents: What can we learn from homo silicus? arXiv preprint arXiv:2301.07543 . [212] M. Horton, S. Mehta, A. Farhadi and M. Rastegari. 2023. Bytes Are All You Need: Transformers Operating Directly On File Bytes. ArXiv:2306.00238 [cs]. [213] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe, A. Gesmundo, M. Attariyan and S. Gelly. 2019. Parameter-efficient transfer learning for nlp. In International Conference on Machine Learning , pages 2790\u20132799. PMLR. 56 [214] D. Houser and K. McCabe. 2014. Experimental eco- nomics and experimental game theory. In Neuroeconomics , pages 19\u201334. Elsevier. [215] J. Howard and S. Ruder. 2018. Universal language model fine-tuning for text classification. In Proceedings of the 56th Annual Meeting of the Association for Com- putational Linguistics (Volume 1: Long Papers) , pages 328\u2013339, Melbourne, Australia. Association for Computa- tional Linguistics. [216] S. Hsiao. 2023. What\u2019s ahead for bard: More global, more visual, more integrated. Available from: https://blog.google/technology/ai/ google-bard-updates-io-2023/ . Accessed: 28/06/2023. [217] B. Hu, J. Xia, J. Zheng, C. Tan, Y . Huang, Y . Xu and S. Z. Li. 2022. Protein language models and structure prediction: Connection and progression. [218] E. J. Hu, Y . Shen, P. Wallis, Z. Allen-Zhu, Y . Li, S. Wang, L. Wang and W. Chen. 2021. Lora: Low-rank adaptation of large language models. [215] J. Howard and S. Ruder. 2018. Universal language model fine-tuning for text classification. In Proceedings of the 56th Annual Meeting of the Association for Com- putational Linguistics (Volume 1: Long Papers) , pages 328\u2013339, Melbourne, Australia. Association for Computa- tional Linguistics. [216] S. Hsiao. 2023. What\u2019s ahead for bard: More global, more visual, more integrated. Available from: https://blog.google/technology/ai/ google-bard-updates-io-2023/ . Accessed: 28/06/2023. [217] B. Hu, J. Xia, J. Zheng, C. Tan, Y . Huang, Y . Xu and S. Z. Li. 2022. Protein language models and structure prediction: Connection and progression. [218] E. J. Hu, Y . Shen, P. Wallis, Z. Allen-Zhu, Y . Li, S. Wang, L. Wang and W. Chen. 2021. Lora: Low-rank adaptation of large language models. [219] Z. Hu, Y . Lan, L. Wang, W. Xu, E.-P. Lim, R. K.-W. Lee, L. Bing and S. Poria. 2023. Llm-adapters: An adapter family for parameter-efficient fine-tuning of large language models. arXiv preprint arXiv:2304.01933 . [220] W. Hua, Z. Dai, H. Liu and Q. Le. 2022. Transformer Quality in Linear Time. In Proceedings of the 39th Interna- tional Conference on Machine Learning , pages 9099\u20139117. PMLR. ISSN: 2640-3498. [221] C.-Z. A. Huang, A. Vaswani, J. Uszkoreit, I. Simon, C. Hawthorne, N. Shazeer, A. M. Dai, M. D. Hoffman et al. 2019. Music transformer. In International Conference on Learning Representations . [222] J. Huang, S. S. Gu, L. Hou, Y . Wu, X. Wang, H. Yu and J. Han. 2022. Large language models can self-improve. [223] J. Huang and K. C.-C. Chang. 2023. Towards Reasoning in Large Language Models: A Survey. ArXiv:2212.10403 [cs]. [224] W. Huang, P. Abbeel, D. Pathak and I. Mordatch. 2022. Language models as zero-shot planners: Extracting action- able knowledge for embodied agents. In International Con- ference on Machine Learning , pages 9118\u20139147. PMLR. [225] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson et al. 2022. Inner monologue: Em- bodied reasoning through planning with language models. arXiv preprint arXiv:2207.05608 . [226] Y . Huang, Y . Cheng, A. Bapna, O. Firat, M. X. Chen, D. Chen, H. Lee, J. Ngiam et al. 2018. Gpipe: Efficient training of giant neural networks using pipeline parallelism. [227] Z. Huang, Y . Shen, X. Zhang, J. Zhou, W. Rong and Z. Xiong. 2023. Transformer-patcher: One mistake worth one neuron. In The Eleventh International Conference on Learning Representations . Language models as zero-shot planners: Extracting action- able knowledge for embodied agents. In International Con- ference on Machine Learning , pages 9118\u20139147. PMLR. [225] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson et al. 2022. Inner monologue: Em- bodied reasoning through planning with language models. arXiv preprint arXiv:2207.05608 . [226] Y . Huang, Y . Cheng, A. Bapna, O. Firat, M. X. Chen, D. Chen, H. Lee, J. Ngiam et al. 2018. Gpipe: Efficient training of giant neural networks using pipeline parallelism. [227] Z. Huang, Y . Shen, X. Zhang, J. Zhou, W. Rong and Z. Xiong. 2023. Transformer-patcher: One mistake worth one neuron. In The Eleventh International Conference on Learning Representations . [228] I. Hubara, B. Chmiel, M. Island, R. Banner, J. Naor and D. Soudry. 2021. Accelerated sparse neural training: A provable and efficient method to find n:m transposable masks. In Advances in Neural Information Processing Sys- tems, volume 34, pages 21099\u201321111. Curran Associates, Inc.[229] HuggingFace. 2023. Huggingchat v0.3.0. Available from: https://huggingface.co/chat . Accessed: 28/06/2023. [230] C. Hwang, W. Cui, Y . Xiong, Z. Yang, Z. Liu, H. Hu, Z. Wang, R. Salas et al. 2022. Tutel: Adaptive mixture-of- experts at scale. arXiv preprint arXiv:2206.03382 . [231] J. P. A. Ioannidis. 2005. Why Most Published Research Findings Are False. PLoS Medicine , 2(8):e124. [232] D. Ippolito, A. Yuan, A. Coenen and S. Burnam. 2022. Creative writing with an ai-powered writing assistant: Perspectives from professional writers. arXiv preprint arXiv:2211.05030 . [233] G. Irving, P. Christiano and D. Amodei. 2018. Ai safety via debate. arXiv preprint arXiv:1805.00899 . [234] K. Y . Iu and V . M.-Y . Wong. 2023. Chatgpt by openai: The end of litigation lawyers? Available at SSRN . [235] S. Iyer, X. V . Lin, R. Pasunuru, T. Mihaylov, D. Simig, P. Yu, K. Shuster, T. Wang et al. 2022. Opt-iml: Scaling language model instruction meta learning through the lens of generalization. [236] G. Izacard, P. Lewis, M. Lomeli, L. Hosseini, F. Petroni, T. Schick, J. Dwivedi-Yu, A. Joulin et al. 2022. Few-shot learning with retrieval augmented language models. arXiv preprint arXiv:2208.03299 . [237] A. Jacovi, A. Caciularu, O. Goldman and Y . Goldberg. 2023. Stop Uploading Test Data in Plain Text: Practical Strategies for Mitigating Data Contamination by Evalua- tion Benchmarks. ArXiv:2305.10160 [cs]. via debate. arXiv preprint arXiv:1805.00899 . [234] K. Y . Iu and V . M.-Y . Wong. 2023. Chatgpt by openai: The end of litigation lawyers? Available at SSRN . [235] S. Iyer, X. V . Lin, R. Pasunuru, T. Mihaylov, D. Simig, P. Yu, K. Shuster, T. Wang et al. 2022. Opt-iml: Scaling language model instruction meta learning through the lens of generalization. [236] G. Izacard, P. Lewis, M. Lomeli, L. Hosseini, F. Petroni, T. Schick, J. Dwivedi-Yu, A. Joulin et al. 2022. Few-shot learning with retrieval augmented language models. arXiv preprint arXiv:2208.03299 . [237] A. Jacovi, A. Caciularu, O. Goldman and Y . Goldberg. 2023. Stop Uploading Test Data in",
    "48": "Plain Text: Practical Strategies for Mitigating Data Contamination by Evalua- tion Benchmarks. ArXiv:2305.10160 [cs]. [238] N. Jain, K. Saifullah, Y . Wen, J. Kirchenbauer, M. Shu, A. Saha, M. Goldblum, J. Geiping et al. 2023. Bring your own data! self-supervised evaluation for large language models. arXiv preprint arXiv:23062.13651 . [239] J. Jang, S. Kim, S. Ye, D. Kim, L. Logeswaran, M. Lee, K. Lee and M. Seo. 2023. Exploring the Benefits of Training Expert Language Models over Instruction Tuning. ArXiv:2302.03202 [cs]. [240] J. R. Jeliazkov, D. del Alamo and J. D. Karpiak. 2023. Esmfold hallucinates native-like protein sequences. bioRxiv . [241] Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y . Xu, E. Ishii, Y . J. Bang et al. 2023. Survey of Hallucination in Natural Language Generation. ACM Computing Surveys , 55(12):1\u2013 38. [242] G. Jiang, M. Xu, S.-C. Zhu, W. Han, C. Zhang and Y . Zhu. 2022. Mpi: Evaluating and inducing person- ality in pre-trained language models. arXiv preprint arXiv:2206.07550 . [243] X. Jiao, Y . Yin, L. Shang, X. Jiang, X. Chen, L. Li, F. Wang and Q. Liu. 2020. TinyBERT: Distilling BERT for natural language understanding. In Findings of the Association for Computational Linguistics: EMNLP 2020 , pages 4163\u20134174, Online. Association for Computational Linguistics. [244] Z. Jin, J. Liu, Z. Lyu, S. Poff, M. Sachan, R. Mihalcea, M. Diab and B. Sch\u00f6lkopf. 2023. Can large language models infer causation from correlation? 57 [245] A. Jinich, S. Z. Nazia, A. V . Tellez, D. Rappoport, M. AlQuraishi and K. Rhee. 2022. Predicting enzyme substrate chemical structure with protein language models. bioRxiv , pages 2022\u201309. [246] Jonathan Frankle [@jefrankle]. 2022. Louder for the people in the back: LARGE MODELS (GPT, DALLE) = DATABASES PROMPTS = QUERIES OUTPUTS = [243] X. Jiao, Y . Yin, L. Shang, X. Jiang, X. Chen, L. Li, F. Wang and Q. Liu. 2020. TinyBERT: Distilling BERT for natural language understanding. In Findings of the Association for Computational Linguistics: EMNLP 2020 , pages 4163\u20134174, Online. Association for Computational Linguistics. [244] Z. Jin, J. Liu, Z. Lyu, S. Poff, M. Sachan, R. Mihalcea, M. Diab and B. Sch\u00f6lkopf. 2023. Can large language models infer causation from correlation? 57 [245] A. Jinich, S. Z. Nazia, A. V . Tellez, D. Rappoport, M. AlQuraishi and K. Rhee. 2022. Predicting enzyme substrate chemical structure with protein language models. bioRxiv , pages 2022\u201309. [246] Jonathan Frankle [@jefrankle]. 2022. Louder for the people in the back: LARGE MODELS (GPT, DALLE) = DATABASES PROMPTS = QUERIES OUTPUTS = RESPONSES NNs find new relations w/in data. Anyone, no matter the resources, can study better querying langs and possibly beat a big model they could never afford to train. [247] D. Jones. 2022. Development and evaluation of speech recognition for the Welsh language. In Proceedings of the 4th Celtic Language Technology Workshop within LREC2022 , pages 52\u201359, Marseille, France. European Lan- guage Resources Association. [248] J. Jumper, R. Evans, A. Pritzel, T. Green, M. Figurnov, O. Ronneberger, K. Tunyasuvunakool, R. Bates et al. 2021. Highly accurate protein structure prediction with alphafold. Nature , 596(7873):583\u2013589. [249] J. Kaddour. 2022. Stop wasting my time! saving days of imagenet and bert training with latest weight averaging. arXiv preprint arXiv:2209.14981 . [250] J. Kaddour. 2023. The MiniPile Challenge for Data- Efficient Language Models. ArXiv:2304.08442 [cs]. [251] J. Kaddour, O. Key, P. Nawrot, P. Minervini and M. J. Kusner. 2023. No Train No Gain: Revisiting Efficient Training Algorithms For Transformer-based Language Models. ArXiv:2307.06440 [cs]. [252] J. Kaddour, L. Liu, R. Silva and M. Kusner. 2022. When do flat minima optimizers work? In Advances in Neural Information Processing Systems . [253] J. Kaddour, A. Lynch, Q. Liu, M. J. Kusner and R. Silva. 2022. Causal machine learning: A survey and open prob- lems. arXiv preprint arXiv:2206.15475 . [254] J. Kaddour, Y . Zhu, Q. Liu, M. J. Kusner and R. Silva. 2021. Causal Effect Inference for Structured Treatments. InAdvances in Neural Information Processing Systems , volume 34, pages 24841\u201324854. Curran Associates, Inc. [255] M. Kale, A. Siddhant, R. Al-Rfou, L. Xue, N. Con- [251] J. Kaddour, O. Key, P. Nawrot, P. Minervini and M. J. Kusner. 2023. No Train No Gain: Revisiting Efficient Training Algorithms For Transformer-based Language Models. ArXiv:2307.06440 [cs]. [252] J. Kaddour, L. Liu, R. Silva and M. Kusner. 2022. When do flat minima optimizers work? In Advances in Neural Information Processing Systems . [253] J. Kaddour, A. Lynch, Q. Liu, M. J. Kusner and R. Silva. 2022. Causal machine learning: A survey and open prob- lems. arXiv preprint arXiv:2206.15475 . [254] J. Kaddour, Y . Zhu, Q. Liu, M. J. Kusner and R. Silva. 2021. Causal Effect Inference for Structured Treatments. InAdvances in Neural Information Processing Systems , volume 34, pages 24841\u201324854. Curran Associates, Inc. [255] M. Kale, A. Siddhant, R. Al-Rfou, L. Xue, N. Con- stant and M. Johnson. 2021. nmT5 - is parallel data still relevant for pre-training massively multilingual language models? In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Pro- cessing (Volume 2: Short Papers) , pages 683\u2013691, Online. Association for Computational Linguistics. [256] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford et al. 2020. Scal- ing laws for neural language models. arXiv preprint arXiv:2001.08361 . [257] A. Karpathy. 2023. Tokenization issues (tweet). [258] D. M. Katz, M. J. Bommarito, S. Gao and P. Arredondo. 2023. Gpt-4 passes the bar exam. Available at SSRN 4389233 . [259] A. Kazemnejad, I. Padhi, K. N. Ramamurthy, P. Das and S. Reddy. 2023. The impact of positional encoding on length generalization in transformers. arXiv preprint arXiv:2305.19466 .[260] Z. Kenton, T. Everitt, L. Weidinger, I. Gabriel, V . Miku- lik and G. Irving. 2021. Alignment of language agents. arXiv preprint arXiv:2103.14659 . [261] N. S. Keskar, B. McCann, L. R. Varshney, C. Xiong and R. Socher. 2019. Ctrl: A conditional transformer language model for controllable generation. arXiv preprint arXiv:1909.05858 . [262] O. Khattab, K. Santhanam, X. L. Li, D. Hall, P. Liang, C. Potts and M. Zaharia. 2023. Demonstrate-Search- Predict: Composing retrieval and language models for knowledge-intensive NLP. ArXiv:2212.14024 [cs]. [263] D. Kiela, M. Bartolo, Y . Nie, D. Kaushik, A. Geiger, Z. Wu, B. Vidgen, G. Prasad et al. 2021. Dyn- abench: Rethinking benchmarking in nlp. arXiv preprint arXiv:2104.14337 . arXiv:2305.19466 .[260] Z. Kenton, T. Everitt, L. Weidinger, I. Gabriel, V . Miku- lik and G. Irving. 2021. Alignment of language agents. arXiv preprint arXiv:2103.14659 . [261] N. S. Keskar, B. McCann, L. R. Varshney, C. Xiong and R. Socher. 2019. Ctrl: A conditional transformer language model for controllable generation. arXiv preprint arXiv:1909.05858 . [262] O. Khattab, K. Santhanam, X. L. Li, D. Hall, P. Liang, C. Potts and M. Zaharia. 2023. Demonstrate-Search- Predict: Composing retrieval and language models for knowledge-intensive NLP. ArXiv:2212.14024 [cs]. [263] D. Kiela, M. Bartolo, Y . Nie, D. Kaushik, A. Geiger, Z. Wu, B. Vidgen, G. Prasad et al. 2021. Dyn- abench: Rethinking benchmarking in nlp. arXiv preprint arXiv:2104.14337 . [264] J. Kim, M. Kim and B. Mozafari. 2022. Provable memo- rization capacity of transformers. In The Eleventh Interna- tional Conference on Learning Representations . [265] S. Kim, K. Mangalam, J. Malik, M. W. Mahoney, A. Gholami and K. Keutzer. 2023. Big little transformer decoder. arXiv preprint arXiv:2302.07863 . [266] T. Kim. 2022. Revisiting the practical effectiveness of constituency parse extraction from pre-trained language models. In Proceedings of the 29th International Con- ference on Computational Linguistics ,",
    "49": "pages 5398\u20135408, Gyeongju, Republic of Korea. International Committee on Computational Linguistics. [267] L. N. Kinch, R. D. Schaeffer, A. Kryshtafovych and N. V . Grishin. 2021. Target classification in the 14th round of the critical assessment of protein structure prediction (casp14). Proteins: Structure, Function, and Bioinformat- ics, 89(12):1618\u20131632. [268] J. Kirchenbauer, J. Geiping, Y . Wen, J. Katz, I. Miers and T. Goldstein. 2023. A Watermark for Large Language Models. ArXiv:2301.10226 [cs]. [269] J. Kirchenbauer, J. Geiping, Y . Wen, M. Shu, K. Sai- fullah, K. Kong, K. Fernando, A. Saha et al. 2023. On the Reliability of Watermarks for Large Language Models. ArXiv:2306.04634 [cs]. [270] R. A. Klein, M. Vianello, F. Hasselman, B. G. Adams, R. B. Adams Jr, S. Alper, M. Aveyard, J. R. Axt et al. 2018. Many labs 2: Investigating variation in replicability across samples and settings. Advances in Methods and Practices in Psychological Science , 1(4):443\u2013490. [271] D. Kocetkov, R. Li, L. B. Allal, J. Li, C. Mou, C. M. Ferrandis, Y . Jernite, M. Mitchell et al. 2022. The stack: 3 tb of permissively licensed source code. [272] J. Koco \u00b4n, I. Cichecki, O. Kaszyca, M. Kochanek, D. Szy- and T. Goldstein. 2023. A Watermark for Large Language Models. ArXiv:2301.10226 [cs]. [269] J. Kirchenbauer, J. Geiping, Y . Wen, M. Shu, K. Sai- fullah, K. Kong, K. Fernando, A. Saha et al. 2023. On the Reliability of Watermarks for Large Language Models. ArXiv:2306.04634 [cs]. [270] R. A. Klein, M. Vianello, F. Hasselman, B. G. Adams, R. B. Adams Jr, S. Alper, M. Aveyard, J. R. Axt et al. 2018. Many labs 2: Investigating variation in replicability across samples and settings. Advances in Methods and Practices in Psychological Science , 1(4):443\u2013490. [271] D. Kocetkov, R. Li, L. B. Allal, J. Li, C. Mou, C. M. Ferrandis, Y . Jernite, M. Mitchell et al. 2022. The stack: 3 tb of permissively licensed source code. [272] J. Koco \u00b4n, I. Cichecki, O. Kaszyca, M. Kochanek, D. Szy- d\u0142o, J. Baran, J. Bielaniewicz, M. Gruza et al. 2023. Chat- gpt: Jack of all trades, master of none. [273] T. Kojima, S. S. Gu, M. Reid, Y . Matsuo and Y . Iwasawa. 2022. Large language models are zero-shot reasoners. In Advances in Neural Information Processing Systems . [274] A. K\u00f6pf, Y . Kilcher, D. von R\u00fctte, S. Anagnostidis, Z.- R. Tam, K. Stevens, A. Barhoum, N. M. Duc et al. 2023. Openassistant conversations\u2013democratizing large language model alignment. arXiv preprint arXiv:2304.07327 . 58 [275] T. Korbak, K. Shi, A. Chen, R. Bhalerao, C. L. Buckley, J. Phang, S. R. Bowman and E. Perez. 2023. Pretraining language models with human preferences. arXiv preprint arXiv:2302.08582 . [276] D. M. Korngiebel and S. D. Mooney. 2021. Consider- ing the possibilities and pitfalls of generative pre-trained transformer 3 (gpt-3) in healthcare delivery. NPJ Digital Medicine , 4(1):1\u20133. [277] M. Kosinski. 2023. Theory of mind may have sponta- neously emerged in large language models. [278] B. Krause, A. D. Gotmare, B. McCann, N. S. Keskar, S. Joty, R. Socher and N. F. Rajani. 2021. GeDi: Genera- tive discriminator guided sequence generation. In Findings of the Association for Computational Linguistics: EMNLP 2021 , pages 4929\u20134952, Punta Cana, Dominican Republic. Association for Computational Linguistics. [279] D. C. Krawczyk. 2018. Introduction to reasoning. Rea- soning\u2014The Neuroscience of How We Think; Academic Press: Cambridge, MA, USA , pages 1\u201311. [280] K. Krishna, Y . Song, M. Karpinska, J. Wieting and M. Iyyer. 2023. Paraphrasing evades detectors of AI- generated text, but retrieval is an effective defense. ArXiv:2303.13408 [cs]. [281] T. Kudo. 2018. Subword regularization: Improving neu- neously emerged in large language models. [278] B. Krause, A. D. Gotmare, B. McCann, N. S. Keskar, S. Joty, R. Socher and N. F. Rajani. 2021. GeDi: Genera- tive discriminator guided sequence generation. In Findings of the Association for Computational Linguistics: EMNLP 2021 , pages 4929\u20134952, Punta Cana, Dominican Republic. Association for Computational Linguistics. [279] D. C. Krawczyk. 2018. Introduction to reasoning. Rea- soning\u2014The Neuroscience of How We Think; Academic Press: Cambridge, MA, USA , pages 1\u201311. [280] K. Krishna, Y . Song, M. Karpinska, J. Wieting and M. Iyyer. 2023. Paraphrasing evades detectors of AI- generated text, but retrieval is an effective defense. ArXiv:2303.13408 [cs]. [281] T. Kudo. 2018. Subword regularization: Improving neu- ral network translation models with multiple subword can- didates. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 66\u201375, Melbourne, Australia. Associ- ation for Computational Linguistics. [282] T. Kudo and J. Richardson. 2018. Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. arXiv preprint arXiv:1808.06226 . [283] A. Kulkarni. 2021. GitHub Copilot AI Is Leaking Func- tional API Keys. [284] S. R. K\u00fcnzel, J. S. Sekhon, P. J. Bickel and B. Yu. 2019. Metalearners for estimating heterogeneous treatment ef- fects using machine learning. Proceedings of the national academy of sciences , 116(10):4156\u20134165. [285] W. Kwon, Z. Li, S. Zhuang, Y . Sheng, L. Zheng, C. Yu, J. Gonzalez, H. Zhang et al. 2023. vllm: Easy, fast, and cheap llm serving with pagedattention. [286] E. K\u0131c\u0131man, R. Ness, A. Sharma and C. Tan. 2023. Causal reasoning and large language models: Opening a new frontier for causality. [287] P. Lab. 2023. Awesome-Prompt-Engineering. Original- date: 2023-02-09T18:22:52Z. [288] A. K. Lampinen, S. C. Chan, I. Dasgupta, A. J. Nam and J. X. Wang. 2023. Passive learning of active causal strategies in agents and language models. arXiv preprint arXiv:2305.16183 . [289] H. Lauren\u00e7on, L. Saulnier, T. Wang, C. Akiki, A. V . del Moral, T. L. Scao, L. V . Werra, C. Mou et al. 2022. The big- science ROOTS corpus: A 1.6TB composite multilingual dataset. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track .[290] A. Lazaridou, E. Gribovskaya, W. Stokowiec and N. Grigorev. 2022. Internet-augmented language mod- Causal reasoning and large language models: Opening a new frontier for causality. [287] P. Lab. 2023. Awesome-Prompt-Engineering. Original- date: 2023-02-09T18:22:52Z. [288] A. K. Lampinen, S. C. Chan, I. Dasgupta, A. J. Nam and J. X. Wang. 2023. Passive learning of active causal strategies in agents and language models. arXiv preprint arXiv:2305.16183 . [289] H. Lauren\u00e7on, L. Saulnier, T. Wang, C. Akiki, A. V . del Moral, T. L. Scao, L. V . Werra, C. Mou et al. 2022. The big- science ROOTS corpus: A 1.6TB composite multilingual dataset. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track .[290] A. Lazaridou, E. Gribovskaya, W. Stokowiec and N. Grigorev. 2022. Internet-augmented language mod- els through few-shot prompting for open-domain question answering. [291] A. Lee, B. Miranda and S. Koyejo. 2023. Beyond Scale: the Diversity Coefficient as a Data Quality Metric Demon- strates LLMs are Pre-trained on Formally Diverse Data. ArXiv:2306.13840 [cs]. [292] D. Lee, J. Lee, J.-W. Ha, J.-H. Kim, S.-W. Lee, H. Lee and H. O. Song. 2023. Query-efficient black-box red teaming via bayesian optimization. arXiv preprint arXiv:2305.17444 . [293] K. Lee, O. Firat, A. Agarwal, C. Fannjiang and D. Sus- sillo. 2018. Hallucinations in neural machine translation. [294] K. Lee, D. Ippolito, A. Nystrom, C. Zhang, D. Eck, C. Callison-Burch and N. Carlini. 2021. Deduplicating training data makes language models better. arXiv preprint arXiv:2107.06499 . [295] N. Lee, W. Ping, P. Xu, M. Patwary, P. Fung, M. Shoeybi and B. Catanzaro. Factuality Enhanced Language Models for Open-Ended Text Generation. [296] P. Lee, S. Bubeck and J. Petro. 2023. Benefits, limits, and risks of gpt-4 as an ai chatbot for medicine. New England Journal of Medicine , 388(13):1233\u20131239. [297]",
    "50": "E. Lehman, E. Hernandez, D. Mahajan, J. Wulff, M. J. Smith, Z. Ziegler, D. Nadler, P. Szolovits et al. 2023. Do we still need clinical language models? [298] D. Lepikhin, H. Lee, Y . Xu, D. Chen, O. Firat, Y . Huang, M. Krikun, N. Shazeer et al. 2020. Gshard: Scaling gi- ant models with conditional computation and automatic sharding. [299] B. Lester, R. Al-Rfou and N. Constant. 2021. The power of scale for parameter-efficient prompt tuning. In Pro- ceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 3045\u20133059, On- line and Punta Cana, Dominican Republic. Association for Computational Linguistics. and risks of gpt-4 as an ai chatbot for medicine. New England Journal of Medicine , 388(13):1233\u20131239. [297] E. Lehman, E. Hernandez, D. Mahajan, J. Wulff, M. J. Smith, Z. Ziegler, D. Nadler, P. Szolovits et al. 2023. Do we still need clinical language models? [298] D. Lepikhin, H. Lee, Y . Xu, D. Chen, O. Firat, Y . Huang, M. Krikun, N. Shazeer et al. 2020. Gshard: Scaling gi- ant models with conditional computation and automatic sharding. [299] B. Lester, R. Al-Rfou and N. Constant. 2021. The power of scale for parameter-efficient prompt tuning. In Pro- ceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 3045\u20133059, On- line and Punta Cana, Dominican Republic. Association for Computational Linguistics. [300] Y . Leviathan, M. Kalman and Y . Matias. 2022. Fast in- ference from transformers via speculative decoding. arXiv preprint arXiv:2211.17192 . [301] D. M. Levine, R. Tuwani, B. Kompa, A. Varma, S. G. Finlayson, A. Mehrotra and A. Beam. 2023. The diagnostic and triage accuracy of the gpt-3 artificial intelligence model. medRxiv , pages 2023\u201301. [302] M. Lewis, S. Bhosale, T. Dettmers, N. Goyal and L. Zettlemoyer. 2021. Base layers: Simplifying training of large, sparse models. [303] M. Lewis, Y . Liu, N. Goyal, M. Ghazvininejad, A. Mo- hamed, O. Levy, V . Stoyanov and L. Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehen- sion. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 7871\u2013 7880, Online. Association for Computational Linguistics. [304] P. Lewis, E. Perez, A. Piktus, F. Petroni, V . Karpukhin, N. Goyal, H. K\u00fcttler, M. Lewis et al. 2020. Retrieval- augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems , 33:9459\u20139474. 59 [305] A. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski, V . Ramasesh, A. Slone, C. Anil et al. 2022. Solving quantitative reasoning problems with lan- guage models. [306] B. Z. Li, M. Nye and J. Andreas. 2021. Implicit repre- sentations of meaning in neural language models. arXiv preprint arXiv:2106.00737 . [307] C. Li, A. A. Awan, H. Tang, S. Rajbhandari and Y . He. 2021. 1-bit lamb: Communication efficient large-scale large-batch training with lamb\u2019s convergence speed. arXiv preprint arXiv:2104.06069 . [308] D. Li, R. Shao, A. Xie, Y . Sheng, L. Zheng, J. E. Gonza- N. Goyal, H. K\u00fcttler, M. Lewis et al. 2020. Retrieval- augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems , 33:9459\u20139474. 59 [305] A. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski, V . Ramasesh, A. Slone, C. Anil et al. 2022. Solving quantitative reasoning problems with lan- guage models. [306] B. Z. Li, M. Nye and J. Andreas. 2021. Implicit repre- sentations of meaning in neural language models. arXiv preprint arXiv:2106.00737 . [307] C. Li, A. A. Awan, H. Tang, S. Rajbhandari and Y . He. 2021. 1-bit lamb: Communication efficient large-scale large-batch training with lamb\u2019s convergence speed. arXiv preprint arXiv:2104.06069 . [308] D. Li, R. Shao, A. Xie, Y . Sheng, L. Zheng, J. E. Gonza- lez, I. Stoica, X. Ma et al. 2023. How long can open-source llms truly promise on context length? [309] H. Li, D. Guo, W. Fan, M. Xu and Y . Song. 2023. Multi- step jailbreaking privacy attacks on chatgpt. arXiv preprint arXiv:2304.05197 . [310] R. Li, J. Su, C. Duan and S. Zheng. 2020. Linear at- tention mechanism: An efficient attention for semantic segmentation. arXiv preprint arXiv:2007.14902 . [311] X. L. Li and P. Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) , pages 4582\u20134597, Online. Association for Computational Linguistics. [312] Y . Li, Z. Lin, S. Zhang, Q. Fu, B. Chen, J.-G. Lou and W. Chen. 2022. On the advance of making language mod- els better reasoners. [313] Y . Li, D. Choi, J. Chung, N. Kushman, J. Schrit- twieser, R. Leblond, T. Eccles, J. Keeling et al. 2022. Competition-level code generation with alphacode. Sci- ence, 378(6624):1092\u20131097. [314] Z. Li, C. You, S. Bhojanapalli, D. Li, A. S. Rawat, S. J. Reddi, K. Ye, F. Chern et al. 2023. The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers. ArXiv:2210.06313 [cs, stat]. [315] L. Lian, B. Li, A. Yala and T. Darrell. 2023. Llm- grounded diffusion: Enhancing prompt understanding of text-to-image diffusion models with large language models. [316] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence and A. Zeng. 2023. Code as policies: Language model programs for embodied control. [317] P. P. Liang, C. Wu, L.-P. Morency and R. Salakhutdi- [313] Y . Li, D. Choi, J. Chung, N. Kushman, J. Schrit- twieser, R. Leblond, T. Eccles, J. Keeling et al. 2022. Competition-level code generation with alphacode. Sci- ence, 378(6624):1092\u20131097. [314] Z. Li, C. You, S. Bhojanapalli, D. Li, A. S. Rawat, S. J. Reddi, K. Ye, F. Chern et al. 2023. The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers. ArXiv:2210.06313 [cs, stat]. [315] L. Lian, B. Li, A. Yala and T. Darrell. 2023. Llm- grounded diffusion: Enhancing prompt understanding of text-to-image diffusion models with large language models. [316] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence and A. Zeng. 2023. Code as policies: Language model programs for embodied control. [317] P. P. Liang, C. Wu, L.-P. Morency and R. Salakhutdi- nov. 2021. Towards understanding and mitigating social biases in language models. In International Conference on Machine Learning , pages 6565\u20136576. PMLR. [318] P. Liang, R. Bommasani, T. Lee, D. Tsipras, D. Soylu, M. Yasunaga, Y . Zhang, D. Narayanan et al. 2022. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110 . [319] O. Lieber, O. Sharir, B. Lenz and Y . Shoham. 2021. Jurassic-1: Technical details and evaluation. White Paper. AI21 Labs , 1. [320] V . Li\u00e9vin, C. E. Hother and O. Winther. 2022. Can large language models reason about medical questions? arXiv preprint arXiv:2207.08143 .[321] C.-C. Lin, A. Jaech, X. Li, M. R. Gormley and J. Eis- ner. 2020. Limitations of autoregressive models and their alternatives. arXiv preprint arXiv:2010.11939 . [322] J. Lin, A. Yang, J. Bai, C. Zhou, L. Jiang, X. Jia, A. Wang, J. Zhang et al. 2021. M6-10t: A sharing- delinking paradigm for efficient multi-trillion parameter pretraining. arXiv preprint arXiv:2110.03888 . [323] S. Lin, J. Hilton and O. Evans. 2021. Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958 . [324] X. V . Lin, T. Mihaylov, M. Artetxe, T. Wang, S. Chen, D. Simig, M. Ott, N. Goyal et al. 2022. Few-shot learning with multilingual generative language models. In Pro- ceedings of the 2022 Conference on Empirical Methods in Natural Language Processing , pages 9019\u20139052, Abu Dhabi, United Arab Emirates. Association for Computa- tional Linguistics. [325] Y .-T. Lin and Y .-N. Chen. 2023. Llm-eval: Unified multi-dimensional automatic evaluation for open-domain conversations with large language models.",
    "51": "arXiv preprint arXiv:2305.13711 . delinking paradigm for efficient multi-trillion parameter pretraining. arXiv preprint arXiv:2110.03888 . [323] S. Lin, J. Hilton and O. Evans. 2021. Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958 . [324] X. V . Lin, T. Mihaylov, M. Artetxe, T. Wang, S. Chen, D. Simig, M. Ott, N. Goyal et al. 2022. Few-shot learning with multilingual generative language models. In Pro- ceedings of the 2022 Conference on Empirical Methods in Natural Language Processing , pages 9019\u20139052, Abu Dhabi, United Arab Emirates. Association for Computa- tional Linguistics. [325] Y .-T. Lin and Y .-N. Chen. 2023. Llm-eval: Unified multi-dimensional automatic evaluation for open-domain conversations with large language models. arXiv preprint arXiv:2305.13711 . [326] Z. Lin, H. Akin, R. Rao, B. Hie, Z. Zhu, W. Lu, A. dos Santos Costa, M. Fazel-Zarandi et al. 2022. Language models of protein sequences at the scale of evolution enable accurate structure prediction. BioRxiv . [327] W. Ling, D. Yogatama, C. Dyer and P. Blunsom. 2017. Program induction by rationale generation: Learning to solve and explain algebraic word problems. In Proceed- ings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 158\u2013167, Vancouver, Canada. Association for Computa- tional Linguistics. [328] B. Liu, J. T. Ash, S. Goel, A. Krishnamurthy and C. Zhang. 2023. Exposing Attention Glitches with Flip- Flop Language Modeling. ArXiv:2306.00946 [cs]. [329] F. Liu, J. M. Eisenschlos, F. Piccinno, S. Krichene, C. Pang, K. Lee, M. Joshi, W. Chen et al. 2022. Deplot: One-shot visual language reasoning by plot-to-table trans- lation. arXiv preprint arXiv:2212.10505 . [330] H. Liu, C. Sferrazza and P. Abbeel. 2023. Languages are rewards: Hindsight finetuning using human feedback. arXiv preprint arXiv:2302.02676 . [331] H. Liu, D. Tam, M. Muqeeth, J. Mohta, T. Huang, M. Bansal and C. A. Raffel. 2022. Few-shot parameter- efficient fine-tuning is better and cheaper than in-context learning. Advances in Neural Information Processing Sys- tems, 35:1950\u20131965. [332] H. Liu, S. M. Xie, Z. Li and T. Ma. 2022. Same pre- training loss, better downstream: Implicit bias matters for language models. ArXiv , abs/2210.14199. [333] N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni and P. Liang. 2023. Lost in the Middle: How Language Models Use Long Contexts. ArXiv:2307.03172 [cs]. lation. arXiv preprint arXiv:2212.10505 . [330] H. Liu, C. Sferrazza and P. Abbeel. 2023. Languages are rewards: Hindsight finetuning using human feedback. arXiv preprint arXiv:2302.02676 . [331] H. Liu, D. Tam, M. Muqeeth, J. Mohta, T. Huang, M. Bansal and C. A. Raffel. 2022. Few-shot parameter- efficient fine-tuning is better and cheaper than in-context learning. Advances in Neural Information Processing Sys- tems, 35:1950\u20131965. [332] H. Liu, S. M. Xie, Z. Li and T. Ma. 2022. Same pre- training loss, better downstream: Implicit bias matters for language models. ArXiv , abs/2210.14199. [333] N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni and P. Liang. 2023. Lost in the Middle: How Language Models Use Long Contexts. ArXiv:2307.03172 [cs]. [334] R. Liu, C. Jia, J. Wei, G. Xu and S. V osoughi. 2022. Quantifying and alleviating political bias in language mod- els.Artificial Intelligence , 304:103654. 60 [335] R. Liu and N. B. Shah. 2023. ReviewerGPT? An Ex- ploratory Study on Using Large Language Models for Pa- per Reviewing. ArXiv:2306.00622 [cs]. [336] S. Liu and Z. Wang. 2023. Ten lessons we have learned in the new\" sparseland\": A short handbook for sparse neu- ral network researchers. arXiv preprint arXiv:2302.02596 . [337] X. Liu, X. Yang, L. Ouyang, G. Guo, J. Su, R. Xi, K. Yuan and F. Yuan. 2022. Protein language model predicts mutation pathogenicity and clinical prognosis. bioRxiv , pages 2022\u201309. [338] Z. Liu, A. Bahety and S. Song. 2023. Reflect: Summa- rizing robot experiences for failure explanation and correc- tion. [339] Z. Liu, E. Gan and M. Tegmark. 2023. Seeing is be- lieving: Brain-inspired modular training for mechanistic interpretability. arXiv preprint arXiv:2305.08746 . [340] S. Longpre, L. Hou, T. Vu, A. Webson, H. W. Chung, Y . Tay, D. Zhou, Q. V . Le et al. 2023. The flan collec- tion: Designing data and methods for effective instruction tuning. [341] S. Longpre, G. Yauney, E. Reif, K. Lee, A. Roberts, B. Zoph, D. Zhou, J. Wei et al. 2023. A Pretrainer\u2019s Guide to Training Data: Measuring the Effects of Data Age, Do- main Coverage, Quality, & Toxicity. ArXiv:2305.13169 [cs]. [342] Y . Lu, M. Bartolo, A. Moore, S. Riedel and P. Stene- torp. 2022. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. InProceedings of the 60th Annual Meeting of the Asso- ciation for Computational Linguistics (Volume 1: Long Papers) , pages 8086\u20138098, Dublin, Ireland. Association [340] S. Longpre, L. Hou, T. Vu, A. Webson, H. W. Chung, Y . Tay, D. Zhou, Q. V . Le et al. 2023. The flan collec- tion: Designing data and methods for effective instruction tuning. [341] S. Longpre, G. Yauney, E. Reif, K. Lee, A. Roberts, B. Zoph, D. Zhou, J. Wei et al. 2023. A Pretrainer\u2019s Guide to Training Data: Measuring the Effects of Data Age, Do- main Coverage, Quality, & Toxicity. ArXiv:2305.13169 [cs]. [342] Y . Lu, M. Bartolo, A. Moore, S. Riedel and P. Stene- torp. 2022. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. InProceedings of the 60th Annual Meeting of the Asso- ciation for Computational Linguistics (Volume 1: Long Papers) , pages 8086\u20138098, Dublin, Ireland. Association for Computational Linguistics. [343] Y . Lu, C. Li, M. Zhang, C. De Sa and Y . He. 2022. Max- imizing communication efficiency for large-scale training via 0/1 adam. arXiv preprint arXiv:2202.06009 . [344] N. Lukas, A. Salem, R. Sim, S. Tople, L. Wutschitz and S. Zanella-B\u00e9guelin. 2023. Analyzing Leakage of Personally Identifiable Information in Language Models. ArXiv:2302.00539 [cs]. [345] B. Luo, R. Y . Lau, C. Li and Y .-W. Si. 2022. A critical review of state-of-the-art chatbot designs and applications. Wiley Interdisciplinary Reviews: Data Mining and Knowl- edge Discovery , 12(1):e1434. [346] Y . Luo, N. Tang, G. Li, C. Chai, W. Li and X. Qin. 2021. Synthesizing natural language to visualization (nl2vis) benchmarks from nl2sql benchmarks. In Proceedings of the 2021 International Conference on Management of Data , pages 1235\u20131247. [347] A. Lynch, G. J. Dovonon, J. Kaddour and R. Silva. 2023. Spawrious: A benchmark for fine control of spurious cor- relation biases. arXiv preprint arXiv:2303.05470 . [348] P. Ma, Z. Li, A. Sun and S. Wang. 2023. \"oops, did i just say that?\" testing and repairing unethical suggestions of large language models with suggest-critique-reflect process. arXiv preprint arXiv:2305.02626 . [349] X. Ma, G. Fang and X. Wang. 2023. Llm-pruner: On the structural pruning of large language models. arXiv preprint arXiv:2305.11627 .[350] X. Ma, X. Kong, S. Wang, C. Zhou, J. May, H. Ma and L. Zettlemoyer. 2021. Luna: Linear unified nested attention. Advances in Neural Information Processing Systems , 34:2441\u20132453. [351] A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U. Alon, N. Dziri et al. 2023. Self-refine: Iterative refinement with self-feedback. relation biases. arXiv preprint arXiv:2303.05470 . [348] P. Ma, Z. Li, A. Sun and S. Wang. 2023. \"oops, did i just say that?\" testing and repairing unethical suggestions of large language models with suggest-critique-reflect process. arXiv preprint arXiv:2305.02626 . [349] X. Ma, G. Fang and X. Wang. 2023. Llm-pruner: On the structural pruning of large language models. arXiv preprint arXiv:2305.11627 .[350] X. Ma, X. Kong, S. Wang, C. Zhou, J. May, H. Ma and L. Zettlemoyer. 2021. Luna: Linear unified nested attention. Advances in Neural",
    "52": "Information Processing Systems , 34:2441\u20132453. [351] A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U. Alon, N. Dziri et al. 2023. Self-refine: Iterative refinement with self-feedback. [352] A. Madani, B. Krause, E. R. Greene, S. Subramanian, B. P. Mohr, J. M. Holton, J. L. Olmos Jr, C. Xiong et al. 2023. Large language models generate functional protein sequences across diverse families. Nature Biotechnology , pages 1\u20138. [353] M. Maddela, M. Ung, J. Xu, A. Madotto, H. Foran and Y .-L. Boureau. 2023. Training Models to Gen- erate, Recognize, and Reframe Unhelpful Thoughts. ArXiv:2307.02768 [cs]. [354] S. Mahdavi, R. Liao and C. Thrampoulidis. 2023. Memo- rization Capacity of Multi-Head Attention in Transformers. ArXiv:2306.02010 [cs]. [355] S. Malladi, T. Gao, E. Nichani, A. Damian, J. D. Lee, D. Chen and S. Arora. 2023. Fine-Tuning Language Mod- els with Just Forward Passes. ArXiv:2305.17333 [cs]. [356] S. Mangrulkar, S. Gugger, L. Debut, Y . Belkada and S. Paul. 2022. Peft: State-of-the-art parameter- efficient fine-tuning methods. https://github.com/ huggingface/peft . [357] P. Maniatis and D. Tarlow. 2023. Large sequence models for software development activities. Available from: https://ai.googleblog.com/2023/ 05/large-sequence-models-for-software. html . Accessed: 26/06/2023. [358] R. R. McCrae and P. T. Costa Jr. 1997. Personality trait structure as a human universal. American psychologist , 52(5):509. [359] I. R. McKenzie, A. Lyzhov, M. Pieler, A. Parrish, A. Mueller, A. Prabhu, E. McLean, A. Kirtland et al. 2023. Inverse Scaling: When Bigger Isn\u2019t Better. ArXiv:2306.09479 [cs]. [360] K. Meng, D. Bau, A. J. Andonian and Y . Belinkov. 2022. Locating and editing factual associations in GPT. In Ad- vances in Neural Information Processing Systems . [361] K. Meng, A. S. Sharma, A. J. Andonian, Y . Belinkov and D. Bau. 2023. Mass-editing memory in a transformer. models for software development activities. Available from: https://ai.googleblog.com/2023/ 05/large-sequence-models-for-software. html . Accessed: 26/06/2023. [358] R. R. McCrae and P. T. Costa Jr. 1997. Personality trait structure as a human universal. American psychologist , 52(5):509. [359] I. R. McKenzie, A. Lyzhov, M. Pieler, A. Parrish, A. Mueller, A. Prabhu, E. McLean, A. Kirtland et al. 2023. Inverse Scaling: When Bigger Isn\u2019t Better. ArXiv:2306.09479 [cs]. [360] K. Meng, D. Bau, A. J. Andonian and Y . Belinkov. 2022. Locating and editing factual associations in GPT. In Ad- vances in Neural Information Processing Systems . [361] K. Meng, A. S. Sharma, A. J. Andonian, Y . Belinkov and D. Bau. 2023. Mass-editing memory in a transformer. InThe Eleventh International Conference on Learning Representations . [362] J. Menick, M. Trebacz, V . Mikulik, J. Aslanides, F. Song, M. Chadwick, M. Glaese, S. Young et al. 2022. Teaching language models to support answers with verified quotes. [363] G. Mialon, R. Dess\u00ec, M. Lomeli, C. Nalmpantis, R. Pa- sunuru, R. Raileanu, B. Rozi\u00e8re, T. Schick et al. 2023. Augmented language models: a survey. arXiv preprint arXiv:2302.07842 . [364] S. Milgram. 1963. Behavioral study of obedience. The Journal of abnormal and social psychology , 67(4):371. [365] S. Min, K. Krishna, X. Lyu, M. Lewis, W.-t. Yih, P. W. Koh, M. Iyyer, L. Zettlemoyer et al. 2023. FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation. ArXiv:2305.14251 [cs]. 61 [366] S. Min, X. Lyu, A. Holtzman, M. Artetxe, M. Lewis, H. Hajishirzi and L. Zettlemoyer. 2022. Rethinking the role of demonstrations: What makes in-context learning work? [367] M. Miotto, N. Rossberg and B. Kleinberg. 2022. Who is gpt-3? an exploration of personality, values and demo- graphics. arXiv preprint arXiv:2209.14338 . [368] P. Mirowski, K. W. Mathewson, J. Pittman and R. Evans. 2022. Co-writing screenplays and theatre scripts with lan- guage models: An evaluation by industry professionals. arXiv preprint arXiv:2209.14958 . [369] A. Mishra, J. A. Latorre, J. Pool, D. Stosic, D. Stosic, G. Venkatesh, C. Yu and P. Micikevicius. 2021. Ac- celerating sparse deep neural networks. arXiv preprint arXiv:2104.08378 . [370] S. Mishra, D. Khashabi, C. Baral and H. Hajishirzi. 2022. Cross-task generalization via natural language crowdsourc- ing instructions. In Proceedings of the 60th Annual Meet- ing of the Association for Computational Linguistics (Vol- [367] M. Miotto, N. Rossberg and B. Kleinberg. 2022. Who is gpt-3? an exploration of personality, values and demo- graphics. arXiv preprint arXiv:2209.14338 . [368] P. Mirowski, K. W. Mathewson, J. Pittman and R. Evans. 2022. Co-writing screenplays and theatre scripts with lan- guage models: An evaluation by industry professionals. arXiv preprint arXiv:2209.14958 . [369] A. Mishra, J. A. Latorre, J. Pool, D. Stosic, D. Stosic, G. Venkatesh, C. Yu and P. Micikevicius. 2021. Ac- celerating sparse deep neural networks. arXiv preprint arXiv:2104.08378 . [370] S. Mishra, D. Khashabi, C. Baral and H. Hajishirzi. 2022. Cross-task generalization via natural language crowdsourc- ing instructions. In Proceedings of the 60th Annual Meet- ing of the Association for Computational Linguistics (Vol- ume 1: Long Papers) , pages 3470\u20133487, Dublin, Ireland. Association for Computational Linguistics. [371] E. Mitchell, Y . Lee, A. Khazatsky, C. D. Manning and C. Finn. 2023. DetectGPT: Zero-Shot Machine- Generated Text Detection using Probability Curvature. ArXiv:2301.11305 [cs]. [372] E. Mitchell, C. Lin, A. Bosselut, C. Finn and C. D. Man- ning. 2022. Fast model editing at scale. In International Conference on Learning Representations . [373] E. Mitchell, C. Lin, A. Bosselut, C. D. Manning and C. Finn. 2022. Memory-based model editing at scale. In Proceedings of the 39th International Conference on Ma- chine Learning , volume 162 of Proceedings of Machine Learning Research , pages 15817\u201315831. PMLR. [374] R. Moriconi, M. P. Deisenroth and K. Sesh Kumar. 2020. High-dimensional bayesian optimization using low- dimensional feature spaces. Machine Learning , 109:1925\u2013 1943. [375] M. Moussa\u00efd, J. E. K\u00e4mmer, P. P. Analytis and H. Neth. 2013. Social influence and the collective dynamics of opinion formation. PloS one , 8(11):e78433. [376] M. Mozes, J. Hoffmann, K. Tomanek, M. Kouate, N. Thain, A. Yuan, T. Bolukbasi and L. Dixon. 2023. To- wards agile text classifiers for everyone. arXiv preprint arXiv:2302.06541 . [377] N. Muennighoff, T. Wang, L. Sutawika, A. Roberts, S. Biderman, T. L. Scao, M. S. Bari, S. Shen et al. 2022. Crosslingual generalization through multitask finetuning. arXiv preprint arXiv:2211.01786 . [378] S. Mukherjee, A. Mitra, G. Jawahar, S. Agarwal, H. Palangi and A. Awadallah. 2023. Orca: Progressive learning from complex explanation traces of gpt-4. arXiv preprint arXiv:2306.02707 . [379] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, [375] M. Moussa\u00efd, J. E. K\u00e4mmer, P. P. Analytis and H. Neth. 2013. Social influence and the collective dynamics of opinion formation. PloS one , 8(11):e78433. [376] M. Mozes, J. Hoffmann, K. Tomanek, M. Kouate, N. Thain, A. Yuan, T. Bolukbasi and L. Dixon. 2023. To- wards agile text classifiers for everyone. arXiv preprint arXiv:2302.06541 . [377] N. Muennighoff, T. Wang, L. Sutawika, A. Roberts, S. Biderman, T. L. Scao, M. S. Bari, S. Shen et al. 2022. Crosslingual generalization through multitask finetuning. arXiv preprint arXiv:2211.01786 . [378] S. Mukherjee, A. Mitra, G. Jawahar, S. Agarwal, H. Palangi and A. Awadallah. 2023. Orca: Progressive learning from complex explanation traces of gpt-4. arXiv preprint arXiv:2306.02707 . [379] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim, C. Hesse, S. Jain et al. 2021. Webgpt: Browser- assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332 .[380] N. Nanda, L. Chan, T. Lieberum, J. Smith and J. Stein- hardt. 2023. Progress measures for grokking via mechanis- tic interpretability. In The Eleventh International",
    "53": "Confer- ence on Learning Representations . [381] S. Nerella, S. Bandyopadhyay, J. Zhang, M. Contreras, S. Siegel, A. Bumin, B. Silva, J. Sena et al. 2023. Trans- formers in healthcare: A survey. [382] A. Nguyen, N. Karampatziakis and W. Chen. 2023. Meet in the middle: A new pre-training paradigm. arXiv preprint arXiv:2303.07295 . [383] E. Nguyen, M. Poli, M. Faizi, A. Thomas, C. Birch- Sykes, M. Wornow, A. Patel, C. Rabideau et al. 2023. Hye- nadna: Long-range genomic sequence modeling at single nucleotide resolution. arXiv preprint arXiv:2306.15794 . [384] A. Nichol, P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin, B. McGrew, I. Sutskever and M. Chen. 2022. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. [385] X. Nie and S. Wager. 2021. Quasi-oracle estimation of heterogeneous treatment effects. Biometrika , 108(2):299\u2013 319. [386] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y . Zhou, S. Savarese and C. Xiong. 2022. Codegen: An open large language model for code with multi-turn pro- gram synthesis. [387] F. Niu, B. Recht, C. Re, S. J. Wright and W. D. St. Hog- wild!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent. [388] H. Nori, N. King, S. M. McKinney, D. Carignan and E. Horvitz. 2023. Capabilities of gpt-4 on medical chal- lenge problems. [389] K. Nottingham, P. Ammanabrolu, A. Suhr, Y . Choi, P. Mishkin, B. McGrew, I. Sutskever and M. Chen. 2022. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. [385] X. Nie and S. Wager. 2021. Quasi-oracle estimation of heterogeneous treatment effects. Biometrika , 108(2):299\u2013 319. [386] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y . Zhou, S. Savarese and C. Xiong. 2022. Codegen: An open large language model for code with multi-turn pro- gram synthesis. [387] F. Niu, B. Recht, C. Re, S. J. Wright and W. D. St. Hog- wild!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent. [388] H. Nori, N. King, S. M. McKinney, D. Carignan and E. Horvitz. 2023. Capabilities of gpt-4 on medical chal- lenge problems. [389] K. Nottingham, P. Ammanabrolu, A. Suhr, Y . Choi, H. Hajishirzi, S. Singh and R. Fox. 2023. Do embod- ied agents dream of pixelated sheep?: Embodied decision making using language guided world modelling. arXiv preprint arXiv:2301.12050 . [390] S. Nurk, S. Koren, A. Rhie, M. Rautiainen, A. V . Bzikadze, A. Mikheenko, M. R. V ollger, N. Altemose et al. 2022. The complete sequence of a human genome. Sci- ence, 376(6588):44\u201353. [391] M. Nye, A. J. Andreassen, G. Gur-Ari, H. Michalewski, J. Austin, D. Bieber, D. Dohan, A. Lewkowycz et al. 2021. Show your work: Scratchpads for intermediate computa- tion with language models. [392] Ofir Press [@OfirPress]. 2022. GPT-3 seems to be nondeterministic even when it should be (i.e. temper- ature == 0). Has anyone else noticed this? Is there a known fix? Video by my collaborator Muru Zhang. https://t.co/dOWYWPBYyP. [393] N. Oh, G.-S. Choi and W. Y . Lee. 2023. Chatgpt goes to operating room: Evaluating gpt-4 performance and its potential in surgical education and training in the era of large language models. medRxiv . [394] C. Olah. Mechanistic Interpretability, Variables, and the Importance of Interpretable Bases. 62 [395] C. Olsson, N. Elhage, N. Nanda, N. Joseph, N. Das- Sarma, T. Henighan, B. Mann, A. Askell et al. 2022. In-context learning and induction heads. arXiv preprint arXiv:2209.11895 . [396] OpenAI. 2022. Chatgpt: Optimizing language mod- els for dialogue. https://openai.com/blog/ chatgpt/ . Accessed: 2023-02-18. [397] OpenAI. 2023. Chat gpt 4 painfully slow. https://community.openai.com/t/ chat-gpt-4-painfully-slow/117996 . [398] OpenAI. 2023. Gpt-4 technical report. [399] P. J. Ortiz Su\u2019arez, B. Sagot and L. Romary. 2019. Asyn- chronous pipelines for processing huge corpora on medium potential in surgical education and training in the era of large language models. medRxiv . [394] C. Olah. Mechanistic Interpretability, Variables, and the Importance of Interpretable Bases. 62 [395] C. Olsson, N. Elhage, N. Nanda, N. Joseph, N. Das- Sarma, T. Henighan, B. Mann, A. Askell et al. 2022. In-context learning and induction heads. arXiv preprint arXiv:2209.11895 . [396] OpenAI. 2022. Chatgpt: Optimizing language mod- els for dialogue. https://openai.com/blog/ chatgpt/ . Accessed: 2023-02-18. [397] OpenAI. 2023. Chat gpt 4 painfully slow. https://community.openai.com/t/ chat-gpt-4-painfully-slow/117996 . [398] OpenAI. 2023. Gpt-4 technical report. [399] P. J. Ortiz Su\u2019arez, B. Sagot and L. Romary. 2019. Asyn- chronous pipelines for processing huge corpora on medium to low resource infrastructures. In Proceedings of the Work- shop on Challenges in the Management of Large Corpora (CMLC-7) 2019. Cardiff, 22nd July 2019 , pages 9 \u2013 16, Mannheim. Leibniz-Institut f\"ur Deutsche Sprache. [400] M. Ott, S. Edunov, A. Baevski, A. Fan, S. Gross, N. Ng, D. Grangier and M. Auli. 2019. fairseq: A fast, ex- tensible toolkit for sequence modeling. arXiv preprint arXiv:1904.01038 . [401] N. Ousidhoum, X. Zhao, T. Fang, Y . Song and D.-Y . Yeung. 2021. Probing toxic content in large pre-trained lan- guage models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) , pages 4262\u20134274. [402] C. Outeiral and C. Deane. 2022. Codon language em- beddings provide strong signals for protein engineering. bioRxiv , pages 2022\u201312. [403] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal et al. 2022. Training lan- guage models to follow instructions with human feedback. InAdvances in Neural Information Processing Systems . [404] M. Pagliardini, D. Paliotta, M. Jaggi and F. Fleuret. 2023. Faster causal attention over large sequences through sparse flash attention. [405] J. Pan, T. Gao, H. Chen and D. Chen. 2023. What in- context learning \"learns\" in-context: Disentangling task recognition and task learning. [406] B. Paranjape, S. Lundberg, S. Singh, H. Hajishirzi, L. Zettlemoyer and M. T. Ribeiro. 2023. Art: Automatic multi-step reasoning and tool-use for large language mod- els. [407] G. Park, B. Park, S. J. Kwon, B. Kim, Y . Lee and D. Lee. 2022. nuqmm: Quantized matmul for efficient inference P. Mishkin, C. Zhang, S. Agarwal et al. 2022. Training lan- guage models to follow instructions with human feedback. InAdvances in Neural Information Processing Systems . [404] M. Pagliardini, D. Paliotta, M. Jaggi and F. Fleuret. 2023. Faster causal attention over large sequences through sparse flash attention. [405] J. Pan, T. Gao, H. Chen and D. Chen. 2023. What in- context learning \"learns\" in-context: Disentangling task recognition and task learning. [406] B. Paranjape, S. Lundberg, S. Singh, H. Hajishirzi, L. Zettlemoyer and M. T. Ribeiro. 2023. Art: Automatic multi-step reasoning and tool-use for large language mod- els. [407] G. Park, B. Park, S. J. Kwon, B. Kim, Y . Lee and D. Lee. 2022. nuqmm: Quantized matmul for efficient inference of large-scale generative language models. arXiv preprint arXiv:2206.09557 . [408] J. S. Park, J. C. O\u2019Brien, C. J. Cai, M. R. Morris, P. Liang and M. S. Bernstein. 2023. Generative agents: Interactive simulacra of human behavior. [409] P. S. Park, P. Schoenegger and C. Zhu. 2023. Artifi- cial intelligence in psychology research. arXiv preprint arXiv:2302.07267 . [410] A. Patel, B. Li, M. S. Rasooli, N. Constant, C. Raffel and C. Callison-Burch. 2023. Bidirectional language models are also few-shot learners.[411] N. D. Patson, E. S. Darowski, N. Moon and F. Ferreira. 2009. Lingering misinterpretations in garden-path sen- tences: evidence from a paraphrasing task. Journal of Experimental Psychology: Learning, Memory, and Cogni- tion, 35(1):280. [412] D. Patterson, J. Gonzalez, U. H\u00f6lzle, Q. Le, C. Liang, L.-M. Munguia, D. Rothchild, D. R. So et al. 2022. The carbon",
    "54": "footprint of machine learning training will plateau, then shrink. Computer , 55(7):18\u201328. [413] A. Paullada, I. D. Raji, E. M. Bender, E. Denton and A. Hanna. 2021. Data and its (dis) contents: A survey of dataset development and use in machine learning research. Patterns , 2(11):100336. [414] M. Pellert, C. M. Lechner, C. Wagner, B. Rammstedt and M. Strohmaier. 2023. Ai psychometrics: Using psy- chometric inventories to obtain psychological profiles of large language models. [415] G. Penedo, Q. Malartic, D. Hesslow, R. Cojocaru, A. Cappelli, H. Alobeidli, B. Pannier, E. Almazrouei et al. 2023. The RefinedWeb Dataset for Falcon LLM: Outper- forming Curated Corpora with Web Data, and Web Data Only. ArXiv:2306.01116 [cs]. [416] B. Peng, E. Alcaide, Q. Anthony, A. Albalak, S. Ar- cadinho, H. Cao, X. Cheng, M. Chung et al. 2023. then shrink. Computer , 55(7):18\u201328. [413] A. Paullada, I. D. Raji, E. M. Bender, E. Denton and A. Hanna. 2021. Data and its (dis) contents: A survey of dataset development and use in machine learning research. Patterns , 2(11):100336. [414] M. Pellert, C. M. Lechner, C. Wagner, B. Rammstedt and M. Strohmaier. 2023. Ai psychometrics: Using psy- chometric inventories to obtain psychological profiles of large language models. [415] G. Penedo, Q. Malartic, D. Hesslow, R. Cojocaru, A. Cappelli, H. Alobeidli, B. Pannier, E. Almazrouei et al. 2023. The RefinedWeb Dataset for Falcon LLM: Outper- forming Curated Corpora with Web Data, and Web Data Only. ArXiv:2306.01116 [cs]. [416] B. Peng, E. Alcaide, Q. Anthony, A. Albalak, S. Ar- cadinho, H. Cao, X. Cheng, M. Chung et al. 2023. RWKV: Reinventing RNNs for the Transformer Era. ArXiv:2305.13048 [cs]. [417] B. Peng, E. Alcaide, Q. Anthony, A. Albalak, S. Arcad- inho, H. Cao, X. Cheng, M. Chung et al. 2023. Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048 . [418] C. Peng, X. Yang, A. Chen, K. E. Smith, N. PourNejatian, A. B. Costa, C. Martin, M. G. Flores et al. 2023. A study of generative large language model for medical research and healthcare. [419] Y . Peng. 2021. A MARVS analysis of two Chinese near- synonymous verbs of jumping based on Chinese corpora. InProceedings of the 35th Pacific Asia Conference on Language, Information and Computation , pages 483\u2013492, Shanghai, China. Association for Computational Lingus- tics. [420] E. Perez, S. Huang, F. Song, T. Cai, R. Ring, J. Aslanides, A. Glaese, N. McAleese et al. 2022. Red teaming lan- guage models with language models. arXiv preprint arXiv:2202.03286 . [421] E. Perez, S. Ringer, K. Luko\u0161i \u00afut\u02d9e, K. Nguyen, E. Chen, S. Heiner, C. Pettit, C. Olsson et al. 2022. Discovering language model behaviors with model-written evaluations. [422] F. Perez and I. Ribeiro. 2022. Ignore previous prompt: Attack techniques for language models. arXiv preprint arXiv:2211.09527 . [423] L. Peric, S. Mijic, D. Stammbach and E. Ash. 2020. Le- gal language modeling with transformers. In Proceedings of the Fourth Workshop on Automated Semantic Analysis of Information in Legal Text (ASAIL 2020) held online in conjunction with te 33rd International Conference on Le- gal Knowledge and Information Systems (JURIX 2020) December 9, 2020 , volume 2764. CEUR-WS. 63 [424] B. Peters and A. F. T. Martins. 2021. Smoothing and arXiv:2202.03286 . [421] E. Perez, S. Ringer, K. Luko\u0161i \u00afut\u02d9e, K. Nguyen, E. Chen, S. Heiner, C. Pettit, C. Olsson et al. 2022. Discovering language model behaviors with model-written evaluations. [422] F. Perez and I. Ribeiro. 2022. Ignore previous prompt: Attack techniques for language models. arXiv preprint arXiv:2211.09527 . [423] L. Peric, S. Mijic, D. Stammbach and E. Ash. 2020. Le- gal language modeling with transformers. In Proceedings of the Fourth Workshop on Automated Semantic Analysis of Information in Legal Text (ASAIL 2020) held online in conjunction with te 33rd International Conference on Le- gal Knowledge and Information Systems (JURIX 2020) December 9, 2020 , volume 2764. CEUR-WS. 63 [424] B. Peters and A. F. T. Martins. 2021. Smoothing and shrinking the sparse Seq2Seq search space. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 2642\u20132654, Online. Associ- ation for Computational Linguistics. [425] J. Peters, D. Janzing and B. Sch\u00f6lkopf. 2017. Elements of causal inference: foundations and learning algorithms . The MIT Press. [426] A. Petrov, E. La Malfa, P. H. Torr and A. Bibi. 2023. Language model tokenizers introduce unfairness between languages. arXiv preprint arXiv:2305.15425 . [427] T. Pettinato Oltz. 2023. Chatgpt, professor of law. Pro- fessor of Law (February 4, 2023) . [428] J. Pfeiffer, A. R\u00fcckl\u00e9, C. Poth, A. Kamath, I. Vuli \u00b4c, S. Ruder, K. Cho and I. Gurevych. 2020. AdapterHub: A framework for adapting transformers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations , pages 46\u2013 54, Online. Association for Computational Linguistics. [429] S. Pichai. 2023. An important next step on our ai jour- ney. https://blog.google/technology/ai/ bard-google-ai-search-updates/ . Accessed: 2023-02-18. [430] M. Poli, S. Massaroli, E. Nguyen, D. Y . Fu, T. Dao, S. Baccus, Y . Bengio, S. Ermon et al. 2023. Hyena Hier- archy: Towards Larger Convolutional Language Models. ArXiv:2302.10866 [cs]. [431] R. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Brad- bury, A. Levskaya, J. Heek, K. Xiao et al. 2022. Efficiently Scaling Transformer Inference. ArXiv:2211.05102 [cs]. [432] R. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Brad- bury, A. Levskaya, J. Heek, K. Xiao et al. 2022. Ef- ficiently scaling transformer inference. arXiv preprint arXiv:2211.05102 . 54, Online. Association for Computational Linguistics. [429] S. Pichai. 2023. An important next step on our ai jour- ney. https://blog.google/technology/ai/ bard-google-ai-search-updates/ . Accessed: 2023-02-18. [430] M. Poli, S. Massaroli, E. Nguyen, D. Y . Fu, T. Dao, S. Baccus, Y . Bengio, S. Ermon et al. 2023. Hyena Hier- archy: Towards Larger Convolutional Language Models. ArXiv:2302.10866 [cs]. [431] R. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Brad- bury, A. Levskaya, J. Heek, K. Xiao et al. 2022. Efficiently Scaling Transformer Inference. ArXiv:2211.05102 [cs]. [432] R. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Brad- bury, A. Levskaya, J. Heek, K. Xiao et al. 2022. Ef- ficiently scaling transformer inference. arXiv preprint arXiv:2211.05102 . [433] V . Prabhakaran, A. Mostafazadeh Davani and M. Diaz. 2021. On releasing annotator-level labels and information in datasets. In Proceedings of the Joint 15th Linguistic Annotation Workshop (LAW) and 3rd Designing Meaning Representations (DMR) Workshop , pages 133\u2013138, Punta Cana, Dominican Republic. Association for Computational Linguistics. [434] O. Press, N. A. Smith and M. Lewis. 2021. Train short, test long: Attention with linear biases enables input length extrapolation. [435] O. Press, M. Zhang, S. Min, L. Schmidt, N. A. Smith and M. Lewis. 2023. Measuring and Narrowing the Com- positionality Gap in Language Models. ArXiv:2210.03350 [cs]. [436] J. Qian, H. Wang, Z. Li, S. Li and X. Yan. 2022. Lim- itations of language models in arithmetic and symbolic induction. arXiv preprint arXiv:2208.05051 . [437] J. Rabelo, R. Goebel, M.-Y . Kim, Y . Kano, M. Yosh- ioka and K. Satoh. 2022. Overview and discussion of the competition on legal information Extraction/Entailment (COLIEE) 2021. The Review of Socionetwork Strategies , 16(1):111\u2013133.[438] A. Radford, R. Jozefowicz and I. Sutskever. 2017. Learn- ing to generate reviews and discovering sentiment. arXiv preprint arXiv:1704.01444 . [439] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey and I. Sutskever. 2022. Robust Speech Recognition via Large-Scale Weak Supervision. ArXiv:2212.04356 [cs, eess]. [440] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei and I. Sutskever. 2019. Language",
    "55": "models are unsupervised multitask learners. [441] J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song, J. Aslanides, S. Henderson et al. 2021. Scaling lan- guage models: Methods, analysis & insights from training competition on legal information Extraction/Entailment (COLIEE) 2021. The Review of Socionetwork Strategies , 16(1):111\u2013133.[438] A. Radford, R. Jozefowicz and I. Sutskever. 2017. Learn- ing to generate reviews and discovering sentiment. arXiv preprint arXiv:1704.01444 . [439] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey and I. Sutskever. 2022. Robust Speech Recognition via Large-Scale Weak Supervision. ArXiv:2212.04356 [cs, eess]. [440] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei and I. Sutskever. 2019. Language models are unsupervised multitask learners. [441] J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song, J. Aslanides, S. Henderson et al. 2021. Scaling lan- guage models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446 . [442] R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning and C. Finn. 2023. Direct preference optimiza- tion: Your language model is secretly a reward model. arXiv preprint arXiv:2305.18290 . [443] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y . Zhou, W. Li et al. 2022. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res. , 21(1). [444] S. Rajbhandari, C. Li, Z. Yao, M. Zhang, R. Y . Aminabadi, A. A. Awan, J. Rasley and Y . He. 2022. DeepSpeed-MoE: Advancing mixture-of-experts inference and training to power next-generation AI scale. In Pro- ceedings of the 39th International Conference on Machine Learning , volume 162 of Proceedings of Machine Learning Research , pages 18332\u201318346. PMLR. [445] S. Rajbhandari, J. Rasley, O. Ruwase and Y . He. 2020. Zero: Memory optimizations toward training trillion param- eter models. In Proceedings of the International Confer- ence for High Performance Computing, Networking, Stor- age and Analysis , SC \u201920. IEEE Press. [446] S. Rajbhandari, O. Ruwase, J. Rasley, S. Smith and Y . He. 2021. Zero-infinity: Breaking the gpu memory wall for extreme scale deep learning. In Proceedings of the In- ternational Conference for High Performance Computing, Networking, Storage and Analysis , SC \u201921, New York, NY , USA. Association for Computing Machinery. [447] I. D. Raji, E. M. Bender, A. Paullada, E. Denton and A. Hanna. 2021. Ai and the everything in the whole wide world benchmark. arXiv preprint arXiv:2111.15366 . [448] A. Rajkomar, E. Loreaux, Y . Liu, J. Kemp, B. Li, M.-J. Chen, Y . Zhang, A. Mohiuddin et al. 2022. Deciphering eter models. In Proceedings of the International Confer- ence for High Performance Computing, Networking, Stor- age and Analysis , SC \u201920. IEEE Press. [446] S. Rajbhandari, O. Ruwase, J. Rasley, S. Smith and Y . He. 2021. Zero-infinity: Breaking the gpu memory wall for extreme scale deep learning. In Proceedings of the In- ternational Conference for High Performance Computing, Networking, Storage and Analysis , SC \u201921, New York, NY , USA. Association for Computing Machinery. [447] I. D. Raji, E. M. Bender, A. Paullada, E. Denton and A. Hanna. 2021. Ai and the everything in the whole wide world benchmark. arXiv preprint arXiv:2111.15366 . [448] A. Rajkomar, E. Loreaux, Y . Liu, J. Kemp, B. Li, M.-J. Chen, Y . Zhang, A. Mohiuddin et al. 2022. Deciphering clinical abbreviations with a privacy protecting machine learning system. Nature Communications , 13(1):7456. [449] R. Ramamurthy, P. Ammanabrolu, K. Brantley, J. Hes- sel, R. Sifa, C. Bauckhage, H. Hajishirzi and Y . Choi. 2022. Is reinforcement learning (not) for natural language processing?: Benchmarks, baselines, and building blocks for natural language policy optimization. arXiv preprint arXiv:2210.01241 . [450] J. Rasley, S. Rajbhandari, O. Ruwase and Y . He. 2020. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Pro- ceedings of the 26th ACM SIGKDD International Confer- ence on Knowledge Discovery & Data Mining , KDD \u201920, page 3505\u20133506, New York, NY , USA. Association for Computing Machinery. 64 [451] P. P. Ray. 2023. ChatGPT: A comprehensive review on background, applications, key challenges, bias, ethics, limitations and future scope. Internet of Things and Cyber- Physical Systems , 3:121\u2013154. [452] E. Razumovskaia, J. Maynez, A. Louis, M. Lapata and S. Narayan. 2022. Little red riding hood goes around the globe: Crosslingual story planning and generation with large language models. arXiv preprint arXiv:2212.10471 . [453] B. Recht, C. Re, S. Wright and F. Niu. 2011. Hogwild!: A lock-free approach to parallelizing stochastic gradient de- scent. Advances in neural information processing systems , 24. [454] J. Ren, S. Rajbhandari, R. Y . Aminabadi, O. Ruwase, S. Yang, M. Zhang, D. Li and Y . He. 2021. {ZeRO- Offload }: Democratizing {Billion-Scale }model training. In2021 USENIX Annual Technical Conference (USENIX ATC 21) , pages 551\u2013564. [455] X. Ren, P. Zhou, X. Meng, X. Huang, Y . Wang, W. Wang, P. Li, X. Zhang et al. 2023. Pangu- Physical Systems , 3:121\u2013154. [452] E. Razumovskaia, J. Maynez, A. Louis, M. Lapata and S. Narayan. 2022. Little red riding hood goes around the globe: Crosslingual story planning and generation with large language models. arXiv preprint arXiv:2212.10471 . [453] B. Recht, C. Re, S. Wright and F. Niu. 2011. Hogwild!: A lock-free approach to parallelizing stochastic gradient de- scent. Advances in neural information processing systems , 24. [454] J. Ren, S. Rajbhandari, R. Y . Aminabadi, O. Ruwase, S. Yang, M. Zhang, D. Li and Y . He. 2021. {ZeRO- Offload }: Democratizing {Billion-Scale }model training. In2021 USENIX Annual Technical Conference (USENIX ATC 21) , pages 551\u2013564. [455] X. Ren, P. Zhou, X. Meng, X. Huang, Y . Wang, W. Wang, P. Li, X. Zhang et al. 2023. Pangu- Sigma: Towards trillion parameter language model with sparse heterogeneous computing. [456] Riley Goodside [@goodside]. 2022. An edge-case in GPT-3 with big implications: Inference is non- deterministic (even at temperature=0) when top-2 token probabilities are <1% different. So temperature=0 output is *very close* to deterministic, but actually isn\u2019t. Worth remembering. [457] X. Robin, J. Haas, R. Gumienny, A. Smolinski, G. Tau- riello and T. Schwede. 2021. Continuous automated model evaluation (cameo)\u2014perspectives on the future of fully automated evaluation of structure prediction meth- ods. Proteins: Structure, Function, and Bioinformatics , 89(12):1977\u20131986. [458] A. Rohrbach, L. A. Hendricks, K. Burns, T. Darrell and K. Saenko. 2018. Object hallucination in image captioning. arXiv preprint arXiv:1809.02156 . [459] S. Roller, S. Sukhbaatar, A. Szlam and J. Weston. 2021. Hash layers for large sparse models. [460] G. M. Rosa, L. Bonifacio, V . Jeronymo, H. Abonizio, R. Lotufo and R. Nogueira. 2022. Billions of parame- ters are worth more than in-domain training data: A case study in the legal case entailment task. arXiv preprint arXiv:2205.15172 . [461] L. Ross, D. Greene and P. House. 1977. The \u201cfalse consensus effect\u201d: An egocentric bias in social perception and attribution processes. Journal of experimental social psychology , 13(3):279\u2013301. [462] Y . Rottenstreich and C. K. Hsee. 2001. Money, kisses, and electric shocks: On the affective psychology of risk. Psychological science , 12(3):185\u2013190. [463] A. Roush. You probably don\u2019t know how to do Prompt Engineering, let me educate you. [464] L. Ruis, A. Khan, S. Biderman, S. Hooker, T. Rock- Hash layers for large sparse models. [460] G. M. Rosa, L. Bonifacio, V . Jeronymo, H. Abonizio, R. Lotufo and R. Nogueira. 2022. Billions of parame- ters are worth more than in-domain training data: A case study in the legal case entailment task. arXiv preprint arXiv:2205.15172 . [461] L. Ross, D. Greene and P. House. 1977. The \u201cfalse consensus effect\u201d: An",
    "56": "egocentric bias in social perception and attribution processes. Journal of experimental social psychology , 13(3):279\u2013301. [462] Y . Rottenstreich and C. K. Hsee. 2001. Money, kisses, and electric shocks: On the affective psychology of risk. Psychological science , 12(3):185\u2013190. [463] A. Roush. You probably don\u2019t know how to do Prompt Engineering, let me educate you. [464] L. Ruis, A. Khan, S. Biderman, S. Hooker, T. Rock- t\u00e4schel and E. Grefenstette. 2022. Large language models are not zero-shot communicators. [465] J. Rumbelow and mwatkins. SolidGoldMagikarp (plus, prompt generation).[466] S. Russell. 2021. Human-compatible artificial intelli- gence. Human-like machine intelligence , pages 3\u201323. [467] P. Rust, J. F. Lotz, E. Bugliarello, E. Salesky, M. de Lhoneux and D. Elliott. 2023. Language Modelling with Pixels. ArXiv:2207.06991 [cs]. [468] A. Sabne. 2020. Xla : Compiling machine learning for peak performance. [469] V . S. Sadasivan, A. Kumar, S. Balasubramanian, W. Wang and S. Feizi. 2023. Can AI-Generated Text be Reliably Detected? ArXiv:2303.11156 [cs]. [470] M. Safdari, G. Serapio-Garc\u00eda, C. Crepy, S. Fitz, P. Romero, L. Sun, M. Abdulhai, A. Faust et al. 2023. Personality traits in large language models. [471] S. Sagawa, P. W. Koh, T. B. Hashimoto and P. Liang. 2020. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. [472] O. Sainz, J. C. Campos, I. Garc\u00eda-Ferrero, J. Etxaniz and E. Agirre. lm-contamination. [473] L. Salewski, S. Alaniz, I. Rio-Torto, E. Schulz and Z. Akata. 2023. In-context impersonation reveals large language models\u2019 strengths and biases. arXiv preprint arXiv:2305.14930 . [474] G. Sanchez, H. Fan, A. Spangher, E. Levi, P. S. Am- manamanchi and S. Biderman. 2023. Stay on topic with Classifier-Free Guidance. ArXiv:2306.17806 [cs]. [475] V . Sanh, A. Webson, C. Raffel, S. Bach, L. Sutawika, Z. Alyafeai, A. Chaffin, A. Stiegler et al. 2022. Multitask prompted training enables zero-shot task generalization. In International Conference on Learning Representations . shifts: On the importance of regularization for worst-case generalization. [472] O. Sainz, J. C. Campos, I. Garc\u00eda-Ferrero, J. Etxaniz and E. Agirre. lm-contamination. [473] L. Salewski, S. Alaniz, I. Rio-Torto, E. Schulz and Z. Akata. 2023. In-context impersonation reveals large language models\u2019 strengths and biases. arXiv preprint arXiv:2305.14930 . [474] G. Sanchez, H. Fan, A. Spangher, E. Levi, P. S. Am- manamanchi and S. Biderman. 2023. Stay on topic with Classifier-Free Guidance. ArXiv:2306.17806 [cs]. [475] V . Sanh, A. Webson, C. Raffel, S. Bach, L. Sutawika, Z. Alyafeai, A. Chaffin, A. Stiegler et al. 2022. Multitask prompted training enables zero-shot task generalization. In International Conference on Learning Representations . [476] S. Sanyal, J. Kaddour, A. Kumar and S. Sanghavi. 2023. Understanding the effectiveness of early weight averaging for training large language models. [477] E. Saravia. 2022. Prompt Engineering Guide. Publica- tion Title: https://github.com/dair-ai/Prompt-Engineering- Guide original-date: 2022-12-16T16:04:50Z. [478] J. Savelka, K. D. Ashley, M. A. Gray, H. Westermann and H. Xu. 2023. Explaining legal concepts with augmented large language models (gpt-4). [479] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ili \u00b4c, D. Hess- low, R. Castagn\u00e9, A. S. Luccioni et al. 2022. Bloom: A 176b-parameter open-access multilingual language model. [480] R. Schaeffer, B. Miranda and S. Koyejo. 2023. Are emergent abilities of large language models a mirage? [481] T. Schick, J. Dwivedi-Yu, R. Dess\u00ec, R. Raileanu, M. Lomeli, L. Zettlemoyer, N. Cancedda and T. Scialom. 2023. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761 . [482] T. Schick, J. Dwivedi-Yu, Z. Jiang, F. Petroni, P. Lewis, G. Izacard, Q. You, C. Nalmpantis et al. 2022. Peer: A collaborative language model. [483] T. Schick and H. Sch\u00fctze. 2021. It\u2019s not just size that matters: Small language models are also few-shot learn- ers. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 2339\u2013 2352. 65 [484] J. Schulman, F. Wolski, P. Dhariwal, A. Radford and O. Klimov. 2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347 . [485] M. Schuster and K. Nakajima. 2012. Japanese and ko- rean voice search. In 2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) , pages 5149\u20135152. [482] T. Schick, J. Dwivedi-Yu, Z. Jiang, F. Petroni, P. Lewis, G. Izacard, Q. You, C. Nalmpantis et al. 2022. Peer: A collaborative language model. [483] T. Schick and H. Sch\u00fctze. 2021. It\u2019s not just size that matters: Small language models are also few-shot learn- ers. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 2339\u2013 2352. 65 [484] J. Schulman, F. Wolski, P. Dhariwal, A. Radford and O. Klimov. 2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347 . [485] M. Schuster and K. Nakajima. 2012. Japanese and ko- rean voice search. In 2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) , pages 5149\u20135152. [486] T. Schuster, R. Schuster, D. J. Shah and R. Barzi- lay. 2020. The limitations of stylometry for detecting machine-generated fake news. Computational Linguistics , 46(2):499\u2013510. [487] R. Schwartz, J. Dodge, N. A. Smith and O. Etzioni. 2019. Green AI. ArXiv:1907.10597 [cs, stat]. [488] S. H. Schwartz, B. Breyer and D. Danner. 2015. Hu- man values scale (ess). Zusammenstellung sozialwis- senschaftlicher Items und Skalen (ZIS) . [489] A. See, A. Pappu, R. Saxena, A. Yerukola and C. D. Manning. 2019. Do massively pretrained language mod- els make better storytellers? In Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL) , pages 843\u2013861, Hong Kong, China. Association for Computational Linguistics. [490] R. Sennrich, B. Haddow and A. Birch. 2015. Neural ma- chine translation of rare words with subword units. arXiv preprint arXiv:1508.07909 . [491] E. Sezgin, J. Sirrianni, S. L. Linwood et al. 2022. Oper- ationalizing and implementing pretrained, large artificial intelligence linguistic models in the us health care system: Outlook of generative pretrained transformer 3 (gpt-3) as a service model. JMIR Medical Informatics , 10(2):e32875. [492] P. Shaw, J. Uszkoreit and A. Vaswani. 2018. Self- attention with relative position representations. In Pro- ceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers) , pages 464\u2013468, New Orleans, Louisiana. Association for Computational Linguistics. [493] N. Shazeer. 2019. Fast transformer decoding: One write- head is all you need. [494] N. Shazeer. 2019. Fast transformer decoding: One write- head is all you need. ationalizing and implementing pretrained, large artificial intelligence linguistic models in the us health care system: Outlook of generative pretrained transformer 3 (gpt-3) as a service model. JMIR Medical Informatics , 10(2):e32875. [492] P. Shaw, J. Uszkoreit and A. Vaswani. 2018. Self- attention with relative position representations. In Pro- ceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers) , pages 464\u2013468, New Orleans, Louisiana. Association for Computational Linguistics. [493] N. Shazeer. 2019. Fast transformer decoding: One write- head is all you need. [494] N. Shazeer. 2019. Fast transformer decoding: One write- head is all you need. [495] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton and J. Dean. 2017. Outrageously large neu- ral networks: The",
    "57": "sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538 . [496] Z. Shen, M. Zhang, H. Zhao, S. Yi and H. Li. 2021. Efficient attention: Attention with linear complexities. In Proceedings of the IEEE/CVF winter conference on appli- cations of computer vision , pages 3531\u20133539. [497] Y . Sheng, L. Zheng, B. Yuan, Z. Li, M. Ryabinin, B. Chen, P. Liang, C. R\u00e9 et al. 2023. High-throughput generative inference of large language models with a single gpu. [498] T. Shevlane, S. Farquhar, B. Garfinkel, M. Phuong, J. Whittlestone, J. Leung, D. Kokotajlo, N. Marchal et al. 2023. Model evaluation for extreme risks. arXiv preprint arXiv:2305.15324 .[499] A. Shirafuji, Y . Watanobe, T. Ito, M. Morishita, Y . Naka- mura, Y . Oda and J. Suzuki. 2023. Exploring the robust- ness of large language models for solving programming problems. [500] O. Shliazhko, A. Fenogenova, M. Tikhonova, V . Mikhailov, A. Kozlova and T. Shavrina. 2022. mgpt: Few-shot learners go multilingual. arXiv preprint arXiv:2204.07580 . [501] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper and B. Catanzaro. 2019. Megatron-lm: Training multi- billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053 . [502] K. Shridhar, J. Macina, M. El-Assady, T. Sinha, M. Ka- pur and M. Sachan. 2022. Automatic generation of socratic subquestions for teaching math word problems. ArXiv , abs/2211.12835. [503] K. Shridhar, A. Stolfo and M. Sachan. 2022. Distilling multi-step reasoning capabilities of large language models into smaller models via semantic decompositions. arXiv problems. [500] O. Shliazhko, A. Fenogenova, M. Tikhonova, V . Mikhailov, A. Kozlova and T. Shavrina. 2022. mgpt: Few-shot learners go multilingual. arXiv preprint arXiv:2204.07580 . [501] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper and B. Catanzaro. 2019. Megatron-lm: Training multi- billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053 . [502] K. Shridhar, J. Macina, M. El-Assady, T. Sinha, M. Ka- pur and M. Sachan. 2022. Automatic generation of socratic subquestions for teaching math word problems. ArXiv , abs/2211.12835. [503] K. Shridhar, A. Stolfo and M. Sachan. 2022. Distilling multi-step reasoning capabilities of large language models into smaller models via semantic decompositions. arXiv preprint arXiv:2212.00193 . [504] D. Shrivastava, H. Larochelle and D. Tarlow. 2022. Repository-level prompt generation for large language mod- els of code. arXiv preprint arXiv:2206.12839 . [505] R. W. Shuai, J. A. Ruffolo and J. J. Gray. 2021. Gen- erative language modeling for antibody design. bioRxiv , pages 2021\u201312. [506] I. Shumailov, Z. Shumaylov, Y . Zhao, Y . Gal, N. Pa- pernot and R. Anderson. 2023. The curse of recursion: Training on generated data makes models forget. [507] K. Shuster, S. Poff, M. Chen, D. Kiela and J. Weston. 2021. Retrieval augmentation reduces hallucination in conversation. arXiv preprint arXiv:2104.07567 . [508] K. Shuster, J. Xu, M. Komeili, D. Ju, E. M. Smith, S. Roller, M. Ung, M. Chen et al. 2022. Blenderbot 3: a deployed conversational agent that continually learns to responsibly engage. [509] S. Sia and K. Duh. 2023. In-context learning as maintain- ing coherency: A study of on-the-fly machine translation using large language models. ArXiv , abs/2305.03573. [510] I. Singh, V . Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox, J. Thomason et al. 2022. Progprompt: Generating situated robot task plans using large language models. [511] K. Singhal, S. Azizi, T. Tu, S. S. Mahdavi, J. Wei, H. W. Chung, N. Scales, A. Tanwani et al. 2022. Large language models encode clinical knowledge. [512] K. Singhal, T. Tu, J. Gottweis, R. Sayres, E. Wulczyn, L. Hou, K. Clark, S. Pfohl et al. 2023. Towards expert-level medical question answering with large language models. arXiv preprint arXiv:2305.09617 . [513] A. Sinitsin, D. Pyrkin, A. Babenko, V . Plokhotnyuk and S. Popov. 2020. EDITABLE NEURAL NETWORKS. [514] S. L. Smith, P.-J. Kindermans, C. Ying and Q. V . Le. using large language models. ArXiv , abs/2305.03573. [510] I. Singh, V . Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox, J. Thomason et al. 2022. Progprompt: Generating situated robot task plans using large language models. [511] K. Singhal, S. Azizi, T. Tu, S. S. Mahdavi, J. Wei, H. W. Chung, N. Scales, A. Tanwani et al. 2022. Large language models encode clinical knowledge. [512] K. Singhal, T. Tu, J. Gottweis, R. Sayres, E. Wulczyn, L. Hou, K. Clark, S. Pfohl et al. 2023. Towards expert-level medical question answering with large language models. arXiv preprint arXiv:2305.09617 . [513] A. Sinitsin, D. Pyrkin, A. Babenko, V . Plokhotnyuk and S. Popov. 2020. EDITABLE NEURAL NETWORKS. [514] S. L. Smith, P.-J. Kindermans, C. Ying and Q. V . Le. 2017. Don\u2019t decay the learning rate, increase the batch size. arXiv preprint arXiv:1711.00489 . 66 [515] S. Smith, M. Patwary, B. Norick, P. LeGresley, S. Ra- jbhandari, J. Casper, Z. Liu, S. Prabhumoye et al. 2022. Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. arXiv preprint arXiv:2201.11990 . [516] I. Solaiman and C. Dennison. 2021. Process for adapting language models to society (palms) with values-targeted datasets. Advances in Neural Information Processing Sys- tems, 34:5861\u20135873. [517] S. Soltan, S. Ananthakrishnan, J. FitzGerald, R. Gupta, W. Hamza, H. Khan, C. Peris, S. Rawls et al. 2022. Alex- atm 20b: Few-shot learning using a large-scale multilingual seq2seq model. arXiv preprint arXiv:2208.01448 . [518] B. Sorscher, R. Geirhos, S. Shekhar, S. Ganguli and A. S. Morcos. 2022. Beyond neural scaling laws: beat- ing power law scaling via data pruning. arXiv preprint arXiv:2206.14486 . [519] A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb, A. Abid, A. Fisch, A. R. Brown, A. Santoro et al. 2022. Beyond the imitation game: Quantifying and extrapolat- ing the capabilities of language models. arXiv preprint arXiv:2206.04615 . [520] J. Steinhardt. 2022. Future ml systems will be qualita- tively different. Accessed May , 20:2022. [521] J. Steinhardt. 2023. Emergent deception and emergent optimization. Available from: https://bounded-regret.ghost.io/ emergent-deception-optimization/ . Ac- cessed: 29/04/2023. [522] M. Stern, N. Shazeer and J. Uszkoreit. 2018. Block- wise parallel decoding for deep autoregressive models. InProceedings of the 32nd International Conference on Neural Information Processing Systems , NIPS\u201918, page arXiv:2206.14486 . [519] A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb, A. Abid, A. Fisch, A. R. Brown, A. Santoro et al. 2022. Beyond the imitation game: Quantifying and extrapolat- ing the capabilities of language models. arXiv preprint arXiv:2206.04615 . [520] J. Steinhardt. 2022. Future ml systems will be qualita- tively different. Accessed May , 20:2022. [521] J. Steinhardt. 2023. Emergent deception and emergent optimization. Available from: https://bounded-regret.ghost.io/ emergent-deception-optimization/ . Ac- cessed: 29/04/2023. [522] M. Stern, N. Shazeer and J. Uszkoreit. 2018. Block- wise parallel decoding for deep autoregressive models. InProceedings of the 32nd International Conference on Neural Information Processing Systems , NIPS\u201918, page 10107\u201310116, Red Hook, NY , USA. Curran Associates Inc. [523] C. Stevenson, I. Smal, M. Baas, R. Grasman and H. van der Maas. 2022. Putting gpt-3\u2019s creativity to the (alternative uses) test. arXiv preprint arXiv:2206.08932 . [524] N. Stiennon, L. Ouyang, J. Wu, D. Ziegler, R. Lowe, C. V oss, A. Radford, D. Amodei et al. 2020. Learning to summarize with human feedback. In Conference on Neural Information Processing Systems . [525] A. Stolfo, Z. Jin, K. Shridhar, B. Sch\u00f6lkopf and M. Sachan. 2022. A causal framework to quantify the robustness of mathematical reasoning with language mod- els. [526] J. Su, Y . Lu, S. Pan, B. Wen and Y . Liu. 2021. Roformer: Enhanced",
    "58": "transformer with rotary position embedding. [527] M. Sun, Z. Liu, A. Bair and J. Z. Kolter. 2023. A simple and effective pruning approach for large language models. [528] T. Sun, Y . Shao, H. Qian, X. Huang and X. Qiu. 2022. Black-box tuning for language-model-as-a-service. In Pro- ceedings of the 39th International Conference on Machine Learning , volume 162 of Proceedings of Machine Learning Research , pages 20841\u201320855. PMLR.[529] X. Sun, T. Ge, F. Wei and H. Wang. 2021. Instanta- neous grammatical error correction with shallow aggres- sive decoding. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) , pages 5937\u20135947, Online. Association for Computational Linguistics. [530] Y . Sun, S. Wang, S. Feng, S. Ding, C. Pang, J. Shang, J. Liu, X. Chen et al. 2021. Ernie 3.0: Large-scale knowl- edge enhanced pre-training for language understanding and generation. arXiv preprint arXiv:2107.02137 . ceedings of the 39th International Conference on Machine Learning , volume 162 of Proceedings of Machine Learning Research , pages 20841\u201320855. PMLR.[529] X. Sun, T. Ge, F. Wei and H. Wang. 2021. Instanta- neous grammatical error correction with shallow aggres- sive decoding. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) , pages 5937\u20135947, Online. Association for Computational Linguistics. [530] Y . Sun, S. Wang, S. Feng, S. Ding, C. Pang, J. Shang, J. Liu, X. Chen et al. 2021. Ernie 3.0: Large-scale knowl- edge enhanced pre-training for language understanding and generation. arXiv preprint arXiv:2107.02137 . [531] Z. Sun. 2023. A short survey of viewing large language models in legal aspect. [532] D. Sur\u00eds, S. Menon and C. V ondrick. 2023. Vipergpt: Visual inference via python execution for reasoning. arXiv preprint arXiv:2303.08128 . [533] Susan Zhang [@suchenzang]. 2023. Piling on to the pile-on (sorry - it\u2019s always easy to criticize), here\u2019s a rant about benchmarks for LLMs that are used to back claims of \"stronger\" or \"better\" models. Let\u2019s start with a tour through GPT-3\u2019s Appendix G... 1/8. [534] M. Suzgun, N. Scales, N. Sch\u00e4rli, S. Gehrmann, Y . Tay, H. W. Chung, A. Chowdhery, Q. V . Le et al. 2022. Chal- lenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261 . [535] S. Swaminathan, A. Dedieu, R. V . Raju, M. Shanahan, M. Lazaro-Gredilla and D. George. 2023. Schema-learning and rebinding as mechanisms of in-context learning and emergence. ArXiv:2307.01201 [cs]. [536] H. Tang, S. Gan, A. A. Awan, S. Rajbhandari, C. Li, X. Lian, J. Liu, C. Zhang et al. 2021. 1-bit adam: Com- munication efficient large-scale training with adam\u2019s con- vergence speed. In Proceedings of the 38th International Conference on Machine Learning , volume 139 of Proceed- ings of Machine Learning Research , pages 10118\u201310129. PMLR. [537] L. Tang, G. Uberti and T. Shlomi. 2023. Baselines for Identifying Watermarked Large Language Models. ArXiv:2305.18456 [cs]. [538] L. Tang, Z. Sun, B. Idnay, J. G. Nestor, A. Soroush, P. A. Elias, Z. Xu, Y . Ding et al. 2023. Evaluating large language models on medical evidence summarization. medRxiv , pages 2023\u201304. [539] R. Tang, Y .-N. Chuang and X. Hu. 2023. The Science of Detecting LLM-Generated Texts. ArXiv:2303.07205 [cs]. [536] H. Tang, S. Gan, A. A. Awan, S. Rajbhandari, C. Li, X. Lian, J. Liu, C. Zhang et al. 2021. 1-bit adam: Com- munication efficient large-scale training with adam\u2019s con- vergence speed. In Proceedings of the 38th International Conference on Machine Learning , volume 139 of Proceed- ings of Machine Learning Research , pages 10118\u201310129. PMLR. [537] L. Tang, G. Uberti and T. Shlomi. 2023. Baselines for Identifying Watermarked Large Language Models. ArXiv:2305.18456 [cs]. [538] L. Tang, Z. Sun, B. Idnay, J. G. Nestor, A. Soroush, P. A. Elias, Z. Xu, Y . Ding et al. 2023. Evaluating large language models on medical evidence summarization. medRxiv , pages 2023\u201304. [539] R. Tang, Y .-N. Chuang and X. Hu. 2023. The Science of Detecting LLM-Generated Texts. ArXiv:2303.07205 [cs]. [540] R. Taori, I. Gulrajani, T. Zhang, Y . Dubois, X. Li, C. Guestrin, P. Liang and T. B. Hashimoto. 2023. Alpaca: A strong, replicable instruction-following model. [541] Y . Tay, D. Bahri, D. Metzler, D.-C. Juan, Z. Zhao and C. Zheng. 2021. Synthesizer: Rethinking self-attention for transformer models. In International conference on machine learning , pages 10183\u201310192. PMLR. [542] Y . Tay, M. Dehghani, S. Abnar, H. W. Chung, W. Fedus, J. Rao, S. Narang, V . Q. Tran et al. 2022. Scaling laws vs model architectures: How does inductive bias influence scaling? 67 [543] Y . Tay, M. Dehghani, D. Bahri and D. Metzler. 2022. Efficient transformers: A survey. ACM Computing Surveys , 55(6):1\u201328. [544] Y . Tay, M. Dehghani, J. Rao, W. Fedus, S. Abnar, H. W. Chung, S. Narang, D. Yogatama et al. 2022. Scale Effi- ciently: Insights from Pre-training and Fine-tuning Trans- formers. ArXiv:2109.10686 [cs]. [545] Y . Tay, M. Dehghani, V . Q. Tran, X. Garcia, J. Wei, X. Wang, H. W. Chung, D. Bahri et al. 2022. Ul2: Unifying language learning paradigms. [546] Y . Tay, V . Q. Tran, S. Ruder, J. Gupta, H. W. Chung, D. Bahri, Z. Qin, S. Baumgartner et al. 2022. Charformer: Fast character transformers via gradient-based subword tokenization. [547] Y . Tay, J. Wei, H. W. Chung, V . Q. Tran, D. R. So, S. Shakeri, X. Garcia, H. S. Zheng et al. 2022. Transcend- ing scaling laws with 0.1% extra compute. [548] R. Taylor, M. Kardas, G. Cucurull, T. Scialom, A. Hartshorn, E. Saravia, A. Poulton, V . Kerkez et al. 2022. Galactica: A large language model for science. arXiv preprint arXiv:2211.09085 . [549] W. L. Taylor. 1953. \u201ccloze procedure\u201d: A new tool for measuring readability. Journalism quarterly , 30(4):415\u2013 X. Wang, H. W. Chung, D. Bahri et al. 2022. Ul2: Unifying language learning paradigms. [546] Y . Tay, V . Q. Tran, S. Ruder, J. Gupta, H. W. Chung, D. Bahri, Z. Qin, S. Baumgartner et al. 2022. Charformer: Fast character transformers via gradient-based subword tokenization. [547] Y . Tay, J. Wei, H. W. Chung, V . Q. Tran, D. R. So, S. Shakeri, X. Garcia, H. S. Zheng et al. 2022. Transcend- ing scaling laws with 0.1% extra compute. [548] R. Taylor, M. Kardas, G. Cucurull, T. Scialom, A. Hartshorn, E. Saravia, A. Poulton, V . Kerkez et al. 2022. Galactica: A large language model for science. arXiv preprint arXiv:2211.09085 . [549] W. L. Taylor. 1953. \u201ccloze procedure\u201d: A new tool for measuring readability. Journalism quarterly , 30(4):415\u2013 433. [550] J. Thiergart, S. Huber and T. \u00dcbellacker. 2021. Under- standing emails and drafting responses\u2013an approach using gpt-3. arXiv preprint arXiv:2102.03062 . [551] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kul- shreshtha, H.-T. Cheng, A. Jin, T. Bos et al. 2022. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239 . [552] R. Tian, S. Narayan, T. Sellam and A. P. Parikh. 2020. Sticking to the Facts: Confident Decoding for Faithful Data-to-Text Generation. ArXiv:1910.08684 [cs]. [553] K. Tirumala, A. H. Markosyan, L. Zettlemoyer and A. Aghajanyan. Memorization Without Overfitting: Ana- lyzing the Training Dynamics of Large Language Models. [554] H. Q. To, N. D. Bui, J. Guo and T. N. Nguyen. 2023. Better language models of code through self-improvement. arXiv preprint arXiv:2304.01228 . [555] A. Tornede, D. Deng, T. Eimer, J. Giovanelli, A. Mohan, T. Ruhkopf, S. Segel, D. Theodorakopoulos et al. 2023. Au- toML in the Age of Large Language Models: Current Chal- lenges, Future Opportunities and Risks. ArXiv:2306.08107 [cs]. [556] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi\u00e8re, N. Goyal et al. 2023. LLaMA: Open and Efficient Foundation",
    "59": "Language Models. ArXiv:2302.13971 [cs]. [557] H. Touvron, L. Martin and K. Stone. Llama 2: Open Foundation and Fine-Tuned Chat Models. [558] C. Tran, S. Khadkikar and A. Porollo. 2023. Survey of protein sequence embedding models. International Journal of Molecular Sciences , 24(4):3775.[559] A. Uchendu, T. Le, K. Shu and D. Lee. 2020. Authorship Attribution for Neural Text Generation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 8384\u20138395, Online. toML in the Age of Large Language Models: Current Chal- lenges, Future Opportunities and Risks. ArXiv:2306.08107 [cs]. [556] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi\u00e8re, N. Goyal et al. 2023. LLaMA: Open and Efficient Foundation Language Models. ArXiv:2302.13971 [cs]. [557] H. Touvron, L. Martin and K. Stone. Llama 2: Open Foundation and Fine-Tuned Chat Models. [558] C. Tran, S. Khadkikar and A. Porollo. 2023. Survey of protein sequence embedding models. International Journal of Molecular Sciences , 24(4):3775.[559] A. Uchendu, T. Le, K. Shu and D. Lee. 2020. Authorship Attribution for Neural Text Generation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 8384\u20138395, Online. Association for Computational Linguistics. [560] J. Uesato, N. Kushman, R. Kumar, F. Song, N. Siegel, L. Wang, A. Creswell, G. Irving et al. 2022. Solving math word problems with process- and outcome-based feedback. [561] S. University. 2023. Holistic evaluation of langauge models results page. Available from: https://crfm. stanford.edu/helm/latest/?groups=1 . Ac- cessed: 23/03/2023. [562] K. Valmeekam, A. Olmo, S. Sreedharan and S. Kamb- hampati. 2023. Large language models still can\u2019t plan (a benchmark for llms on planning and reasoning about change). [563] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u. Kaiser and I. Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems , volume 30. Curran Asso- ciates, Inc. [564] S. Vemprala, R. Bonatti, A. Bucker and A. Kapoor. 2023. Chatgpt for robotics: Design principles and model abilities. [565] A. Venigalla, J. Frankle and M. Carbin. 2022. Pubmed gpt: A domain- specific large language model for biomed- ical text. https://www.mosaicml.com/blog/ introducing-pubmed-gpt . Accessed: 2023-01-24. [566] R. Verkuil, O. Kabeli, Y . Du, B. I. Wicky, L. F. Milles, J. Dauparas, D. Baker, S. Ovchinnikov et al. 2022. Lan- guage models generalize beyond natural proteins. bioRxiv , pages 2022\u201312. [567] A. Vijayakumar, M. Cogswell, R. Selvaraju, Q. Sun, S. Lee, D. Crandall and D. Batra. 2018. Diverse beam search for improved description of complex scenes. Pro- ceedings of the AAAI Conference on Artificial Intelligence , 32(1). [568] P. Villalobos, J. Sevilla, L. Heim, T. Besiroglu, M. Hobb- hahn and A. Ho. 2022. Will we run out of data? an analysis [565] A. Venigalla, J. Frankle and M. Carbin. 2022. Pubmed gpt: A domain- specific large language model for biomed- ical text. https://www.mosaicml.com/blog/ introducing-pubmed-gpt . Accessed: 2023-01-24. [566] R. Verkuil, O. Kabeli, Y . Du, B. I. Wicky, L. F. Milles, J. Dauparas, D. Baker, S. Ovchinnikov et al. 2022. Lan- guage models generalize beyond natural proteins. bioRxiv , pages 2022\u201312. [567] A. Vijayakumar, M. Cogswell, R. Selvaraju, Q. Sun, S. Lee, D. Crandall and D. Batra. 2018. Diverse beam search for improved description of complex scenes. Pro- ceedings of the AAAI Conference on Artificial Intelligence , 32(1). [568] P. Villalobos, J. Sevilla, L. Heim, T. Besiroglu, M. Hobb- hahn and A. Ho. 2022. Will we run out of data? an analysis of the limits of scaling datasets in machine learning. arXiv preprint arXiv:2211.04325 . [569] H. Viswanath and T. Zhang. 2023. Fairpy: A toolkit for evaluation of social biases and their mitigation in large language models. arXiv preprint arXiv:2302.05508 . [570] J. von Oswald, E. Niklasson, E. Randazzo, J. Sacra- mento, A. Mordvintsev, A. Zhmoginov and M. Vladymy- rov. 2022. Transformers learn in-context by gradient de- scent. arXiv preprint arXiv:2212.07677 . [571] H. d. Vries. 2023. Go smol or go home. [572] T. Vu, B. Lester, N. Constant, R. Al-Rfou\u2019 and D. Cer. 2022. SPoT: Better frozen model adaptation through soft prompt transfer. In Proceedings of the 60th Annual Meet- ing of the Association for Computational Linguistics (Vol- ume 1: Long Papers) , pages 5039\u20135059, Dublin, Ireland. Association for Computational Linguistics. [573] J. P. Wahle, T. Ruas, T. Folt `ynek, N. Meuschke and B. Gipp. 2022. Identifying machine-paraphrased plagia- rism. In International Conference on Information , pages 393\u2013413. Springer. 68 [574] J. P. Wahle, T. Ruas, F. Kirstein and B. Gipp. 2022. How large language models are transforming machine- paraphrased plagiarism. arXiv preprint arXiv:2210.03568 . [575] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy and S. Bowman. 2018. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP , pages 353\u2013355, Brussels, Belgium. Association for Com- putational Linguistics. [576] B. Wang and A. Komatsuzaki. 2021. GPT-J- 6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/ mesh-transformer-jax . rism. In International Conference on Information , pages 393\u2013413. Springer. 68 [574] J. P. Wahle, T. Ruas, F. Kirstein and B. Gipp. 2022. How large language models are transforming machine- paraphrased plagiarism. arXiv preprint arXiv:2210.03568 . [575] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy and S. Bowman. 2018. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP , pages 353\u2013355, Brussels, Belgium. Association for Com- putational Linguistics. [576] B. Wang and A. Komatsuzaki. 2021. GPT-J- 6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/ mesh-transformer-jax . [577] C. Wang, K. Cho and J. Gu. 2020. Neural machine translation with byte-level subwords. Proceedings of the AAAI Conference on Artificial Intelligence , 34(05):9154\u2013 9160. [578] C. Wang, X. Liu, Z. Chen, H. Hong, J. Tang and D. Song. 2022. DeepStruct: Pretraining of language models for structure prediction. In Findings of the Association for Computational Linguistics: ACL 2022 , pages 803\u2013823, Dublin, Ireland. Association for Computational Linguis- tics. [579] G. Wang, Y . Xie, Y . Jiang, A. Mandlekar, C. Xiao, Y . Zhu, L. Fan and A. Anandkumar. 2023. V oyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291 . [580] H. Wang, J. Kaddour, S. Liu, J. Tang, M. Kusner, J. Lasenby and Q. Liu. 2022. Evaluating self-supervised learning for molecular graph embeddings. arXiv preprint arXiv:2206.08005 . [581] P. Wang, L. Li, L. Chen, D. Zhu, B. Lin, Y . Cao, Q. Liu, T. Liu et al. 2023. Large Language Models are not Fair Evaluators. ArXiv:2305.17926 [cs]. [582] R. Wang, H. Wang, F. Mi, Y . Chen, R. Xu and K.- F. Wong. 2023. Self-critique prompting with large lan- guage models for inductive instructions. arXiv preprint arXiv:2305.13733 . [583] S. Wang, Y . Liu, Y . Xu, C. Zhu and M. Zeng. 2021. Want to reduce labeling cost? gpt-3 can help. [584] S. Wang, S. Menon, T. Long, K. Henderson, D. Li, K. Crowston, M. Hansen, J. V . Nickerson et al. 2023. Reel- framer: Co-creating news reels on social media with gener- ative ai. arXiv preprint arXiv:2304.09653 . [585] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery and D. Zhou. 2022. Self- consistency improves chain of thought reasoning in lan- guage models. [586] Y . Wang, Z. Yu, Z. Zeng, L. Yang, C. Wang, H. Chen, Evaluators. ArXiv:2305.17926 [cs]. [582] R. Wang, H. Wang, F.",
    "60": "Mi, Y . Chen, R. Xu and K.- F. Wong. 2023. Self-critique prompting with large lan- guage models for inductive instructions. arXiv preprint arXiv:2305.13733 . [583] S. Wang, Y . Liu, Y . Xu, C. Zhu and M. Zeng. 2021. Want to reduce labeling cost? gpt-3 can help. [584] S. Wang, S. Menon, T. Long, K. Henderson, D. Li, K. Crowston, M. Hansen, J. V . Nickerson et al. 2023. Reel- framer: Co-creating news reels on social media with gener- ative ai. arXiv preprint arXiv:2304.09653 . [585] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery and D. Zhou. 2022. Self- consistency improves chain of thought reasoning in lan- guage models. [586] Y . Wang, Z. Yu, Z. Zeng, L. Yang, C. Wang, H. Chen, C. Jiang, R. Xie et al. 2023. Pandalm: An automatic eval- uation benchmark for llm instruction tuning optimization. arXiv preprint arXiv:2306.05087 . [587] Y . Wang. 2021. Comment section personalization: Algo- rithmic, interface, and interaction design. In Proceedings of the EACL Hackashop on News Media Content Analysis and Automated Report Generation , pages 84\u201388, Online. Association for Computational Linguistics.[588] Y . Wang, Y . Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi and H. Hajishirzi. 2022. Self-instruct: Align- ing language model with self generated instructions. [589] Y . Wang, S. Mishra, P. Alipoormolabashi, Y . Kordi, A. Mirzaei, A. Naik, A. Ashok, A. S. Dhanasekaran et al. 2022. Super-naturalinstructions: Generalization via declar- ative instructions on 1600+ nlp tasks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing , pages 5085\u20135109. [590] Y . Wang, Y . Zhao and L. Petzold. 2023. Are large lan- guage models ready for healthcare? a comparative study on clinical language understanding. [591] Z. Wang, S. Cai, A. Liu, X. Ma and Y . Liang. 2023. Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. arXiv preprint arXiv:2302.01560 . [592] Z. Wang, J. Wohlwend and T. Lei. 2019. Struc- tured pruning of large language models. arXiv preprint arXiv:1910.04732 . [593] Z. Wang, Z. Dai, B. P\u00f3czos and J. Carbonell. 2019. Char- acterizing and avoiding negative transfer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 11293\u201311302. [594] Z. Wang, M. Zoghi, F. Hutter, D. Matheson, N. De Fre- [590] Y . Wang, Y . Zhao and L. Petzold. 2023. Are large lan- guage models ready for healthcare? a comparative study on clinical language understanding. [591] Z. Wang, S. Cai, A. Liu, X. Ma and Y . Liang. 2023. Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. arXiv preprint arXiv:2302.01560 . [592] Z. Wang, J. Wohlwend and T. Lei. 2019. Struc- tured pruning of large language models. arXiv preprint arXiv:1910.04732 . [593] Z. Wang, Z. Dai, B. P\u00f3czos and J. Carbonell. 2019. Char- acterizing and avoiding negative transfer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 11293\u201311302. [594] Z. Wang, M. Zoghi, F. Hutter, D. Matheson, N. De Fre- itas et al. 2013. Bayesian optimization in high dimensions via random embeddings. In IJCAI , volume 13, pages 1778\u2013 1784. [595] T. Webb, K. J. Holyoak and H. Lu. 2022. Emergent analogical reasoning in large language models. [596] A. Webson and E. Pavlick. 2022. Do prompt-based models really understand the meaning of their prompts? In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 2300\u20132344, Seattle, United States. Association for Computational Linguistics. [597] A. Wei, N. Haghtalab and J. Steinhardt. 2023. Jailbroken: How Does LLM Safety Training Fail? ArXiv:2307.02483 [cs]. [598] J. Wei, M. Bosma, V . Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai et al. 2022. Finetuned language models are zero-shot learners. In International Conference on Learning Representations . [599] J. Wei, Y . Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yogatama, M. Bosma et al. 2022. Emer- gent abilities of large language models. [600] J. Wei, Y . Tay and Q. V . Le. 2022. Inverse scaling can become u-shaped. arXiv preprint arXiv:2211.02011 . [601] J. Wei, X. Wang, D. Schuurmans, M. Bosma, brian ichter, F. Xia, E. H. Chi, Q. V . Le et al. 2022. Chain of thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems . [602] L. Weidinger, J. Mellor, M. Rauh, C. Griffin, J. Uesato, P.-S. Huang, M. Cheng, M. Glaese et al. 2021. Ethical and social risks of harm from language models. arXiv preprint arXiv:2112.04359 . [603] M. Weiss. 2019. Deepfake bot submissions to federal public comment websites cannot be distinguished from S. Borgeaud, D. Yogatama, M. Bosma et al. 2022. Emer- gent abilities of large language models. [600] J. Wei, Y . Tay and Q. V . Le. 2022. Inverse scaling can become u-shaped. arXiv preprint arXiv:2211.02011 . [601] J. Wei, X. Wang, D. Schuurmans, M. Bosma, brian ichter, F. Xia, E. H. Chi, Q. V . Le et al. 2022. Chain of thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems . [602] L. Weidinger, J. Mellor, M. Rauh, C. Griffin, J. Uesato, P.-S. Huang, M. Cheng, M. Glaese et al. 2021. Ethical and social risks of harm from language models. arXiv preprint arXiv:2112.04359 . [603] M. Weiss. 2019. Deepfake bot submissions to federal public comment websites cannot be distinguished from human submissions. Technology Science , 2019121801. 69 [604] S. Welleck, I. Kulikov, S. Roller, E. Dinan, K. Cho and J. Weston. 2019. Neural text generation with unlikelihood training. arXiv preprint arXiv:1908.04319 . [605] L. Weng. 2023. Large transformer model inference opti- mization. Lil\u2019Log . [606] L. Weng. 2023. Prompt engineering. lilian- weng.github.io . [607] M. Willig, M. ZE \u02c7CEVI \u00b4C, D. S. Dhami and K. Kersting. 2023. Causal parrots: Large language models may talk causality but are not causal. preprint . [608] F. Winkelmolen, N. Ivkin, H. F. Bozkurt and Z. Karnin. 2020. Practical and sample efficient zero-shot hpo. arXiv preprint arXiv:2007.13382 . [609] Y . Wolf, N. Wies, Y . Levine and A. Shashua. 2023. Fun- damental limitations of alignment in large language models. arXiv preprint arXiv:2304.11082 . [610] M. Wornow, Y . Xu, R. Thapa, B. Patel, E. Steinberg, S. Fleming, M. A. Pfeffer, J. Fries et al. 2023. The shaky foundations of clinical foundation models: A survey of large language models and foundation models for emrs. [611] F. Wu, D. Radev and J. Xu. 2023. When geometric deep learning meets pretrained protein language models. bioRxiv , pages 2023\u201301. [612] J. Wu, L. Ouyang, D. M. Ziegler, N. Stiennon, R. Lowe, J. Leike and P. Christiano. 2021. Recursively sum- marizing books with human feedback. arXiv preprint arXiv:2109.10862 . [613] J. Wu, F. Wu, B. Jiang, W. Liu and P. Zhao. 2022. tfold- ab: Fast and accurate antibody structure prediction without sequence homologs. bioRxiv , pages 2022\u201311. [614] P. Y . Wu, J. A. Tucker, J. Nagler and S. Messing. 2023. Large language models can be used to estimate the ideolo- gies of politicians in a zero-shot learning setting. foundations of clinical foundation models: A survey of large language models and foundation models for emrs. [611] F. Wu, D. Radev and J. Xu. 2023. When geometric deep learning meets pretrained protein language models. bioRxiv , pages 2023\u201301. [612] J. Wu, L. Ouyang, D. M. Ziegler, N. Stiennon, R. Lowe, J. Leike and P. Christiano. 2021. Recursively sum- marizing books with human feedback. arXiv preprint arXiv:2109.10862 . [613] J. Wu, F. Wu, B. Jiang, W. Liu and P. Zhao. 2022. tfold- ab: Fast and accurate antibody structure prediction without sequence homologs. bioRxiv , pages 2022\u201311. [614] P. Y . Wu, J. A. Tucker, J. Nagler and S. Messing. 2023. Large language models can be used",
    "61": "to estimate the ideolo- gies of politicians in a zero-shot learning setting. [615] S. Wu, X. Zhao, T. Yu, R. Zhang, C. Shen, H. Liu, F. Li, H. Zhu et al. 2021. Yuan 1.0: Large-scale pre-trained language model in zero-shot and few-shot learning. [616] S. Wu, O. Irsoy, S. Lu, V . Dabravolski, M. Dredze, S. Gehrmann, P. Kambadur, D. Rosenberg et al. 2023. Bloomberggpt: A large language model for finance. [617] Y . Wu, M. Schuster, Z. Chen, Q. V . Le, M. Norouzi, W. Macherey, M. Krikun, Y . Cao et al. 2016. Google\u2019s neu- ral machine translation system: Bridging the gap between human and machine translation. [618] Y . Wu, M. Gardner, P. Stenetorp and P. Dasigi. 2022. Generating data to mitigate spurious correlations in natural language inference datasets. arXiv preprint arXiv:2203.12942 . [619] Z. Wu, L. Qiu, A. Ross, E. Aky\u00fcrek, B. Chen, B. Wang, N. Kim, J. Andreas et al. 2023. Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks. ArXiv:2307.02477 [cs]. [620] Y . Xiao and W. Y . Wang. 2021. On Hallucination and Predictive Uncertainty in Conditional Language Genera- tion. ArXiv:2103.15025 [cs].[621] Q. Xie, Z. Luo, B. Wang and S. Ananiadou. 2023. A survey on biomedical text summarization with pre-trained language model. [622] S. M. Xie, H. Pham, X. Dong, N. Du, H. Liu, Y . Lu, P. Liang, Q. V . Le et al. 2023. DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining. ArXiv:2305.10429 [cs]. [623] S. M. Xie, A. Raghunathan, P. Liang and T. Ma. 2022. An Explanation of In-context Learning as Implicit Bayesian Inference. ArXiv:2111.02080 [cs]. [624] S. M. Xie, S. Santurkar, T. Ma and P. Liang. 2023. Data Selection for Language Models via Importance Resam- pling. ArXiv:2302.03169 [cs]. [cs]. [620] Y . Xiao and W. Y . Wang. 2021. On Hallucination and Predictive Uncertainty in Conditional Language Genera- tion. ArXiv:2103.15025 [cs].[621] Q. Xie, Z. Luo, B. Wang and S. Ananiadou. 2023. A survey on biomedical text summarization with pre-trained language model. [622] S. M. Xie, H. Pham, X. Dong, N. Du, H. Liu, Y . Lu, P. Liang, Q. V . Le et al. 2023. DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining. ArXiv:2305.10429 [cs]. [623] S. M. Xie, A. Raghunathan, P. Liang and T. Ma. 2022. An Explanation of In-context Learning as Implicit Bayesian Inference. ArXiv:2111.02080 [cs]. [624] S. M. Xie, S. Santurkar, T. Ma and P. Liang. 2023. Data Selection for Language Models via Importance Resam- pling. ArXiv:2302.03169 [cs]. [625] C. Xu, Q. Sun, K. Zheng, X. Geng, P. Zhao, J. Feng, C. Tao and D. Jiang. 2023. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244 . [626] F. F. Xu, U. Alon, G. Neubig and V . J. Hellendoorn. 2022. A systematic evaluation of large language models of code. [627] M. Xu, X. Yuan, S. Miret and J. Tang. 2023. Protst: Multi-modality learning of protein sequences and biomedi- cal texts. arXiv preprint arXiv:2301.12040 . [628] Y . Xu, H. Lee, D. Chen, B. Hechtman, Y . Huang, R. Joshi, M. Krikun, D. Lepikhin et al. 2021. Gspmd: general and scalable parallelization for ml computation graphs. arXiv preprint arXiv:2105.04663 . [629] L. Xue, A. Barua, N. Constant, R. Al-Rfou, S. Narang, M. Kale, A. Roberts and C. Raffel. 2022. ByT5: Towards a token-free future with pre-trained byte-to-byte models. ArXiv:2105.13626 [cs]. [630] L. Xue, A. Barua, N. Constant, R. Al-Rfou, S. Narang, M. Kale, A. Roberts and C. Raffel. 2022. ByT5: Towards a token-free future with pre-trained byte-to-byte models. Transactions of the Association for Computational Linguis- tics, 10:291\u2013306. [631] L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant, A. Barua and C. Raffel. 2021. mT5: A mas- sively multilingual pre-trained text-to-text transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 483\u2013498, Online. Association for Computational Linguistics. [632] L. Yan, L. Sha, L. Zhao, Y . Li, R. Martinez-Maldonado, G. Chen, X. Li, Y . Jin et al. 2023. Practical and ethical chal- lenges of large language models in education: A systematic literature review. M. Kale, A. Roberts and C. Raffel. 2022. ByT5: Towards a token-free future with pre-trained byte-to-byte models. Transactions of the Association for Computational Linguis- tics, 10:291\u2013306. [631] L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant, A. Barua and C. Raffel. 2021. mT5: A mas- sively multilingual pre-trained text-to-text transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 483\u2013498, Online. Association for Computational Linguistics. [632] L. Yan, L. Sha, L. Zhao, Y . Li, R. Martinez-Maldonado, G. Chen, X. Li, Y . Jin et al. 2023. Practical and ethical chal- lenges of large language models in education: A systematic literature review. [633] G. Yang, E. Hu, I. Babuschkin, S. Sidor, X. Liu, D. Farhi, N. Ryder, J. Pachocki et al. 2021. Tuning large neural net- works via zero-shot hyperparameter transfer. Advances in Neural Information Processing Systems , 34:17084\u201317097. [634] J. Yang, H. Jin, R. Tang, X. Han, Q. Feng, H. Jiang, B. Yin and X. Hu. 2023. Harnessing the power of llms in practice: A survey on chatgpt and beyond. [635] K. Yang and D. Klein. 2021. Fudge: Controlled text generation with future discriminators. arXiv preprint arXiv:2104.05218 . 70 [636] K. Yang, D. Klein, N. Peng and Y . Tian. 2022. Doc: Im- proving long story coherence with detailed outline control. arXiv preprint arXiv:2212.10077 . [637] K. Yang, N. Peng, Y . Tian and D. Klein. 2022. Re3: Generating longer stories with recursive reprompting and revision. arXiv preprint arXiv:2210.06774 . [638] X. Yang, K. Chen, W. Zhang, C. Liu, Y . Qi, J. Zhang, H. Fang and N. Yu. 2023. Watermarking Text Generated by Black-Box Language Models. ArXiv:2305.08883 [cs]. [639] S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y . Cao and K. Narasimhan. 2023. Tree of Thoughts: De- liberate Problem Solving with Large Language Models. ArXiv:2305.10601 [cs]. [640] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan and Y . Cao. 2022. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629 . [641] X. Yao, Y . Zheng, X. Yang and Z. Yang. 2022. NLP From Scratch Without Large-Scale Pretraining: A Simple and Efficient Framework. In Proceedings of the 39th Inter- national Conference on Machine Learning , pages 25438\u2013 25451. PMLR. ISSN: 2640-3498. [642] Y . Yao, P. Wang, B. Tian, S. Cheng, Z. Li, S. Deng, H. Fang and N. Yu. 2023. Watermarking Text Generated by Black-Box Language Models. ArXiv:2305.08883 [cs]. [639] S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y . Cao and K. Narasimhan. 2023. Tree of Thoughts: De- liberate Problem Solving with Large Language Models. ArXiv:2305.10601 [cs]. [640] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan and Y . Cao. 2022. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629 . [641] X. Yao, Y . Zheng, X. Yang and Z. Yang. 2022. NLP From Scratch Without Large-Scale Pretraining: A Simple and Efficient Framework. In Proceedings of the 39th Inter- national Conference on Machine Learning , pages 25438\u2013 25451. PMLR. ISSN: 2640-3498. [642] Y . Yao, P. Wang, B. Tian, S. Cheng, Z. Li, S. Deng, H. Chen and N. Zhang. 2023. Editing Large Lan- guage Models: Problems, Methods, and Opportunities. ArXiv:2305.13172 [cs]. [643] Z. Yao, R. Y . Aminabadi, M. Zhang, X. Wu, C. Li and Y . He. 2022. Zeroquant: Efficient and affordable post- training quantization for large-scale transformers. arXiv preprint arXiv:2206.01861 . [644] M. Yasunaga, A. Bosselut, H. Ren, X. Zhang, C. D. Man- ning, P. Liang and J. Leskovec. 2022. Deep bidirectional language-knowledge graph pretraining. arXiv",
    "62": "preprint arXiv:2210.09338 . [645] S. Yi, R. Goel, C. Khatri, A. Cervone, T. Chung, B. He- dayatnia, A. Venkatesh, R. Gabriel et al. 2019. Towards coherent and engaging spoken dialog response generation using automatic conversation evaluators. In Proceedings of the 12th International Conference on Natural Language Generation , pages 65\u201375, Tokyo, Japan. Association for Computational Linguistics. [646] D. Yogatama, C. de Masson d\u2019Autume and L. Kong. 2021. Adaptive semiparametric language models. Trans- actions of the Association for Computational Linguistics , 9:362\u2013373. [647] T. Yoneda, J. Fang, P. Li, H. Zhang, T. Jiang, S. Lin, B. Picker, D. Yunis et al. 2023. Statler: State-maintaining language models for embodied reasoning. [648] K. M. Yoo, D. Park, J. Kang, S.-W. Lee and W. Park. 2021. GPT3Mix: Leveraging large-scale language models for text augmentation. In Findings of the Association for Computational Linguistics: EMNLP 2021 , pages 2225\u2013 2239, Punta Cana, Dominican Republic. Association for Computational Linguistics. [649] K. Yoo, W. Ahn, J. Jang and N. Kwak. 2023. Robust Nat- ural Language Watermarking through Invariant Features. Computational Linguistics. [646] D. Yogatama, C. de Masson d\u2019Autume and L. Kong. 2021. Adaptive semiparametric language models. Trans- actions of the Association for Computational Linguistics , 9:362\u2013373. [647] T. Yoneda, J. Fang, P. Li, H. Zhang, T. Jiang, S. Lin, B. Picker, D. Yunis et al. 2023. Statler: State-maintaining language models for embodied reasoning. [648] K. M. Yoo, D. Park, J. Kang, S.-W. Lee and W. Park. 2021. GPT3Mix: Leveraging large-scale language models for text augmentation. In Findings of the Association for Computational Linguistics: EMNLP 2021 , pages 2225\u2013 2239, Punta Cana, Dominican Republic. Association for Computational Linguistics. [649] K. Yoo, W. Ahn, J. Jang and N. Kwak. 2023. Robust Nat- ural Language Watermarking through Invariant Features. ArXiv:2305.01904 [cs].[650] R. You, Y . Liu, H. Mamitsuka and S. Zhu. 2021. Bertmesh: deep contextual representation learning for large-scale high-performance mesh indexing with full text. Bioinformatics , 37(5):684\u2013692. [651] F. Yu, L. Quartey and F. Schilder. 2022. Legal prompting: Teaching a language model to think like a lawyer. arXiv preprint arXiv:2212.01326 . [652] L. Yu, D. Simig, C. Flaherty, A. Aghajanyan, L. Zettle- moyer and M. Lewis. 2023. Megabyte: Predicting million-byte sequences with multiscale transformers. arXiv preprint arXiv:2305.07185 . [653] P. Yu, M. Artetxe, M. Ott, S. Shleifer, H. Gong, V . Stoy- anov and X. Li. 2022. Efficient language modeling with sparse all-mlp. [654] P. Yu, T. Wang, O. Golovneva, B. Alkhamissy, G. Ghosh, M. Diab and A. Celikyilmaz. 2022. Alert: Adapting language models to reasoning tasks. arXiv preprint arXiv:2212.08286 . [655] L. Yunxiang, L. Zihan, Z. Kai, D. Ruilong and Z. You. 2023. Chatdoctor: A medical chat model fine-tuned on llama model using medical domain knowledge. [656] E. Zelikman, Y . Wu, J. Mu and N. Goodman. 2022. STar: Bootstrapping reasoning with reasoning. In Advances in Neural Information Processing Systems . [657] R. Zellers, A. Holtzman, H. Rashkin, Y . Bisk, A. Farhadi, F. Roesner and Y . Choi. 2019. Defending against neural fake news. Advances in neural information processing systems , 32. [658] A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y . Xu et al. 2022. Glm-130b: An open bilingual pre-trained model. [659] W. Zeng, X. Ren, T. Su, H. Wang, Y . Liao, Z. Wang, X. Jiang, Z. Yang et al. 2021. Pangu- \u03b1: Large-scale au- toregressive pretrained chinese language models with auto- parallel computation. 2023. Chatdoctor: A medical chat model fine-tuned on llama model using medical domain knowledge. [656] E. Zelikman, Y . Wu, J. Mu and N. Goodman. 2022. STar: Bootstrapping reasoning with reasoning. In Advances in Neural Information Processing Systems . [657] R. Zellers, A. Holtzman, H. Rashkin, Y . Bisk, A. Farhadi, F. Roesner and Y . Choi. 2019. Defending against neural fake news. Advances in neural information processing systems , 32. [658] A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y . Xu et al. 2022. Glm-130b: An open bilingual pre-trained model. [659] W. Zeng, X. Ren, T. Su, H. Wang, Y . Liao, Z. Wang, X. Jiang, Z. Yang et al. 2021. Pangu- \u03b1: Large-scale au- toregressive pretrained chinese language models with auto- parallel computation. [660] F. Zhang, B. Chen, Y . Zhang, J. Liu, D. Zan, Y . Mao, J.- G. Lou and W. Chen. 2023. Repocoder: Repository-level code completion through iterative retrieval and generation. [661] H. Zhang, L. H. Li, T. Meng, K.-W. Chang and G. V . d. Broeck. 2022. On the Paradox of Learning to Reason from Data. ArXiv:2205.11502 [cs]. [662] H. Zhang, D. Duckworth, D. Ippolito and A. Neelakan- tan. 2021. Trading off diversity and quality in natural language generation. In Proceedings of the Workshop on Human Evaluation of NLP Systems (HumEval) , pages 25\u2013 33, Online. Association for Computational Linguistics. [663] M. Zhang and Y . He. 2020. Accelerating training of transformer-based language models with progressive layer dropping. [664] M. Zhang, O. Press, W. Merrill, A. Liu and N. A. Smith. 2023. How Language Model Hallucinations Can Snowball. ArXiv:2305.13534 [cs]. [665] S. Zhang. 2023. [...] that\u2019s an unhelpful order of magnitude difference in how large of a model you should be training in order to be considered \u201ccompute optimal\u201d. https://twitter.com/suchenzang/ status/1616752494608007171?s=20 . Accessed: 2023-06-06. 71 [666] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab et al. 2022. Opt: Open pre- trained transformer language models. [667] T. Zhang, V . Kishore, F. Wu, K. Q. Weinberger and Y . Artzi. 2019. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675 . [668] T. Zhang, F. Ladhak, E. Durmus, P. Liang, K. McKeown and T. B. Hashimoto. 2023. Benchmarking large language models for news summarization. [669] Z. Zhang, Y . Gu, X. Han, S. Chen, C. Xiao, Z. Sun, Y . Yao, F. Qi et al. 2021. Cpm-2: Large-scale cost-effective pre-trained language models. should be training in order to be considered \u201ccompute optimal\u201d. https://twitter.com/suchenzang/ status/1616752494608007171?s=20 . Accessed: 2023-06-06. 71 [666] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab et al. 2022. Opt: Open pre- trained transformer language models. [667] T. Zhang, V . Kishore, F. Wu, K. Q. Weinberger and Y . Artzi. 2019. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675 . [668] T. Zhang, F. Ladhak, E. Durmus, P. Liang, K. McKeown and T. B. Hashimoto. 2023. Benchmarking large language models for news summarization. [669] Z. Zhang, Y . Gu, X. Han, S. Chen, C. Xiao, Z. Sun, Y . Yao, F. Qi et al. 2021. Cpm-2: Large-scale cost-effective pre-trained language models. [670] Z. Zhang, Y . Lin, Z. Liu, P. Li, M. Sun and J. Zhou. 2022. Moefication: Transformer feed-forward layers are mixtures of experts. [671] Z. Zhang, A. Zhang, M. Li and A. Smola. 2022. Auto- matic chain of thought prompting in large language models. [672] S. Zhao, J. Wen, L. A. Tuan, J. Zhao and J. Fu. 2023. Prompt as triggers for backdoor attack: Examin- ing the vulnerability in language models. arXiv preprint arXiv:2305.01219 . [673] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y . Hou, Y . Min, B. Zhang et al. 2023. A Survey of Large Language Models. ArXiv:2303.18223 [cs]. [674] Y . Zhao, A. Gu, R. Varma, L. Luo, C.-C. Huang, M. Xu, L. Wright, H. Shojanazeri et al. 2023. Pytorch fsdp: experi- ences on scaling fully sharded data parallel. arXiv preprint arXiv:2304.11277 . [675] Z. Zhao, E. Wallace, S. Feng, D. Klein and S. Singh. 2021. Calibrate before use: Improving few-shot perfor- mance of language models. In Proceedings of the 38th International Conference on Machine Learning , volume 139 of Proceedings of Machine Learning Research , pages 12697\u201312706. PMLR. [676]",
    "63": "B. Zheng, L. Dong, S. Huang, S. Singhal, W. Che, T. Liu, X. Song and F. Wei. 2021. Allocating large vocabulary ca- pacity for cross-lingual language model pre-training. arXiv preprint arXiv:2109.07306 . [677] L. Zheng, Z. Li, H. Zhang, Y . Zhuang, Z. Chen, Y . Huang, Y . Wang, Y . Xu et al. 2022. Alpa: Automat- ing inter- and Intra-Operator parallelism for distributed deep learning. In 16th USENIX Symposium on Operat- ing Systems Design and Implementation (OSDI 22) , pages 559\u2013578, Carlsbad, CA. USENIX Association. [678] R. Zheng, S. Dou, S. Gao, W. Shen, B. Wang, Y . Liu, S. Jin, Q. Liu et al. 2023. Secrets of RLHF in Large International Conference on Machine Learning , volume 139 of Proceedings of Machine Learning Research , pages 12697\u201312706. PMLR. [676] B. Zheng, L. Dong, S. Huang, S. Singhal, W. Che, T. Liu, X. Song and F. Wei. 2021. Allocating large vocabulary ca- pacity for cross-lingual language model pre-training. arXiv preprint arXiv:2109.07306 . [677] L. Zheng, Z. Li, H. Zhang, Y . Zhuang, Z. Chen, Y . Huang, Y . Wang, Y . Xu et al. 2022. Alpa: Automat- ing inter- and Intra-Operator parallelism for distributed deep learning. In 16th USENIX Symposium on Operat- ing Systems Design and Implementation (OSDI 22) , pages 559\u2013578, Carlsbad, CA. USENIX Association. [678] R. Zheng, S. Dou, S. Gao, W. Shen, B. Wang, Y . Liu, S. Jin, Q. Liu et al. 2023. Secrets of RLHF in Large Language Models Part I: PPO. ArXiv:2307.04964 [cs]. [679] W. Zhong, R. Cui, Y . Guo, Y . Liang, S. Lu, Y . Wang, A. Saied, W. Chen et al. 2023. Agieval: A human- centric benchmark for evaluating foundation models. arXiv preprint arXiv:2304.06364 . [680] A. Zhou, Y . Ma, J. Zhu, J. Liu, Z. Zhang, K. Yuan, W. Sun and H. Li. 2021. Learning N: M fine-grained structured sparse neural networks from scratch. In 9th In- ternational Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021 . OpenRe- view.net.[681] C. Zhou, P. Liu, P. Xu, S. Iyer, J. Sun, Y . Mao, X. Ma, A. Efrat et al. 2023. LIMA: Less Is More for Alignment. ArXiv:2305.11206 [cs]. [682] D. Zhou, N. Sch\u00e4rli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schuurmans, C. Cui et al. 2022. Least-to-most prompt- ing enables complex reasoning in large language models. [683] Y . Zhou, A. I. Muresanu, Z. Han, K. Paster, S. Pitis, H. Chan and J. Ba. 2023. Large language models are human-level prompt engineers. In International Confer- ence on Learning Representations . [684] Y . Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urta- sun, A. Torralba and S. Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by watch- ing movies and reading books. [685] B. Zhuang, J. Liu, Z. Pan, H. He, Y . Weng and C. Shen. 2023. A survey on efficient training of transformers. arXiv preprint arXiv:2302.01107 . [686] D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Rad- ford, D. Amodei, P. Christiano and G. Irving. 2019. Fine- tuning language models from human preferences. arXiv preprint arXiv:1909.08593 . [687] B. Zoph, I. Bello, S. Kumar, N. Du, Y . Huang, J. Dean, N. Shazeer and W. Fedus. 2022. St-moe: Designing stable human-level prompt engineers. In International Confer- ence on Learning Representations . [684] Y . Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urta- sun, A. Torralba and S. Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by watch- ing movies and reading books. [685] B. Zhuang, J. Liu, Z. Pan, H. He, Y . Weng and C. Shen. 2023. A survey on efficient training of transformers. arXiv preprint arXiv:2302.01107 . [686] D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Rad- ford, D. Amodei, P. Christiano and G. Irving. 2019. Fine- tuning language models from human preferences. arXiv preprint arXiv:1909.08593 . [687] B. Zoph, I. Bello, S. Kumar, N. Du, Y . Huang, J. Dean, N. Shazeer and W. Fedus. 2022. St-moe: Designing stable and transferable sparse expert models. [688] M. Zvyagin, A. Brace, K. Hippe, Y . Deng, B. Zhang, C. O. Bohorquez, A. Clyde, B. Kale et al. 2022. Genslms: Genome-scale language models reveal sars-cov-2 evolu- tionary dynamics. bioRxiv , pages 2022\u201310. 72 Attention Is All You Need Ashish Vaswani\u0003 Google Brain avaswani@google.comNoam Shazeer\u0003 Google Brain noam@google.comNiki Parmar\u0003 Google Research nikip@google.comJakob Uszkoreit\u0003 Google Research usz@google.com Llion Jones\u0003 Google Research llion@google.comAidan N. Gomez\u0003y University of Toronto aidan@cs.toronto.edu\u0141ukasz Kaiser\u0003 Google Brain lukaszkaiser@google.com Illia Polosukhin\u0003z illia.polosukhin@gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signi\ufb01cantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. 1 Introduction Recurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks in particular, have been \ufb01rmly established as state of the art approaches in sequence modeling and \u0003Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the \ufb01rst Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and in particular, have been \ufb01rmly established as state of the art approaches in sequence modeling and \u0003Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the \ufb01rst Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and ef\ufb01cient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research. yWork performed while at Google Brain. zWork performed while at Google Research. 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v5 [cs.CL] 6 Dec 2017 transduction problems such as language modeling and machine translation [ 35,2,5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder",
    "64": "architectures [38, 24, 15]. Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden statesht, as a function of the previous hidden state ht\u00001and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved signi\ufb01cant improvements in computational ef\ufb01ciency through factorization tricks [ 21] and conditional computation [ 32], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains. Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved signi\ufb01cant improvements in computational ef\ufb01ciency through factorization tricks [ 21] and conditional computation [ 32], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains. Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms are used in conjunction with a recurrent network. In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for signi\ufb01cantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs. 2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more dif\ufb01cult to learn dependencies between distant positions [ 12]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2. Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22]. it more dif\ufb01cult to learn dependencies between distant positions [ 12]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2. Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22]. End-to-end memory networks are based on a recurrent attention mechanism instead of sequence- aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [34]. To the best of our knowledge, however, the Transformer is the \ufb01rst transduction model relying entirely on self-attention to compute representations of its input and output without using sequence- aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9]. 3 Model Architecture Most competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35]. Here, the encoder maps an input sequence of symbol representations (x1;:::;x n)to a sequence of continuous representations z= (z1;:::;z n). Given z, the decoder then generates an output sequence (y1;:::;y m)of symbols one element at a time. At each step the model is auto-regressive [10], consuming the previously generated symbols as additional input when generating the next. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively. 2 Figure 1: The Transformer - model architecture. 3.1 Encoder and Decoder Stacks Encoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two sub-layers. The \ufb01rst is a multi-head self-attention mechanism, and the second is a simple, position- wise fully connected feed-forward network. We employ a residual connection [ 11] around each of the two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is [10], consuming the previously generated symbols as additional input when generating the next. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively. 2 Figure 1: The Transformer - model architecture. 3.1 Encoder and Decoder Stacks Encoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two sub-layers. The \ufb01rst is a multi-head self-attention mechanism, and the second is a simple, position- wise fully connected feed-forward network. We employ a residual connection [ 11] around each of the two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is LayerNorm( x+ Sublayer( x)), where Sublayer(x)is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512 . Decoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position ican depend only on the known outputs at positions less than i. 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility",
    "65": "function of the query with the corresponding key. 3 Scaled Dot-Product Attention Multi-Head Attention Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel. 3.2.1 Scaled Dot-Product Attention We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. 3 Scaled Dot-Product Attention Multi-Head Attention Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel. 3.2.1 Scaled Dot-Product Attention We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the query with all keys, divide each bypdk, and apply a softmax function to obtain the weights on the values. In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices KandV. We compute the matrix of outputs as: Attention(Q;K;V ) = softmax(QKT pdk)V (1) The two most commonly used attention functions are additive attention [ 2], and dot-product (multi- plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of1pdk. Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-ef\ufb01cient in practice, since it can be implemented using highly optimized matrix multiplication code. While for small values of dkthe two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk[3]. We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients4. To counteract this effect, we scale the dot products by1pdk. 3.2.2 Multi-Head Attention Instead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it bene\ufb01cial to linearly project the queries, keys and values htimes with different, learned linear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional dot product attention without scaling for larger values of dk[3]. We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients4. To counteract this effect, we scale the dot products by1pdk. 3.2.2 Multi-Head Attention Instead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it bene\ufb01cial to linearly project the queries, keys and values htimes with different, learned linear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional output values. These are concatenated and once again projected, resulting in the \ufb01nal values, as depicted in Figure 2. 4To illustrate why the dot products get large, assume that the components of qandkare independent random variables with mean 0and variance 1. Then their dot product, q\u0001k=Pdk i=1qiki, has mean 0and variance dk. 4 Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this. MultiHead( Q;K;V ) = Concat(head 1;:::;head h)WO where head i= Attention( QWQ i;KWK i;VWV i) Where the projections are parameter matrices WQ i2Rdmodel\u0002dk,WK i2Rdmodel\u0002dk,WV i2Rdmodel\u0002dv andWO2Rhdv\u0002dmodel. In this work we employ h= 8 parallel attention layers, or heads. For each of these we use dk=dv=dmodel=h= 64 . Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality. 3.2.3 Applications of Attention in our Model The Transformer uses multi-head attention in three different ways: \u000fIn \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9]. \u000fThe encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder. 3.2.3 Applications of Attention in our Model The Transformer uses multi-head attention in three different ways: \u000fIn \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9]. \u000fThe encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder. \u000fSimilarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information \ufb02ow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to \u00001) all values in the input of the softmax which correspond to illegal connections. See Figure 2. 3.3 Position-wise Feed-Forward Networks In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between. FFN(x) = max(0;xW 1+b1)W2+b2 (2) While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality dff= 2048 . 3.4 Embeddings and Softmax Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor- mation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [ 30]. In the embedding layers, we multiply those weights bypdmodel. 3.5 Positional Encoding from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is dmodel = 512 , and the inner-layer",
    "66": "has dimensionality dff= 2048 . 3.4 Embeddings and Softmax Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor- mation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [ 30]. In the embedding layers, we multiply those weights bypdmodel. 3.5 Positional Encoding Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the 5 Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types. nis the sequence length, dis the representation dimension, kis the kernel size of convolutions and rthe size of the neighborhood in restricted self-attention. Layer Type Complexity per Layer Sequential Maximum Path Length Operations Self-Attention O(n2\u0001d) O(1) O(1) Recurrent O(n\u0001d2) O(n) O(n) Convolutional O(k\u0001n\u0001d2)O(1) O(logk(n)) Self-Attention (restricted) O(r\u0001n\u0001d)O(1) O(n=r) tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and \ufb01xed [9]. In this work, we use sine and cosine functions of different frequencies: PE(pos;2i)=sin(pos=100002i=d model) PE(pos;2i+1)=cos(pos=100002i=d model) whereposis the position and iis the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2\u0019to10000\u00012\u0019. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any \ufb01xed offset k,PEpos+kcan be represented as a linear function of PEpos. We also experimented with using learned positional embeddings [ 9] instead, and found that the two as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and \ufb01xed [9]. In this work, we use sine and cosine functions of different frequencies: PE(pos;2i)=sin(pos=100002i=d model) PE(pos;2i+1)=cos(pos=100002i=d model) whereposis the position and iis the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2\u0019to10000\u00012\u0019. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any \ufb01xed offset k,PEpos+kcan be represented as a linear function of PEpos. We also experimented with using learned positional embeddings [ 9] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training. 4 Why Self-Attention In this section we compare various aspects of self-attention layers to the recurrent and convolu- tional layers commonly used for mapping one variable-length sequence of symbol representations (x1;:::;x n)to another sequence of equal length (z1;:::;z n), withxi;zi2Rd, such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata. One is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required. The third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [ 12]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types. As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n)sequential operations. In terms of dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [ 12]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types. As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n)sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence lengthnis smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [38] and byte-pair [ 31] representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size rin 6 the input sequence centered around the respective output position. This would increase the maximum path length to O(n=r). We plan to investigate this approach further in future work. A single convolutional layer with kernel width k<n does not connect all pairs of input and output positions. Doing so requires a stack of O(n=k)convolutional layers in the case of contiguous kernels, orO(logk(n))in the case of dilated convolutions [ 18], increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity considerably, to O(k\u0001n\u0001d+n\u0001d2). Even with k=n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model. As side bene\ufb01t, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences. 5 Training between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity considerably, to O(k\u0001n\u0001d+n\u0001d2). Even with k=n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model. As side bene\ufb01t, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences. 5 Training This section describes the training regime for our models. 5.1 Training Data and",
    "67": "Batching We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source- target vocabulary of about 37000 tokens. For English-French, we used the signi\ufb01cantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [ 38]. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens. 5.2 Hardware and Schedule We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days). 5.3 Optimizer We used the Adam optimizer [ 20] with 1= 0:9, 2= 0:98and\u000f= 10\u00009. We varied the learning rate over the course of training, according to the formula: lrate =d\u00000:5 model\u0001min(step_num\u00000:5;step _num\u0001warmup _steps\u00001:5) (3) This corresponds to increasing the learning rate linearly for the \ufb01rst warmup _steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup _steps = 4000 . 5.4 Regularization We employ three types of regularization during training: trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days). 5.3 Optimizer We used the Adam optimizer [ 20] with 1= 0:9, 2= 0:98and\u000f= 10\u00009. We varied the learning rate over the course of training, according to the formula: lrate =d\u00000:5 model\u0001min(step_num\u00000:5;step _num\u0001warmup _steps\u00001:5) (3) This corresponds to increasing the learning rate linearly for the \ufb01rst warmup _steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup _steps = 4000 . 5.4 Regularization We employ three types of regularization during training: Residual Dropout We apply dropout [ 33] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of Pdrop= 0:1. 7 Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost. ModelBLEU Training Cost (FLOPs) EN-DE EN-FR EN-DE EN-FR ByteNet [18] 23.75 Deep-Att + PosUnk [39] 39.2 1:0\u00011020 GNMT + RL [38] 24.6 39.92 2:3\u000110191:4\u00011020 ConvS2S [9] 25.16 40.46 9:6\u000110181:5\u00011020 MoE [32] 26.03 40.56 2:0\u000110191:2\u00011020 Deep-Att + PosUnk Ensemble [39] 40.4 8:0\u00011020 GNMT + RL Ensemble [38] 26.30 41.16 1:8\u000110201:1\u00011021 ConvS2S Ensemble [9] 26.36 41.29 7:7\u000110191:2\u00011021 Transformer (base model) 27.3 38.1 3:3\u00011018 Transformer (big) 28.4 41.8 2:3\u00011019 Label Smoothing During training, we employed label smoothing of value \u000fls= 0:1[36]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score. 6 Results 6.1 Machine Translation On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2:0 BLEU, establishing a new state-of-the-art BLEU score of 28:4. The con\ufb01guration of this model is listed in the bottom line of Table 3. Training took 3:5days on 8P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models. Transformer (big) 28.4 41.8 2:3\u00011019 Label Smoothing During training, we employed label smoothing of value \u000fls= 0:1[36]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score. 6 Results 6.1 Machine Translation On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2:0 BLEU, establishing a new state-of-the-art BLEU score of 28:4. The con\ufb01guration of this model is listed in the bottom line of Table 3. Training took 3:5days on 8P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models. On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41:0, outperforming all of the previously published single models, at less than 1=4the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate Pdrop= 0:1, instead of 0:3. For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of 4and length penalty = 0:6[38]. These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length + 50, but terminate early when possible [38]. Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of \ufb02oating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision \ufb02oating-point capacity of each GPU5. 6.2 Model Variations To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the development set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3. In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision \ufb02oating-point capacity of each GPU5. 6.2 Model Variations To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the development set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3. In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads. 5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively. 8 Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities. N d modeldffh d kdvPdrop\u000flstrain PPL BLEU params steps (dev) (dev)\u0002106 base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65 (A)1 512 512 5.29 24.9 4 128 128 5.00 25.5 16 32 32 4.91 25.8 32 16 16 5.01 25.4 (B)16 5.16 25.1 58 32 5.01 25.4 60 (C)2 6.11 23.7 36 4 5.19 25.3 50 8 4.88 25.5 80 256 32 32",
    "68": "5.75 24.5 28 1024 128 128 4.66 26.0 168 1024 5.12 25.4 53 4096 4.75 26.2 90 (D)0.0 5.77 24.6 0.2 4.95 25.5 0.0 4.67 25.3 0.2 5.47 25.7 (E) positional embedding instead of sinusoids 4.92 25.7 big 6 1024 4096 16 0.3 300K 4.33 26.4 213 Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ) Parser Training WSJ 23 F1 Vinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3 Petrov et al. (2006) [29] WSJ only, discriminative 90.4 Zhu et al. (2013) [40] WSJ only, discriminative 90.4 Dyer et al. (2016) [8] WSJ only, discriminative 91.7 Transformer (4 layers) WSJ only, discriminative 91.3 Zhu et al. (2013) [40] semi-supervised 91.3 Huang & Harper (2009) [14] semi-supervised 91.3 McClosky et al. (2006) [26] semi-supervised 92.1 Vinyals & Kaiser el al. (2014) [37] semi-supervised 92.1 Transformer (4 layers) semi-supervised 92.7 Luong et al. (2015) [23] multi-task 93.0 Dyer et al. (2016) [8] generative 93.3 big 6 1024 4096 16 0.3 300K 4.33 26.4 213 Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ) Parser Training WSJ 23 F1 Vinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3 Petrov et al. (2006) [29] WSJ only, discriminative 90.4 Zhu et al. (2013) [40] WSJ only, discriminative 90.4 Dyer et al. (2016) [8] WSJ only, discriminative 91.7 Transformer (4 layers) WSJ only, discriminative 91.3 Zhu et al. (2013) [40] semi-supervised 91.3 Huang & Harper (2009) [14] semi-supervised 91.3 McClosky et al. (2006) [26] semi-supervised 92.1 Vinyals & Kaiser el al. (2014) [37] semi-supervised 92.1 Transformer (4 layers) semi-supervised 92.7 Luong et al. (2015) [23] multi-task 93.0 Dyer et al. (2016) [8] generative 93.3 In Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be bene\ufb01cial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-\ufb01tting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [ 9], and observe nearly identical results to the base model. 6.3 English Constituency Parsing To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents speci\ufb01c challenges: the output is subject to strong structural 9 constraints and is signi\ufb01cantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37]. We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank [ 25], about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-con\ufb01dence and BerkleyParser corpora from with approximately 17M sentences [37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting. We performed only a small number of experiments to select the dropout, both attention and residual (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters remained unchanged from the English-to-German base translation model. During inference, we We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank [ 25], about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-con\ufb01dence and BerkleyParser corpora from with approximately 17M sentences [37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting. We performed only a small number of experiments to select the dropout, both attention and residual (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters remained unchanged from the English-to-German base translation model. During inference, we increased the maximum output length to input length + 300. We used a beam size of 21and = 0:3 for both WSJ only and the semi-supervised setting. Our results in Table 4 show that despite the lack of task-speci\ufb01c tuning our model performs sur- prisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8]. In contrast to RNN sequence-to-sequence models [ 37], the Transformer outperforms the Berkeley- Parser [29] even when training only on the WSJ training set of 40K sentences. 7 Conclusion In this work, we presented the Transformer, the \ufb01rst sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention. For translation tasks, the Transformer can be trained signi\ufb01cantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles. We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to ef\ufb01ciently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours. The code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor . Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration. model outperforms even all previously reported ensembles. We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to ef\ufb01ciently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours. The code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor . Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration. References [1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450 , 2016. [2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. CoRR , abs/1409.0473, 2014. [3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural machine translation architectures. CoRR , abs/1703.03906, 2017. [4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading. arXiv preprint arXiv:1601.06733 , 2016. [5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. CoRR , abs/1406.1078, 2014. [6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv preprint arXiv:1610.02357 , 2016. 10 [7]Junyoung Chung, \u00c7aglar G\u00fcl\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014. [8]Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural network grammars. In Proc. of NAACL , 2016. [9]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu- tional sequence to sequence learning.",
    "69": "arXiv preprint arXiv:1705.03122v2 , 2017. [10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850 , 2013. [11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im- age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 770\u2013778, 2016. of gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014. [8]Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural network grammars. In Proc. of NAACL , 2016. [9]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu- tional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017. [10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850 , 2013. [11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im- age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 770\u2013778, 2016. [12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\u00fcrgen Schmidhuber. Gradient \ufb02ow in recurrent nets: the dif\ufb01culty of learning long-term dependencies, 2001. [13] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation , 9(8):1735\u20131780, 1997. [14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing , pages 832\u2013841. ACL, August 2009. [15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016. [16] \u0141ukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural Information Processing Systems, (NIPS) , 2016. [17] \u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference on Learning Representations (ICLR) , 2016. [18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko- ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 , 2017. [19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks. InInternational Conference on Learning Representations , 2017. [20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015. [21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint arXiv:1703.10722 , 2017. [22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130 , 2017. [18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko- ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 , 2017. [19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks. InInternational Conference on Learning Representations , 2017. [20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015. [21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint arXiv:1703.10722 , 2017. [22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130 , 2017. [23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task sequence to sequence learning. arXiv preprint arXiv:1511.06114 , 2015. [24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention- based neural machine translation. arXiv preprint arXiv:1508.04025 , 2015. [25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated corpus of english: The penn treebank. Computational linguistics , 19(2):313\u2013330, 1993. [26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference , pages 152\u2013159. ACL, June 2006. 11 [27] Ankur Parikh, Oscar T\u00e4ckstr\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model. In Empirical Methods in Natural Language Processing , 2016. [28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization. arXiv preprint arXiv:1705.04304 , 2017. [29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL , pages 433\u2013440. ACL, July 2006. [30] O\ufb01r Press and Lior Wolf. Using the output embedding to improve language models. arXiv preprint arXiv:1608.05859 , 2016. [31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909 , 2015. [32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, summarization. arXiv preprint arXiv:1705.04304 , 2017. [29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL , pages 433\u2013440. ACL, July 2006. [30] O\ufb01r Press and Lior Wolf. Using the output embedding to improve language models. arXiv preprint arXiv:1608.05859 , 2016. [31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909 , 2015. [32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538 , 2017. [33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi- nov. Dropout: a simple way to prevent neural networks from over\ufb01tting. Journal of Machine Learning Research , 15(1):1929\u20131958, 2014. [34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28 , pages 2440\u20132448. Curran Associates, Inc., 2015. [35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems , pages 3104\u20133112, 2014. [36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015. [37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In Advances in Neural Information Processing Systems , 2015. [38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\u2019s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144 , 2016. [39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with fast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016. [40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate Rethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015. [37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In Advances in Neural Information Processing Systems , 2015. [38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le,",
    "70": "Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\u2019s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144 , 2016. [39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with fast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016. [40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate shift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume 1: Long Papers) , pages 434\u2013443. ACL, August 2013. 12 Attention Visualizations Input-Input Layer5 It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb \u2018making\u2019, completing the phrase \u2018making...more dif\ufb01cult\u2019. Attentions here shown only for the word \u2018making\u2019. Different colors represent different heads. Best viewed in color. 13 Input-Input Layer5 The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad> The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad> Input-Input Layer5 The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad> The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word \u2018its\u2019 for attention heads 5 13 Input-Input Layer5 The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad> The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad> Input-Input Layer5 The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad> The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word \u2018its\u2019 for attention heads 5 and 6. Note that the attentions are very sharp for this word. 14 Input-Input Layer5 The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad> The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad> Input-Input Layer5 The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad> The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the sentence. We give two such examples above, from two different heads from the encoder self-attention at layer 5 of 6. The heads clearly learned to perform different tasks. 15 attention_p1.png\"The image you provided is a page from a PDF paper titled \"Attention Is All You Need.\" Here's the requested information: ### Diagrams, Graphics, and Equations The provided page does not contain any diagrams, graphics, or equations. ### Data Representation and Summary The page contains the title, author information, abstract, and the beginning of the introduction, structured as follows: #### Title - Attention Is All You Need #### Authors - Ashish Vaswani, Google Brain, avaswani@google.com - Noam Shazeer, Google Brain, noam@google.com - Niki Parmar, Google Research, nikip@google.com - Jakob Uszkoreit, Google Research, usz@google.com - Llion Jones, Google Research, llion@google.com - Aidan N. Gomez, University of Toronto, aidan@cs.toronto.edu - \u0141ukasz Kaiser, Google Brain, lukaszkaiser@google.com - Illia Polosukhin, illia.polosukhin@gmail.com #### Abstract ``` The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a de coder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely . Experiments on two machine translation tasks show these models to\" attention_p10.png\"### Content Identified **Table 4:** - The table describes the results of experiments comparing the newly introduced model with previous models. Below is the structured form of the data found in Table 4. **Diagrams/Graphics:** - There are no specific diagrams or graphics present in the shown image. **Equations:** - There are no specific equations present in the shown image. ### Table 4 Details Here is the structured representation of the table content found in the image: ``` Table 4: Experi mental Results Comparing Models Performance | Model | Setting | Performance Indicator | | ------------------------------------ | ---------- ------------ | --------------------- | | Transformer -based Model | WSJ only | Better | | Transformer - based Model | Se mi-supervised | Better | | Previous Expert Models (RNN, etc.) | Various | Not as good | | Best Previous Model | Recurrent Neural Network Grammar | Comparable | ``` This representation captures the essence of the content described in Table 4 of the research paper.\" diagrams or graphics present in the shown image. **Equations:** - There are no specific equations present in the shown image. ### Table 4 Details Here is the structured representation of the table content found in the image: ``` Table 4: Experi mental Results Comparing Models Performance | Model | Setting | Performance Indicator | | ------------------------------------ | ---------- ------------ | --------------------- | | Transformer -based Model | WSJ only | Better | | Transformer - based Model | Se mi-supervised | Better | | Previous Expert Models (RNN, etc.) | Various | Not as good | | Best Previous Model | Recurrent Neural Network Grammar | Comparable | ``` This representation captures the essence of the content described in Table 4 of the research paper.\" attention_p11.png\"The provided image appears to depict a list of references from a research paper. There are no diagrams, graphics, equations, or tables within the image. The text consists solely of reference entries; it does not contain any structured dat a that could be directly translated into tabular form, nor does it present any visual or mathematical content needing transcription or summarization. The references list includes various citation styles (such as article titles, authors, publication venues, and years), but without further structural information or graphical content.\" attention_p12.png\"The provided image page from a PDF consists of references to various research papers, with no diagrams, graphics, or equations present. It does not contain tables or structured data that can be represented in a different format. Below is a structured text summarizing the references for possible database input. ``` References: - Ankur Parikh, Oscar T\u00e4ckstr\u00f6m, Dipanjan Das, and Jakob",
    "71": "Uszkoreit. A decomposable attention model. In Empirical Methods in Natural Language Processing, 2016. - Romai n Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization. arXiv preprint arXiv:1705.04304, 2017. - Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact, and interpretable tree annota tion. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 433 \u2013440. ACL, July 2006. - Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv preprint arXiv:1608.05859, 2016. - Rico Sennrich, Barry Hadd ow, and Alexandra T\u00e4ckstr\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model. In Empirical Methods in Natural Language Processing, 2016. - Romai n Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization. arXiv preprint arXiv:1705.04304, 2017. - Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact, and interpretable tree annota tion. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 433 \u2013440. ACL, July 2006. - Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv preprint arXiv:1608.05859, 2016. - Rico Sennrich, Barry Hadd ow, and Alexandra Birch. Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909, 2015. - Noam Shazeer\" attention_p13.png\"**Diagrams and Graphics:** The image contains a graphic titled \"Attention Visualizations.\" The graphic illustrates the attention mechanism in a layer of a neural network, focusing on the word \"making.\" Attention heads are depicted with li nes and colors connecting the word \"making\" to other words in the sequence. Different colors represent different heads. **Figure:** The caption of the figure reads: \"Figure 3: An example of the attention mechanism following long -distance dependencies in th e encoder self -attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb \u2018making\u2019, completing the phrase \u2018making...more difficult\u2019. Attentions here shown only for the word \u2018making\u2019. Different colors represent differe nt heads. Best viewed in color.\" **Structured Form Output:** ``` { \"image_title\": \"Attention Visualizations\", \"figure_summary\": \"An example of the attention mechanism following long -distance dependencies in the encoder self -attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb \u2018making\u2019, completing the phrase \u2018making...more difficult\u2019. Attentions here shown only for the word \u2018making\u2019. Different colors represent different heads. Best viewed in color.\", \"words\": [ \"I t\", \"is\", \"in\", \"the\", \"spirit\", \"of\", \"a\", \"majority\", \"of\", \"American\", \"govern\" attention_p14.png\"The image contains two sets of graphical diagrams depicting attention heads, likely from a transformer model used for natural language processing. ### Diagrams #### Top { \"image_title\": \"Attention Visualizations\", \"figure_summary\": \"An example of the attention mechanism following long -distance dependencies in the encoder self -attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb \u2018making\u2019, completing the phrase \u2018making...more difficult\u2019. Attentions here shown only for the word \u2018making\u2019. Different colors represent different heads. Best viewed in color.\", \"words\": [ \"I t\", \"is\", \"in\", \"the\", \"spirit\", \"of\", \"a\", \"majority\", \"of\", \"American\", \"govern\" attention_p14.png\"The image contains two sets of graphical diagrams depicting attention heads, likely from a transformer model used for natural language processing. ### Diagrams #### Top Diagram - This diagram illustrates full attention heads for head 5 of layer 5 of 6. - Shows how various words in a sentence attend to each other. - Words: [The, Law, will, never, be, perfect, but, its, application, should, be, just, . This, is, what, we, are, missing, in, my, opinion, . <EOS>, <pad>] #### Bottom Diagram - This diagram shows isolated attentions from just the word \"its\" for attention heads 5 and 6. - Focus is sharper, indicating the word \"its\" and its relevant connections to other words in the sentence. - Words: [The, Law, will, never, be, perfect, but, its, application, should, be, just, . This, is, what, we, are, missing, in, my, opinion, . <EOS>, <pad>] #### Data Translation into Structured Form No distinct tables are extracted from the diagrams/graphics present. ### Summary in Simple Format for Database: - Diagram Type: Attention Heads - Layer: 5 of 6 - Attention Heads: 5, 6 - Top Diagram: - Full attentions for head 5 - Words: [The, Law\" attention_p15.png\"**Image Analysis:** **Diagram Description:** - The image contains two diagrams that depict the attention behavior of heads in an encoder self -attention mechanism, most likely from a transformer model. - These diagrams are visualized as co nnection matrices between the words of a sentence and their aligned pairs through attention weights. **Top Diagram (in green):** - The sentence: \"The Law will never be perfect, but its application should be just. This is what we are missing, in my opinion <EOS> <pad>\" - Connections between words are visualized with lines of varying thickness and opacity, indicating the strength of the attention weights. **Bottom Diagram (in red):** - The same sentence is used: \"The Law will never be 5 - Words: [The, Law\" attention_p15.png\"**Image Analysis:** **Diagram Description:** - The image contains two diagrams that depict the attention behavior of heads in an encoder self -attention mechanism, most likely from a transformer model. - These diagrams are visualized as co nnection matrices between the words of a sentence and their aligned pairs through attention weights. **Top Diagram (in green):** - The sentence: \"The Law will never be perfect, but its application should be just. This is what we are missing, in my opinion <EOS> <pad>\" - Connections between words are visualized with lines of varying thickness and opacity, indicating the strength of the attention weights. **Bottom Diagram (in red):** - The same sentence is used: \"The Law will never be perfect, but its application should be just. This is what we are missing, in my opinion <EOS> <pad>\" - Similar to the top diagram, the connections between words are visualized with lines, indicating the attention weights but in a different pattern compared to the top diagram. **Caption:** - The caption below the diagrams states: \"Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the sentence. We give two such examples above, from two different heads from the enco der self -attention at layer 5 of 6. The heads clearly learned to perform different tasks.\" **Structured Summary:** _Strucutred Data:_ - **Diagrams:** - Color: Green, Red \" attention_p2.png\"**Structure of the Content:** *Diagrams/Graphics:* 1. **Figure 1**: This is referenced in the text as consisting of left and right halves representing the encoder and decoder, respectively. --- *Equations:* There are no explicit equations present within the provided page of the document. --- *Table:* There are no tables present within the provided page of the document. --- *Summary of Graphics and Diagrams:* 1. **Figure 1**: - **Description**: The diagram (Figure 1) illustrated describes th e Transformer architecture's Encoder and Decoder blocks. - **Details**: - The left half of Figure 1 demonstrates the Encoder, which likely consists of stacked self -attention mechanisms and fully connected layers. - The right half of Figure 1 shows the Deco der, probably constructed similarly but incorporating mechanisms to attend to the encoder's output. The detailed theoretical description of the Transformer model outlines several respectively. --- *Equations:* There are no explicit equations",
    "72": "present within the provided page of the document. --- *Table:* There are no tables present within the provided page of the document. --- *Summary of Graphics and Diagrams:* 1. **Figure 1**: - **Description**: The diagram (Figure 1) illustrated describes th e Transformer architecture's Encoder and Decoder blocks. - **Details**: - The left half of Figure 1 demonstrates the Encoder, which likely consists of stacked self -attention mechanisms and fully connected layers. - The right half of Figure 1 shows the Deco der, probably constructed similarly but incorporating mechanisms to attend to the encoder's output. The detailed theoretical description of the Transformer model outlines several aspects of language modeling and parallelization advantages compared to recur rent models and other attention mechanisms. The page does not contain any structured data within tables, specific numerical data, or equations that require direct transcription into text form. --- **Database Representation:** - **Figure 1**: - **Type**: Di agram - **Description**: Illustration of the Transformer model's Encoder and Decoder structures. - **Details**: Encoder and Decoder using stacked self -attention mechanisms and point -wise, fully connected layers. --- No tables\" attention_p3.png\"### Diagrams and Graphics: **Figure 1: The Transformer - model architecture.** - The diagram shows the architecture of the Transformer model. - It consists of two main parts: the Encoder and the Decoder. - Both the Encoder and the Decoder are composed of a stack of `N` identical layers. - The Encoder and Decoder each have sub -layers that include Multi -Head Attention and Feed Forward mechanisms with normalization steps. - The Encoder processes the input embeddings with positional encoding. - The Decoder processes the output embeddings with positional encoding and includes an additional Masked Multi -Head Attention sub-layer. - Both processes lead to the generation of output probabilities through a linear transformation followed by a softmax ac tivation function. ### Equations: **Equation for Encoder:** \\[ \\text{LayerNorm}(x + \\text{Sublayer}(x)) \\] Where Sublayer(x) is the function implemented by the sub -layer. **Output Dimension:** \\[ d_{ \\text{model}} = 512 \\] ### Summary of Sections: #### 3.1 Encoder and Decoder Stacks **Encoder:** - Composed of a Multi -Head Attention and Feed Forward mechanisms with normalization steps. - The Encoder processes the input embeddings with positional encoding. - The Decoder processes the output embeddings with positional encoding and includes an additional Masked Multi -Head Attention sub-layer. - Both processes lead to the generation of output probabilities through a linear transformation followed by a softmax ac tivation function. ### Equations: **Equation for Encoder:** \\[ \\text{LayerNorm}(x + \\text{Sublayer}(x)) \\] Where Sublayer(x) is the function implemented by the sub -layer. **Output Dimension:** \\[ d_{ \\text{model}} = 512 \\] ### Summary of Sections: #### 3.1 Encoder and Decoder Stacks **Encoder:** - Composed of a stack of N = 6 identical layers. - Each layer has two sub -layers: 1. Multi -head self -attention mechanism. 2. Simple, position -wise fully connected feed -forward network. - Each sub -layer employs residu al connections followed by layer normalization. - The output of each sub -layer is LayerNorm(x + Sublayer(x)). \" attention_p4.png\"The image contains the following notable elements: 1. **Diagrams:** - **Scaled Dot -Product Attention Diagram:** This diagram illustrates the process of scaled dot - product attention. It shows the inputs \\( Q \\), \\( K \\), and \\( V \\) going t hrough several layers: - MatMul (Matrix Multiplication) - Scale - Mask (optional) - SoftMax - MatMul (Matrix Multiplication again) - **Multi -Head Attention Diagram:** This diagram depicts multi -head attention consisting of several parallel attention layers : - Linear transformations performed on \\( Q \\), \\( K \\), and \\( V \\) - Scaled Dot -Product Attention applied in parallel (h times) - Concatenation of the outputs - Another Linear transformation 2. **Equation:** - The equation provided in the image represen ts the scaled dot -product attention: \\[ \\text{Attention}(Q, K, V) = \\text{softmax} \\left(\\frac{QK^T}{ \\sqrt{d_k}} \\right)V \\tag{1} \\] There are no tables within the given image. ### Summary for Database Insertion **Diagrams:** - **Scaled Dot -Product Attention Diagram:** Depicts the process involving MatMul, Scale, optional Mask, SoftMax, and another MatMul on inputs \\( Q \\), \\( K \\), and \\( V \\). \" attention_p5.png\"Here's the structured data derived from the image provided, organized for easy input into a database: --- **Diagrams, Graphics, and Equations:** 1. **Equation 1:** - Concatenation of the outputs - Another Linear transformation 2. **Equation:** - The equation provided in the image represen ts the scaled dot -product attention: \\[ \\text{Attention}(Q, K, V) = \\text{softmax} \\left(\\frac{QK^T}{ \\sqrt{d_k}} \\right)V \\tag{1} \\] There are no tables within the given image. ### Summary for Database Insertion **Diagrams:** - **Scaled Dot -Product Attention Diagram:** Depicts the process involving MatMul, Scale, optional Mask, SoftMax, and another MatMul on inputs \\( Q \\), \\( K \\), and \\( V \\). \" attention_p5.png\"Here's the structured data derived from the image provided, organized for easy input into a database: --- **Diagrams, Graphics, and Equations:** 1. **Equation 1:** - Description: Multi -head attention combining multiple attention heads. - Representation: \\[ \\text{MultiHead}(Q, K, V) = \\text{Concat}( \\text{head}_1, ..., \\text{head}_h)W^O \\] where \\(\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V) \\) 2. **Equation 2:** - Description: Position -wise feed -forward network. - Representation: \\[ \\text{FFN}(x) = \\text{max}(0, xW_1 + b_1)W_2 + b_2 \\] **Tables:** - **Table of Parameters:** - Description: Parameters used in multi -head attention. - Representation: - \\( W_i^Q \\in \\mathbb{R}^{d_{ \\text{model}} \\times d_k} \\) - \\( W_i^K \\in \\mathbb{R}^{ d_{\\text{model}} \\times d_k} \\) - \\( W_i^V \\in \\mathbb{R}^{d_{ \\text{model}} \\times d_k}\" attention_p6.png\"## Table **Table 1: Maximum path lengths, per -layer complexity and minimum number of sequential operations for different layer types.** | Layer Type | Complexity per Layer | Sequential Operations | Maximum Path Length | | ------------------ --------- |---------------- ------ |----------------------- |-------------------------- | | Self -Attention | O(n\u00b2\u00b7d) | O(1) | O(1) | | Recurrent | O(n\u00b7d\u00b2) | O(n) | O(n) | | Convolutional | O(k\u00b7n\u00b7d\u00b2) | O(1) | O(log\u2096(n)) | | Self -Attention (restricted)| O(r\u00b7n\u00b7d) | O(1) | O(n/r) | ## Equations ### Positional Encodings: 1. \\( PE_{ \\text{pos}, 2i} = \\sin(\\text{pos}/10000^{2i/d_{ \\text{model}}}) \\) 2. \\( PE_{ \\text{pos}, 2i+1} = \\cos(\\text{pos}/10000^{2i/d_{ \\text{model}}}) \\) where pos is the position and i is the dimens ion.\" attention_p7.png\"The image contains various sections with numerical data and a mathematical equation. Here's a summary including the structured form of the data: ### Equation: \\[ lrate = d^{-0.5}_{model} \\cdot \\min(step \\_num^{ -0.5}, step \\_num \\cdot warmup \\_steps^{ -1.5}) \\] ### ------ |----------------------- |-------------------------- | | Self -Attention | O(n\u00b2\u00b7d) | O(1) | O(1) | | Recurrent | O(n\u00b7d\u00b2) | O(n) | O(n) | | Convolutional | O(k\u00b7n\u00b7d\u00b2) | O(1) | O(log\u2096(n)) | | Self -Attention (restricted)| O(r\u00b7n\u00b7d) | O(1) | O(n/r) | ## Equations ### Positional Encodings: 1. \\( PE_{ \\text{pos}, 2i} = \\sin(\\text{pos}/10000^{2i/d_{ \\text{model}}}) \\) 2. \\( PE_{ \\text{pos}, 2i+1} = \\cos(\\text{pos}/10000^{2i/d_{ \\text{model}}}) \\) where pos is the position and i is the dimens ion.\" attention_p7.png\"The image contains various sections with numerical data and a mathematical equation. Here's a summary including the structured form of the data: ### Equation: \\[ lrate = d^{-0.5}_{model} \\cdot \\min(step \\_num^{ -0.5}, step \\_num \\cdot warmup \\_steps^{ -1.5}) \\] ###",
    "73": "Table 3 Reference: The text references a table (Table 3), but the table itself is not visible in this image. Specific data related to training step times and model training are mentioned. ### Structured Data: - **Training Data and Bat ching:** - WMT 2014 English -German dataset - Total: \u2248 4.5 million sentence pairs - Byte -pair encoding: shared source -target vocabulary \u2248 37000 tokens - English -French dataset - Total: 36M sentences - V ocabulary: 32000 word -piece tokens - Training batch: \u2248 25000 source tokens and \u2248 25000 target tokens - **Hardware and Schedule:** - Machine: 8 NVIDIA P100 GPUs - Training step time (base models): 0.4 seconds - Number of steps (base models): 100,000 steps (12 hours) - Training step time (big models): 1.0 second s - Number of steps (big models): 300,000 steps (3.\" attention_p8.png\"### Table Information Table 2 provides information about the BLEU scores achieved by different models on the English -to-German (EN -DE) and English -to-French (EN - FR) translation tasks, along with their training costs measured in floating -point operations per second (FLOPs). ``` ------------------------------------------ | Model | BLEU | Training Cost (FLOPs) | | | EN -DE | EN -FR | EN -DE | EN -FR | ------------------------------------------ | ByteNet [18] | 23.75 | | | 1.0 * 10^20 | | Deep -Att + PosUnk [39] | | 39.2 | | 1.4 * 10^20 | | GNMT + RL [38] | 24.6 | 39.92 | 2.3 * 10^19 | | | ConvS2S [9] | 25.16 | 40.46 | 9.6 * 10^18 | 1.5 * 10^20 | | MoE [32] | 26.03 | 40.56 | 2.0 * 10^19 | 1.2 * 10^20 | | Deep -Att + PosUnk Ensemble [39] | | 40.4 |\" attention_p9.png\"**Diagrams, Graphics, Equations, or Tables within the Image** - **Tables**: achieved by different models on the English -to-German (EN -DE) and English -to-French (EN - FR) translation tasks, along with their training costs measured in floating -point operations per second (FLOPs). ``` ------------------------------------------ | Model | BLEU | Training Cost (FLOPs) | | | EN -DE | EN -FR | EN -DE | EN -FR | ------------------------------------------ | ByteNet [18] | 23.75 | | | 1.0 * 10^20 | | Deep -Att + PosUnk [39] | | 39.2 | | 1.4 * 10^20 | | GNMT + RL [38] | 24.6 | 39.92 | 2.3 * 10^19 | | | ConvS2S [9] | 25.16 | 40.46 | 9.6 * 10^18 | 1.5 * 10^20 | | MoE [32] | 26.03 | 40.56 | 2.0 * 10^19 | 1.2 * 10^20 | | Deep -Att + PosUnk Ensemble [39] | | 40.4 |\" attention_p9.png\"**Diagrams, Graphics, Equations, or Tables within the Image** - **Tables**: - **Table 3: Variations on the Transformer architecture.** - **Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ).** **Table 3: Data Representation** | N | d_model | d_ff | h | d_k | d_v | P_drop | \u03b5_ls | train steps | PPL (dev) | PPL (test) | BLEU (dev) | BLEU (test) | params x10^6 | | ----|--------- |------ |----|-----|-----|---- ----|------ |------------- |----------- |------------ |------------ |------------- |-------------- | | 6 | 512 | 2048 | 8 | 64 | 64 | 0.1 | 0.1 | 100K | 4.92 | 4.85 | 25.8 | 26.4 | 65 | | (A)| 1 | 512 | 512 | | | | | | 5.29 | 5.00 | 24.9 | 25.5 | | | | 4 | 128 | 128 | | | | \" Challenges LLM July 19_23_p1.png\"The image contains the following elements: ### Diagram 1. **Figure 1: Overview of LLM Challenges** - **Design** - Unfathomable Datasets - Tokenizer -Reliance - Fine-Tuning Overhead - **Behavior** - Prompt Brittleness - Misal igned Behavior - Outdated Knowledge - **Science** - Evaluations Based on Static, Human -Written Ground Truth - Lacking Experimental Designs - Lack of Reproducibility - Common Challenges: - High Inference Latency - Limited Context Length - Hallucinations - Tasks Not Solvable by Scale - High Pre -Training Costs - Detecting Generated Texts - Brittle Evaluations ### Structured Data Representation **Table of Contents** ``` Contents 1 Introduction ........................................... 1 2 Challenges 2.1 Unfat homable Datasets .............................. 2 2.2 Tokenizer -Reliance ...................................... 4 2.3 High Pre - Training Costs ............................... 6 2.4 Fine -Tuning Overhead .................................. 10 2.5 Behavior - Outdated Knowledge - **Science** - Evaluations Based on Static, Human -Written Ground Truth - Lacking Experimental Designs - Lack of Reproducibility - Common Challenges: - High Inference Latency - Limited Context Length - Hallucinations - Tasks Not Solvable by Scale - High Pre -Training Costs - Detecting Generated Texts - Brittle Evaluations ### Structured Data Representation **Table of Contents** ``` Contents 1 Introduction ........................................... 1 2 Challenges 2.1 Unfat homable Datasets .............................. 2 2.2 Tokenizer -Reliance ...................................... 4 2.3 High Pre - Training Costs ............................... 6 2.4 Fine -Tuning Overhead .................................. 10 2.5 High Inferenc e Latency ................................. 11 2.6 Limited Context Length ................................ 14 2.7 Prompt Brittleness ...................................... 17 2.8 Hallucinations ............................................ 19 2.9\" Challenges LLM July 19_23_p10.png\"The image includes the following types of content: ### Graphics/Diagrams: 1. **Warning Diagram 1**: - **Title:** Overhead of Storing and Loading Fine-Tuned LLMs - **Content:** \"When adapting an LLM via full -model fine -tuning, an individual copy of the model must be stored (consuming data storage) and loaded (expending memory allocation, etc.) for each task.\" 2. **Warning Diagram 2**: - **Title:** Large Memory Requirements - **Content:** \"Fine -tuning entire LLMs requires the same amount of memory as pre-training, rendering it infeasible for many practitioners.\" ### Structured Data (as a table representation): | **ID** | **Reference** | | -------- |------------------------------------------------------- ---| | 514 | Sanyal et al. | | 476 | Sanyal et al. | | 249 | (Cited paper reference) | | 1 \u201312B | (LLM parameter range) | | 6.9B | (Specific parameter model) | | 4,200 | (GPU hours) | | 251 | (Cited paper reference) | | 663 | (Cited paper reference) | | 685 | (Cited paper reference) | | 215 | (Cited paper reference) | |\" Challenges LLM July 19_23_p11.png\"### Diagrams #### Figure 5 Diagrams 1. **Figure 5(a):** - **Sentiment model** - Fine-tuning LLM #1 - Sentiment analysis task - **QA model** - Fine-tuning LLM #2 - Question answering task - **Hate speech model** - Fine-tuning LLM #3 - Hate speech task 2. **Figure 5(b):** - **Base LLM (PEFT -adaptable)** - Sentiment model: ---| | 514 | Sanyal et al. | | 476 | Sanyal et al. | | 249 | (Cited paper reference) | | 1 \u201312B | (LLM parameter range) | | 6.9B | (Specific parameter model) | | 4,200 | (GPU hours) | | 251 | (Cited paper reference) | | 663 | (Cited paper reference) | | 685 | (Cited paper reference) | | 215 | (Cited paper reference) | |\" Challenges LLM July 19_23_p11.png\"### Diagrams #### Figure 5 Diagrams 1. **Figure 5(a):** - **Sentiment model** - Fine-tuning LLM #1 - Sentiment analysis task - **QA model** - Fine-tuning LLM #2 - Question answering task - **Hate speech model** - Fine-tuning LLM #3 - Hate speech task 2. **Figure 5(b):** - **Base LLM (PEFT -adaptable)** - Sentiment model: - PEFT weights - Sentiment analysis task - QA model: - PEFT weights - Question answering task - Hate speech model: - PEFT weights - Hate speech task ### Sum mary of Graphics - **Full Matrix Multiplications:** - The graphic warns that parameter -efficient fine -tuning of LLMs still requires computing full forward/backward passes throughout the whole network. - **High Inference Latency:** - The graphic warns that LLM inference latencies remain high because of low parallelizability and large memory footprints. ### Equations There are no visible equations on this page. \" Challenges LLM July 19_23_p12.png\"The provided image contains primarily text from a research paper. Based on your instructions, I'm identifying the",
    "74": "relevant technical details such as equations, tables, and diagrams without focusing on the plain text. ### E quations There are two equations mentioned in the text: 1. `FlashAttention` 2. `LLM.int8()` ### Summarized Content about Methods, Techniques, and Models: 1. **Efficient Attention**: - Two lines of work to accelerate attention mechanisms: 1. Lower -level har dware -aware modifications. 2. Higher -level sub-quadratic approximations of the attention mechanism. - Methodologies: - **Multi -query attention**: Reduces memory bandwidth bottlenecks. - **FlashAttention**: Minimizes I/O operations by proposing an alter -native computation method for multi -head self -attention. - **Pruning**: Removes parts of the model to reduce complexity without degrading performance. 2. **Quantization**: - Techniques to reduce memory footprint or increase throughput: - **NUQMM and ZeroQuant **: Non -uniform quantization methods. - **Degradation -free LLM.int8()**: Efficient inference for multi -billion parameter LLMs. 3. **Pruning**: - accelerate attention mechanisms: 1. Lower -level har dware -aware modifications. 2. Higher -level sub-quadratic approximations of the attention mechanism. - Methodologies: - **Multi -query attention**: Reduces memory bandwidth bottlenecks. - **FlashAttention**: Minimizes I/O operations by proposing an alter -native computation method for multi -head self -attention. - **Pruning**: Removes parts of the model to reduce complexity without degrading performance. 2. **Quantization**: - Techniques to reduce memory footprint or increase throughput: - **NUQMM and ZeroQuant **: Non -uniform quantization methods. - **Degradation -free LLM.int8()**: Efficient inference for multi -billion parameter LLMs. 3. **Pruning**: - Structured and unstructured pruning to reduce model complexity. - Methods: - **Dense sections with sparsity sub stitutions**. - **Structured vs. Unstructured pruning**: - LLM -Pruner\" Challenges LLM July 19_23_p13.png\"**Equations:** The text includes the following equations: 1. G(x) and Ei(x): \\[G(x) \\quad \\text{and} \\quad Ei(x) \\] 2. Summation of gating and expert network outputs: \\[ y = \\sum_{i=1}^{n} G(x)_i E_i(x) \\] 3. Condition that if \\(G(x)_i = 0 \\): \\[ G(x)_i = 0 \\implies Ei(x) \\text{ is not computed during inference.} \\] **Diagrams or Graphics:** The page does not have any diagrams or graphics. **Tables:** The page does not include any tables.\" Challenges LLM July 19_23_p14.png\"### Diagrams, Graphics, and Tables: - **Graphics:** - A red-bordered box with an alert icon contains a note regarding \"Limited Context Length\". **Graphic Summary:** The graphic is a warning box highlighting the challenges of handling long inputs in NLP tasks. --- ### Equations: There are no equations present in the image. --- ### Transcription: #### Graphic (Warning Box): ``` Limited Context Length Limited context lengths are a barrier for handling long inputs well to facilitate applications like novel or textbook writing or summarizing. ``` --- ### Summary: The image contains a page discussing strategies to mitigate limitations in processing long contexts in NLP ta sks. It highlights the importance of understanding broader context for NLP applications, specifically drawing from different research and implementations to address these challenges. The warning graphic emphasizes the issue of limited context length in han dling long inputs efficiently.\" long inputs in NLP tasks. --- ### Equations: There are no equations present in the image. --- ### Transcription: #### Graphic (Warning Box): ``` Limited Context Length Limited context lengths are a barrier for handling long inputs well to facilitate applications like novel or textbook writing or summarizing. ``` --- ### Summary: The image contains a page discussing strategies to mitigate limitations in processing long contexts in NLP ta sks. It highlights the importance of understanding broader context for NLP applications, specifically drawing from different research and implementations to address these challenges. The warning graphic emphasizes the issue of limited context length in han dling long inputs efficiently.\" Challenges LLM July 19_23_p15.png\"The provided image contains text from a PDF document. Within this text, several references to diagrams, tables, or graphics might be implied, but the image itself shows none of these explicitly. Instead, there are distinct sections, citations, and references to research names and works. Below is a structured representation of the data as inferred from the text: ### Equations: - The provided page does not contain explicit equations to be transcribed. ### Diagrams/Graphics/Vi suals: - The image provided does not include any visible diagrams, graphics, or visuals. ### Structured Data from Table: - No tables are present within the provided page. ### Summary: The text on the page discusses various advancements and methodologies in attention mechanisms within transformer -based language models (LLMs). It highlights several key points: 1. **Efficient Attention Mechanisms**: - **Luna**: A unified nested attention mechanism by Ma et al. - **COltS**: Handles sequences up to 64,000 tokens by splitting computations. - **Transient Global Attention**: Each token attends to nearby and global tokens. - **Synthesizer**: Introduces token -free interactions for synthetic attention weights. - **Speed -up Techniques**: Hua et al. and other models like those by Tay et al. and Ding et al. focused on speed and memory efficiency. 2. **Length Generalization**: - Necessary for scaling LLMs. - Addressed through various proposed\" Challenges LLM July 19_23_p16.png\"The image contains equations and references to figures and methods. Here is the structured representation of the data from the image: ### Equations: 1. **Equation 4**: \\[ \\text{softmax} \\left( \\frac{1}{ \\sqrt{d}} \\sum_{i,j} x_{i}^T W_{q} splitting computations. - **Transient Global Attention**: Each token attends to nearby and global tokens. - **Synthesizer**: Introduces token -free interactions for synthetic attention weights. - **Speed -up Techniques**: Hua et al. and other models like those by Tay et al. and Ding et al. focused on speed and memory efficiency. 2. **Length Generalization**: - Necessary for scaling LLMs. - Addressed through various proposed\" Challenges LLM July 19_23_p16.png\"The image contains equations and references to figures and methods. Here is the structured representation of the data from the image: ### Equations: 1. **Equation 4**: \\[ \\text{softmax} \\left( \\frac{1}{ \\sqrt{d}} \\sum_{i,j} x_{i}^T W_{q} R_{\\Theta, k}^{d (i -j)} W_{k} x_{j} \\right) \\] 2. **Equation 5**: \\[ \\text{softmax} \\left( \\frac{1}{ \\sqrt{d}} \\sum_{i,j} x_{i}^T W_{q} W_{k} x_{j} + b_{i -j} \\right) \\] 3. **Equation 6**: \\[ \\text{softmax} \\left( \\frac{1}{ \\sqrt{d}} \\sum_{i,j} x_ {i}^T W_{q} W_{k} x_{j} + m \\cdot (i -j) \\right) \\] ### Summary of Methods/References: 1. **Rotary Positional Embeddings (RoPE)**: References [526] and [576, 47, 86]. RoPE incorporates absolute positional information into embeddings through rotation, improv ing performance on long text tasks. 2. **Relative Positional Bias**: Reference [443].\" Challenges LLM July 19_23_p17.png\"The provided image contains several notable elements that can be structured for database input. The elements are as follows: ### Table Data A table labeled \"Prompt Brittleness\" with references to several citations in diffe rent colors. - **Caption**: Prompt Brittleness - **Content**: Variations of the prompt syntax, often occurring in ways unintuitive to humans, can result in dramatic output changes. - **References**: - [675, 596, 342] ### Graphical and Diagram Data There is a diagram in red with a warning sign icon and text within it: - **Diagram Title**: Prompt Brittleness - **Description**: Variations of the prompt syntax, often occurring in ways unintuitive to humans, can result in dramatic output changes. - **References* *: - [675, 596, 342] ### Equations There are no equations",
    "75": "present in the image. By parsing this image, the following structured information could be passed to a database: ```json { \"tables\": [ { \"title\": \"Prompt Brittleness\", \"content\": \"Variations of the prompt syntax, often occurring in ways unintuitive to humans, can res ult in dramatic output changes.\", in ways unintuitive to humans, can result in dramatic output changes. - **References**: - [675, 596, 342] ### Graphical and Diagram Data There is a diagram in red with a warning sign icon and text within it: - **Diagram Title**: Prompt Brittleness - **Description**: Variations of the prompt syntax, often occurring in ways unintuitive to humans, can result in dramatic output changes. - **References* *: - [675, 596, 342] ### Equations There are no equations present in the image. By parsing this image, the following structured information could be passed to a database: ```json { \"tables\": [ { \"title\": \"Prompt Brittleness\", \"content\": \"Variations of the prompt syntax, often occurring in ways unintuitive to humans, can res ult in dramatic output changes.\", \"references\": [675, 596, 342] } ], \"diagrams\": [ { \"title\": \"Prompt Brittleness\", \"description\": \"Variations of the prompt syntax, often occurring in\" Challenges LLM July 19_23_p18.png\"The provided document page mainly contains graphics and diagrams related to Single -Turn and Multi -Turn Prompting. Below is a structured format for these elements: ### Graphics and Diagrams #### Single -Turn Prompting 1. **I n-Context Learning** - **Input:** - Q: Lisa has four pets. She buys three more with four times the amount of money. How many more days does it take her to feed them? - A: The answer is 42. 2. **Instruction -Following** - **Input:** - Q: Here is an arithmeti c question: Lisa has four pets. She buys three more with four times the amount of money. How many more days does it take her to feed them? - A: The answer is 42. 3. **Chain -of-Thought** - **Input:** - Q: Lisa has four pets. She buys three more with four ti mes the amount of money. How many more days does it take her to feed them? - A: The other dogs have arrival schedules. 4. **Prompt Tuning** - **Input:** - Q: Lisa has four pets. She buys three more with four times the amount of money. How many more days do es it take her to feed them? - A: The answer is 42. #### Multi -Turn Prompting 1. **Self -Consistency** - **Input:** - Q\" Challenges LLM July 19_23_p19.png\"The image contains text and references from a research paper, but no diagrams, graphics, or equations are present. Below is the table content transcribed in a structured format: ### Table Data 1. **Chain -of-Thought (CoT)** - [327, 601]: Describes a technique to construct few -shot prompts via a series of intermediate reasoning steps leading to take her to feed them? - A: The other dogs have arrival schedules. 4. **Prompt Tuning** - **Input:** - Q: Lisa has four pets. She buys three more with four times the amount of money. How many more days do es it take her to feed them? - A: The answer is 42. #### Multi -Turn Prompting 1. **Self -Consistency** - **Input:** - Q\" Challenges LLM July 19_23_p19.png\"The image contains text and references from a research paper, but no diagrams, graphics, or equations are present. Below is the table content transcribed in a structured format: ### Table Data 1. **Chain -of-Thought (CoT)** - [327, 601]: Describes a technique to construct few -shot prompts via a series of intermediate reasoning steps leading to the final output. 2. **Impersonation** - [473]: A technique where the prompt for the model asks it to pretend to be a domain expert w hen answering a domain -specific question. 3. **Multi -Turn Prompting** - Iteratively chains prompts and their answers together. 4. **Ask Me Anything** - [24]: Uses multiple prompt templates to reformat few -shot example inputs into an open -ended question -answering format. 5. **Self -consistency** - [585]: Extends chain -of-thought prompting by sampling multiple reasoning paths and selecting the most consistent answer via a majority vote. 6. **Least -to-Most** - [682]: Uses a set of constant prompts to decompose a given complex problem into a series of subproblems. 7. **Scratchpad** - [391]: A method to fine -tune LLMs on multi -step computation tasks by outputting intermediate reasoning steps into a \"scratchpad.\" 8. **ReAct** - [640]: Combines reasoning and acting by prompting LLMs to generate reasoning traces\" Challenges LLM July 19_23_p2.png\"The provided image contains some structured data and graphics. Here is the structured representation and summary of the relevant elements: **Tables:** There are no tables present in the image. **Graphics/Diagrams:** There a re some highlighted boxes, referred to as \"Challenge\" and \"Unfathomable Datasets.\" The descriptions within these boxes are: 1. **Challenge Box:** - **Title:** Challenge - **Description:** This box highlights a challenge. 2. **Unfathomable Datasets Box:** - **Title:** Unfathomable Datasets - **Description:** The size of modern pre -training datasets renders it impractical for any individual to read or conduct quality assessments on the encompassed documents generate reasoning traces\" Challenges LLM July 19_23_p2.png\"The provided image contains some structured data and graphics. Here is the structured representation and summary of the relevant elements: **Tables:** There are no tables present in the image. **Graphics/Diagrams:** There a re some highlighted boxes, referred to as \"Challenge\" and \"Unfathomable Datasets.\" The descriptions within these boxes are: 1. **Challenge Box:** - **Title:** Challenge - **Description:** This box highlights a challenge. 2. **Unfathomable Datasets Box:** - **Title:** Unfathomable Datasets - **Description:** The size of modern pre -training datasets renders it impractical for any individual to read or conduct quality assessments on the encompassed documents thoroughly. **Equations:** There are no equations present in the image. **Summary:** The image contains information about the challenges posed by unfathomable datasets and near - duplicates in pre -training data for Large Language Models (LLMs). It discusses the difficulty in scaling up pre -training data, issues with detecting and handling near -duplicates, and problems arising from benchmark data contamination. - **Unfathomable Datasets:** Discusses the impracticality of manual quality checks due to the large size of pre -training datasets. - **Near - Duplicates:** Highlights the challenge of identifying and handling near -duplicates that degrade model performance. Mentions methods like MinHash and SemDeDup for deduplication. - **Benchmark\" Challenges LLM July 19_23_p20.png\"The image contains the following elements: ### Figure: **Figure 7: Example of Hallucinations with GPT -4.** This figure contains three different examples of papers that illustrate specific aspects of large language models, such as the availability of influential papers, understanding the limitations and power of language models, and a survey of the Transformers. Additionally, it shows an example where GPT -4 generates text that appears fluent but contains inaccuracies. ### Ta ble: A table titled **\"Hallucination\"** states the following information: ``` | Warning Icon | Hallucination [293, 458, 241] | | -------------- |-------- ------------------------------------------------------------------ | | | Generated text that is fluent and natural but | | | unfaithful to the source content (intrinsic) | | | and/or under -determined (extrinsic). examples of papers that illustrate specific aspects of large language models, such as the availability of influential papers, understanding the limitations and power of language models, and a survey of the Transformers. Additionally, it shows an example where GPT -4 generates text that appears fluent but contains inaccuracies. ### Ta ble: A table titled **\"Hallucination\"** states the following information: ``` | Warning Icon | Hallucination [293, 458, 241] | | --------------",
    "76": "|-------- ------------------------------------------------------------------ | | | Generated text that is fluent and natural but | | | unfaithful to the source content (intrinsic) | | | and/or under -determined (extrinsic). | ``` ### Graphics: - An example output graphic titled **\"Example of Hallucinations with GPT - 4\"** accessed on 02/06/2023. - This graphic contains sni ppets of text -aligning suggested different influential papers on large language models showing both correct and incorrect factual information. ### Equations: There are no explicit equations presented in this page from the PDF. ### Summary: The page discuss es hallucinations in large language models (LLMs), particularly focusing on intrinsic and extrinsic hallucinations. Intrinsic hallucinations arise when the generated text logically contradicts the source content, whereas extrinsic hallucinations arise when \" Challenges LLM July 19_23_p21.png\"The page includes diagrams that illustrate intrinsic and extrinsic hallucinations with their corresponding solutions. Below is a summary and structured transcription of the diagrams and their content. ### Diagrams - **Diag ram Title: Illustration of a) intrinsic and b) extrinsic hallucinations in user interaction with an LLM** #### Intrinsic Hallucination (Problems P.1) - **Description:** - **Problem Statement:** \u201cBob\u2019s wife is Amy. Bob\u2019s daughter is Cindy. Who is Cindy to A my?\u201d - **Incorrect Answer:** \"Cindy is Amy's daughter -in-law.\" - **Visual:** The incorrect answer is marked with an \"X.\" #### Intrinsic Hallucination (Solutions S.1) Decoding Strategies) - **Description:** - **Problem Statement:** \u201cBob\u2019s wife is Amy. Bob\u2019s daughter is Cindy. Who is Cindy to Amy?\u201d - **Correct Answer:** \"Cindy is Amy's daughter.\" - **Visual:** The correct answer is marked with a check mark \" \u2714.\" #### Extrinsic Hallucination (Problems P.2) - **Description:** - **Problem Statement:** a) intrinsic and b) extrinsic hallucinations in user interaction with an LLM** #### Intrinsic Hallucination (Problems P.1) - **Description:** - **Problem Statement:** \u201cBob\u2019s wife is Amy. Bob\u2019s daughter is Cindy. Who is Cindy to A my?\u201d - **Incorrect Answer:** \"Cindy is Amy's daughter -in-law.\" - **Visual:** The incorrect answer is marked with an \"X.\" #### Intrinsic Hallucination (Solutions S.1) Decoding Strategies) - **Description:** - **Problem Statement:** \u201cBob\u2019s wife is Amy. Bob\u2019s daughter is Cindy. Who is Cindy to Amy?\u201d - **Correct Answer:** \"Cindy is Amy's daughter.\" - **Visual:** The correct answer is marked with a check mark \" \u2714.\" #### Extrinsic Hallucination (Problems P.2) - **Description:** - **Problem Statement:** \u201cExplain RLHF for LLMs.\u201d - **Incorrect Answer:** \"RLHF stands for 'Rights, Limitations, Harms and Freedoms' and is a framework for models like LLM -X.\" - **Visual:** The incorrect answer is marked with an \"X.\" #### Extrinsic Hallucination (Solutions S\" Challenges LLM July 19_23_p22.png\"The image contains the following elements: 1. **Figure**: - **Caption**: Example of Retrieval -Augmented GPT -4, accessed on 02/06/2023. - **Content Description**: The figure shows an example of a model responding to a query \"Which review papers discuss challenges and applications of large language models?\" along with the model's response highlighting several papers and their key points. 2. **Diagram**: - **Misaligned Behavior** box with a warning icon - **Content**: - **Titl e**: Misaligned Behavior - **Text**: \"LLMs often generate outputs that are not well -aligned with human values or intentions, which can have unintended or negative consequences.\" 3. **Equations**: - There is a mention of threshold and control token -based co nditions in conditional training. - **Equation**: `R(x) >= t` - **Condition**: `c = <|good|> or <|bad|>` Since there is no table present in the image, there is no tabular data to be represented. **Description and Summary of Graphics and Diagrams**: - **Ret rieval -Augmented GPT -4 Example Figure**: - The figure displays an interface where a user queries a language model about review papers concerning challenges and applications of large language models. The response from the model lists several papers with the ir titles and brief summaries highlighting their contributions and focus areas\" Challenges LLM July 19_23_p23.png\"**Table Data:** | Detecting Misaligned Behavior | is a mention of threshold and control token -based co nditions in conditional training. - **Equation**: `R(x) >= t` - **Condition**: `c = <|good|> or <|bad|>` Since there is no table present in the image, there is no tabular data to be represented. **Description and Summary of Graphics and Diagrams**: - **Ret rieval -Augmented GPT -4 Example Figure**: - The figure displays an interface where a user queries a language model about review papers concerning challenges and applications of large language models. The response from the model lists several papers with the ir titles and brief summaries highlighting their contributions and focus areas\" Challenges LLM July 19_23_p23.png\"**Table Data:** | Detecting Misaligned Behavior | Aligning Model Behavior | | ------------------------------- |------------------------ | | Evaluation and Auditing | Pre -training with Human Feedback | | Mechanistic Interpreta bility | Instruction Fine - Tuning | | Red Teaming | RLHF | **Graphic/Diagram Description:** - **Figure 10: Alignment** - Lower right corner of the page contains a table titled \"Figure 10: Alignment.\" - The categorized alignment work is divided into two broa d categories: - Detecting Misaligned Behavior - Aligning Model Behavior - Under \"Detecting Misaligned Behavior\" are: - Evaluation and Auditing - Mechanistic Interpretability - Red Teaming - Under \"Aligning Model Behavior\" are: - Pre- training with Human Fee dback - Instruction Fine -Tuning - RLHF **Equations:** There are no equations on this page. **Summary:** This page discusses various aspects of fine -tuning and aligning large language models (LLMs) using different approaches and techniques such as Reinforce ment Learning from Human Feedback (RLHF). It touches on the importance of feedback from humans in generating improved instructions and points out the challenges and potential issues like the introduction of unwanted biases. The discussion also includes dif ferent methods categorized for detecting misaligned behavior and aligning model behavior effectively.\" Challenges LLM July 19_23_p24.png\"The provided page from the PDF contains text, but there are no diagrams, graphics, equations, or tables present on this page. Since the task involves identifying specific elements other than basic text, no further structur ed data or text format can be provided as there are no tables, graphics, diagrams, or equations in the image to transcribe or describe.\" Reinforce ment Learning from Human Feedback (RLHF). It touches on the importance of feedback from humans in generating improved instructions and points out the challenges and potential issues like the introduction of unwanted biases. The discussion also includes dif ferent methods categorized for detecting misaligned behavior and aligning model behavior effectively.\" Challenges LLM July 19_23_p24.png\"The provided page from the PDF contains text, but there are no diagrams, graphics, equations, or tables present on this page. Since the task involves identifying specific elements other than basic text, no further structur ed data or text format can be provided as there are no tables, graphics, diagrams, or equations in the image to transcribe or describe.\" Challenges LLM July 19_23_p25.png\"The provided image contains a page from a document. There are no diagrams, tables, or equations present on this page. The text mainly discusses topics such as red -teaming, debate training for LLMs (Large Language Models), emergent capabilities of LLMs, deception in AI models, and mechanistic interpretability. Here is a summary of the key points from the text: - Red Teaming LLMs: The process involves using",
    "77": "classifiers to detect undesired outputs and propose a three -stage app roach for evaluating the model's behavior. - Debate Training: Aims to evaluate the model\u2019s behavior where agents take turns making strong statements, and a human judge decides the most accurate and useful information. - Emergent Capabilities: Understanding these capabilities is critical for safe and aligned AI models. Training on large -scale models uncovers emergent abilities not present in smaller models. - Deception in AI: Various studies have shown that deception can appear in AI models, which can be a result of the model\u2019s strategic advantage, and needs to be studied to prevent malicious behavior. - Mechanistic Interpretability: Important for AI alignment by reverse -engineering learned behaviors into interpretable components to understand and locate undesirable behaviors. There are no additional st ructured data, equations, or graphical descriptions to be transcribed based on the content of the image.\" Challenges LLM July 19_23_p26.png\"The provided image contains no diagrams, graphics, or tables. However, I can transcribe and summarize some of the relevant text related to equations, on large -scale models uncovers emergent abilities not present in smaller models. - Deception in AI: Various studies have shown that deception can appear in AI models, which can be a result of the model\u2019s strategic advantage, and needs to be studied to prevent malicious behavior. - Mechanistic Interpretability: Important for AI alignment by reverse -engineering learned behaviors into interpretable components to understand and locate undesirable behaviors. There are no additional st ructured data, equations, or graphical descriptions to be transcribed based on the content of the image.\" Challenges LLM July 19_23_p26.png\"The provided image contains no diagrams, graphics, or tables. However, I can transcribe and summarize some of the relevant text related to equations, methods, or other details as found within the text: 1. **Equation/Method :** - A method to automate the identification of important units in a neural network: - \"Given a model\u2019s computational graph, this algorithm finds subgraphs that explain a particular behavior of the model.\" (Conmy et al. [99]). - A method for making neural networks modular and interpretable: - \"Embedding neurons in a geometric space and augmenting the loss function with a cost proportional to the length of each neuron connection.\" (Liu et al. [339]). 2. **Methods for Understanding LLM Predictions:** - \"Deve lop a method that can decode any hidden state into a distribution over the vocabulary.\" (Belrose et al. [39]). - A method that can recover diverse knowledge represented in LLMs across multiple models and datasets without human supervision or model outputs: - \"This approach produced prompt sensitivity in half and maintained a high accuracy even when the language models are prompted to generate incorrect answers.\" (Burns et al. [62]). 3. **Biases in LLMs:** - \"Recent papers discuss the potential origins of bi ases in LLMs (such as training data or model specification), ethical concerns when deploying biased LLMs in various applications, as\" Challenges LLM July 19_23_p27.png\" - **Graphics/Diagrams:** - There are two warning boxes with icons. 1. First Box: - Title: **\"Isolated Model Updates without Side -Effects \\[205\\]\"** - Content: ``` Updating isolated model behavior or factual knowledge can b e expensive and untargeted, which might cause unintended side -effects. ``` 2. Second Box: - Title: **\"Brittle or model outputs: - \"This approach produced prompt sensitivity in half and maintained a high accuracy even when the language models are prompted to generate incorrect answers.\" (Burns et al. [62]). 3. **Biases in LLMs:** - \"Recent papers discuss the potential origins of bi ases in LLMs (such as training data or model specification), ethical concerns when deploying biased LLMs in various applications, as\" Challenges LLM July 19_23_p27.png\" - **Graphics/Diagrams:** - There are two warning boxes with icons. 1. First Box: - Title: **\"Isolated Model Updates without Side -Effects \\[205\\]\"** - Content: ``` Updating isolated model behavior or factual knowledge can b e expensive and untargeted, which might cause unintended side -effects. ``` 2. Second Box: - Title: **\"Brittle Evaluations\"** - Content: ``` Slight modifications of the benchmark prompt or evaluation protocol can give drastically different results. ``` - **Tables:** None found. - **Equations:** None found. **Summary:** The page discusses several topics related to large language models (LLMs): 1. **Outdated Knowledge:** - Discusses how factual information learned during pre - training can become outdated, and r etraining models with updated data is costly. Current model editing techniques often face limitations in effectively managing these updates. 2. **Modifying Model Parameters:** - Techniques like locate -then-edit and meta -learning methods for changing model behaviors are mentioned. 3. **Preserving Model Parameters:** - Methods that utilize post-edit models or insert new weights to update models without fully retraining them. 4. **Brittle Evaluations:** - Highlights the challenges in evaluating LLMs due to the ir uneven capabilities and emphasizes the need for holistic benchmarking.\" Challenges LLM July 19_23_p28.png\"### Diagrams and Graphics Summary: 1. **Diagram/Flowchart:** - **Title:** Solutions to outdated knowledge - **Description:** The diagram illustrates two approaches to addressing outdated knowledge: - **1) Retrieval Augment ation**: Updating an underlying retrieval index with up -to-date knowledge. - **2) Model Editing**: Applying techniques to adapt the model to current knowledge. - There are arrows indicating a transitional process for each solution. Historical data points ( e.g., year 2019) are updated with current data points (e.g., year 2023). 2. **Warning Graphic:** - **Title:** **Brittle Evaluations:** - Highlights the challenges in evaluating LLMs due to the ir uneven capabilities and emphasizes the need for holistic benchmarking.\" Challenges LLM July 19_23_p28.png\"### Diagrams and Graphics Summary: 1. **Diagram/Flowchart:** - **Title:** Solutions to outdated knowledge - **Description:** The diagram illustrates two approaches to addressing outdated knowledge: - **1) Retrieval Augment ation**: Updating an underlying retrieval index with up -to-date knowledge. - **2) Model Editing**: Applying techniques to adapt the model to current knowledge. - There are arrows indicating a transitional process for each solution. Historical data points ( e.g., year 2019) are updated with current data points (e.g., year 2023). 2. **Warning Graphic:** - **Title:** Reliance on Static, Human -Written Ground Truth - **Text:** ``` Static benchmarks become less useful over time due to changing capabilities while u pdating them often relies on human -written ground truth. ``` ### Table Representation: - **Title:** Problems due to reliance on outdated training data - **Data (Structured Form):** | Year | Issue | | ------ |--------------------------------------- ----------- -| | 2015 | AI system trained, predicts David Cameron as prime minister of the UK. | | 2017 | System trained, predicts Theresa May as prime minister of the UK. | | 2019 | System trained, predicts Theresa May instead of Boris Johnson. | | 2021 | System trai ned, predicts Theresa May instead of Boris Johnson due to outdated data. | | 2023 | Knowledge updated, correct\" Challenges LLM July 19_23_p29.png\"The page contains various elements such as a diagram, figure, or table, as follows: 1. **Diagram:** - **Detecting LLM -generated Text** - This is a highlighted section describing the difficulty in distinguishing whether a t ext is generated by a Language Learning Model (LLM) or written by a human. 2. **Equations:** - No explicit mathematical equations are identified in the text. 3. **Tables:** - There are no tables present in the image. 4.",
    "78": "**Graphics:** - There is a highlight ed text box with a warning icon for \"Detecting LLM -generated Text.\" #### Summary of Graphics and Diagram: - **Detecting LLM -generated Text:** - The graphic element is a highlighted box that signals the difficulty in differentiating whether a text is genera ted by LLMs or humans. It implies that detecting generated text is figure, or table, as follows: 1. **Diagram:** - **Detecting LLM -generated Text** - This is a highlighted section describing the difficulty in distinguishing whether a t ext is generated by a Language Learning Model (LLM) or written by a human. 2. **Equations:** - No explicit mathematical equations are identified in the text. 3. **Tables:** - There are no tables present in the image. 4. **Graphics:** - There is a highlight ed text box with a warning icon for \"Detecting LLM -generated Text.\" #### Summary of Graphics and Diagram: - **Detecting LLM -generated Text:** - The graphic element is a highlighted box that signals the difficulty in differentiating whether a text is genera ted by LLMs or humans. It implies that detecting generated text is significantly important. Summary Data: - **Diagram:** \"Detecting LLM -generated Text\" - **Description**: The difficulty in classifying whether a text is LLM -generated or written by a human. Text Example: ```json { \"diagram\": [ { \"name\": \"Detecting LLM -generated Text\", \"description\": \"The difficulty in classifying whether a text is LLM -generated or written by a human.\" } ], \"equations\": [], \"tables\": [], \"graphics\": [ { \"type\": \"\" Challenges LLM July 19_23_p3.png\"## Diagrams, Graphics, Equations, or Tables in the Image ### Tables: Table 1: Overview of Selected Pre -Training Datasets. **Structured Form of the Data:** | Date | Name | Size | Sources | Public | | ------------ |------------ ----------------------------- |---- ------------- |---------------------------------------------- |----------- | | 2014 | BookCorpus [684, 36] | 5 GB 11 B tokens| Webpages | No | | 2019 | OSCAR [399] | 6.3 T ? | Webpages in 166 languages | Yes | | 2019 | WebTex t [440] | 40 GB ? | Webpages | No | | 12.2020 | CC -100 [100] | 2.5 TB 292 B | Webpages in 100 languages | Yes | | 12.2020 | The Pile [165, 41] | 825 GB 300 B | Science, Webpages, GitHub Code, Law, etc. | Yes | | 2020 | C4 [443] | 745 GB 156 B | Webpages | Yes | | 10.2020 | mC4 [631] | ? 6.3 T | Webpages in 101 languages | Yes\" Challenges LLM July 19_23_p30.png\"Here is a structured representation of the non -text items found in the image: ### Diagrams and Graphics - **Paraphrasing Attacks Warning Icon Graphic**: Indicates that paraphrasing attacks involve another LLM rewriting text to preserve approximately the same meaning but changing the words or sentence structure. - **Tasks Not Yes | | 2019 | WebTex t [440] | 40 GB ? | Webpages | No | | 12.2020 | CC -100 [100] | 2.5 TB 292 B | Webpages in 100 languages | Yes | | 12.2020 | The Pile [165, 41] | 825 GB 300 B | Science, Webpages, GitHub Code, Law, etc. | Yes | | 2020 | C4 [443] | 745 GB 156 B | Webpages | Yes | | 10.2020 | mC4 [631] | ? 6.3 T | Webpages in 101 languages | Yes\" Challenges LLM July 19_23_p30.png\"Here is a structured representation of the non -text items found in the image: ### Diagrams and Graphics - **Paraphrasing Attacks Warning Icon Graphic**: Indicates that paraphrasing attacks involve another LLM rewriting text to preserve approximately the same meaning but changing the words or sentence structure. - **Tasks Not Solvable by Scale Warning Icon Graphic**: Indicates tasks that seem not to be solvable by further data/model scaling. ### Equations - There are no equations presented in the image. ### Tables - There are no tables presented in the image. ### Summary of Key Points in Graphic Diagrams - **Paraphrasing Attacks Section:** - Parap hrasing attacks involve another LLM rewriting text to retain the same meaning while altering words or sentence structure. - Detectors can be evaded by training paraphrase generation models. - Suggestion to store model generations in a database for retrievi ng semantically similar texts. - Retrieval approach is shown to be robust to paraphrasing attacks. - Watermarking generated text is claimed to be impractical in detecting generated content. - **Tasks Not Solvable By Scale Section (Inverse Scaling):** - Inverse Scaling (IS): Task performance worsens as model size and training loss performance improve. - Common objectives may induce false answers. - Identified potential causes: memorizing data, undesirable training patterns, performing distractor tasks, spuri ous correlations. - U\" Challenges LLM July 19_23_p31.png\"The image contains several important elements including a diagram and a table. Here are the details: ### Diagram There is a red highlighted box with the following text: ``` Uncontrolled Experiments Papers presenting novel LLMs often lack controlled experiments, likely due to the prohibitive costs of training enough models. ``` ### Table - Title: \"Table 2 shows a (non -exhaustive) overview of selected LLMs within the scope of this review, described in academic papers.\" The table data is not directly visible but there is a Common objectives may induce false answers. - Identified potential causes: memorizing data, undesirable training patterns, performing distractor tasks, spuri ous correlations. - U\" Challenges LLM July 19_23_p31.png\"The image contains several important elements including a diagram and a table. Here are the details: ### Diagram There is a red highlighted box with the following text: ``` Uncontrolled Experiments Papers presenting novel LLMs often lack controlled experiments, likely due to the prohibitive costs of training enough models. ``` ### Table - Title: \"Table 2 shows a (non -exhaustive) overview of selected LLMs within the scope of this review, described in academic papers.\" The table data is not directly visible but there is a reference to the table and it likely contains an overview of selected LLMs with reference to the papers that describe them. ### Equations There are no equatio ns present in the visible portion of the image. ### Summary of Graphics/Diagrams/Text 1. **Lacking Experimental Designs:** - Discusses that many papers do not include controlled ablations, which is problematic for large design spaces. This hinders scientif ic comprehension and advancement. 2. **Lack of Controlled Ablations:** - Many papers vary one factor at a time due to computational costs, example cited as Chowdhery et al. where PaLM might outperform GPT -3. - Many adopt hyper -parameters from previous work s without further tuning after changes. - Important implementation details are sometimes not mentioned. 3. **Uncontrolled Experiments:** - The red highlighted box emphasizes that papers often lack controlled experiments, likely\" Challenges LLM July 19_23_p32.png\"Here is the structured data extracted from the table in the image: ``` [ { \"Date\": \"2018.11\", \"Name\": \"GPT2\", \"Organization\": \"OpenAI\", \"Language\": \"Eng\", \"Parameters\": \"1.5B\", \"FLOPs\": \"300B\", \"Architecture\": \"Dec. -Only\", \"Train. Obj.\": \"NTP\", \"Tokenizer\": \"BPE\", \"Pre. -Trained\": \"Learned\", \"RF/Interpolation\": \" \u2714\", \"GFLOPs\": \"N/A\", \"Open -Source\": \" \u2714\", \"Checkpoints\": \" \u2714\", \"Bias/Fairness\": \" \u2714\", \"Layperson Summary\": \"\u2714\" }, { \"Date\": \"2020.05\", \"Name\": \"GPT -3\", \"Organization\": \"OpenAI\", \"Language\": \"Eng\", \"Parameters\": \"175B\", \"FLOPs\": \"300B\", \"Architecture\": \"Dec. -Only\", \"Train. Obj.\": \"NTP\", \"Tokenizer\": \"BPE\", \"Pre. -Trained\": \"Learned\", \"RF/Interpolation\": \" \u2714\", \"GFLOPs\": \"N/A\", \"Open -Source\": \"x\", \"Checkpoints\": \"x\", \"Bias/F\" Challenges LLM July 19_23_p32.png\"Here is the structured data extracted from the table in the image: ``` [ { \"Date\": \"2018.11\", \"Name\": \"GPT2\", \"Organization\": \"OpenAI\", \"Language\": \"Eng\", \"Parameters\": \"1.5B\", \"FLOPs\": \"300B\", \"Architecture\": \"Dec. -Only\", \"Train. Obj.\": \"NTP\", \"Tokenizer\": \"BPE\", \"Pre. -Trained\": \"Learned\", \"RF/Interpolation\": \" \u2714\", \"GFLOPs\": \"N/A\", \"Open -Source\": \" \u2714\", \"Checkpoints\": \"",
    "79": "\u2714\", \"Bias/Fairness\": \" \u2714\", \"Layperson Summary\": \"\u2714\" }, { \"Date\": \"2020.05\", \"Name\": \"GPT -3\", \"Organization\": \"OpenAI\", \"Language\": \"Eng\", \"Parameters\": \"175B\", \"FLOPs\": \"300B\", \"Architecture\": \"Dec. -Only\", \"Train. Obj.\": \"NTP\", \"Tokenizer\": \"BPE\", \"Pre. -Trained\": \"Learned\", \"RF/Interpolation\": \" \u2714\", \"GFLOPs\": \"N/A\", \"Open -Source\": \"x\", \"Checkpoints\": \"x\", \"Bias/F\" Challenges LLM July 19_23_p33.png\"**Tables:** 1. **Table 2**: | Characteristic | Models Differentiating Attribute | | ----------------------------------------------------- |------------------------------- ------------- | | Training Datasets or Fine -Grained Architectural | Multi -head [563] or multi -query attention [494] | **Diagrams or Graphics:** 1. **Warning Graphic (Red alert icon with white exclamation mark)**: - **Label**: Curse of (Design) Dimensionalit y - **Description**: Common design spaces of LLM experiments are high -dimensional. 2. **Warning Graphic (Red alert icon with white exclamation mark)**: - **Label**: Irrepeatable Training Runs - **Description**: Parallelism strategies designed to distribute the training process across many accelerators are typically non -deterministic, rendering LLM training irreproducible. **Equations:** None present in the provided image. **Summary of Graphics and Diagrams:** 1. The first warning highlights the issue of hig h-dimensional design spaces in LLM experiments, making it a challenge to cover their vast design space. 2. The second warning indicates that the use of parallelism strategies in distributing the training process across various accelerators leads to non -deterministic outcomes, making it difficult to reproduce LLM training results.\" Challenges LLM July 19_23_p34.png\"The image contains three boxed sections, each with specific information. 1. **Warning Box: Irreproducible API Inference** - **Title:** Irreproducible API Inference - **Content:** API -served models are often irreproducible. 2. **Warning Box: Maintaining Coherence** - **Title:** Maintaining Coherence - **Content:** 1. The first warning highlights the issue of hig h-dimensional design spaces in LLM experiments, making it a challenge to cover their vast design space. 2. The second warning indicates that the use of parallelism strategies in distributing the training process across various accelerators leads to non -deterministic outcomes, making it difficult to reproduce LLM training results.\" Challenges LLM July 19_23_p34.png\"The image contains three boxed sections, each with specific information. 1. **Warning Box: Irreproducible API Inference** - **Title:** Irreproducible API Inference - **Content:** API -served models are often irreproducible. 2. **Warning Box: Maintaining Coherence** - **Title:** Maintaining Coherence - **Content:** Multi -turn interactions make Chatbots easily \u201cforget\u201d earlier parts of the conversation or repeat themselves [53, 451]. 3. **Constraint Box** - **Title:** Constrai nt - **Content:** This box highlights a constraint. There are no tables, diagrams, or equations in the provided image. The data can be summarized and represented in the following structured format suitable for database entry: ```plaintext Warnings: - Title: Irreproducible API Inference Content: API -served models are often irreproducible. - Title: Maintaining Coherence Content: Multi -turn interactions make Chatbots easily \u201cforget\u201d earlier parts of the conversation or repeat themselves [53, 451]. Constraints: - Title: Constraint Content: This box highlights a constraint. ```\" Challenges LLM July 19_23_p35.png\"The image contains a structured diagram titled \"Figure 12: Overview of LLM Applications. Color = Level of Model Adaption (Pre -Trained, Fine -Tuned, Prompting Strategy, Evaluation)\". This diagram categorizes various applicat ions of LLMs (Large Language Models) under different sections: ### Chatbots 3.1 - BlenderBot3 (OPT -175B) [508], Bard (LaMDA), PaLM2) [551], Sparrow (Chinchilla) [170], ChatGPT (GPT -3.5, GPT -4) [596], OpenAssistant (LLaMA) [74] - GPT-4 Technical Report [597 ], Sparks of AGI (GPT -4) [61], Capabilities of ChatGPT [27] ### Computational Biology 3.2 **Proteins** - ESM -2 [326], ProtT5 [319], ProtGPT [627], Galactica [402], ProGen [352], IgLM [505], xTrimoGPT [73] **Genomics** - GenSLMs [358], Nucleotide Transforme rs [106] ### Computer Programming 3.3 - InCoder [154], CodeGen [386], AlphaCode [313], SantaCoder [17], PolyCoder [626], phi -1 [182] - Codex (GPT -3) [171] - Self-Debugging (Codex) [81], ViperGPT (Codex) [\" Language Models) under different sections: ### Chatbots 3.1 - BlenderBot3 (OPT -175B) [508], Bard (LaMDA), PaLM2) [551], Sparrow (Chinchilla) [170], ChatGPT (GPT -3.5, GPT -4) [596], OpenAssistant (LLaMA) [74] - GPT-4 Technical Report [597 ], Sparks of AGI (GPT -4) [61], Capabilities of ChatGPT [27] ### Computational Biology 3.2 **Proteins** - ESM -2 [326], ProtT5 [319], ProtGPT [627], Galactica [402], ProGen [352], IgLM [505], xTrimoGPT [73] **Genomics** - GenSLMs [358], Nucleotide Transforme rs [106] ### Computer Programming 3.3 - InCoder [154], CodeGen [386], AlphaCode [313], SantaCoder [17], PolyCoder [626], phi -1 [182] - Codex (GPT -3) [171] - Self-Debugging (Codex) [81], ViperGPT (Codex) [\" Challenges LLM July 19_23_p36.png\"The page contains the following elements: ### Sections - Transfer to Downstream Applications (Sidebar) - High Inference Latency (Sidebar) - Computational Biology - Protein Embeddings (Subsection) ### Sidebars **Transfer to Downstream Applications:** - Description: - The ultimate objective of protein language models is to deploy them in real -world projects such as drug design. Evaluations often target smaller and/or specialized datasets, not considering how the models could contribute to protein design in vitro or in vivo. **High Inference Latency:** - Description: - High inference latency hinders the user experience, especially in multi -turn interaction with chatbots. ### Structured Data (References): There are several references cited throughout the text. ### Equations There are no visible equations on this page. ### Summary of Graphics or Diagrams There are no graphics or diagrams on this page. ### Data to be stored ```json { \"sections\": [ { \"title\": \"Transfer to Downstream Applications\", \"description\": \" The ultimate objective of protein language models is to deploy them in real -world projects such as drug design. Evaluations often target smaller and/or specialized datasets, not considering how the models could contribute to protein design in vitro or in v ivo.\" }, { \"title\": \"High Inference Latency\", \"description\": \"High inference latency hinders the user experience, especially in multi -turn interaction with chatbots.\" \" Challenges LLM July 19_23_p37.png\"The page contains one highlighted text box and several references to diagrams and models, which I will summarize below based on the context provided. ### Highlighted Text Box **Title: \"Limited Context Window\"** - \"The larg est Downstream Applications\", \"description\": \" The ultimate objective of protein language models is to deploy them in real -world projects such as drug design. Evaluations often target smaller and/or specialized datasets, not considering how the models could contribute to protein design in vitro or in v ivo.\" }, { \"title\": \"High Inference Latency\", \"description\": \"High inference latency hinders the user experience, especially in multi -turn interaction with chatbots.\" \" Challenges LLM July 19_23_p37.png\"The page contains one highlighted text box and several references to diagrams and models, which I will summarize below based on the context provided. ### Highlighted Text Box **Title: \"Limited Context Window\"** - \"The larg est genomes have vastly longer DNA sequences than existing genomic LLM's context windows can handle, constraining the types of genomes that can be successfully modeled using these approaches.\" ### Models and Studies Described 1. **xTrimoPGLM -100B** - *Desc ription*: Trained simultaneously for protein embedding",
    "80": "and generation tasks. - *Performance*: Outperforms existing approaches on 13 out of 15 evaluated tasks. 2. **CaLM (Codon adaptation Language Model)** - *Description*: Utilizes codons instead of amino a cids, training on 86 million parameter protein LLM. 3. **ProGen** - *Description*: 1.2B parameter protein embedding model trained on 280 million protein amino acid sequences. - *Key Features*: Additional control tags specifying protein properties. 4. **Pro tST** - *Description*: Protein language model focusing on protein sequences and text descriptions for classification and retrieval tasks. 5. **Immunoglobulin Language Model (IgLM)** - *Description*: 13 million parameters, aims for the generation of immunog lobulin sequences using a masked language modeling approach. 6. **GenSLM (Genome -scale Language Models)** - *Parameters*: Up\" Challenges LLM July 19_23_p38.png\"### Tables There is one table in the image: #### Table 1: Long -range Dependencies - **Row 1**: Long -range dependencies across a code repository usually cannot be regarded because of limited context lengths (Sec. 2.6). ### Diagrams and Graphics 1. **Graphic:** There is an icon indicating a warning about long -range dependencies in the middle of the page. It shows an exclamation mark inside a triangle. ### Equations There are no equations present in the image. ### Summary The page discusses various studies related parameters, aims for the generation of immunog lobulin sequences using a masked language modeling approach. 6. **GenSLM (Genome -scale Language Models)** - *Parameters*: Up\" Challenges LLM July 19_23_p38.png\"### Tables There is one table in the image: #### Table 1: Long -range Dependencies - **Row 1**: Long -range dependencies across a code repository usually cannot be regarded because of limited context lengths (Sec. 2.6). ### Diagrams and Graphics 1. **Graphic:** There is an icon indicating a warning about long -range dependencies in the middle of the page. It shows an exclamation mark inside a triangle. ### Equations There are no equations present in the image. ### Summary The page discusses various studies related to the development and improvement of large language models (LLMs) specialized in code generation. It includes: 1. Description of Codex, a fine -tuned GPT -3 LLM used for generating standalone Python functions. 2. Self -debugging prompting approaches improving the performance of Codex. 3. Training methods and results for models like PolyCoder and AlphaCode. 4. Challenges such as handling long -range dependencies in code repositories due to limited context lengths. 5. Intro duction of frameworks like RepoCoder for better code completion considering broader context of the repository. ### Final Data Representation for Database ```json { \"tables\": [ { \"title\": \"Long -range Dependencies\", \"content\": [ [\"Warning\", \"Long -range depen dencies across a code repository usually cannot be regarded because of limited context lengths (Sec. 2.6).\"] ] } ], \"diagrams\": [ { \" Challenges LLM July 19_23_p39.png\"The image contains a diagram and some structured code snippets outlined below. There is no table in the displayed content. ### Diagram: #### Title: API Definition Framework #### Description: The diagram illustrates the pro cess of defining an API using Language Learning Models (LLMs) and highlights the following components: 1. **Prompt:** Instruction to generate a program using the provided API functions. 2. **API Definition:** Snippet of code for an API definition, for exam ple: ```python def move_to_location(x, y, z) def move_in_circle(radius, x, y, z) def jump_in_place ( w, x, y) def dig(location ) ``` 3. **LLM:** The LLM processes prompts and definitions. 4. **Function Implementation:** This part interprets current actions to use the functions provided. 5. **Self - snippets outlined below. There is no table in the displayed content. ### Diagram: #### Title: API Definition Framework #### Description: The diagram illustrates the pro cess of defining an API using Language Learning Models (LLMs) and highlights the following components: 1. **Prompt:** Instruction to generate a program using the provided API functions. 2. **API Definition:** Snippet of code for an API definition, for exam ple: ```python def move_to_location(x, y, z) def move_in_circle(radius, x, y, z) def jump_in_place ( w, x, y) def dig(location ) ``` 3. **LLM:** The LLM processes prompts and definitions. 4. **Function Implementation:** This part interprets current actions to use the functions provided. 5. **Self - debugging:** LLM\u2019s function in debugging APIs that do not execute properly. #### Process Flow: 1. **Prompt:** Generates a query for writing a program using API functions. 2. **API Definition:** Specifies the functi on structure needed. 3. **Function Implementation:** Handles execution based on the defined API. 4. **Self -debugging:** LLM self -checks the API execution to ensure correctness. ### Code snippets within the Diagram: #### Function Definitions: ```python def move_to_location(x, y, z) def move_in_circle(radius, x, y, z) def jump_in_place(w, x\" Challenges LLM July 19_23_p4.png\"The image contains the following: ### Graphics/Diagrams: 1. **Warning Box Graphic**: - **Title**: Tokenizer -Reliance - **Content**: Tokenizers introduce several challenges, e.g., computational overhead, language dependence, handling of novel words, fixed vocabulary size, information loss, and low human interpretability. ### Equations: There are no explicit equations present in the image. ### Tables: There are no tables present in the image. ### Summary: This page discusses v arious aspects of tokenization and its challenges in language models. The key points include: - **Negative Transfer in MTLMs**: Multitask learning models (MTLMs) may perform worse than models focused on single tasks due to issues like negative task transfe r and catastrophic forgetting. - **Experimentation on Task Sets and Prompts**: Studies show varying approaches in fine -tuning to balance task sets and the importance of prompt templates. - **Closed -Source Model Imitation**: It appears models like ChatGPT b y OpenAI are emulated but show substantial capability gaps when using fine -tuned Equations: There are no explicit equations present in the image. ### Tables: There are no tables present in the image. ### Summary: This page discusses v arious aspects of tokenization and its challenges in language models. The key points include: - **Negative Transfer in MTLMs**: Multitask learning models (MTLMs) may perform worse than models focused on single tasks due to issues like negative task transfe r and catastrophic forgetting. - **Experimentation on Task Sets and Prompts**: Studies show varying approaches in fine -tuning to balance task sets and the importance of prompt templates. - **Closed -Source Model Imitation**: It appears models like ChatGPT b y OpenAI are emulated but show substantial capability gaps when using fine -tuned open -sourced models. - **Tokenizer -Relevance**: Tokenization splits words or characters into smaller units (subwords or WordPieces). This approach helps handle rare and out -of-vocabulary words efficiently but introduces challenges such as computational overhead and language dependence. The key graphic (warning box) highlights the overarching issues with tokenization in models, alerting to complexities like handling\" Challenges LLM July 19_23_p40.png\"The image contains a highlighted section denoted by a box, identified by an exclamation mark icon, which is likely a warning or important information. There are no tables, equations, or diagrams present on this page. **Hig hlighted Section:** - **Title:** Limited Context Window - **References:** [368, 637] - **Text Content:** The inability of current LLMs to keep the entire generated work within the context window currently constrains their long -form applications and generat es the need for modular prompting (14). There are various textual references which could be linked to",
    "81": "citations within the document. Here is how they are referenced: 1. **Yang et al. [637]** 2. **J. et al. [303]** 3. **Wang et al. [584]** 4. **Ippolito et al. [232]** 5. **Calderwood et al. [63]** 6. **Haase and Hanel [187]** 7. **Feng et al. [148]** 8. **Lian et al. [315]** 9. **Eloundou et al. [140]** 10. **Bommarito et al. [49]** 11. **Eloundou et al. [140]** (Repeat Reference) 12. **Chakrabarty et al. [69]** 13. **Razumovskaia et al. [452]** There are\" Challenges LLM July 19_23_p41.png\"**Diagrams/Graphics:** 1. **Figure 14: Modular Prompting** - Description: Diagram showing a process using separate modules to enable an constrains their long -form applications and generat es the need for modular prompting (14). There are various textual references which could be linked to citations within the document. Here is how they are referenced: 1. **Yang et al. [637]** 2. **J. et al. [303]** 3. **Wang et al. [584]** 4. **Ippolito et al. [232]** 5. **Calderwood et al. [63]** 6. **Haase and Hanel [187]** 7. **Feng et al. [148]** 8. **Lian et al. [315]** 9. **Eloundou et al. [140]** 10. **Bommarito et al. [49]** 11. **Eloundou et al. [140]** (Repeat Reference) 12. **Chakrabarty et al. [69]** 13. **Razumovskaia et al. [452]** There are\" Challenges LLM July 19_23_p41.png\"**Diagrams/Graphics:** 1. **Figure 14: Modular Prompting** - Description: Diagram showing a process using separate modules to enable an LLM to perform tasks that are complex or cannot be easily specified in a single prompt . - **Module 1:** - General Prompt - LLM - Output (e.g., Generate a plot outline for a new novel as paragraph headings) - **Module 2:** - Pre-processing - General Prompt - LLM Residual - Output (e.g., Using the outline, generate a draft for the nth paragra ph heading) - **Module 3:** - Pre-processing - General Prompt - LLM - Output (e.g., Check the spelling and consistency of this paragraph given the outline and summary) - Additional Components: - User Prompt - Re-run - Iterate **Equations:** - No equations are visible in the provided image. **Tables:** - No tables are visible in the provided image. Here's a simplified representation: **Graphic: \"Figure 14: Modular Prompting\"** - **Module 1:** - General Prompt - LLM - Output: \"e.g., Generate a plot outline fo r a new novel as paragraph headings\" - **Module 2:** - Pre-processing - General Prompt - L\" Challenges LLM July 19_23_p42.png\"**Diagrams, Graphics, and Tables:** 1. There is a highlighted notice in the document: - **Warning Box Title:** \" Out of Date Information\" - **Warning Box Content:** - \"Due to regularly updated laws and new precedents, the training/retrieval data be - come outdated frequently [195].\" Description/Summary: - The highlighted notice informs the readers that due to the reg ularly updated nature of laws and legal precedents, the training and retrieval data may frequently become outdated. **Equations:** - No equations are present on the provided page. **Structured Data from the Table:** - There appears to be no traditional tab ular data in the provided document. **Text Summary of Graphics and Prompt - L\" Challenges LLM July 19_23_p42.png\"**Diagrams, Graphics, and Tables:** 1. There is a highlighted notice in the document: - **Warning Box Title:** \" Out of Date Information\" - **Warning Box Content:** - \"Due to regularly updated laws and new precedents, the training/retrieval data be - come outdated frequently [195].\" Description/Summary: - The highlighted notice informs the readers that due to the reg ularly updated nature of laws and legal precedents, the training and retrieval data may frequently become outdated. **Equations:** - No equations are present on the provided page. **Structured Data from the Table:** - There appears to be no traditional tab ular data in the provided document. **Text Summary of Graphics and Diagrams:** - The warning box, which is the only graphic element, stresses the importance of ensuring that legal information remains current. The inclusion of a citation ([195]) provides a reference that could be researched further if required. If you need any further details or have another page to share, please let me know! \" Challenges LLM July 19_23_p43.png\"The provided image contains one highlighted box and references in the text to multiple citations but no diagrams, graphics, or equations are present. Here is the data from the highlighted box in a structured format: **High lighted Box:** - **Title:** Hallucination and Bias [538, 388, 511] - **Content:** - \"The safety -critical nature of the medical domain means the possibility of hallucinations significantly limits the current use cases. Further work is also needed to reduce the risk of LLMs perpetuating existing bias in clinical datasets.\" No tables or non -textual data are identified in the image. This summary should provide you with a clear structured representation of the highlighted information without including basic text content from the paper.\" Challenges LLM July 19_23_p44.png\"The provided page contains the following notable content: 1. **Table** - **Description**: Comparison of LLMs on mathematical reasoning tasks. - **Content**: - **Tasks**: Word -based math problems in GSM8K - **Fine -Tuning St eps**: 1. Supervised and RLHF prompts 2. Zero -shot and few -shot 3. Majority voting and reward model 4. Process -based vs. Outcome -based on final answer correctness. 2. **Diagrams/Graphics/Equations/Tables Summary** - **No diagrams or graphical illustrations** are visible in the provided page excerpt. - **No specific equations** are provide you with a clear structured representation of the highlighted information without including basic text content from the paper.\" Challenges LLM July 19_23_p44.png\"The provided page contains the following notable content: 1. **Table** - **Description**: Comparison of LLMs on mathematical reasoning tasks. - **Content**: - **Tasks**: Word -based math problems in GSM8K - **Fine -Tuning St eps**: 1. Supervised and RLHF prompts 2. Zero -shot and few -shot 3. Majority voting and reward model 4. Process -based vs. Outcome -based on final answer correctness. 2. **Diagrams/Graphics/Equations/Tables Summary** - **No diagrams or graphical illustrations** are visible in the provided page excerpt. - **No specific equations** are mentioned in the provid ed page excerpt. 3. **Structured Data Representation of the Table**: ```plaintext { \"Table\": { \"Comparison of LLMs on Mathematical Reasoning Tasks\": [ { \"Tasks\": \"Word -based math problems in GSM8K\", \"Fine -Tuning Steps\": [ \"Supervised and RLHF prompts\", \"Ze ro-shot and few -shot\", \"Majority voting and reward model\", \"Process -based vs. Outcome -based on final answer correctness\" ] } ] } } ``` 4. **Graphics/Diagrams** - There are no diagrams or graphics to describe from the image provided. Note: The description i s based on the provided text regarding different fine -tuning and\" Challenges LLM July 19_23_p45.png\"### Summary of Non -Text Elements #### Diagram - **Sub -Human -Performance Warning**: - **Label**: \"Sub -Human -Performance [562, 607]\" - **Description**: Indicates that existing LLMs struggle to match human performance on reasoning benchmarks. - **Visual Element**: Contains a warning triangle, signaling caution. #### References - Citation markers indicate sources of various claims and points discussed in the text: - Webb et al. [595] - Yu et al. [654] - Ruis et al. [464] - Valmeekam et al. [562] - Kiciman et al. [425, 253] - Ahn et al. [14] - Driess et al. [129] - Vemprala et al. [564] - Gao et al. [164] - Srivastava et al. [519] - Jin et al. [244] - Lampinen et al. [288] - Willi g et al. [607] #### Equations - None present in the provided image. #### Tables - No structured tables are present. ### Structured Data Representation ```json { \"diagrams\": [ { \"label\": \"Sub -Human -Performance [562, 607]\", \"description\": \"Existing LLMs struggle to m atch human\" Challenges LLM July 19_23_p46.png\"**Data in the table:** ``` LLMs in the Social Sciences & ####",
    "82": "References - Citation markers indicate sources of various claims and points discussed in the text: - Webb et al. [595] - Yu et al. [654] - Ruis et al. [464] - Valmeekam et al. [562] - Kiciman et al. [425, 253] - Ahn et al. [14] - Driess et al. [129] - Vemprala et al. [564] - Gao et al. [164] - Srivastava et al. [519] - Jin et al. [244] - Lampinen et al. [288] - Willi g et al. [607] #### Equations - None present in the provided image. #### Tables - No structured tables are present. ### Structured Data Representation ```json { \"diagrams\": [ { \"label\": \"Sub -Human -Performance [562, 607]\", \"description\": \"Existing LLMs struggle to m atch human\" Challenges LLM July 19_23_p46.png\"**Data in the table:** ``` LLMs in the Social Sciences & Psychology --------------------------------------------------- | Using LLMs to model | Analyzing behavioral | Simulating social | | human behavior | characteristics of LLMs | relationships with LLMs | --------------------------------------------------- | Milgram Shock Experiment | Big Five personality traits | Interacting artificial agents | | Illusory Truth Effect | Gulliver's Alternative Use | LLMs to simulate socie ties | ``` **Descriptions and summaries of graphics and diagrams:** **Graph/Diagram Description:** Title: \"LLMs in the Social Sciences & Psychology\" Use Cases and Examples: This diagram suggests three main use cases for Large Language Models (LLMs) in soci al sciences and psychology, presented in a table format. Each category includes examples: 1. Using LLMs to model human behavior (Examples: Milgram Shock Experiment, Illusory Truth Effect). 2. Analyzing behavioral characteristics of LLMs (Examples: Big Five personality traits, Gulliver's Alternative Use). 3. Simulating social relationships with LLMs (Examples: Interacting artificial agents, LLMs to simulate societies). **Equations:** There are no equations present in the image. **General Summary:** The image from the PDF page contains a section on the application of Large Language Models (LLMs) in social sciences and psychology. It includes a\" Challenges LLM July 19_23_p47.png\"**Table:** - **Title:** Social Biases [12, 367] - **Content:** - Unbalanced views and opinions in the training data skew the LLMs towards biased human behaviors. **Summary of Graphics and Diagrams:** - There are no visible graphics or diagrams on this page. **Equations:** - There are no equations present in the Gulliver's Alternative Use). 3. Simulating social relationships with LLMs (Examples: Interacting artificial agents, LLMs to simulate societies). **Equations:** There are no equations present in the image. **General Summary:** The image from the PDF page contains a section on the application of Large Language Models (LLMs) in social sciences and psychology. It includes a\" Challenges LLM July 19_23_p47.png\"**Table:** - **Title:** Social Biases [12, 367] - **Content:** - Unbalanced views and opinions in the training data skew the LLMs towards biased human behaviors. **Summary of Graphics and Diagrams:** - There are no visible graphics or diagrams on this page. **Equations:** - There are no equations present in the provided page. **Summarized Content:** This page of the PDF discusses various experiments and studies analyzing the behavior of large language models (LLMs) in replicating human psychological traits and biases. The content focuses on several key findings: 1. **Modeling Psychological Change:** Studies on LLM responses to behavioral tests suggest that LLMs can mirror human judgments in certain scenarios, such as political view changes and behavioral effects. 2. **The Turing Experiment (TE):** A framework to assess LLMs' behavior across different demographics using test items derived from social ps ychology and behavioral economics. The findings show alignment with human behavior, but larger models yield results more akin to human responses. 3. **Behavioral Experiments Reproduction:** Studies replicating known psychological tests demonstrated that GP T-3 replicates human participants' behaviors to some extent but misses the nuances of human cognitive systems. 4. **Personality Traits Examination:** Research on LLMs' personality traits using the Machine Personality Inventory (MPI) dataset and other scale s found that GPT -3 and models like it exhibit traits similar to human personalities when analyzed\" Challenges LLM July 19_23_p48.png\"**Diagrams/Graphics** 1. **Figure 16: Modality Conversion.** - Description: The diagram illustrates the use of models with different input modalities in pre - or post -processing steps in an LLM pipeline. The figure is divid ed into two sections: Pre -processing and Post -processing. **Pre -processing:** - Three types of inputs are shown: speech input (microphone symbol), image input (image icon), text input. - These inputs behaviors to some extent but misses the nuances of human cognitive systems. 4. **Personality Traits Examination:** Research on LLMs' personality traits using the Machine Personality Inventory (MPI) dataset and other scale s found that GPT -3 and models like it exhibit traits similar to human personalities when analyzed\" Challenges LLM July 19_23_p48.png\"**Diagrams/Graphics** 1. **Figure 16: Modality Conversion.** - Description: The diagram illustrates the use of models with different input modalities in pre - or post -processing steps in an LLM pipeline. The figure is divid ed into two sections: Pre -processing and Post -processing. **Pre -processing:** - Three types of inputs are shown: speech input (microphone symbol), image input (image icon), text input. - These inputs go through a \"Modality -to-Text\" conversion. - The output text goes into a \"Prompt\" box. - The prompt is then fed into the \"LLM.\" - The LLM produces an \"Output.\" **Post -processing:** - A \"Prompt\" is fed into the \"LLM.\" - The LLM output is then processed through \"Code X -> Modality (speech, image, text)\". - Diffe rent outputs like speech, image, and text are shown. **Equations** - There are no visible equations on this page. **Tables** - There are no visible tables on this page. Based on the image, no tables or structured data extraction is necessary. Summary of th e diagram: The figure demonstrates the implementation of using models with different input modalities, either in the initial stage (Pre -processing) or in the final stage (Post - processing) of an LLM pipeline. Pre -processing involves converting various input types (speech, image, text\" Challenges LLM July 19_23_p49.png\"Here is a description and transcription of the non -basic - text elements within the provided image of the PDF page: ### Graphics and Diagrams 1. **Warning Box:** - Title: Hallucinated Distributions [506] - Content: \"Using LL Ms for fully synthetic data generation is currently constrained by our inability to verify whether the synthetic data generated is representative of the true distribution in the corresponding real -world data.\" ### Tables There are no tables present in the image. ### Equations No equations are visible in the image. This summary captures the structured form of the graphics and diagrams and ensures the essential information is ready to be passed back to a database.\" Challenges LLM July 19_23_p5.png\"### Diagrams, Graphics, Equations, and Tables text elements within the provided image of the PDF page: ### Graphics and Diagrams 1. **Warning Box:** - Title: Hallucinated Distributions [506] - Content: \"Using LL Ms for fully synthetic data generation is currently constrained by our inability to verify whether the synthetic data generated is representative of the true distribution in the corresponding real -world data.\" ### Tables There are no tables present in the image. ### Equations No equations are visible in the image. This summary captures the structured form of the graphics and diagrams and ensures the essential information is ready to be passed back to a database.\" Challenges LLM July 19_23_p5.png\"### Diagrams, Graphics, Equations, and Tables Description",
    "83": "#### Diagrams and Graphics **Figure 2: Exemplary Drawbacks of relying on Tokenization** 1. **Tokenizer Training Costs** - **Training Sequences**: - English: Includes example sequences like \"if instance\", \"for look\", \"level array\", etc. - Chinese: Displays examples in Chinese characters. - Python: Shows Python code snippets like `for`, `False`, `def`, `array`, etc. - **V ocabulary**: - Includes example tokens like \"where\", \"do\", \"it\", \"look\", \"chinese\", \"example\", special tokens, Chinese characters, and Python keywords. 2. **Architecture Dependency on V ocabulary** - **Structure**: - Embedding Matrix `E` of dimens ion \\(E \\in \\mathbb{R}^{ \\lvert V \\rvert \\times D} \\). - Transformer Blocks (with placeholder symbol `...` indicating multiple transformer blocks). - Softmax over V ocabulary Matrix `W` of dimension \\(W \\in \\mathbb{R}^{D_{model} \\times \\lvert V \\rvert} \\). ### # Equations 1. \\(E \\in \\mathbb{R}^{ \\lvert V \\rvert \\times D} \\) 2. \\(W \\in \\mathbb{R}^{D_{model} \\times \\lvert V \\rvert} \\) ###\" Challenges LLM July 19_23_p50.png\"The given image is a page from a PDF document. It consists mainly of text without any visible diagrams, graphics, equations, or tables. Below is a transcription of some relevant textual references provided in a structured form suitable for a database: ```plaintext References: 1. A blog post detailed a Sam Altman freakout about a huge chips shortage threatening OpenAI. Then it was taken down. 2. Open LLM Leaderboard - a Hugging Face Space by HuggingFace. 3. Reproducibility \u2014 PyTorch 2.0 documentation. 4. 2023. Negative prompts for text generation. Section: Prompting. 5. 2023. Reproducibility. Page Version ID: 1163331755. 6. A. Abbas, K. Tirumala, D. Simig, S. Ganguli and A. S. Morcos. \\rvert} \\) ###\" Challenges LLM July 19_23_p50.png\"The given image is a page from a PDF document. It consists mainly of text without any visible diagrams, graphics, equations, or tables. Below is a transcription of some relevant textual references provided in a structured form suitable for a database: ```plaintext References: 1. A blog post detailed a Sam Altman freakout about a huge chips shortage threatening OpenAI. Then it was taken down. 2. Open LLM Leaderboard - a Hugging Face Space by HuggingFace. 3. Reproducibility \u2014 PyTorch 2.0 documentation. 4. 2023. Negative prompts for text generation. Section: Prompting. 5. 2023. Reproducibility. Page Version ID: 1163331755. 6. A. Abbas, K. Tirumala, D. Simig, S. Ganguli and A. S. Morcos. 2023. Semdedup: Data -efficient learning a t web -scale through semantic deduplication. arXiv preprint arXiv:2303.09540. 7. J. D. Abernethy, A. Agarwal, T. V . Marinov and M. K. Warmuth. 2023. A mechanism for sample -efficient in -context learning for sparse retrieval tasks. ArXiv, abs/2305.17040. 8. D . Aidwardana, M. -T. Luong, R. Dr. So, J. Hall, N.\" Challenges LLM July 19_23_p51.png\"The provided image contains references from a research paper, without any diagrams, graphics, equations, or tables. Below is the transcription of the references captured from the image: ```plaintext [33] P. Bajaj, C. Xiong , G. Ke, X. Liu, D. He, S. Tiwary, T. -Y . Liu, P. Bennett et al. 2022. Metro: E\ufb03cient denoising pre - training of large scale autoregressive language models with model generated signals. arXiv preprint arXiv:2204.06644. [34] A. Bakhtin, S. Gross, M. Ott, Y . Deng, M. Ranzato and A. Szlam. 2019. Real or Fake? Learning to Discriminate Machine from Human Generated Text. ArXiv:1906.03351 [cs, stat]. [35] R. Balesteriro, J. Pesenti and Y . LeCun. 2021. Learning in high dimension always amounts to extrapolation. arXi v preprint arXiv:2110.09485. [36] J. Bandy and N. Vincent. 2021. Addressing \"documenta - tion debt\" in machine learning research: A retrospective dataset for bootcorpus. [37] P. Barham, A. Chowdhery, J. Dean, S. Ghemawat, S\" Challenges LLM July 19_23_p52.png\"The provided image is a page from a bibliography section of a document. It does not contain diagrams, graphics, equations, or tables. Instead, it lists references in a structured numbered format. Here is a structured repre sentation of the references: Learning to Discriminate Machine from Human Generated Text. ArXiv:1906.03351 [cs, stat]. [35] R. Balesteriro, J. Pesenti and Y . LeCun. 2021. Learning in high dimension always amounts to extrapolation. arXi v preprint arXiv:2110.09485. [36] J. Bandy and N. Vincent. 2021. Addressing \"documenta - tion debt\" in machine learning research: A retrospective dataset for bootcorpus. [37] P. Barham, A. Chowdhery, J. Dean, S. Ghemawat, S\" Challenges LLM July 19_23_p52.png\"The provided image is a page from a bibliography section of a document. It does not contain diagrams, graphics, equations, or tables. Instead, it lists references in a structured numbered format. Here is a structured repre sentation of the references: 1. C. Burns, H. Ye, D. Klein, J. Steinhardt. 2022. Discovering latent knowledge in language models without supervision. 2. A. Calderwood, N. Wardrip -Fruin, M. Mateas. 2022. Spinning coherent interactive fiction through foundati on model prompts. International Conference of Computation and Creativity. 3. N. Carlini, M. Jagielski, C. A. Choquette -Choo, D. Paleka, W. Pearce, H. Anderson, A. Terzis, K. Thomas et al. 2023. Poisoning Web -Scale Training Datasets is Practical. ArXiv:2302 .10149 [cs]. 4. N. Carlini, C. Liu, \u00d6. Erlingsson, J. Kos, D. Song. 2019. The secret sharer: Evaluating and testing unintended memorization in neural networks. In USENIX Security Symposium, volume 267. 5. N. Carlini, M. Nasr, C. A. Choquette -Choo, M. Jagielski, I. Gao, A. Awadalla, P. W. Koh, D. Ipp\" Challenges LLM July 19_23_p53.png\"The provided image of the page from a PDF includes references to various papers. There are no diagrams, graphics, equations, or tables present in the image. The page consists solely of text from bibliographic entries. Here is a structured summary of the referenced papers: 1. Cobbe, K., Kosaraju, M., Bavarian, M., Chen, H., Jun, L., Kaiser, M., Plappert, J., and Tworek, J. (2021). Training verifiers to solve math word problems. 2. Cohen, M., Ryu, M., Chow, O., Keller, I., Gr eenberg, A., Hassidim, M., Fink, Y ., and Matias, Y . (2022). Dynamic planning in open -ended dialogue using reinforcement learning. arXiv preprint arXiv:2208.02294. 3. Cohen, M., Hamri, M., Geva, M., and Globerson, A. (2023). LM vs LM: Detecting factual erro rs via cross examination. arXiv:2305.13281. 4. Computer, T. (2023). Redpajama: An open source recipe to reproduce llama training dataset. 5. Conmy, A., Mavor - image. The page consists solely of text from bibliographic entries. Here is a structured summary of the referenced papers: 1. Cobbe, K., Kosaraju, M., Bavarian, M., Chen, H., Jun, L., Kaiser, M., Plappert, J., and Tworek, J. (2021). Training verifiers to solve math word problems. 2. Cohen, M., Ryu, M., Chow, O., Keller, I., Gr eenberg, A., Hassidim, M., Fink, Y ., and Matias, Y . (2022). Dynamic planning in open -ended dialogue using reinforcement learning. arXiv preprint arXiv:2208.02294. 3. Cohen, M., Hamri, M., Geva, M., and Globerson, A. (2023). LM vs LM: Detecting factual erro rs via cross examination. arXiv:2305.13281. 4. Computer, T. (2023). Redpajama: An open source recipe to reproduce llama training dataset. 5. Conmy, A., Mavor - Parker, A., Lynch, S., Heimersheim, A., and Garriga -Alonso, A. (2023). Towards automated circuit d iscovery for mechanistic interpretability. arXiv pre\" Challenges LLM July 19_23_p54.png\"The image contains only text references and no diagrams, graphics, equations, or tables. Thus, there is no data to structure or transcribe. Based on the content of the page, it consists mainly of bibliographic entries.\" Challenges LLM July 19_23_p55.png\"The image of the PDF page contains references and does not include any diagrams, graphics, tables, or equations that need detailed transcription or summarization. Here is a plain text extraction of the references from the image: ``` [151] C.",
    "84": "Fourrier, N. Habib, J. Launay and T. Wolf. 2023. What's going on with the open l \\lm leaderboard? Available from: https://huggingface.co/blog/evaluating -mmlu -leaderboard. Accessed: 27/06/2023. [152] E. Frantar and D. Alistarh. 2023. Mass ive language models can be accurately pruned in one -shot. arXiv preprint arXiv:2301.00774. [153] E. Frantar, S. Ashkboos, R. Hoefler and D. Alistarh. 2022. GPTQ: Accurate post -training quantization for generative pre - trained transformers. arXiv preprint ar Xiv:2210.17323. [154] D. Fried, A. Aghajanyan, J. Lin, S. Wang, E. Wallace, F. Shi, R. Zhong, W. -t. Yin et al. 2022. InCoder: A generative model for code infilling and synthesis. [155] A. Fr\u00f6mmgen and L. Kharytan. 2023. Resol\" Challenges LLM July 19_23_p56.png\"The provided image contains a list of references from a research paper. There are no diagrams, graphics, or equations present within this image. Additionally, there are no tables to extract data from. Since the request was to identify any accurately pruned in one -shot. arXiv preprint arXiv:2301.00774. [153] E. Frantar, S. Ashkboos, R. Hoefler and D. Alistarh. 2022. GPTQ: Accurate post -training quantization for generative pre - trained transformers. arXiv preprint ar Xiv:2210.17323. [154] D. Fried, A. Aghajanyan, J. Lin, S. Wang, E. Wallace, F. Shi, R. Zhong, W. -t. Yin et al. 2022. InCoder: A generative model for code infilling and synthesis. [155] A. Fr\u00f6mmgen and L. Kharytan. 2023. Resol\" Challenges LLM July 19_23_p56.png\"The provided image contains a list of references from a research paper. There are no diagrams, graphics, or equations present within this image. Additionally, there are no tables to extract data from. Since the request was to identify any diagrams, graphics, equations, or tables, and none are present, no detailed summary is required. The page simply lists references cited in a research paper, all of which are formatted in a standard bibliographic style, typically used in ac ademic writing.\" Challenges LLM July 19_23_p57.png\"This page from the PDF does not contain any diagrams, graphics, equations, or tables. It consists solely of references to various academic papers and sources. Here's a transcribed structured representation: ```plaintext Re ferences: ----------- [214] D. Houser and K. McCabe. 2014. Experimental economics and experimental game theory. In Neuroeconomics, pages 19 \u201334. Elsevier. [215] J. Howard and S. Ruder. 2018. Universal language model fine -tuning for text classification. In P roceedings of the 56th Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers), pages 328 \u2013339, Melbourne, Australia. Association for Computational Linguistics. [216] S. Hsiao. 2023. What\u2019s ahead for bard: More global, more visual, more integrated. Available from: https://blog.google/technology/ai/ google -bard-updates -io-2023/. Accessed: 28/06/2023. [217] B. Hu, J. Xia, J. Zheng, C. Tan, Y . Huang, Y . Xu and S. Z. Li. 2022. Protein language models and structure prediction: Connection and progression. [218] E. J. Hu, Y . Shen, P. Wallis, Z. Allen - Zhu, Y . Li, S. Wang, L. Wang and W. Chen. 202\" Challenges LLM July 19_23_p58.png\"Based on the image of the page you provided, I don't see any diagrams, graphics, or equations. The entire page consists only of text, including references to various studies, papers, and preprints related to language model s and natural language Melbourne, Australia. Association for Computational Linguistics. [216] S. Hsiao. 2023. What\u2019s ahead for bard: More global, more visual, more integrated. Available from: https://blog.google/technology/ai/ google -bard-updates -io-2023/. Accessed: 28/06/2023. [217] B. Hu, J. Xia, J. Zheng, C. Tan, Y . Huang, Y . Xu and S. Z. Li. 2022. Protein language models and structure prediction: Connection and progression. [218] E. J. Hu, Y . Shen, P. Wallis, Z. Allen - Zhu, Y . Li, S. Wang, L. Wang and W. Chen. 202\" Challenges LLM July 19_23_p58.png\"Based on the image of the page you provided, I don't see any diagrams, graphics, or equations. The entire page consists only of text, including references to various studies, papers, and preprints related to language model s and natural language processing. Since there are no tables or other structured data elements to transcribe or describe from the image, here\u2019s a summary of the relevant content: **Summary:** The image contains a list of references primarily related to adv ancements in language models, protein structure prediction, and transformer -based models. These references include studies on more efficient querying and data -training methods, alignment in language agents, causal reasoning in machine learning, and handlin g models with open problems. Some particular studies touch on topics like MiniPile challenge for data -efficient language models, causal inference for structured treatments, effectiveness of weight averager, and large pre -trained language models alignment, among others. Each reference includes authors, year of publication, title of the work, and sometimes the conference or journal where the paper was presented or published. No structured data (such as tables or equations) were identified on the provided page .\" Challenges LLM July 19_23_p59.png\"The provided image is a page from a PDF document. It contains a list of academic references formatted in a standard citation style. There are no diagrams, graphics, equations, or tables within the provided image. Due to th e absence of structured data types like tables, there is no data to represent for database entry. Here is a brief overview of the document content: - The page lists multiple references for academic papers and articles covering various topics related to mac hine learning, artificial intelligence, natural language processing, and computational linguistics. Each reference follows a format that tables or equations) were identified on the provided page .\" Challenges LLM July 19_23_p59.png\"The provided image is a page from a PDF document. It contains a list of academic references formatted in a standard citation style. There are no diagrams, graphics, equations, or tables within the provided image. Due to th e absence of structured data types like tables, there is no data to represent for database entry. Here is a brief overview of the document content: - The page lists multiple references for academic papers and articles covering various topics related to mac hine learning, artificial intelligence, natural language processing, and computational linguistics. Each reference follows a format that includes the authors' names, publication year, title of the paper, publication venue, and other relevant publication de tails such as page numbers, DOIs, and arXiv identifiers. If you wish to extract and structure citation information for database entry purposes, I can transcribe the references. However, be aware that these references may require a bibliographic database fo rmat such as BibTeX, RIS, or a custom schema depending on your use case.\" Challenges LLM July 19_23_p6.png\"**Diagrams:** 1. There is a diagram labeled \"Fig. 3.\" It appears to represent pre -training objectives, such as target span length, high/low corruption. 2. There is a diagram labeled \"Fig. 4.\" It seems to illustrate a concep t related to scaling laws, up - stream scaling, and down -stream setups. **Graphics:** 1. A warning symbol followed by a highlighted text section labeled \"Unsustainable Loss Power -Law [256],\" which describes performance increases through larger compute budget s but at a decreasing rate if the model or dataset size is fixed. **Equations:** There are no explicit equations in the provided text. **Tables:** No tables are present in the provided text. **Summary:** The provided page discusses high pre -training costs for large language models (LLMs), the scaling laws related to model performance, and ways to optimize compute costs. It suggests that while scaling laws can be used to predict",
    "85": "performance based on compute budgets, the exac t coefficients are debated. Additionally, the text mentions various pre -training objectives and their impact on self - supervised training efficiency. **Structured Data Representation:** - Diagrams: - Fig 3: Pre - training objectives (target span length, high/ low corruption) - Fig 4: Scaling laws, up -stream dataset size is fixed. **Equations:** There are no explicit equations in the provided text. **Tables:** No tables are present in the provided text. **Summary:** The provided page discusses high pre -training costs for large language models (LLMs), the scaling laws related to model performance, and ways to optimize compute costs. It suggests that while scaling laws can be used to predict performance based on compute budgets, the exac t coefficients are debated. Additionally, the text mentions various pre -training objectives and their impact on self - supervised training efficiency. **Structured Data Representation:** - Diagrams: - Fig 3: Pre - training objectives (target span length, high/ low corruption) - Fig 4: Scaling laws, up -stream scaling, down -stream setups - Graphics: - Warning symbol with text on \"Unsustainable Loss Power -Law [256]\": ``` Performance increases through larger compute budgets but\" Challenges LLM July 19_23_p60.png\"The page you provided contains only basic text citations and does not appear to contain any tables, diagrams, graphics, or equations. Therefore, there's no structured data or graphical summary to be transcribed into a data base. If you have other pages or documents with tables or equations, feel free to share them, and I can help you transcribe the structured data accordingly.\" Challenges LLM July 19_23_p61.png\"The page from the provided PDF does not contain any diagrams, graphics, equations, or tables. It consists of references in a numbered list format. Below is a text representation of the content found on the page, organized as references: 1. R. Liu and N. B. Shah, 2023. ReviewerGPT? An Exploratory Study on Using Large Language Models for Paper Reviewing. ArXiv:2306.00622 [cs]. 2. S. Liu and Z. Wang, 2023. Ten lessons we have learned in the new 'sparseland': A short handbook f or sparse neural network research. ArXiv:2302.02596. 3. X. Liu, X. Yang, L. Ouyang, G. Guo, J. Su, R. Xi, K. Yuan and F. Yuan, 2022. Protein language model predicts mutation pathogenicity and clinical prognosis. bioRxiv, pages 2022 \u201309. 4. Z. Liu, A. Bahety and S. Song, 2023. Reflect: Summarizing robot experiences for failure explanation and correction. 5. Z. Liu, E. Gan, and M. Tegmark, 2023. Seeing is believing: Brain -inspired modular training for mechanistic interpretability. arXiv preprint arXiv:2305.087 46. 6. S. Longpre, L. Hou, T. Vu, A. Webson,\" Models for Paper Reviewing. ArXiv:2306.00622 [cs]. 2. S. Liu and Z. Wang, 2023. Ten lessons we have learned in the new 'sparseland': A short handbook f or sparse neural network research. ArXiv:2302.02596. 3. X. Liu, X. Yang, L. Ouyang, G. Guo, J. Su, R. Xi, K. Yuan and F. Yuan, 2022. Protein language model predicts mutation pathogenicity and clinical prognosis. bioRxiv, pages 2022 \u201309. 4. Z. Liu, A. Bahety and S. Song, 2023. Reflect: Summarizing robot experiences for failure explanation and correction. 5. Z. Liu, E. Gan, and M. Tegmark, 2023. Seeing is believing: Brain -inspired modular training for mechanistic interpretability. arXiv preprint arXiv:2305.087 46. 6. S. Longpre, L. Hou, T. Vu, A. Webson,\" Challenges LLM July 19_23_p62.png\"The page from the PDF provided does not contain any diagrams, graphics, equations, or tables. It primarily consists of a list of references. No specific data is structured in a table format that can be transcribed for a da tabase. The page number at the bottom indicates it is page 62. References are listed in a numerical sequence and include authors, titles, publication years, and journal or conference names along with additional details like volume, page numbers, and digita l object identifiers (DOIs). If you need specific references to be extracted or processed, please provide further instructions on the format or details you need.\" Challenges LLM July 19_23_p63.png\"The image provided is a page from a PDF that contains a reference list. There are no diagrams, graphics, equations, or tables on this page. The text consists solely of bibliographic entries. Since there are no specific dat a tables to structure or equations to transcribe and no diagrams to describe, no additional structured data or summaries are necessary from this page. If you require any other type of processing or have another page with diagrams, tables, or equations, ple ase provide that additional information or page.\" Challenges LLM July 19_23_p64.png\"The provided image contains bibliographic references from a research paper. The page does not contain any diagrams, graphics, equations, or tables. Here is the structured text from the references into a simplified form: ``` { \"references\": [ { \"id\": 424, \"authors\": \"B. Peters and A. F. T. Martins\", \"year\": 2021, \"title\": \"Smoothing and shrinking the sparse Seq2Seq search space\", \"source\": \"Proceedings of the 2021 Conference of the North equations to transcribe and no diagrams to describe, no additional structured data or summaries are necessary from this page. If you require any other type of processing or have another page with diagrams, tables, or equations, ple ase provide that additional information or page.\" Challenges LLM July 19_23_p64.png\"The provided image contains bibliographic references from a research paper. The page does not contain any diagrams, graphics, equations, or tables. Here is the structured text from the references into a simplified form: ``` { \"references\": [ { \"id\": 424, \"authors\": \"B. Peters and A. F. T. Martins\", \"year\": 2021, \"title\": \"Smoothing and shrinking the sparse Seq2Seq search space\", \"source\": \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\", \"pages\": \"2462 \u20132654\", \"publisher\": \"Online. Association for Computational Linguistics.\" }, { \"id\": 425, \"authors\": \"J. Pet ers, D. Janzing and B. Sch \\\"olkopf\", \"year\": 2017, \"title\": \"Elements of causal inference: foundations and learning algorithms\", \"source\": \"The MIT Press.\" }, { \"id\": 426, \"authors\": \"A. Petrov, E. La Malfa, P. H. Torr and A. Bibi\", \"year\": 2023, \"title\": \"Language model tokenizers introduce unfairness between languages\", \"source\": \"arXiv arXiv:2305.15425.\" }, \" Challenges LLM July 19_23_p65.png\"The provided page of the PDF document does not contain any diagrams, graphics, equations, or tables. The content includes only textual references and citations. Therefore, there are no structured tabular data, diagrams, or equations to represent from this page.\" Challenges LLM July 19_23_p66.png\"The page from the PDF you provided contains references. There are no diagrams, graphics, tables, or complex structured data elements other than basic references. There are no equations to transcribe. Below is a structured representation of the data found on the page: ```plaintext [484] J. Schulman, F. Wolski, P. Dhariwal, A. Radford and O. Klimov. 2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347. [485] M. Schuster and K. Nakajima. 2012. Japanes e and korean voice search. In 2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5149 \u2013 5152. [486] T. Schuster, R. Schuster, D. J. Shah and R. Barzilay. 2020. The limitations of this page.\" Challenges LLM July 19_23_p66.png\"The page from the PDF you provided contains references. There are no diagrams, graphics, tables, or complex structured data elements other than basic references. There are no equations to transcribe. Below is a structured representation of the data found on the page: ```plaintext [484] J. Schulman, F. Wolski, P. Dhariwal, A. Radford and O. Klimov. 2017. Proximal policy",
    "86": "optimization algorithms. arXiv preprint arXiv:1707.06347. [485] M. Schuster and K. Nakajima. 2012. Japanes e and korean voice search. In 2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5149 \u2013 5152. [486] T. Schuster, R. Schuster, D. J. Shah and R. Barzilay. 2020. The limitations of stylometry for detecting machine -generated fake news. Computational Linguistics, 46(2):499 \u2013 510. [487] R. Schwartz, J. Dodge, N. A. Smith and O. Etzioni. 2019. Green AI. ArXiv:1907.10597 [cs, stat]. [488] S. H. Schwartz, B. Breyer and D. Danner. 2015. Human values scale (ess). Zusammenstellun g sozialwissenschaftlicher Items und Skalen (ZIS). \" Challenges LLM July 19_23_p67.png\"The provided image contains a list of bibliographic references from a paper. There are no diagrams, graphics, or tables. Only bibliographic entries are present, and there are no equations either. Here is the structured rep resentation of the references as text for database entry: 1. S. Smith, M. Patwary, B. Roop, L. P. LeGresley, S. Rajbhandari, J. Casper, Z. Liu, S. Prabhumooye et al. 2022. Using deepspeed and megatron to train megatron -turing NLG 530b, a large -scale genera tive language model. arXiv preprint arXiv:2201.11990. 2. I. Solaiman and C. Dennison. 2021. Process for adapting language models to society (palms) with values -targeted datasets. Advances in Neural Information Processing Systems 34: 5861 \u20135873. 3. S. Soltan , S. Ananthakrishnan, J. FitzGerald, R. Gupta, W. Hamma, K. Khan, C. Peris, S. Rawls et al. 2022. AlexaTM 20b: Few -shot learning using a large -scale multilingual seq2seq model. arXiv preprint arXiv:2208.01448. 4. B. Sorscher, R. Geirhos, S. Shekhar\" Challenges LLM July 19_23_p68.png\"The provided image contains a list of references from a document. There are no diagrams, graphics, equations, or tables visible in the image. The content is primarily bibliographic citations. Here's a brief summary for con text, formatted as plain text: arXiv:2201.11990. 2. I. Solaiman and C. Dennison. 2021. Process for adapting language models to society (palms) with values -targeted datasets. Advances in Neural Information Processing Systems 34: 5861 \u20135873. 3. S. Soltan , S. Ananthakrishnan, J. FitzGerald, R. Gupta, W. Hamma, K. Khan, C. Peris, S. Rawls et al. 2022. AlexaTM 20b: Few -shot learning using a large -scale multilingual seq2seq model. arXiv preprint arXiv:2208.01448. 4. B. Sorscher, R. Geirhos, S. Shekhar\" Challenges LLM July 19_23_p68.png\"The provided image contains a list of references from a document. There are no diagrams, graphics, equations, or tables visible in the image. The content is primarily bibliographic citations. Here's a brief summary for con text, formatted as plain text: ``` The provided references are related to machine learning, specifically focusing on language models, their evaluation, encoding, and various methods for improving their efficiency and accuracy. Some prominent themes include surveys on transformers, insig hts on pre -training and fine-tuning transformers, unifying learning paradigms, and analyzing different ways of measuring readability, understanding, and drafting responses using advanced AI models like GPT-3. There is also a focus on domain -specific langua ge models, such as those used for biomedical text, and challenges and future opportunities in the age of large language models. Additionally, the references touch on issues like model scaling, handling data, dealing with scaling limits in machine learning, improving code models for self -improvement, and approaches to model adaptation through prompt transfer. ``` There's no data to be represented in a structured format as there are no tables provided in the image.\" Challenges LLM July 19_23_p69.png\"The image provided is a page from a PDF document consisting mainly of bibliographic references. There are no diagrams, graphics, tables, or equations present in the image. The document contains a numbered list of reference s in a structured format. No diagrams, graphics, equations, or tables were found within the image. The data is only basic text consisting of referenced documents.\" Challenges LLM July 19_23_p7.png\"**Diagrams and Graphics:** 1. **Figure 3: Masking Strategies** - **Description:** The figure illustrates three distinct input masking strategies used in language modeling: - **Masked LM:** All tokens are masked. - **Languag e Modeling:** No format as there are no tables provided in the image.\" Challenges LLM July 19_23_p69.png\"The image provided is a page from a PDF document consisting mainly of bibliographic references. There are no diagrams, graphics, tables, or equations present in the image. The document contains a numbered list of reference s in a structured format. No diagrams, graphics, equations, or tables were found within the image. The data is only basic text consisting of referenced documents.\" Challenges LLM July 19_23_p7.png\"**Diagrams and Graphics:** 1. **Figure 3: Masking Strategies** - **Description:** The figure illustrates three distinct input masking strategies used in language modeling: - **Masked LM:** All tokens are masked. - **Languag e Modeling:** No tokens are masked. - **Prefix LM:** Some leading tokens are unmasked, and the rest are masked. - **Visualization:** The figure uses distinct color codings (assumed red for unmasked and blue for masked tokens) to represent tokens \\(x_1\\) to \\(x_9\\) (input) against outputs \\(y_1\\) to \\(y_9\\) (rows). - **Key Identifier:** Each row represents which inputs \\(x_i\\) (columns) a particular output \\(y_i\\) (row) can attend to (either uni - or bi-directionally). **Equations:** 1. **Equation 1: Language Modeling Objective** \\[ L(x) = \\sum_{i=1}^{N} \\log P(x_i|x_1, \\ldots, x_{i-1}; \\theta). \\] 2. **Equation 2: Masked Language Modeling Objective** \\[ L(x_{ \\text{MASK}}|x_{ -\\text{MASK}}) = \\frac{1}{|x_{ \\text{MASK}}|} \\sum_{i \\in x_{\\text{MASK}}\" Challenges LLM July 19_23_p70.png\"The provided image contains a page from an academic paper with a list of references. It does not contain any diagrams, graphics, or tables. Here are the equations transcribed from this image: No equations are present. The text references are formatted as follows: 1. S. Welleck, I. Kulikov, S. Roller, E. Dinan, K. Cho and J. Weston. 2019. Neural text generation with unlikelihood training. arXiv preprint arXiv:1908.04319. 2. L. Weng. 2023. Large transformer model inference op timization. Lil' Log. 3. L. Weng. 2023. Prompt engineering. lillianweng.github.io. 4. M. Willig, M. \u017dE\u010cEV\u00cdC, D. S. Dhami and K. Kersting. 2023. Causal parrots: Large language models may talk casually but are not causal. preprint. 5. F. Winkelmolen, N. Ivki n, H. F. Bozkurt and Z. Karnin. 2020. Practical and sample efficient zero - shot hop. arXiv preprint arXiv:2007.13882. 6. Y . Wolf, N. Wies, Y . Levine and A. Shashua. 2023. equations transcribed from this image: No equations are present. The text references are formatted as follows: 1. S. Welleck, I. Kulikov, S. Roller, E. Dinan, K. Cho and J. Weston. 2019. Neural text generation with unlikelihood training. arXiv preprint arXiv:1908.04319. 2. L. Weng. 2023. Large transformer model inference op timization. Lil' Log. 3. L. Weng. 2023. Prompt engineering. lillianweng.github.io. 4. M. Willig, M. \u017dE\u010cEV\u00cdC, D. S. Dhami and K. Kersting. 2023. Causal parrots: Large language models may talk casually but are not causal. preprint. 5. F. Winkelmolen, N. Ivki n, H. F. Bozkurt and Z. Karnin. 2020. Practical and sample efficient zero - shot hop. arXiv preprint arXiv:2007.13882. 6. Y . Wolf, N. Wies, Y . Levine and A. Shashua. 2023. Fundamental limitations of aligning in large language models. arXiv preprint arXiv:230 4.\" Challenges LLM July 19_23_p71.png\"Based on the image provided, here is a summary of the identified elements: It is a text -heavy document with citations. There are no diagrams, graphics, equations, or tables visible in the image provided. Therefore, there is no data to extract",
    "87": "in a structured form or transcribe. Each entry appears to be a reference with details including authors, title, publication year, and source, followed by a citation identifier. If you have any specific needs or additional images, feel free to provide them!\" Challenges LLM July 19_23_p72.png\"The given image is of a page from a PDF that contains references. There are no diagrams, graphics, tables, or equations presented in the image. The content includes a list of references from various research papers and art icles. Therefore, no structured data tables, graphics summary, or mathematical equations are available to transcribe or describe. For clarity and accuracy, here is the list of references transcribed as text: 1. S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, D. Dewan, M. Diab et al. 2022. Opt: Open pre - trained transformer language models. > arXiv preprint arXiv:2205.01068. 2. T. Zhang, V . Kishore, F. Wu, K. Q. Weinberger and Y . Artzi. 2019. Bertscore: Evaluating text generation with bert. > arX iv preprint arXiv:1904.09675. 3. T. Zhang, F. Ladhak, E. Durmus, P. Liang, K. McKeown and T. B. Hashimoto. 2023. Benchmarking large language models for news summarization. 4. Z. Zhang, Y . Gu, X. Han, S. Chen, C. Xiao, Z. Sun, Y . Yao, F. Qi et al. 2021\" structured data tables, graphics summary, or mathematical equations are available to transcribe or describe. For clarity and accuracy, here is the list of references transcribed as text: 1. S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, D. Dewan, M. Diab et al. 2022. Opt: Open pre - trained transformer language models. > arXiv preprint arXiv:2205.01068. 2. T. Zhang, V . Kishore, F. Wu, K. Q. Weinberger and Y . Artzi. 2019. Bertscore: Evaluating text generation with bert. > arX iv preprint arXiv:1904.09675. 3. T. Zhang, F. Ladhak, E. Durmus, P. Liang, K. McKeown and T. B. Hashimoto. 2023. Benchmarking large language models for news summarization. 4. Z. Zhang, Y . Gu, X. Han, S. Chen, C. Xiao, Z. Sun, Y . Yao, F. Qi et al. 2021\" Challenges LLM July 19_23_p8.png\"The page contains diagrams illustrating different pre - training objectives for a language model, focusing on self -supervised data construction. Here's a structured form of the content and description: ### Diagrams and Data: 1. **Span Corruption (R-Denoising)** - **Inputs:** ``` [Some proponents of AI consciousness adhere to functionalism, the view that mental states are defined more by their functions than their underlying physical structures. In other words, if an AI can res pond to inputs and produce outputs similar to a conscious being, then it could be considered conscious. However, this view doesn't account for subjective] ``` - **Targets:** ``` [4] [3] [2] ``` 2. **Prefix Language Modeling (S -Denoising)** - **Inputs:** `` ` [Some proponents of AI consciousness adhere to functionalism, the view that mental states are defined more by their functions than their underlying physical structures. In other] ``` - **Targets:** ``` [56] ``` 3. **Long Span Corruption (one form of X -Denoising)** - **Inputs:** ``` [Some proponents of AI consciousness adhere to functionalism, the] [12] [underlying physical structures. In other words, if an AI can respond to] [13] [considered conscious. However, this view doesn't account for subjective] [1 4\" Challenges LLM July 19_23_p9.png\"### Analysis of the PDF Page #### Diagrams and Graphics: There are no diagrams or graphics present on the page. #### Equations: There is a single equation labeled (3): (3) \\[ \\begin{aligned} & \\sum_{x \\in S} \\Bigg( -\\log \\overrightarrow{p} \\left(x_{i} \\mid x_{<i}; \\theta \\right) \\\\ & \\text{NLL for forward model} \\\\ & underlying physical structures. In other] ``` - **Targets:** ``` [56] ``` 3. **Long Span Corruption (one form of X -Denoising)** - **Inputs:** ``` [Some proponents of AI consciousness adhere to functionalism, the] [12] [underlying physical structures. In other words, if an AI can respond to] [13] [considered conscious. However, this view doesn't account for subjective] [1 4\" Challenges LLM July 19_23_p9.png\"### Analysis of the PDF Page #### Diagrams and Graphics: There are no diagrams or graphics present on the page. #### Equations: There is a single equation labeled (3): (3) \\[ \\begin{aligned} & \\sum_{x \\in S} \\Bigg( -\\log \\overrightarrow{p} \\left(x_{i} \\mid x_{<i}; \\theta \\right) \\\\ & \\text{NLL for forward model} \\\\ & -\\log \\overleftarrow{p} \\left(x_{i} \\mid x_{>i}; \\theta \\right) \\\\ & \\text{NLL for backward model} \\\\ & + \\beta D_{i}^{ \\mathrm{TV}} \\left(\\overrightarrow{p} \\| \\overleftarrow{p} \\right) \\Bigg) \\end{aligned} \\] where \\( D_{i}^{ \\mathrm{TV}} \\left(\\overrightarrow{p} \\| \\overleftarrow{p} \\right) \\) is the total variation distance among the two models on the \\( i \\)-th token. #### Tables: There are no tables present on the pa ge. ### Summary: 1. **Equations**: There is a single equation describing the pre -training loss for sequence models using forward and backward language models, incorporating a total variation distance term as a regularizer. ```plaintext equation: \\sum_{x \\in\" Continual_Pretraining_p1.png\"The image contains a bar chart (Figure 1) showing domain - specific task performance in biomedicine, finance, and law. Below is the extracted and structured data from the table: **Biomedicine** - CHEMPROT - General LLM: 56 - DAPT : 57 - AdaptLLM: 59 - RCT - General LLM: 53 - DAPT: 54 - AdaptLLM: 56 - HoC - General LLM: 51 - DAPT: 52 - AdaptLLM: 54 - PubMedQA - General LLM: 43 - DAPT: 45 - AdaptLLM: 47 **Finance** - CONDQA - General LLM: 66 - DAPT: 70 - AdaptLLM: 71 - PTB - General LLM: 61 - DAPT: 63 - AdaptLLM: 65 - FCSA - General LLM: 58 - DAPT: 59 - AdaptLLM: 60 - Headline - General LLM: 43 - DAPT: 49 - AdaptLLM: 51 **Law** - SCOTUS - General L\" Continual_Pretraining_p10.png\"The page contains primarily text, with a few features of note: ### Diagrams and Graphics: 1. There is a large \"X\" mark on the page, likely indicating a highlight or a correction. ### Equations: There are no equations present o n this page. ### Tables: data from the table: **Biomedicine** - CHEMPROT - General LLM: 56 - DAPT : 57 - AdaptLLM: 59 - RCT - General LLM: 53 - DAPT: 54 - AdaptLLM: 56 - HoC - General LLM: 51 - DAPT: 52 - AdaptLLM: 54 - PubMedQA - General LLM: 43 - DAPT: 45 - AdaptLLM: 47 **Finance** - CONDQA - General LLM: 66 - DAPT: 70 - AdaptLLM: 71 - PTB - General LLM: 61 - DAPT: 63 - AdaptLLM: 65 - FCSA - General LLM: 58 - DAPT: 59 - AdaptLLM: 60 - Headline - General LLM: 43 - DAPT: 49 - AdaptLLM: 51 **Law** - SCOTUS - General L\" Continual_Pretraining_p10.png\"The page contains primarily text, with a few features of note: ### Diagrams and Graphics: 1. There is a large \"X\" mark on the page, likely indicating a highlight or a correction. ### Equations: There are no equations present o n this page. ### Tables: There are no tables present on this page. ### Summary of Key Sections: **Section: Instruction Fine-tuning** - Fine-tuning LLMs on domain -specific tasks improves their performance. - Utilizes question -answering instructions. - Effec tive in specific domains: [Singhal et al., 2022], [Le et al., 2023b], [Wang et al., 2023], [Han et al., 2023], [Xiong et al., 2023], [Huang et al., 2023]. - Challenges in creating large -scale supervised datasets for instruction tuning. - Supervised fine -tuning data often limited. - Generative models like ChatGPT and GPT -4 are used to produce data. - Pre-training can alleviate some limitations of inferred codes from closed - source models. **Section: Retrieval -augmented Prompting** - Enhances LLMs by integrati ng external information without altering model parameters [Li et al., 2023b], [Cui et al., 2023] [Huang et al., 2023]. - Utilizes domain -specific knowledge from documents, domain -specific knowledge graphs, neural networks. - Improvement through better cont ext understanding from\" Continual_Pretraining_p11.png\"The provided image includes a list of references from a paper. The references contain",
    "88": "various textual details, but there are no diagrams, graphics, or equations present on the page. Here's the identified structured data from t he text extracted from tables: 1. Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. - Title: A large annotated corpus for learning natural language inference. - Conference: EMNLP - Pages: 632-642 - Year: 2015 - Association: Ass ociation for Computational Linguistics 2. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind knowledge graphs, neural networks. - Improvement through better cont ext understanding from\" Continual_Pretraining_p11.png\"The provided image includes a list of references from a paper. The references contain various textual details, but there are no diagrams, graphics, or equations present on the page. Here's the identified structured data from t he text extracted from tables: 1. Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. - Title: A large annotated corpus for learning natural language inference. - Conference: EMNLP - Pages: 632-642 - Year: 2015 - Association: Ass ociation for Computational Linguistics 2. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert - V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and D ario Amodei. - Title: Language models are few -shot learners. - Conference: NeurIPS - Year: 2020 3. Ilias Chalkidis. \" Continual_Pretraining_p12.png\"The provided image primarily contains a list of references from a research paper, which includes the titles, authors, publication forums, and years of various cited works. There are no diagrams, graphics, specific equations, o r structured tables in the provided image. Here\u2019s the formatted information of the references for database entry: 1. Title: The false promise of imitating proprietary llms. Authors: Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, P ieter Abbeel, Sergey Levine, Dawn Song. Forum: CoRR Reference ID/DOI: abs/2305.15171 Year: 2023 2. Title: Don\u2019t stop pretraining: Adapt language models to domains and tasks. Authors: Suchin Gururangan, Ana Marasovic, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, Noah A. Smith. Forum: ACL Pages: 8342 \u20138360 Year: 2020 3. Title: MedAlpaca - an open -source collection of medical conversational AI models and training data. Authors: Tianyu Han, Lisa C. Adams, Jens - Michalis Papaioannou, Paul Grundmann, Tom Oberhauser, Alexander L\u00f6ser, Daniel Truhn, Keno K. Bressem. Forum: CoRR Reference ID/DOI: abs/\" promise of imitating proprietary llms. Authors: Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, P ieter Abbeel, Sergey Levine, Dawn Song. Forum: CoRR Reference ID/DOI: abs/2305.15171 Year: 2023 2. Title: Don\u2019t stop pretraining: Adapt language models to domains and tasks. Authors: Suchin Gururangan, Ana Marasovic, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, Noah A. Smith. Forum: ACL Pages: 8342 \u20138360 Year: 2020 3. Title: MedAlpaca - an open -source collection of medical conversational AI models and training data. Authors: Tianyu Han, Lisa C. Adams, Jens - Michalis Papaioannou, Paul Grundmann, Tom Oberhauser, Alexander L\u00f6ser, Daniel Truhn, Keno K. Bressem. Forum: CoRR Reference ID/DOI: abs/\" Continual_Pretraining_p13.png\"There are no diagrams, graphics, or equations in this image. However, there appears to be a table of references formatted in a specific citation style. Below is the structured data of the table in text format: ``` [ { \"Author( s)\": \"Marco Lippi, Przemyslaw Palka, Giuseppe Contissa, Francesca Lagioia, Hans -Wolfgang Micklitz, Giovanni Sartor, and Paolo Torroni\", \"Title\": \"CLAUDETTE: an automated detector of potentially unfair clauses in online terms of service\", \"Journal\": \"Artif. Intell. Law\", \"V olume\": \"27(2)\", \"Pages\": \"117 \u2013139\", \"Year\": \"2019\" }, { \"Author(s)\": \"Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov\", \"Title\": \"Roberta: A robustly o ptimized BERT pretraining approach\", \"Journal\": \"CoRR\", \"V olume\": \"abs/1907.11692\", \"Year\": \"2019\" }, { \"Author(s)\": \"Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V .\" Continual_Pretraining_p14.png\"The page consists of a list of bibliographic references. There are no diagrams, graphics, tables, or equations present. Therefore, there is no data to represent in a structured form or text. The content includes references to various academic papers and articles related to computational linguistics, language models, and machine learning. Each entry lists the authors, title, conference or journal, page numbers, year, and sometimes URLs. This text data does not require transcript ion for any diagrams, graphics, tables, or equations as none are present.\" Continual_Pretraining_p15.png\"The image contains a list of references from a research paper. Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V .\" Continual_Pretraining_p14.png\"The page consists of a list of bibliographic references. There are no diagrams, graphics, tables, or equations present. Therefore, there is no data to represent in a structured form or text. The content includes references to various academic papers and articles related to computational linguistics, language models, and machine learning. Each entry lists the authors, title, conference or journal, page numbers, year, and sometimes URLs. This text data does not require transcript ion for any diagrams, graphics, tables, or equations as none are present.\" Continual_Pretraining_p15.png\"The image contains a list of references from a research paper. There are no diagrams, graphics, or equations present on the page. Here's a structured representation of the data listed in the table (reference list): 1. Adina Wi lliams, Nikita Nangia, and Samuel R. Bowman. A broad -coverage challenge corpus for sentence understanding through inference. *NAACL -HLT*, pp. 1112 -1122. Association for Computational Linguistics, 2018. 2. Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, a nd Weidi Xie. Pmc -llama: Further fine - tuning llama on medical papers. *CoRR*, abs/2304.14454, 2023a. 3. Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhakaran Kambadur, David S. Rosenberg, and Gideon Mann. Bloombe rggpt: A large language model for finance. *CoRR*, abs/2303.17564, 2023b. 4. Honglin Xiong, Sheng Wang, Yitao Zhu, Zihao Zhao, Yuxiao Liu, Linlin Huang, Qian Wang, and Dinggang Shen. Doctorglm: Fine -tuning your chinese doctor is not a herculean task. *CoRR *, abs/2304.01097, 2023\" Continual_Pretraining_p16.png\"**Diagrams, Graphics, and Tables Identified:** - **Table 6: Domain knowledge probing results.** --- **Structured Data from Table 6:** | Domain | General LLM | Raw Text | Read. Text | Read. Compre. | | ---------------- |--------- ----|---------- |------------ |----- ---------- | | BioMed | 36.5 | 36.9 | 36.8 | | Law | 45.0 | 45.6 | 46.4 | --- **Summary of Graphics and Diagrams:** - **Table 6** provides the results of the domain knowledge probing. The table compares performance metrics for two domains (BioMed and Law) across four models: General LLM, Raw Text, Read. Text, and Read. Compre. The values in the cells represe nt scores indicating the performance of the models on domain -specific tasks. **Equations:** No equations Continual_Pretraining_p16.png\"**Diagrams, Graphics, and Tables Identified:** - **Table 6: Domain knowledge probing results.** --- **Structured Data from Table 6:** | Domain | General LLM | Raw Text | Read. Text | Read. Compre. | |",
    "89": "---------------- |--------- ----|---------- |------------ |----- ---------- | | BioMed | 36.5 | 36.9 | 36.8 | | Law | 45.0 | 45.6 | 46.4 | --- **Summary of Graphics and Diagrams:** - **Table 6** provides the results of the domain knowledge probing. The table compares performance metrics for two domains (BioMed and Law) across four models: General LLM, Raw Text, Read. Text, and Read. Compre. The values in the cells represe nt scores indicating the performance of the models on domain -specific tasks. **Equations:** No equations were present on the page. --- The above data representation can be passed directly back to a database.\" Continual_Pretraining_p17.png\"**Tables:** 1. **Table 7: Pre -training corpora** | Domain | Data Source | Raw Size | # Tokens | # Docs | | ---------- |-------------------------------- |---------- |-------- --|-------- | | BioMed | PubMed Abstracts (Gao et al., 202 1) | 19.3 GiB | 5.4 B | 15.5 M | | Finance | Stock News (Gao et al., 2021) | 5.1 GiB | 1.2 B | 1.1 M | | Law | FreeLaw Opinions (Gao et al., 2021) | 51.2 GiB | 16.7 B | 3.6 M | 2. **Table 8: Hyper -parameters of domain -adaptive pre - training** | Hyperparamet er | Assignment | | ----------------------------- |------------------------------- | | Computing infrastructure | 32 V100 -32GB GPUs | | Runtime | 24 Hours | | Number of steps | 10,000 | | Batch size | 32 | | Maximum sequence length | 2,048 | | Maximum learnin g rate | 1e -5 | | Optimizer | Adam | | Adam beta weights | 0.9,\" Continual_Pretraining_p18.png\"**Diagrams/Graphics/Equations/Tables Summary** 1. **Table 9: Keywords that compile into regular expressions.** - **Columns:** Keyword, Regex - **Data:** - {VERBAL}: Replaced with the verbalizer - {WORD}: regex: (([ \u30fb!?\u3002\uff0c]\\\\n ) [\\^\\\\s]{10,}) - Matches a single word having more than 9 characters - {SENT}: regex: (([\u30fb!?\u3002\uff0c]{50,} ) [!? \\\\+ }) - Matches a single sentence having more than 50 characters 2. **Table 10: Specifications of the domain -specific task datasets.** - **Columns:** Task, Type, Metric, # Demos - **Data:** - BioMed: - MOP: Binary classification, Accuracy, 4 - PubMedQA: Binary classification, Accuracy, 0 - USMLE: Multi -choice QA, Accuracy, 0 - RCT: Optimizer | Adam | | Adam beta weights | 0.9,\" Continual_Pretraining_p18.png\"**Diagrams/Graphics/Equations/Tables Summary** 1. **Table 9: Keywords that compile into regular expressions.** - **Columns:** Keyword, Regex - **Data:** - {VERBAL}: Replaced with the verbalizer - {WORD}: regex: (([ \u30fb!?\u3002\uff0c]\\\\n ) [\\^\\\\s]{10,}) - Matches a single word having more than 9 characters - {SENT}: regex: (([\u30fb!?\u3002\uff0c]{50,} ) [!? \\\\+ }) - Matches a single sentence having more than 50 characters 2. **Table 10: Specifications of the domain -specific task datasets.** - **Columns:** Task, Type, Metric, # Demos - **Data:** - BioMed: - MOP: Binary classification, Accuracy, 4 - PubMedQA: Binary classification, Accuracy, 0 - USMLE: Multi -choice QA, Accuracy, 0 - RCT: Multi -class classification, Micro F1, 10 - ChemProt: Multi -class classification, Micro F1, 13 - Finance: - FiQA SA: Multi -class classification, Weighted F1, 5 - FPB: Multi -class classification, Weighted F1, 5 - NER: Named entity recognition, Entity -level F1, 20 - Head\" Continual_Pretraining_p19.png\"**Table Data** ``` TableName: PromptTemplates Columns: - Task - Template Rows: 1: Task: BioMed. Template: - MQP - Question 1: {QUESTION1} - Question 2: {QUESTION2} - Are questions 1 and 2 asking the same thing? {ANSWER} - PubM edQA - Context: {CONTEXT} - Question: {QUESTION} - Answer: {ANSWER} - USMLE - Question: {QUESTION} - Answer: {ANSWER} - RCT - {SENTENCE} - Question: What is the role of this sentence in an abstract? - Answer: {ANSWER} - ChemProt - {SENTENCE} - Question: Wh at is the relation? - Answer: {ANSWER} 2: Task: Finance Template: - FiQA SA - {SENTENCE} - Question: What is the sentiment on {TARGET}? - Answer: {ANSWER} - FPB - {SENTENCE} - Question: What is the sentiment? - Answer: {ANSWER} - NER - {SENTENCE} - Extract named entity: {ANSWER} - Headline - {SENTENCE} - Question: {QUESTION} -\" Continual_Pretraining_p2.png\"The image contains a figure (Figure 2) and a brief description of it. Here is the transcription and structured summary: --- **Diagrams and Graphics:** **Figure Description:** Figure 2: A simplified example of a reading comprehe nsion text, wherein the raw text is followed by a series of tasks constructed from it, including: - Summarization (purple) - Word -to-Text (blue) - Natural Language Inference (red) - Commonsense Reasoning (teal) - Paraphrase Detection (yellow) - Text Completion (green). --- **Summary of Answer: {ANSWER} - FPB - {SENTENCE} - Question: What is the sentiment? - Answer: {ANSWER} - NER - {SENTENCE} - Extract named entity: {ANSWER} - Headline - {SENTENCE} - Question: {QUESTION} -\" Continual_Pretraining_p2.png\"The image contains a figure (Figure 2) and a brief description of it. Here is the transcription and structured summary: --- **Diagrams and Graphics:** **Figure Description:** Figure 2: A simplified example of a reading comprehe nsion text, wherein the raw text is followed by a series of tasks constructed from it, including: - Summarization (purple) - Word -to-Text (blue) - Natural Language Inference (red) - Commonsense Reasoning (teal) - Paraphrase Detection (yellow) - Text Completion (green). --- **Summary of Graphics/Diagrams:** - **Raw Text Section:** This section includes a snippet about \"Glottic Carcinoma in Young Patients.\" Various elements are highlighted in different colors (Title, Domain Keywords, Entailment Relation, Cause & Effect, Semantic Similar ity, and Text Ending). - **Reading Comprehension Section:** This section is based on the Raw Text. It includes various types of questions and prompts: - Summarization: \"What is a summary Glottic Carcinoma in Young Patients.\" - Entailment: \"Generate a sente nce that includes these biomedicine keywords (carcinoma, oropharyngeal, papillomavirus) Recent reported evidence indicates that vocal cord carcinoma is evolving...\" - Explanation Tasks: \"What is the reason for ...?\" - Contradiction Tasks: \"Compose a senten ce that contradicts the meaning...\" - Completion\" Continual_Pretraining_p20.png\"**Diagrams and Graphics:** 1. **Pie Charts (Figure 4):** - **Biomedicine** - Word -to-Text: 50.3 - Summarize: 26.3 - Text Completion: 17.4 - Common Reason: 0.3 - Paraphrase: 3.5 - NIL: 2.2 - **Finance** - Word -to-Text: 35.8 - Summarize: 31.4 - Text Completion: 20.7 - Common Reason: 0.5 - Paraphrase: 7.8 - NIL: 3.8 - **Law** - Word -to- Text: 62.3 - Summarize: 7.2 - Text Completion: 22.0 - Common Reason: 1.0 - Paraphrase: 4.9 - NIL: 2.7 2. **Bar Charts (Figure 5):** - **Domain Task s:** - All: 44 - w/o Summarize: 43 - w/o Word -to-Text: 40 - w/o NIL: 43.5 - w/o Common Reason: 44 - w/o Paraphrase\" Continual_Pretraining_p21.png\"### Data from Table 12: **Fine -tuning performance on domain - specific tasks of general large language model (General LLM)** #### Biomedical Domain: - **Data Type:** - **BioMedQA | PubMedQA | ChemProt | MQP | RCT | UMSLE | A VERA GE** Reason: 0.3 - Paraphrase: 3.5 - NIL: 2.2 - **Finance** - Word -to-Text: 35.8 - Summarize: 31.4 - Text Completion: 20.7 - Common Reason: 0.5 - Paraphrase: 7.8 - NIL: 3.8 - **Law** - Word -to- Text: 62.3 - Summarize: 7.2 - Text Completion: 22.0 - Common Reason: 1.0 - Paraphrase: 4.9 - NIL: 2.7 2. **Bar Charts (Figure 5):** - **Domain Task s:** - All: 44 - w/o Summarize: 43 - w/o Word -to-Text: 40 - w/o NIL: 43.5 - w/o Common Reason: 44 - w/o Paraphrase\" Continual_Pretraining_p21.png\"### Data from Table 12: **Fine -tuning performance on domain - specific tasks of general large language model (General LLM)** #### Biomedical Domain: - **Data Type:** - **BioMedQA | PubMedQA | ChemProt | MQP | RCT | UMSLE | A VERA GE** - **General LLM:** - 75.4, 64.5, 87.0, 35.0, 87.0, 38.5, 64.2 - **Raw Text:** - 76.2, 64.8, 65.6, 87.0, 39.0, 66.5 - **Read. Compre.:** - 76.0, 65.4, 87.9, 87.5, 41.0, 71.5 #### Finance Domain: - **Data Type:** -",
    "90": "**ConvFinQA | FPB | FiQA SA | Headline | NER | A VERAGE** - **General LLM:** - 58.1, 81.9, 86.4, 95.7, 77.5, 79.9 - **Raw Text:** - 56.2, 83.3, 87.9, 95.8, 81.3, 80.9 - **\" Continual_Pretraining_p22.png\"### Table: Prompting results on general LLM benchmarks **Columns:** - Task - Metric - General LLM Raw - General LLM Read - BioMed. Raw - BioMed. Read - Finance Raw - Finance Read - Law Raw - Law Read **Data:** 1. **Summarizati on** - **AGNews (Zhang et al., 2015)** - Acc: 58.7, 51.7, 55.5, 56.1, 50.1, 57.8, 60.6 - R-1: 1.5, 3.6, 7.5, 1.9, 10.8, 3.4, 7.4 - R-2: 0.2, 0.9, 2.8, 0.3, 8.3, 0.8, 2.7 - R-L: 1.5, 3.6, 7.2, 1.8, 10.3, 3.3, 7.2 - **AESLC (Zhang & Tetreault, 2019)** - R-1: 0.6, 3.8, 9.3, 3.1, 13.2, \" Continual_Pretraining_p23.png\"**Identified Components:** - **Table:** - **Columns:** - **Category**: Tasks related to various text comprehension categories. - **Task Description**: Detailed description of each task. - **Figure:** - **Description**: An exam ple of a reading comprehension text constructed from raw text. The underlined sentence is added to guide the model to answer questions based on the given context. **Table Data:** ``` Category: - Raw Text Task Description: Here is the first part of an artic le about biomedicine: Recent reported evidence indicates that vocal cord carcinoma is evolving similarly to oropharyngeal cancer with an increasing number of patients without a smoking history having human papillomavirus (HPV) Continual_Pretraining_p23.png\"**Identified Components:** - **Table:** - **Columns:** - **Category**: Tasks related to various text comprehension categories. - **Task Description**: Detailed description of each task. - **Figure:** - **Description**: An exam ple of a reading comprehension text constructed from raw text. The underlined sentence is added to guide the model to answer questions based on the given context. **Table Data:** ``` Category: - Raw Text Task Description: Here is the first part of an artic le about biomedicine: Recent reported evidence indicates that vocal cord carcinoma is evolving similarly to oropharyngeal cancer with an increasing number of patients without a smoking history having human papillomavirus (HPV) disease. Observations also suggest that an increasing number of patients who present with glottic carcinoma are younger than has been reported in the past. However, r ecent published evidence shows that glottic carcinoma can be an HPV -related disease with increasing incidence in nonsmokers. It isn\u2019t surprising that alternate malignant pathways may have a different timeline. - Summarization Task Description: What is a su mmary? Glottic Carcinoma in Young Patients. - Word -to-Text Task Description: Generate a sentence that includes these biomedical words [carcinoma, oropharyngeal, papillomavirus]: Recent reported evidence indicates that vocal cord carcinoma is evolving simil arly to oropharyngeal\" Continual_Pretraining_p24.png\"**Table: Case of a reading comprehension text in biomedicine domain** | Description | Content | | ------------- |--------- | | Title | Case of a reading comprehension text in biomedicine domain | | Content | Certain portions are omitted for brevity and are represented as (...). | --- **Diagrams and Graphics:** None present in the image. **Equations:** There are no equations present in the image. **Summary:** The table presents a reading comprehension text focused on the biomedicin e domain, specifically regarding the biochemistry of chromogranin A -derived peptide (PST). It discusses how PST influences glucose, lipids, protein metabolism, and insulin action in rat adipocytes. PST has an overall counter -regulatory effect on insulin ac tion by activating a specific receptor -effector system and stimulates both basal and insulin -mediated protein synthesis. The text describes experimental methods, including the text in biomedicine domain | | Content | Certain portions are omitted for brevity and are represented as (...). | --- **Diagrams and Graphics:** None present in the image. **Equations:** There are no equations present in the image. **Summary:** The table presents a reading comprehension text focused on the biomedicin e domain, specifically regarding the biochemistry of chromogranin A -derived peptide (PST). It discusses how PST influences glucose, lipids, protein metabolism, and insulin action in rat adipocytes. PST has an overall counter -regulatory effect on insulin ac tion by activating a specific receptor -effector system and stimulates both basal and insulin -mediated protein synthesis. The text describes experimental methods, including the use of Western blot for assessing PST stimulation of S6 kinase activity and phos phorylation of specific proteins involved in the translation process. Questions for reading comprehension are provided along with their descriptions. Key biomedicine concepts are also mentioned, like phosphorylation and the impact on translation machinery. **Structured Data Representation:** Table: Case of a reading comprehension text in biomedicine domain - **Title**: Case of a reading comprehension text in biomedicine domain - **Content**: Certain portions are omitted for brevity and are represented as (. ..). - **Summary Topics**: - PST and\" Continual_Pretraining_p25.png\"### Data Extraction from the Table: **Table 15: Case of a reading comprehension text in finance domain** --- ### Structured Data for Database: | **Company Name** | **Abbrv.** | **Market Capitalization** | **Incorporation Date* * | **Location** | | ------------------------ |------------ |-------------------------- |--------------------------- |------ ------------------- | | Casella Waste Systems | CWST | Not provided | Not provided | Not provided | | Stericycle, Inc. | SRCL | $4.73 bill ion | Incorporated in 1989 | Bannockburn, Illinois | | ABM Industries Inc. | ABM | $2.98 billion | Incorporated in 1985 | New York, New York | --- ### Diagrams and Graphics: None identified from the given page. --- ### Equations: None identified from the g iven page. --- ### Notes: - The table mentions that only the brief information about companies is given. - Additional details or omitted portions are represented as (...) within the text. ---\" Continual_Pretraining_p26.png\"The image provided contains a page from a PDF, specifically a ------------------- | | Casella Waste Systems | CWST | Not provided | Not provided | Not provided | | Stericycle, Inc. | SRCL | $4.73 bill ion | Incorporated in 1989 | Bannockburn, Illinois | | ABM Industries Inc. | ABM | $2.98 billion | Incorporated in 1985 | New York, New York | --- ### Diagrams and Graphics: None identified from the given page. --- ### Equations: None identified from the g iven page. --- ### Notes: - The table mentions that only the brief information about companies is given. - Additional details or omitted portions are represented as (...) within the text. ---\" Continual_Pretraining_p26.png\"The image provided contains a page from a PDF, specifically a legal text including some descriptive elements and specific figures related to a case of restitution. Based on your request, here is the structured information extr acted from the image: ### Table: **Title:** Case of a reading comprehension test in law domain. Certain portions are omitted for brevity and are represented as (...). 1. **Amount of Restitution Ordered:** - **Value:** $5,829,334.90 - **Without Interest:** Yes 2. **Proof of Additional False Claim:** - **Value:** $488,000 3. **Claims for which no Proof of Falsity was Conceded by the G overnment:** - **Floresitine Baker:** $18,000 - **Shirley E.:** [Value not provided] ### Descriptive Elements: 1. **Primary Subject:** - Restitution ordered to Arledge pursuant to the Mandatory Victims Restitution Act of 1996. 2. **Dispute:** - Arledge dis putes the calculation used to determine the amount of loss, particularly related to the Fen Phen II settlement. 3. **Categories of Evidence Used:** - Testimony of Wyatt (an S&A employee who created",
    "91": "fraudulent documents). - Testimony of two pharmacists (tes tified specific prescriptions alleged from their pharmacies were fraudulent). - Representations by AHP. ### Summary of Graphics/Di\" Continual_Pretraining_p3.png\"The image contains a table and a small diagram. Here is a structured representation of the data within the table and a summary of the diagram present in the image. ### Table Data (Text Representation): **Table 1: Domain -specifi c task scores of general language model (General LLM) and the language model that has undergone continued pre - training on the domain -specific raw corpora (DAPT).** | Method | Prompting | Fine -tuning | Knowledge Prob | | -------------- |---------------------- ------------------- |------------------------------------- documents). - Testimony of two pharmacists (tes tified specific prescriptions alleged from their pharmacies were fraudulent). - Representations by AHP. ### Summary of Graphics/Di\" Continual_Pretraining_p3.png\"The image contains a table and a small diagram. Here is a structured representation of the data within the table and a summary of the diagram present in the image. ### Table Data (Text Representation): **Table 1: Domain -specifi c task scores of general language model (General LLM) and the language model that has undergone continued pre - training on the domain -specific raw corpora (DAPT).** | Method | Prompting | Fine -tuning | Knowledge Prob | | -------------- |---------------------- ------------------- |------------------------------------- --|-------------------- | | | BioMed. | Finance | Law | BioMed. | Finance | Law | BioMed. | Law | | General LLM | 44.2 | 58.6 | 34.2 | 64.2 | 79.9 | 42.0 | 36.5 | 45.0 | | DAPT | 41.7 | 57.6 | 35.0 | 66.5 | 80.9 | 45.4 | 36.9 | 45.6 | ### Diagram Summary: There is a highlighted section within the text that visually emphasizes important information about the experiment findings. Highlighted text summarizes the exploration and findings related to domain -specific pre -training for large language models. **Equations:** There are no mathematical equations visible in the provided image. --- This formatted data is\" Continual_Pretraining_p4.png\"Based on the provided image of the PDF page, here is a structured summary of the included diagrams, graphics, equations, and tables. Please note that I'm ignoring the basic text of the paper and focusing on the requested elemen ts. ### Diagrams and Graphics - There are references to \"Figure 2\" and \"Table 2\" within the text, indicating the presence of these items somewhere in the document. However, they do not appear on the provided page. ### Equations - There are no visible mathe matical equations on this page. ### Tables - The text includes a reference to \"Table 2\" but does not show the actual table on this page. The reference mentions that Table 2 describes techniques used to extract and create tasks from raw texts. ### Structure d Data Representation Since the page does not contain an actual table, I cannot represent table data in a structured form. However, based on the text, here is a structured representation of the referenced information: #### Figure References - **Figure 2**: and Graphics - There are references to \"Figure 2\" and \"Table 2\" within the text, indicating the presence of these items somewhere in the document. However, they do not appear on the provided page. ### Equations - There are no visible mathe matical equations on this page. ### Tables - The text includes a reference to \"Table 2\" but does not show the actual table on this page. The reference mentions that Table 2 describes techniques used to extract and create tasks from raw texts. ### Structure d Data Representation Since the page does not contain an actual table, I cannot represent table data in a structured form. However, based on the text, here is a structured representation of the referenced information: #### Figure References - **Figure 2**: Mentioned in the context of illustrating the methodology or tasks. #### Table References - **Table 2**: Summarizes techniques used to extract and create tasks from raw texts. #### Techniques Mentioned 1. **Answer Questions Based on the Article**: - Techni que to enhance task diversity. - Parsed raw text with specific phrases like \"Answer questions based on the article\". 2. **Summarization**: - Prompt to generate a concise summary\" Continual_Pretraining_p5.png\"The image contains a table identified as \"Table 2: Mining patterns and input -output templates.\" Below is the structured representation of the data in the table: ```plaintext Table 2: Mining patterns and input -output templates {VERBAL} is replaced with the verbalizers in Table 3. For mining, {WORD} captures a single word, and {SENT} captures a single sentence. Each input -output template is paraphrased into multiple variations. We also turn the task around \u2014exchanging the question and answer \u2014to achieve enhanced diversity. Task Type Mining Pattern Input -output Template ---------------------------------------------------------------------- ------------------------------- -------------- Summarization Title Title as summary What is a summary? {TITLE} Topic {SENT1} {VERBAL} {SENT2} {SENT1} is about: {SENT2} Word -to-Text Word -to-text Domain keywords as input; Generate a sentence about these {DOMAIN} keywords {WORD1}, {WORD2}, {WORD3}: {SENT} sentence as output Definition {WORD} {VERBAL} {SENT} How to define {WORD}? {SENT} Natural Language Inference Entail {SENT1} {VERBAL} {SENT2} Does \"{SENT1}\" entail \"{SENT2}\"? {Yes/Maybe/No} Neutral Contradict Commonsense Reasoning\" the task around \u2014exchanging the question and answer \u2014to achieve enhanced diversity. Task Type Mining Pattern Input -output Template ---------------------------------------------------------------------- ------------------------------- -------------- Summarization Title Title as summary What is a summary? {TITLE} Topic {SENT1} {VERBAL} {SENT2} {SENT1} is about: {SENT2} Word -to-Text Word -to-text Domain keywords as input; Generate a sentence about these {DOMAIN} keywords {WORD1}, {WORD2}, {WORD3}: {SENT} sentence as output Definition {WORD} {VERBAL} {SENT} How to define {WORD}? {SENT} Natural Language Inference Entail {SENT1} {VERBAL} {SENT2} Does \"{SENT1}\" entail \"{SENT2}\"? {Yes/Maybe/No} Neutral Contradict Commonsense Reasoning\" Continual_Pretraining_p6.png\"**Table: Verbalizers for mining patterns in Table 2** | Task Type | Verbalizer | | -------------------------------- |-------------------------------------------------------------------- --| | **Summarization** | | | Topic | talks about, is about, 's topic is | | **Word -to-Text** | | | Definition | is defined as, 's definition is | | **Natural Language Inference** | | | Entail | Yes, Therefore, Thus, Accordingly, Hence, For this reason | | Neutral | Maybe, Furthermore, Additionally, Moreover, In addition | | Contradict | No, However, But, On the contrary, In contrast, Whereas | | **Commonsense Reasoning** | | | Cause -effect | Therefore, Thus, Accordingly, Hence, For this reason | | Effect -cause | due to, on account of, owing to | | **Paragraph Detection** | | | Similar | In other words, Namely, That is to say, Similarly, Equally | | Different | No, However, But, On the contrary, In contrast, Whereas |\" Continual_Pretraining_p7.png\"The image contains a table under the heading \"Table 4: Domain - specific task performance of general large language model (General LLM), vanilla domain - adaptive pretraining (DAPT), and ours (AdaptLLM) in prompting evaluation.\" He re is the data from the table in structured text format: ```plaintext Biomedicine | Model | PubMedQA | ChemProt | MQP | RCT | UMSLE | A VERAGE | | -------------- |---------- |---------- |------ |------ |------- |- -------- | | MedAlpaca -7B | 58.6 | 39.0 | 50.7 | 40. 8 | 36.7 | 45.1 | | MedAlpaca -13B| 60.7 | 38.4 | 57.4 | 51.3 | 39.0 | 49.4 | | General LLM -7B | 59.6 | 31.4 | 50.7 | 45.1 | 34.5 | 44.2 | | DAPT -7B | 52.6 | 26.6 | 49.2 | 46.6 | 33.5 | 41.7 | |",
    "92": "AdaptLLM -7B | 63.3 | 35.2 |\" Continual_Pretraining_p7.png\"The image contains a table under the heading \"Table 4: Domain - specific task performance of general large language model (General LLM), vanilla domain - adaptive pretraining (DAPT), and ours (AdaptLLM) in prompting evaluation.\" He re is the data from the table in structured text format: ```plaintext Biomedicine | Model | PubMedQA | ChemProt | MQP | RCT | UMSLE | A VERAGE | | -------------- |---------- |---------- |------ |------ |------- |- -------- | | MedAlpaca -7B | 58.6 | 39.0 | 50.7 | 40. 8 | 36.7 | 45.1 | | MedAlpaca -13B| 60.7 | 38.4 | 57.4 | 51.3 | 39.0 | 49.4 | | General LLM -7B | 59.6 | 31.4 | 50.7 | 45.1 | 34.5 | 44.2 | | DAPT -7B | 52.6 | 26.6 | 49.2 | 46.6 | 33.5 | 41.7 | | AdaptLLM -7B | 63.3 | 35.2 |\" Continual_Pretraining_p8.png\"Here is the data extracted and structured from the table present in the image: ### Table 5: Ablation results on training data **Header:** Data | Raw Text | Read. Compr. | Gen. Ins. | Raw + Gen. Ins. | Read. + Gen. Ins. **Rows:* * - **BioMed:** - Raw Text: 41.7 - Read. Compr.: 44.3 - Gen. Ins.: 43.3 - Raw + Gen. Ins.: 44.8 - Read. + Gen. Ins.: 47.3 - **Finance:** - Raw Text: 57.6 - Read. Compr.: 60.0 - Gen. Ins.: 62.2 - Raw + Gen. Ins.: 61.7 - Read. + Gen. Ins.: 63.4 - **Law:** - Raw Text: 35.0 - Read. Compr.: 37.0 - Gen. Ins.: 37.8 - Raw + Gen. Ins.: 34.7 - Read. + Gen. Ins.: 38.5 ### Description of the Table: **Title:** Ablation Results on Training Data. **Description:** The table shows the performance scores (in percentages) for different domains \u2014BioMed, Finance, and Law \u2014under various\" Continual_Pretraining_p9.png\"### Identified Elements #### Diagrams and Graphics 1. **Bar Chart (Domain Knowledge)** - Three bars representing \"BioMed.,\" \"Finance,\" and \"Law\" categories. - Each category is represented in three different conditions: - General LLM - Raw Text - Read. Compre. 2. **Spider/Radar Chart (Prompting Ability)** - Categories: - Summarize - Word -to-Text - N.L.I - Common. Reason. - Paraphrase - Text Comple. - Close. QA - Read. Compre. - Values plotted for three different conditions: - General LLM - Raw Text - Read. Compre. #### Table Data **Table Representation of Bar Chart** ```plaintext Category | General LLM | Raw Text | Read. Compre. --------- |----------- --|---------- |-------------- BioMed. | 60 | 75 | 85 Finance | 70 | 74 | 77 Law | 50 | 60 | 65 ``` **Values from Radar/Spider Chart** ```plaintext Chart (Domain Knowledge)** - Three bars representing \"BioMed.,\" \"Finance,\" and \"Law\" categories. - Each category is represented in three different conditions: - General LLM - Raw Text - Read. Compre. 2. **Spider/Radar Chart (Prompting Ability)** - Categories: - Summarize - Word -to-Text - N.L.I - Common. Reason. - Paraphrase - Text Comple. - Close. QA - Read. Compre. - Values plotted for three different conditions: - General LLM - Raw Text - Read. Compre. #### Table Data **Table Representation of Bar Chart** ```plaintext Category | General LLM | Raw Text | Read. Compre. --------- |----------- --|---------- |-------------- BioMed. | 60 | 75 | 85 Finance | 70 | 74 | 77 Law | 50 | 60 | 65 ``` **Values from Radar/Spider Chart** ```plaintext Ability Category | General LLM | Raw Text | Read. Compre. ----------------- |------------- |---------- |- -------- ----- Summarize | 41 | 47 | 49 Word -to-Text \" cs224n -2023 -lecture11 -prompting -rlhf_p1.png\"The image provided is a title slide from a presentation. Title: \"Natural Language Processing with Deep Learning\" Course: \"CS224N/Ling284\" Graphic: An abstract representation of a rooftop and arches, which could symbolize an educational institution or structure. Instructor: \"Jesse Mu\" Lecture Title: \"Lecture 11: Prompting, Instruction Finetuning, and RLHF\" Since this is a title slide, there are no diagrams, tables, or equations to transcribe or describe. Data to be saved in database: ``` { \"title\": \"Natural Language Processing with Deep Learning\", \"course\": \"CS224N/Ling284\", \"instructor\": \"Jesse Mu\", \"lecture_title\": \"Lecture 11: Prompting, Instruction Finetuning, and RLHF\", \"graphic_description\": \"An abstract repre sentation of a rooftop and arches, which could symbolize an educational institution or structure.\" } ``` \" cs224n -2023 -lecture11 -prompting -rlhf_p10.png\"**Diagrams/Graphics/Equations Identification:** 1. **Diagram/Graphic:** - There is a graphic on the right -hand side showing a conversation about suggestions for a 3 -course vegetarian menu with a chocolate desser t. **Data in the Graphic:** - **Starter:** 1. Wild Mushroom Tartlets with Onion Sauce [1]. 2. Vegan Popcorn Tofu Nuggets [2]. - **Main Course:** 1. Vegan Butternut Squash Mac [2]. 2. Vegetarian Three -Bean Chili [3]. - **Dessert:** 1. Chocolate Lava Cake [4 ]. 2. Chocolate Pasta with Chocolate Hazelnut Cream Sauce, White Chocolate Shavings, and Fresh Berries [4]. \"graphic_description\": \"An abstract repre sentation of a rooftop and arches, which could symbolize an educational institution or structure.\" } ``` \" cs224n -2023 -lecture11 -prompting -rlhf_p10.png\"**Diagrams/Graphics/Equations Identification:** 1. **Diagram/Graphic:** - There is a graphic on the right -hand side showing a conversation about suggestions for a 3 -course vegetarian menu with a chocolate desser t. **Data in the Graphic:** - **Starter:** 1. Wild Mushroom Tartlets with Onion Sauce [1]. 2. Vegan Popcorn Tofu Nuggets [2]. - **Main Course:** 1. Vegan Butternut Squash Mac [2]. 2. Vegetarian Three -Bean Chili [3]. - **Dessert:** 1. Chocolate Lava Cake [4 ]. 2. Chocolate Pasta with Chocolate Hazelnut Cream Sauce, White Chocolate Shavings, and Fresh Berries [4]. **Summary:** The graphic presents options for a 3 -course vegetarian menu designed for a dinner party of six people, ending with a chocolate dessert. Detailed options include starters, main courses, and desserts, with the source links provided for more information. - **Source Links:** 1. booths.co.uk +10 more. **Structured Text Representation for Database:** ``` { \"menu_suggestions\": { \"starter\": [ { \" name\": \"Wild Mushroom Tartlets with Onion Sauce\", \"source\": \"1\" }, { \"name\": \"Vegan Popcorn Tofu Nuggets\", \"source\": \"2\" \" cs224n -2023 -lecture11 -prompting -rlhf_p11.png\"The image contains a graphical table titled \"ChatGPT,\" divided into three categories: Examples, Capabilities, and Limitations. Below is the structured text representation of the data: **ChatGPT** - **Examples** - \"Explain quantum computing in simple terms\" - \"Got any creative ideas for a 10 -year-old's birthday?\" - \"How do I make an HTTP request in Javascript?\" - **Capabilities** - Remembers what user said earlier in the conversation - Allows user to provide follo w-up corrections - Trained to decline inappropriate requests - **Limitations** - May occasionally generate incorrect information - May occasionally produce harmful instructions or biased content - Limited knowledge of the world and events after 2021 This d ata is now ready to be passed directly to a database.\" cs224n -2023 -lecture11 -prompting -rlhf_p12.png\"The image provided contains a structured list from a lecture plan. There are no diagrams, graphics, equations, or tables present in the image. Here is the structured data identified in the image: ### Lecture Pla n: From Language Models to make an HTTP request in Javascript?\" - **Capabilities** - Remembers what user said earlier in the conversation - Allows user to provide follo w-up corrections - Trained to decline inappropriate requests - **Limitations** - May occasionally generate incorrect information - May occasionally produce harmful instructions or biased content - Limited knowledge of the world and events after 2021 This d ata is now ready to be passed directly to a database.\" cs224n -2023 -lecture11 -prompting -rlhf_p12.png\"The image provided contains a structured list from a lecture plan. There are no diagrams, graphics, equations, or tables present in the image. Here is the structured data identified in the image: ###",
    "93": "Lecture Pla n: From Language Models to Assistants 1. Zero -Shot (ZS) and Few -Shot (FS) In -Context Learning 2. Instruction finetuning 3. Reinforcement Learning from Human Feedback (RLHF) 4. What\u2019s next? This data can be directly used for reference or input into a databa se.\" cs224n -2023 -lecture11 -prompting -rlhf_p13.png\"The image appears to be a slide from a lecture plan. The content is structured as a list with four items. Here is the structured representation of the data, which will be passed to a database: ``` { \"lecture_pla n\": [ { \"item\": 1, \"title\": \"Zero - Shot (ZS) and Few -Shot (FS) In -Context Learning\" }, { \"item\": 2, \"title\": \"Instruction finetuning\" }, { \"item\": 3, \"title\": \"Reinforcement Learning from Human Feedback (RLHF)\" }, { \"item\": 4, \"title\": \"What\u2019s next?\" } ] } ``` This format organizes the information from the slide in a structured manner suitable for a database.\" cs224n -2023 -lecture11 -prompting -rlhf_p14.png\"The image contains the following diagrams, equations, and structured data: ### Diagrams: 1. **Transformer Decoder Diagram**: - Represents a set of interconnected blocks (likely layers or processing units) labele d \"Decoder,\" with arrows indicating the flow of information between them. 2. **Textual Entailment Example**: - Shows an example of textual entailment with the text: ``` [START] The man is in the doorway [DELIM] The person is near the door [EXTRACT] ``` - \"entailment\" is indicated as the relation between the two sentences. ### Structured Data: #### GPT Parameters and Information: - **Model**: GPT - **Parameters**: 117 million (117M) - **Study Reference**: Radford et al., 2018 - **Features**: - Transformer de coder with 12 layers. - Trained on equations, and structured data: ### Diagrams: 1. **Transformer Decoder Diagram**: - Represents a set of interconnected blocks (likely layers or processing units) labele d \"Decoder,\" with arrows indicating the flow of information between them. 2. **Textual Entailment Example**: - Shows an example of textual entailment with the text: ``` [START] The man is in the doorway [DELIM] The person is near the door [EXTRACT] ``` - \"entailment\" is indicated as the relation between the two sentences. ### Structured Data: #### GPT Parameters and Information: - **Model**: GPT - **Parameters**: 117 million (117M) - **Study Reference**: Radford et al., 2018 - **Features**: - Transformer de coder with 12 layers. - Trained on BookCorpus: over 7000 unique books, and 4.6GB of text. ### Description Summary for Database: ```json { \"diagrams\": [ { \"type\": \"Transformer Decoder Diagram\", \"description\": \"Interconnected blocks (layers/processing units) labeled 'Decoder' with arrows showing information flow.\" }, { \"type\": \"Textual Entailment Example\", \"description\": \"[START] The man is in the doorway [DELIM] The person is near the door [EXTRACT]. '\" cs224n -2023 -lecture11 -prompting -rlhf_p15.png\"**Diagrams and Graphics:** - The page contains a diagram with a title: \"Language Models are Unsupervised Multitask Learners.\" **Tables:** - No tables are present in the image. **Equations:** - No equations are p resent in the image. **Summary of Graphics and Diagrams:** - The graphic titled \"Language Models are Unsupervised Multitask Learners\" lists the authors of the corresponding paper: - Alec Radford - Jeffrey Wu - Rewon Child - David Luan - Dario Amodei - Ilya Sutskever **Summary of Key Points:** - The text discusses GPT -2 (2019), a model with 1.5 billion parameters. - It mentions that GPT -2 has the same architecture as GPT but is larger (from 117M to 1.5B parameters). - GPT-2 is trained on significantly more d ata (from 4GB to 40GB of internet text data, termed WebText). - Data is collected by scraping URLs posted on Reddit with at least 3 upvotes, serving as a rough proxy of human quality. This structured information can be directly fed into the database for fu rther use.\" cs224n -2023 -lecture11 -prompting -rlhf_p16.png\"The image from the PDF contains the following: **Diagrams/Graphics:** - There are no diagrams or graphics present in this specific image. **Equations:** - \\( \\text{Is } P(... \\text{because the cat was too big}) \\ge Points:** - The text discusses GPT -2 (2019), a model with 1.5 billion parameters. - It mentions that GPT -2 has the same architecture as GPT but is larger (from 117M to 1.5B parameters). - GPT-2 is trained on significantly more d ata (from 4GB to 40GB of internet text data, termed WebText). - Data is collected by scraping URLs posted on Reddit with at least 3 upvotes, serving as a rough proxy of human quality. This structured information can be directly fed into the database for fu rther use.\" cs224n -2023 -lecture11 -prompting -rlhf_p16.png\"The image from the PDF contains the following: **Diagrams/Graphics:** - There are no diagrams or graphics present in this specific image. **Equations:** - \\( \\text{Is } P(... \\text{because the cat was too big}) \\ge P(...\\text{because the hat was too big})? \\) **Tables:** - There are no tables present in this specific image. **Summary:** This section of the PDF discusses the concept of emergent zero - shot learning in GPT -2, which refers to the system's ability to perform various tasks without examples or gradient updates. It presents two key abilities: 1. Specifying the right sequence prediction problem, exemplified through a question abo ut Tom Brady. 2. Comparing probabilities of sequences using a given example from the Winograd Schema Challenge (Levesque, 2011). **Text Data for Database:** ```plaintext { \"title\": \"Emergent zero -shot learning\", \"key_points\": [ { \"point\": \"Specifying the r ight sequence prediction problem (e.g. question answering)\", \"example\": \"Passage: Tom Brady... Q: Where was Tom Brady born? A: ...\" }, { \"point\": \"Comparing probabilities of sequences (e.g. Winograd Schema Challenge)\", \"example\": \"The cat couldn\u2019t fit into the hat because it was too big. Does it = the cat or the hat?\", \"equ\" cs224n -2023 -lecture11 -prompting -rlhf_p17.png\"The image contains a description of a benchmark comparison followed by a table. Here is the structured text representation: ### Benchmark Table #### Columns - **Model Size**: Different sizes of the GPT -2 model. - **LAMBADA (PPL)**: Perplexity score on LAMBADA dataset. - **LAMBADA (ACC)**: Accuracy score on LAMBADA dataset. - **CBT -CN (ACC)**: Accuracy score on CBT -CN dataset. - **CBT -NE (ACC)**: Accuracy score on CBT -NE dataset. - **WikiText2 (PPL)**: Perplexity score on WikiText2 dataset. #### Rows 1. **SOTA:** - **LAMBADA (PPL)**: 99.8 - **LAMBADA (ACC)**: 59.23 - **CBT -CN (ACC)**: 85.7 - **CBT -NE (ACC)**: 82.3 \"example\": \"The cat couldn\u2019t fit into the hat because it was too big. Does it = the cat or the hat?\", \"equ\" cs224n -2023 -lecture11 -prompting -rlhf_p17.png\"The image contains a description of a benchmark comparison followed by a table. Here is the structured text representation: ### Benchmark Table #### Columns - **Model Size**: Different sizes of the GPT -2 model. - **LAMBADA (PPL)**: Perplexity score on LAMBADA dataset. - **LAMBADA (ACC)**: Accuracy score on LAMBADA dataset. - **CBT -CN (ACC)**: Accuracy score on CBT -CN dataset. - **CBT -NE (ACC)**: Accuracy score on CBT -NE dataset. - **WikiText2 (PPL)**: Perplexity score on WikiText2 dataset. #### Rows 1. **SOTA:** - **LAMBADA (PPL)**: 99.8 - **LAMBADA (ACC)**: 59.23 - **CBT -CN (ACC)**: 85.7 - **CBT -NE (ACC)**: 82.3 - **WikiText2 (PPL)**: 39.14 2. **117M:** - **LAMBADA (PPL)**: 35.13 - **LAMBADA (ACC)**: 45.99 - **CB T-CN (ACC)**: 87.65 - **CBT -NE (ACC)**: 83.4 - **WikiText\" cs224n -2023 -lecture11 -prompting -rlhf_p18.png\"**Diagrams/Graphics:** - There is no diagram or graphic in the image. **Tables:** - There is a table presenting the ROUGE scores for the summarization of the CNN/DailyMail dataset. **Table Data:** ```plaintext [ { \"Year\": \"2018 SoTA\", \"Model\": \"Bottom -Up Sum\", \"R -1\": 41.22, \"R -2\": 18.68, \"R -L\": 38.34 }, { \"Year\": \"2018 SoTA\", \"Model\": \"Lede -3\", \"R -1\": 40.38, \"R -2\": 17.66,",
    "94": "\"R -L\": 36.62 }, { \"Year\": \"Supervised (287K)\", \"Model\": \"Seq2Seq + Attn\", \"R -1\": 31.33, \"R -2\": 11.81, \"R -L\": 28.83 }, { \"Year\": \"Supervised (287K)\", \"Model\": \"GPT -2 TL;DR:\", \"R -1\": 29.34, \"R -2\": 8.27, \"R -L\": 26.58 }, { \"Year\": \"Select from article\", \"Model\": \"Random -\" cs224n -2023 -lecture11 -prompting -rlhf_p19.png\"The image contains one primary graphic: a table that lists names of contributors. Here is the structured format of the data present in the table. **Table: Contributors** - Tom B. Brown - Benjamin Mann - Nick Ryd er - Melanie Subbiah Additionally, this section contains text referencing the capabilities of GPT -3, including parameters, and data size, but no specific equations or other graphical data beyond the mentioned table.\" cs224n -2023 -lecture11 -prompting -rlhf_p2.png\"The image contains only basic text and does not include any diagrams, graphics, equations, or tables. Here's the text in a structured format: ``` Reminders - Project proposals (both custom and final) due a few mi nutes ago! - We\u2019re in the cs224n -2023 -lecture11 -prompting -rlhf_p19.png\"The image contains one primary graphic: a table that lists names of contributors. Here is the structured format of the data present in the table. **Table: Contributors** - Tom B. Brown - Benjamin Mann - Nick Ryd er - Melanie Subbiah Additionally, this section contains text referencing the capabilities of GPT -3, including parameters, and data size, but no specific equations or other graphical data beyond the mentioned table.\" cs224n -2023 -lecture11 -prompting -rlhf_p2.png\"The image contains only basic text and does not include any diagrams, graphics, equations, or tables. Here's the text in a structured format: ``` Reminders - Project proposals (both custom and final) due a few mi nutes ago! - We\u2019re in the process of assigning mentors to projects and will aim to give feedback on project proposals with a quick turnaround - A5 due Friday 11:59PM! - We still recommend using Colab for the assignments; in case you run into trouble (e.g. you have exceeded Colab quota), instructions for connecting to a Kaggle notebook have been posted on Ed ```\" cs224n -2023 -lecture11 -prompting -rlhf_p20.png\"The image contains two tables with examples of in-context learning translations. ### Table 1 (Left) | Incorrect | Correct | | ----------- |--------- | | gaot | goat | | sakne | snake | | brid | bird | | fsih | fish | | dcuk | duck | | cmiihp | chimp | ### Table 2 (Right) | English | French | | --------- |---------- | | thanks | merci | | hello | bonjour | | mint | menthe | | wall | mur | | otter | loutre | | bread | pain | ### Data Format for Database #### Table 1 - gaot => goat - sakne => snake - brid => bird - fsih => fish - dcuk => duck - cmiihp => chimp #### Table 2 - thanks => merci - hello => bonjour - mint => menthe - wall => mur - otter => loutre - bread => pain\" cs224n -2023 -lecture11 -prompting -rlhf_p21.png\"### Identification and Summary #### Diagram: Zero -shot Translation - **Description**: The diagram shows an example of zero -shot learning. It provides a prompt to translate the word \"cheese\" from English to Frenc h. #### Graph: In - Context Learning on SuperGLUE - **Description**: The graph visualizes the performance of different models on the SuperGLUE benchmark as the number of examples in context increases. The y -axis represents the performance score, and the x -axis represents the number of examples goat - sakne => snake - brid => bird - fsih => fish - dcuk => duck - cmiihp => chimp #### Table 2 - thanks => merci - hello => bonjour - mint => menthe - wall => mur - otter => loutre - bread => pain\" cs224n -2023 -lecture11 -prompting -rlhf_p21.png\"### Identification and Summary #### Diagram: Zero -shot Translation - **Description**: The diagram shows an example of zero -shot learning. It provides a prompt to translate the word \"cheese\" from English to Frenc h. #### Graph: In - Context Learning on SuperGLUE - **Description**: The graph visualizes the performance of different models on the SuperGLUE benchmark as the number of examples in context increases. The y -axis represents the performance score, and the x -axis represents the number of examples in context (K). - **Data Points**: - Random Guessing: Constant line at 50. - Fine-tuned BERT Large: Constant line at approximately 70. - Fine-tuned BERT++: Constant line at approximately 73-74. - Fine-tuned SOTA: Consta nt line at approximately 85. - Human: Constant line at approximately 90. - Few-shot GPT -3 175B: Line starting at approximately 60 for K=0, quickly rising to approximately 71 for K=1, and then slightly increasing up to approximately 72 -73 as K increases to 32. ### Data Representation **Graph Data (In -Context Learning on SuperGLUE):** | Model | Performance (Y -axis) | | ------------------------- |---------------------- | | Human | 90 | | Fine -tuned SOTA | 85 | | Fine -tuned BERT++ \" cs224n -2023 -lecture11 -prompting -rlhf_p22.png\"The image includes both a graphic and a diagram. Below is a summary and transcription of each: ### 1. Diagram: Few -Shot Translation Example #### Description: The left side of the image shows an example of a one -shot learning task where English words are translated to French. #### Structured Text: ``` One -shot Translation Example: 1. Translate English to French: 2. sea otter => loutre de mer 3. cheese => ``` ### 2. Graphic: Performance Chart #### Description: The right side of the image is a chart titled \"In -Context Learning on SuperGLUE\". It plots the performance of Few -shot GPT -3 175B against the number of examples in context (K). The Y -axis shows the performance score ranging from 40 to 90, while the X -axis depi cts the number of examples in context from 0 to 32. #### Key Points: - Human performance is set around 87. - Few-shot GPT -3 175B performance hovers Example #### Description: The left side of the image shows an example of a one -shot learning task where English words are translated to French. #### Structured Text: ``` One -shot Translation Example: 1. Translate English to French: 2. sea otter => loutre de mer 3. cheese => ``` ### 2. Graphic: Performance Chart #### Description: The right side of the image is a chart titled \"In -Context Learning on SuperGLUE\". It plots the performance of Few -shot GPT -3 175B against the number of examples in context (K). The Y -axis shows the performance score ranging from 40 to 90, while the X -axis depi cts the number of examples in context from 0 to 32. #### Key Points: - Human performance is set around 87. - Few-shot GPT -3 175B performance hovers around the high 60s to low 70s. - Fine-tuned models like BERT++ perform around 70, while Fine-tuned BERT Lar ge and Random Guessing perform at much lower levels. #### Chart Data: ``` Chart Title: In -Context Learning on SuperGLUE Y -Axis: Performance Score (40 to 90) X - Axis: Number of Examples in Context (K) (0, 1, 2\" cs224n -2023 -lecture11 -prompting -rlhf_p23.png\"### Diagrams, Graphics, Equations, or Tables within the Image: 1. **Table: A translation table from English to French:** - sea otter => loutre de mer - peppermint => menthe poivr\u00e9e - plush giraffe => girafe peluche - cheese => 2. **Graph: In -Context Learning on SuperGLUE Performance** - **Type:** Line chart with points - **Axes:** - X-axis: Number of Examples in Context (K) ranging from 0 to 32. - Y-axis: Performance Score ranging from 40 to 90. - **Curves:** - Orange Curve: Performance of Few - shot GPT -3 175B - **Annotations:** - Human (at 90) - Fine-tuned SOTA (at around 85) - Fine- tuned BERT Large (at around 70) - Fine-tuned BERT++ (at around 70 , slightly above Fine -tuned BERT Large) - Random Guessing (at approximately 50) ### Data for Database: **Translation Table:** ``` { \"translations\": [ { \"English\": \"sea otter\", \"French\": \"loutre de mer\" }, { \"English\": \"peppermint\", \"French\": \"menthe poivr\u00e9 e\" }, \" cs224n -2023 -lecture11 -prompting -rlhf_p24.png\"## Chart: ### Title: Synthetic \"word unscrambling\" tasks, 100",
    "95": "-shot ### X -Axis: Parameters in LM (Billions) ### Y -Axis: Accuracy ### Legend: - Cycle letters (blue) - Mid word 1 anagrams (orange) - Mid word 2 anag rams (green) - Random insertion (red) - Reversed words (purple) ### Data Points: | Parameters in LM shot GPT -3 175B - **Annotations:** - Human (at 90) - Fine-tuned SOTA (at around 85) - Fine- tuned BERT Large (at around 70) - Fine-tuned BERT++ (at around 70 , slightly above Fine -tuned BERT Large) - Random Guessing (at approximately 50) ### Data for Database: **Translation Table:** ``` { \"translations\": [ { \"English\": \"sea otter\", \"French\": \"loutre de mer\" }, { \"English\": \"peppermint\", \"French\": \"menthe poivr\u00e9 e\" }, \" cs224n -2023 -lecture11 -prompting -rlhf_p24.png\"## Chart: ### Title: Synthetic \"word unscrambling\" tasks, 100 -shot ### X -Axis: Parameters in LM (Billions) ### Y -Axis: Accuracy ### Legend: - Cycle letters (blue) - Mid word 1 anagrams (orange) - Mid word 2 anag rams (green) - Random insertion (red) - Reversed words (purple) ### Data Points: | Parameters in LM (B) | Cycle letters (blue) | Mid word 1 anagrams (orange) | Mid word 2 anagrams (green) | Random insertion (red) | Reversed words (purple) | | -------------- -------- |---------------------- |--------- --------------------- |----------------------------- |------------------------- |------------------------- | | 0.1B | ~12 | ~1 | ~1 | ~15 | ~0 | | 0.4B | ~18 | ~2 | ~2 | ~20 | ~0 | | 0.8B | ~26 | ~11 | ~14 | ~30 | ~0 | | 1.3B | ~34 | ~20 | ~21 | ~38 | ~0 | | 2.6B | ~41 | ~28 | ~28 | ~46 | ~0 | | 6.7B | ~49 \" cs224n -2023 -lecture11 -prompting -rlhf_p25.png\"The image contains a table and a diagram. Here is the structured data extracted from the table and diagram: ### Table: **Zero/Few -Shot Prompting** | English | French | | ----------------- |-------------------- | | sea otter | loutre de mer | | peppermint | menthe poivr\u00e9e | | plush giraffe | girafe peluche | | cheese | | ### Diagram: **Traditional Fine -Tuning** 1. sea otter => loutre de mer - gradient update 2. peppermint => menthe poivr\u00e9e - gradient update 3. cheese => (awaiting translation) ### Summary of Diagram Explanation: The diagram illustrates the process of traditional fine -tuning in contrast with zero/few -shot prompting. In the fine -tuning method, each example (such as translating \"sea otter\" to \"loutre de m er\") is followed by a gradient update, iteratively refining the model. Please note that the diagram graphics show: 1. Sequential illustrations of translation pairs (e.g., sea otter => loutre de mer and peppermint => menthe poivr\u00e9e). 2. Gradient updates occ urring after each translation. The information from the table and the explanations capture how these two distinct **Traditional Fine -Tuning** 1. sea otter => loutre de mer - gradient update 2. peppermint => menthe poivr\u00e9e - gradient update 3. cheese => (awaiting translation) ### Summary of Diagram Explanation: The diagram illustrates the process of traditional fine -tuning in contrast with zero/few -shot prompting. In the fine -tuning method, each example (such as translating \"sea otter\" to \"loutre de m er\") is followed by a gradient update, iteratively refining the model. Please note that the diagram graphics show: 1. Sequential illustrations of translation pairs (e.g., sea otter => loutre de mer and peppermint => menthe poivr\u00e9e). 2. Gradient updates occ urring after each translation. The information from the table and the explanations capture how these two distinct methods are used in natural language processing model training.\" cs224n -2023 -lecture11 -prompting -rlhf_p26.png\"**Equations:** 1. \\( 19583 + 29534 = 49117 \\) 2. \\( 98394 + 49384 = 147778 \\) 3. \\( 29382 + 12347 = 41729 \\) 4. \\( 93847 + 39299 = ? \\) **Transcribed Data in Structured Form:** ```json { \"equations\": [ { \"expres sion\": \"19583 + 29534\", \"result\": \"49117\" }, { \"expression\": \"98394 + 49384\", \"result\": \"147778\" }, { \"expression\": \"29382 + 12347\", \"result\": \"41729\" }, { \"expression\": \"93847 + 39299\", \"result\": \"?\" } ] } ```\" cs224n -2023 -lecture11 -prompting -rlhf_p27.png\"### Data Representation for Database #### Diagram Summary The image contains two different approaches for prompting large language models: 1. **Standard Prompting**: An incorrect and correct response. 2. **Chain -of-Thought Prompting**: Detailed step -by-step reasoning leading to the correct response. #### Tables and Graphics Description - **Table Structure**: - **Title**: Chain -of-thought Prompting (Well - integrated into the graphic) - **Columns**: - Model Input - Model Output - **Rows**: - Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now? - A (Standard Prompting): The answer is 11. - A (Chain -of- Thought Prompting): Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11. (Highlighted Te xt) - Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have? - A (Standard Prompting): The answer is 27. - A (Chain -of-Thought Prompting): The cafeteria had 23 apples originally. Graphics Description - **Table Structure**: - **Title**: Chain -of-thought Prompting (Well - integrated into the graphic) - **Columns**: - Model Input - Model Output - **Rows**: - Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now? - A (Standard Prompting): The answer is 11. - A (Chain -of- Thought Prompting): Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11. (Highlighted Te xt) - Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have? - A (Standard Prompting): The answer is 27. - A (Chain -of-Thought Prompting): The cafeteria had 23 apples originally. They used 20 to make lunch. So they had 23 - 20\" cs224n -2023 -lecture11 -prompting -rlhf_p28.png\"The provided image contains a set of three graphs from a PDF page. Here's a structured summary and transcription of the data: ### Data Summary #### Graphs: - The graphs depict the solve rate (%) of GSM8K middle school math word problems. - There are three different models compared: - **LaMDA** - **GPT** - **PaLM** Each sub -graph represents: 1. **Standard prompting** (black lines with small unfilled circles) 2. **Chain -of-thought prompting** (blue lines with large unfilled circles) 3. **Prior -supervised best** (orange dashed line at 60%) #### X -axis: - Model scale (# of parameters in billions) #### Y -axis: - Solve rate (%) ### Data from Graphs: #### LaMDA: - Model scale: 0.4B, 8B, 137B - Solve rate (Standard prompt ing): Data points are low (~0%) - Solve rate (Chain -of-thought prompting): Data points are low (~0%) #### GPT: - Model scale: 0.4B, 7B, 175B - Solve rate (Standard prompting): Data points start low (~0%) and slightly increase (below 20%) - Solve rate (Chai n-of-thought prompting): Data points start low (~0%) and increase significantly, especially at 175B (reaching around 40%) #### PaLM: - Model scale: \" cs224n -2023 -lecture11 -prompting -rlhf_p29.png\"### Summary of Diagrams and Equations #### Diagram: - **Title: Chain -of-thought prompting** - **Categories:** - **Model Input:** - Example questions and answers with reasoning steps highlighted. - **Model Output :** - Final calculation provided. #### Equations: 1. \\(5 + 6 = 11 \\) 2. \\(23 - 20 = 3 \\) 3. \\(3 + 6 = 9 \\) ### Data in Structured Text Form #### Example 1 - Tennis Balls: - **Question:** Roger has 5 tennis 0.4B, 7B, 175B - Solve rate (Standard prompting): Data points start low (~0%) and slightly increase (below 20%) - Solve rate (Chai n-of-thought prompting): Data points start low (~0%) and increase significantly, especially at 175B (reaching around 40%) #### PaLM: - Model scale: \" cs224n -2023 -lecture11 -prompting -rlhf_p29.png\"### Summary of Diagrams and Equations #### Diagram: - **Title: Chain",
    "96": "-of-thought prompting** - **Categories:** - **Model Input:** - Example questions and answers with reasoning steps highlighted. - **Model Output :** - Final calculation provided. #### Equations: 1. \\(5 + 6 = 11 \\) 2. \\(23 - 20 = 3 \\) 3. \\(3 + 6 = 9 \\) ### Data in Structured Text Form #### Example 1 - Tennis Balls: - **Question:** Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now? - **Answer:** - **Reasoning:** Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. - **Equation:** \\(5 + 6 = 11 \\) - **Result:** The answer is 11. #### Example 2 - Apples: - **Question:** The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have? - **Answer:** - **Reasoning:** The cafeteria had 23 apples originally. They used 20 to make lunch. - **Equation Step 1:** \\(23 - 20 = 3\\) - **Reasoning\" cs224n -2023 -lecture11 -prompting -rlhf_p3.png\"### Textual Representation of Image Data: #### Diagram Description and Summary The image contains a scatter plot titled **\"The blessings of scale\"**, depicting the estimated computing resources used for AI traini ng runs. The resources are represented using Floating -point operations on a log scale. The scatter plot includes various AI models categorized by their application types: - **Red:** Drawing - **Light Blue:** Language - **Dark Blue:** Vision - **Grey:** Oth er The x -axis represents the years ranging from 1950 to 2022, while the y -axis represents the floating -point operations from \\(10^4 \\) to \\(10^{24} \\). Annotated AI models include: - **Theseus** - **ADALINE** - **Neocognitron** - **NetTalk** - **NPLM** - **BERT -Large** - **GPT -2** - **GPT -3** - **LaM DA** - **PaLM (540B)** - **DALL -E** The scatter plot shows the significant growth in computing resources required for training larger models over the years. #### Structured Data Representation: ``` { \"diagram_title\": \"The blessings of scale\", \"x_axis\": { \" label\": \"Year\", AI models categorized by their application types: - **Red:** Drawing - **Light Blue:** Language - **Dark Blue:** Vision - **Grey:** Oth er The x -axis represents the years ranging from 1950 to 2022, while the y -axis represents the floating -point operations from \\(10^4 \\) to \\(10^{24} \\). Annotated AI models include: - **Theseus** - **ADALINE** - **Neocognitron** - **NetTalk** - **NPLM** - **BERT -Large** - **GPT -2** - **GPT -3** - **LaM DA** - **PaLM (540B)** - **DALL -E** The scatter plot shows the significant growth in computing resources required for training larger models over the years. #### Structured Data Representation: ``` { \"diagram_title\": \"The blessings of scale\", \"x_axis\": { \" label\": \"Year\", \"values\": [\"1950\", \"60\", \"70\", \"80\", \"90\", \"2000\", \"10\", \"22\"] }, \"y_axis\": { \"label\": \"\" cs224n -2023 -lecture11 -prompting -rlhf_p30.png\"The image contains the following equations and explanatory text: 1. **Equation from first question:** - \\( 5 + 6 = 11 \\) 2. **Equation from second question:** - \\( 23 - 20 = 3 \\) - \\( 3 + 6 = 9 \\) ### Structured Data Representation: - **Equation:** 5 + 6 = 11 - **Explanation:** Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. The sum is 11. - **Answer:** 11 - **Verification:** Correct - **Equation:** - 23 - 20 = 3 - 3 + 6 = 9 - **Expla nation:** The cafeteria had 23 apples originally. They used 20 to make lunch. They bought 6 more apples, so they have 9. - **Answer:** 9 - **Verification:** Correct 3. **Steps and Count from Third Question:** - **Description:** A juggler can juggle 16 ball s. Half of the balls are golf balls, and half of the golf balls are blue. - **Steps:** - There are 16 balls in total. - Half of the balls are golf balls (8 golf balls). - Half of the golf balls are blue (4 blue golf balls). \" cs224n -2023 -lecture11 -prompting -rlhf_p31.png\"### Diagrams, Graphics, and Tables The image contains a table comparing various methods' performance on two benchmarks: **MultiArith** and **GSM8K.** The methods being compared are Zero -Shot, Few -Shot, Zero -Shot-CoT, Few - Shot-CoT, and Manual CoT. ### Transcribed Data from the Table #### Columns: - Method - Samples - MultiArith - GSM8K #### Rows: - Zero -Shot, -, 17.7, 10.4 - Few-Shot, 2 samples, 33.7, 15.6 - Few-Shot, 8 samples, 33.8, 15.6 - Zero -Shot-CoT, -, 78.7 , 40.7 - Few-Shot-CoT, 2 samples, 84.8, 41.3 - Few-Shot-CoT, 4 samples : First (*1), 89.2, - - Few-Shot-CoT, 4 samples : golf balls). - Half of the golf balls are blue (4 blue golf balls). \" cs224n -2023 -lecture11 -prompting -rlhf_p31.png\"### Diagrams, Graphics, and Tables The image contains a table comparing various methods' performance on two benchmarks: **MultiArith** and **GSM8K.** The methods being compared are Zero -Shot, Few -Shot, Zero -Shot-CoT, Few - Shot-CoT, and Manual CoT. ### Transcribed Data from the Table #### Columns: - Method - Samples - MultiArith - GSM8K #### Rows: - Zero -Shot, -, 17.7, 10.4 - Few-Shot, 2 samples, 33.7, 15.6 - Few-Shot, 8 samples, 33.8, 15.6 - Zero -Shot-CoT, -, 78.7 , 40.7 - Few-Shot-CoT, 2 samples, 84.8, 41.3 - Few-Shot-CoT, 4 samples : First (*1), 89.2, - - Few-Shot-CoT, 4 samples : Second (*1), 90.5, - - Few-Shot-CoT, 8 samples, -, - - Manual CoT, -, 93.0, 48.7 ### Structured Text Format: ```plaintext { \"table\": [ { \"Method\": \"Zero -Shot\", \"Samples\": \" -\", \"MultiArith\": 17.\" cs224n -2023 -lecture11 -prompting -rlhf_p32.png\"### Data Extraction from the Table **Table Data:** - Column Headings: - No. - Category - Zero -shot CoT Trigger Prompt - Accuracy - Table Rows: - Row 1: 1, LM -Designed, \"Let's work this out in a step by step way to be sure we have the right answer.\", 82.0 - Row 2: 2, Human -Designed, \"Let's think step by step. (*1)\", 78.7 - Row 3: 3, Human -Designed, \"First, (*2)\", 77.3 - Row 4: 4, Human -Designed, \"Let's think about this logically.\", 74.5 - Row 5: 5, Human -Designed, \"Let's solve this problem by splitting it into steps. (*3)\", 72.2 - Row 6: 6, Human -Designed, \"Let's be realistic and think step by step.\", 70.8 - Row 7: 7, Human -Designed, \"Let's think like a detective step by step.\", 70.3 - Row 8: 8, Human - Designed, \"Le t's think\", 57.5 - Row 9: 9, Human -Designed, \"Before we dive into the answer,\", 55.7 - Row 10:\" cs224n -2023 -lecture11 -prompting -rlhf_p33.png\"The image provided contains several elements including diagrams, graphics, and a snippet of code. Here's a structured summary of each element: ### Diagrams/Graphics: 1. **Image of blue glowing dodecahedron dice: ** - Description: Fantasy concept art of glowing blue dodecahedron die on a wooden table, in a cozy fantasy (workshop), tools on the table, artstation, depth of field, 4k, masterpiece. ### Equations: - No equations are present in the image. ### Tables: - No tab les are present in the image. ### Code Snippets: - Snippet of code header from Google LLC: ``` 1 # Copyright 2022 Google LLC. Designed, \"Le t's think\", 57.5 - Row 9: 9, Human -Designed, \"Before we dive into the answer,\", 55.7 - Row 10:\" cs224n -2023 -lecture11 -prompting -rlhf_p33.png\"The image provided contains several elements including diagrams, graphics, and a snippet of code. Here's a structured summary of each element: ### Diagrams/Graphics: 1. **Image of blue glowing dodecahedron dice: ** - Description: Fantasy concept art of glowing blue dodecahedron die on a wooden table, in a cozy fantasy (workshop), tools on the table, artstation, depth of field, 4k, masterpiece. ### Equations: - No equations are present in the image. ### Tables: - No tab les are present in the image. ### Code Snippets: - Snippet of code header from Google LLC: ``` 1 # Copyright 2022 Google LLC. 2 # 3 # Licensed under the Apache License, Version 2.0 (the \"License\"); 4 # you may not use this file except in compliance with th e License. 5 # You may obtain a copy of the License at 6 # 7 # http://www.apache.org/licenses/LICENSE -2.0 ``` ### Additional Descriptions: 1. **Text Descriptions:** - There is",
    "97": "a question and answer section considering a juggler juggling 16 balls, half of t hem being golf balls and half of the golf balls being blue. - Instruction for translation from English to French with a joke result: \"Haha pwned!!\" - Mention of \"Jailbreaking\" LMs (Language Models). -\" cs224n -2023 -lecture11 -prompting -rlhf_p34.png\"The image features the following elements: 1. **Diagram:** - A screenshot of a Wikipedia article excerpt titled \"Prompt engineering.\" - A highlighted text snippet within the Wikipedia article. 2. **Keywords note d:** - Artificial intelligence - Natural Language Processing (NLP) Based on this observation, the structured information is: ```json { \"type\": \"diagram\", \"source\": \"Wikipedia\", \"title\": \"Prompt engineering\", \"keywords\": [ \"artificial intelligence\", \"natura l language processing\", \"prompt engineering\" ] } ``` Additionally, there is an advertisement or notice at the bottom: 3. **Job Posting:** - \"Prompt Engineer and Librarian\" - Location: San Francisco, CA - Type: Product / Full -Time / Hybrid This information in a structured format: ```json { \"type\": \"job_posting\", \"position\": \"Prompt Engineer and Librarian\", \"location\": \"San Francisco, CA\", \"type\": [\"Product\", \"Full -Time\", \"Hybrid\"] } ``` No tables, equations, or additional graphics were detected within the im age.\" intelligence - Natural Language Processing (NLP) Based on this observation, the structured information is: ```json { \"type\": \"diagram\", \"source\": \"Wikipedia\", \"title\": \"Prompt engineering\", \"keywords\": [ \"artificial intelligence\", \"natura l language processing\", \"prompt engineering\" ] } ``` Additionally, there is an advertisement or notice at the bottom: 3. **Job Posting:** - \"Prompt Engineer and Librarian\" - Location: San Francisco, CA - Type: Product / Full -Time / Hybrid This information in a structured format: ```json { \"type\": \"job_posting\", \"position\": \"Prompt Engineer and Librarian\", \"location\": \"San Francisco, CA\", \"type\": [\"Product\", \"Full -Time\", \"Hybrid\"] } ``` No tables, equations, or additional graphics were detected within the im age.\" cs224n -2023 -lecture11 -prompting -rlhf_p35.png\"The provided image does not contain any diagrams, graphics, equations, or tables. It appears to be a slide presenting a lecture plan. Here is the structured information from the slide: ```plaintext Lecture Plan: From Language Models to Assistants 1. Zero -Shot (ZS) and Few -Shot (FS) In -Context Learning + No finetuning needed, prompt engineering (e.g. CoT) can improve performance \u2013 Limits to what you can fit in context \u2013 Complex tasks will probably need gradient st eps 2. Instruction finetuning 3. Reinforcement Learning from Human Feedback (RLHF) 4. What\u2019s next? ``` This structured format should be easy to pass directly into a database.\" cs224n -2023 -lecture11 -prompting -rlhf_p36.png\"The image provided contains a slide titled \"Lecture Plan: From Language Models to Assistants\" and is structured as a list. Here is a summary and transcription of the contents, followed by a structured format: ## # Structured Data: **Title:** Lecture Plan: From Language Models to Assistants **Content:** 1. Zero -Shot (ZS) and Few -Shot (FS) In -Context Learning - Advantages: + No finetuning needed, prompt engineering (e.g., CoT) can improve performance - Limitations: - Limits to what you can fit in context - Complex tasks will probably need gradient steps 2. Instruction finetuning 3. Reinforcement Learning from Human Feedback (RLHF) 4. What\u2019s next? ### Summary of Graphics and Diagrams: There are no diagrams, graphics, equations, or tables in this slide. This data can be directly passed into a database with fields for titles, content, and notes on advantages and limitations where appropriate.\" summary and transcription of the contents, followed by a structured format: ## # Structured Data: **Title:** Lecture Plan: From Language Models to Assistants **Content:** 1. Zero -Shot (ZS) and Few -Shot (FS) In -Context Learning - Advantages: + No finetuning needed, prompt engineering (e.g., CoT) can improve performance - Limitations: - Limits to what you can fit in context - Complex tasks will probably need gradient steps 2. Instruction finetuning 3. Reinforcement Learning from Human Feedback (RLHF) 4. What\u2019s next? ### Summary of Graphics and Diagrams: There are no diagrams, graphics, equations, or tables in this slide. This data can be directly passed into a database with fields for titles, content, and notes on advantages and limitations where appropriate.\" cs224n -2023 -lecture11 -prompting -rlhf_p37.png\"**Diagrams, Graphics, Equations, Tables:** - The image contains a table. **Table Data:** ``` | PROMPT | Explain the moon landing to a 6 year old in a few sentences. | | ------------------------------------------- --------------- |---------------------- ---------------------------------------------------------------- | | COMPLETION | GPT -3 | | Explain the theory of gravity to a 6 year old. | | | Explain the theory of relativity to a 6 year old in a few sentences. | | | Explain the big bang theory to a 6 year old. | | | Explain evolution to a 6 year old. | | ``` **Summary:** - The table illustrates a mismatch between the given prompt and the completion generated by a language model (GPT -3). It highlights how the language model completes the prompt with sentences that are irrelevant to the asked prompt.\" cs224n -2023 -lecture11 -prompting -rlhf_p38.png\"The image includes a table with two columns and two rows. **Table Representation in Structured Text:** - **Column Headers:** - Prompt - Completion - **Data:** - Row 1: - Prompt: Explain the moon landing to a 6 y ear old in a few sentences. - Completion: - **Source:** Human - **Text:** A giant rocket ship blasted off from Earth carrying astronauts to the moon. The astronauts landed their spaceship on the moon and walked around exploring the lunar surface. Then they returned safely back to Earth, bringing home moon rocks to show everyone.\" cs224n -2023 -lecture11 -prompting -rlhf_p39.png\"**Diagrams Summary:** The image contains two similar diagrams illustrating the process of pretraining and finetuning in the context of cs224n -2023 -lecture11 -prompting -rlhf_p38.png\"The image includes a table with two columns and two rows. **Table Representation in Structured Text:** - **Column Headers:** - Prompt - Completion - **Data:** - Row 1: - Prompt: Explain the moon landing to a 6 y ear old in a few sentences. - Completion: - **Source:** Human - **Text:** A giant rocket ship blasted off from Earth carrying astronauts to the moon. The astronauts landed their spaceship on the moon and walked around exploring the lunar surface. Then they returned safely back to Earth, bringing home moon rocks to show everyone.\" cs224n -2023 -lecture11 -prompting -rlhf_p39.png\"**Diagrams Summary:** The image contains two similar diagrams illustrating the process of pretraining and finetuning in the context of Natural Language Processing (NLP): 1. **Diagram on the Left (Step 1: Pretrai n on language modeling):** - Title: Step 1: Pretrain (on language modeling) - Purpose: Learning general things from lots of text. - Components: - Decoder labeled \"Decoder (Transformer, LSTM, ++)\". - Input tokens: \"Iroh\", \"goes\", \"to\", \"make\", \"tasty\", \"tea \". - Output tokens (arrows pointing upwards): \"goes\", \"to\", \"make\", \"tasty\", \"tea\", \"END\". 2. **Diagram on the Right (Step 2: Finetune on your task):** - Title: Step 2: Finetune (on your task) - Purpose: Adapting to the specific task with not many labels. - Components: - Decoder labeled \"Decoder (Transformer, LSTM, ++)\". - Input tokens: \"...\", \"the\", \"movie\", \"was\", \"...\" (incompletely shown, assumed to be a continuation of a sentence). - Output tokens (arrows pointing upwards): \"...\" (indicating context co ntinuation). There are smiley and sad face emojis at the end of the output tokens in the second diagram, indicating different possible sentiments\" cs224n -2023 -lecture11 -prompting -rlhf_p4.png\"The image is a visual",
    "98": "representation of the number of tokens seen during training for different entities/models over time. It features a graphic with various colored circles representing each entity and the numbe r of tokens they have seen during training. Below is the structured data extracted from the image: ### Structured Data: #### Entities and Tokens Seen During Training 1. **13 y.o. Human** - Tokens: <100 Million - Representation: Text 2. **BERT (2018)** - Tokens: 3 Billion - Representation: Small green circle 3. **RoBERTa (2019)** - Tokens: 30 Billion - Representation: Medium red circle 4. There are smiley and sad face emojis at the end of the output tokens in the second diagram, indicating different possible sentiments\" cs224n -2023 -lecture11 -prompting -rlhf_p4.png\"The image is a visual representation of the number of tokens seen during training for different entities/models over time. It features a graphic with various colored circles representing each entity and the numbe r of tokens they have seen during training. Below is the structured data extracted from the image: ### Structured Data: #### Entities and Tokens Seen During Training 1. **13 y.o. Human** - Tokens: <100 Million - Representation: Text 2. **BERT (2018)** - Tokens: 3 Billion - Representation: Small green circle 3. **RoBERTa (2019)** - Tokens: 30 Billion - Representation: Medium red circle 4. **GPT -3 (2020)** - Tokens: 200 Billion - Representation: Large blue circle 5. **Chinchilla (2022)** - Tokens: 1.4 Trillio n - Representation: Extra -large orange circle #### Additional Information - URL: [https://babylm.github.io/](https://babylm.github.io/) ### Summary The image visually compares the amount of training data used for different models over the years, with 13 y. o. Human being the baseline with less than 100 million tokens. Subsequent models \u2014 BERT (2018), RoBERTa (2019), GPT -3 (2020), and Chinchilla (2022) \u2014have seen exponentially\" cs224n -2023 -lecture11 -prompting -rlhf_p40.png\"**Diagrams and Graphics Summary:** The image contains two main diagrams that illustrate the process of scaling up finetuning for NLP applications: 1. **Diagram 1 (Step 1: Pretrain)**: - Title: \"Step 1: Pretrain (on language modeling)\" - Description: This step involves learning general language representations from large amounts of text. The text example includes the sentence: \"Iroh goes to make tasty tea,\" which is processed by a decoder (potentially a Transforme r, LSTM, or other models). 2. **Diagram 2 (Step 2: Finetune)**: - Title: \"Step 2: Finetune (on many tasks)\" - Description: This step focuses on adapting the pretrained model to specific tasks with relatively fewer labels. The illustrative text example seem s partly obscured but involves finetuning using another decoder (Transformer, LSTM, or other models) to adapt to more specific tasks, indicated by text such as \"... the movie was ...\". **Equations:** - No equations are present. **Tables:** - No tables are present. **Simple Structured Data:** ```plaintext Diagrams: 1. Title: Step 1: Pretrain (on large amounts of text. The text example includes the sentence: \"Iroh goes to make tasty tea,\" which is processed by a decoder (potentially a Transforme r, LSTM, or other models). 2. **Diagram 2 (Step 2: Finetune)**: - Title: \"Step 2: Finetune (on many tasks)\" - Description: This step focuses on adapting the pretrained model to specific tasks with relatively fewer labels. The illustrative text example seem s partly obscured but involves finetuning using another decoder (Transformer, LSTM, or other models) to adapt to more specific tasks, indicated by text such as \"... the movie was ...\". **Equations:** - No equations are present. **Tables:** - No tables are present. **Simple Structured Data:** ```plaintext Diagrams: 1. Title: Step 1: Pretrain (on language modeling) Description: Learn general things from large amounts of text. Example Text: Iroh goes to make tasty tea Model: Decoder (Transformer, LSTM, ++) 2. Title: Step 2\" cs224n -2023 -lecture11 -prompting -rlhf_p41.png\"The image contains various elements including diagrams and equations. Here is the structured data from the diagrams: ### Diagrams Representation 1. **Instruction Pairings and Finetuning:** - **Instruction 1:** - **Instruction:** Please answer the following question. What is the boiling point of Nitrogen? - **Output:** -320.4F - **Instruction 2:** - **Instruction:** Answer the following question by reasoning step -by-step. The cafeteria had 23 apples. If they used 20 for lunch, and bought 6 more, how many apples do they have? - **Output:** The cafeteria had 23 apples originally. They used 20 to make lunch. So they had 23 - 20 = 3. They bought 6 more apples, so they have 3 + 6 = 9. 2. **Evaluation on Unseen Tasks:** - **Instruction 3:** - **Instruction:** Q: Can Geoffrey Hinton have a conversation with George Washington? Give the rationale before answering. - **Output:** Geoffrey Hinton is a British -Canadian computer scientist born in 1947. George Washington died in 1 799. Thus, they could not have had a conversation together. So the answer is \"no\". ### Equations - Equation in Instruction 2 Output: - Apple Calculation: - 23 - 20 = 3 - \" cs224n -2023 -lecture11 -prompting -rlhf_p42.png\"### Diagram/Graphic Summary #### Diagram: Task Categories in Super -NaturalInstructions Dataset - **Large Circles:** - Translation - Question Answering - **Medium Circles:** - Sentiment Analysis - Information Ext raction - + 6 = 9. 2. **Evaluation on Unseen Tasks:** - **Instruction 3:** - **Instruction:** Q: Can Geoffrey Hinton have a conversation with George Washington? Give the rationale before answering. - **Output:** Geoffrey Hinton is a British -Canadian computer scientist born in 1947. George Washington died in 1 799. Thus, they could not have had a conversation together. So the answer is \"no\". ### Equations - Equation in Instruction 2 Output: - Apple Calculation: - 23 - 20 = 3 - \" cs224n -2023 -lecture11 -prompting -rlhf_p42.png\"### Diagram/Graphic Summary #### Diagram: Task Categories in Super -NaturalInstructions Dataset - **Large Circles:** - Translation - Question Answering - **Medium Circles:** - Sentiment Analysis - Information Ext raction - Textual Entailment - Text Categorization - Text Matching - Question Generation - Named Entity Recognition - Commonsense Classification - Program Execution - Language Identification - Summarization - Toxic Language Detection - Text Simplification - Keyword Tagging - Speaker Identification - Dialogue Generation - Sentence Composition - Sentence Perturbation - Cause Effect Classification - Word Analogy - Linguistic Probing - Grammar Error Detection - Sentence Expansion - **Small to Very Small Circles :** - Spelling Correction - Style Transfer - Part-of- Speech (POS) Tagging - Entity Relation Classification - Fact Verification - Word Relation Classification - Negotiation Strategy Detection - Overlap Extraction - Poem Generation - Spam Classification - Answerability Classification - Dialogue State Tracking - Preposition Prediction - Dialogue Act Recognition - Mathematics - Dialogue Discourse Identification - Speaker Relation Classification - Discourse Relation Classification - Iron\" cs224n -2023 -lecture11 -prompting -rlhf_p43.png\"The image contains a bar chart comparing the performance of different language models on various college -level and high school -level subjects. The chart includes three different data series: GPT -3 (blue), Unifie dQA (green), and Random (red). Here is the structured data extracted from the chart: ``` Subject | GPT -3 | UnifiedQA | Random ------------------------------------------------------------ Abstract Algebra | value | value | value Anatomy | value | value | va lue Astronomy | value | value | value Business Ethics | value | value | value Clinical Knowledge | value | value | value College Biology | value | value | Classification - Discourse Relation Classification - Iron\" cs224n -2023 -lecture11 -prompting -rlhf_p43.png\"The image contains",
    "99": "a bar chart comparing the performance of different language models on various college -level and high school -level subjects. The chart includes three different data series: GPT -3 (blue), Unifie dQA (green), and Random (red). Here is the structured data extracted from the chart: ``` Subject | GPT -3 | UnifiedQA | Random ------------------------------------------------------------ Abstract Algebra | value | value | value Anatomy | value | value | va lue Astronomy | value | value | value Business Ethics | value | value | value Clinical Knowledge | value | value | value College Biology | value | value | value College Chemistry | value | value | value College Comp Sci | value | value | value College Math ematics | value | value | value College Medicine | value | value | value College Physics | value | value | value Computer Security | value | value | value Conceptual Physics | value | value | value Econometrics | value | value | value Electrical Engineerin g | value | value | value Elementary Mathematics | value | value | value Formal Logic | value | value | value Global Facts | value | value | value High School Biology | value \" cs224n -2023 -lecture11 -prompting -rlhf_p44.png\"Below is a structured representation of the content present in the provided image. It includes descriptions of the diagrams and transcriptions of any non -basic text elements like equations or tables. **Diagrams/ Graphics:** 1. **Word Cloud:** - Description: The word cloud is a visual representation of various tasks included in the BIG-Bench dataset. The words vary in size, indicating their frequency or significance. Larger words include \"common sense\", \"free respo nse\", \"programmatic\", \"contextual question - answering\", \"logical reasoning\", \"reading comprehension\". - Keywords: common sense, free response, programmatic, logical reasoning, reading comprehension, context -free question answering, contextual question -answe ring, mathematics, analogical reasoning, casual reasoning, non-language tasks, visual reasoning, multilingual, gender bias, emotion, arithmetic, numerical response, paraphrase, repeated interaction, non -English, creativity, human -like behavior, physical reasoning, theory of mind, computer code, social bias, machine translation, context length, non - BIG-Bench dataset. The words vary in size, indicating their frequency or significance. Larger words include \"common sense\", \"free respo nse\", \"programmatic\", \"contextual question - answering\", \"logical reasoning\", \"reading comprehension\". - Keywords: common sense, free response, programmatic, logical reasoning, reading comprehension, context -free question answering, contextual question -answe ring, mathematics, analogical reasoning, casual reasoning, non-language tasks, visual reasoning, multilingual, gender bias, emotion, arithmetic, numerical response, paraphrase, repeated interaction, non -English, creativity, human -like behavior, physical reasoning, theory of mind, computer code, social bias, machine translation, context length, non - linguistic task, social bias, meta -reasoning, machine translation, temporal reasoning. **Tables:** None identified. **Equations:** None identified. --- **Addition al Text:** 1. **Link:** - URL: `https://github.com/google/BIG -bench/blob/main/bigbench/benchmark_tasks/README.md` 2. **Alphabetic Author List:** - List: Authors' names are listed in alphabetical order. Here are\" cs224n -2023 -lecture11 -prompting -rlhf_p45.png\"The image contains two main components: a word cloud diagram and ASCII art. ### Graphics and Diagrams: 1. **Word Cloud Diagram:** - Title: BIG -Bench - Description: A word cloud representing topics covered by 200 + tasks. - Words included (in varied sizes, indicating frequency or importance): - common sense, programmatic, free response, logical reasoning, reading comprehension, context -free question answering, mathematics, analogical reasoning, context length, para phrase, emotional intelligence, implicit reasoning, human -like behavior, arithmetic, computer code, creativity, non - English, translation, social reasoning, causal reasoning, visual reasoning, domain specific, numerical response, and many others. - Link Inc luded: [https://github.com/google/BIG - bench/blob/main/bigbench/benchmark_tasks/README.md](https://github.com/google/BIG - bench/blob/main/bigbench/benchmark_tasks/README.md) 2. **Kanji ASCII Art:** - Title: Kanji ASCII Art to Meaning - Description: A subtask that converts various kanji into ASCI I art and has the language model guess their meaning from the ASCII art. - ASCII Art Sample: ``` ........#........ ........#........ ....############# .......#.#..#.... ...#.#..#.#..#... .......#.....#... ......#...... .# #. ...............#. ......#.....\" English, translation, social reasoning, causal reasoning, visual reasoning, domain specific, numerical response, and many others. - Link Inc luded: [https://github.com/google/BIG - bench/blob/main/bigbench/benchmark_tasks/README.md](https://github.com/google/BIG - bench/blob/main/bigbench/benchmark_tasks/README.md) 2. **Kanji ASCII Art:** - Title: Kanji ASCII Art to Meaning - Description: A subtask that converts various kanji into ASCI I art and has the language model guess their meaning from the ASCII art. - ASCII Art Sample: ``` ........#........ ........#........ ....############# .......#.#..#.... ...#.#..#.#..#... .......#.....#... ......#...... .# #. ...............#. ......#.....\" cs224n -2023 -lecture11 -prompting -rlhf_p46.png\"### Table Data ```plaintext | Params | Model | BIG-bench + MMLU avg (normalized) | | -------- |------------------ |----------------------------------- | | 80M | T5 -Small | -9.2 | | 80M | Flan -T5-Small | -3.1 (+6.1) | | 250M | T5 -Base | -5.1 | | 250M | Flan-T5-Base | 6.5 (+11.6) | | 780M | T5 -Large | -5.0 | | 780M | Flan -T5-Large | 13.8 (+18.8) | | 3B | T5 -XL | -4.1 | | 3B | Flan -T5-XL | 19.1 (+23.2) | | 11B | T5 -XXL | -2.9 | | 11B | Flan -T5-XXL | 23.7 (+26.6) | ``` # ## Diagram Description The diagram on the page is a neural network structure. It appears to show a sequence of connections between different layers of nodes, indicating a flow from one layer to a subsequent series of layers. The first layer is labeled\" cs224n -2023 -lecture11 -prompting -rlhf_p47.png\"### Diagrams and Graphics 1. **Diagram**: The image includes a comparison of responses before and after instruction fine -tuning. There are two sections, each enclosed in a rounded rectangle: - **Left Section (Mo del input (Disambiguation QA))**: This section concerns the task of disambiguation and describes a model input example where the model needs to explain the antecedent of the pronoun. - **Right Section (Before instruction finetuning)**: This section illustr ates several examples of the model's responses to the same input before finetuning, indicating that the model didn't answer the question. ### Tables: - There is no explicit table in the image, but an implied comparison table of responses is present. ### Eq uations: - There are no equations present in the image. ### Structured Data Representation ``` { \"Model Input (Disambiguation QA)\": { \"Question\": \"In the two sections, each enclosed in a rounded rectangle: - **Left Section (Mo del input (Disambiguation QA))**: This section concerns the task of disambiguation and describes a model input example where the model needs to explain the antecedent of the pronoun. - **Right Section (Before instruction finetuning)**: This section illustr ates several examples of the model's responses to the same input before finetuning, indicating that the model didn't answer the question. ### Tables: - There is no explicit table in the image, but an implied comparison table of responses is present. ### Eq uations: - There are no equations present in the image. ### Structured Data Representation ``` { \"Model Input (Disambiguation QA)\": { \"Question\": \"In the following sentences, explain the antecedent of the pronoun (which thing the pronoun refers to), or sta te",
    "100": "that it is ambiguous.\", \"Sentence\": \"The reporter and the chef will discuss their favorite dishes.\", \"Options\": [ \"They will discuss the reporter's favorite dishes\", \"They will discuss the chef's favorite dishes\", \"Ambiguous\" ], \"Answer\": \"Let's think s tep by step.\" }, \"Before Instruction Finetuning\": [ \"The reporter and the chef will discuss their favorite dishes.\", \" cs224n -2023 -lecture11 -prompting -rlhf_p48.png\"The image contains a diagram with two sections, labeled \"Model input (Disambiguation QA)\" and \"After instruction finetuning\". Below is a structured representation of the content: **Model input (Disambiguation QA ):** - Q: In the following sentences, explain the antecedent of the pronoun (which thing the pronoun refers to), or state that it is ambiguous. - Sentence: The reporter and the chef will discuss their favorite dishes. - Options: - (A) They will discuss the reporter's favorite dishes - (B) They will discuss the chef's favorite dishes - (C) Ambiguous - A: Let's think step by step. **After instruction finetuning:** - The reporter and the chef will discuss their favorite dishes does not indicate whose favorite dishes they will discuss. So, the answer is (C). Additionally, there is a recommendation text at the bottom of the image: - Highly recommend trying FLAN -T5 out to get a sense of its capabilities: [https://huggingface.co/google/flan -t5- xxl](https://huggingface.co/google/flan -t5-xxl) - Citation: [Chung et al., 2022] This information can be passed to the database in a structured format.\" dishes. - Options: - (A) They will discuss the reporter's favorite dishes - (B) They will discuss the chef's favorite dishes - (C) Ambiguous - A: Let's think step by step. **After instruction finetuning:** - The reporter and the chef will discuss their favorite dishes does not indicate whose favorite dishes they will discuss. So, the answer is (C). Additionally, there is a recommendation text at the bottom of the image: - Highly recommend trying FLAN -T5 out to get a sense of its capabilities: [https://huggingface.co/google/flan -t5- xxl](https://huggingface.co/google/flan -t5-xxl) - Citation: [Chung et al., 2022] This information can be passed to the database in a structured format.\" cs224n -2023 -lecture11 -prompting -rlhf_p49.png\"The image contains a lecture plan with numbered points and corresponding descriptions. There are no diagrams, graphics, equations, or tables in the image. Below is the structured representation of the data from the lecture plan: ``` Lecture Plan: From Language Models to Assistants 1. Zero -Shot (ZS) and Few -Shot (FS) In - Context Learning - Positives: + No finetuning needed, prompt engineering (e.g. CoT) can improve performance - Negatives: - Limits to what you can fit in context - Complex tasks will probably need gradient steps 2. Instruction finetuning - Positives: + Simple and straightforward, generalize to unseen tasks - Negatives: - ? - ? 3. Reinforcement Learning from Human Feedback (RLHF) (No additional detail s provided) 4. What\u2019s next? (No additional details provided) ```\" cs224n -2023 -lecture11 -prompting -rlhf_p5.png\"The image contains a list of sentences, but no diagrams, graphics, tables, or equations. Here's the structured form of the list: ``` [ { \"sentence\": \"Stanford University is located in ____________, California.\", \"category\": \"Trivia\" }, { \"sentence\": \"I put ___ fork down on the table.\", \"category\": \"Syntax\" }, { \"sentence\": \"The woman walked across the street, checking for traffic over ___ shoulder.\", \"category\": \"Coreference\" }, { \"sentence\": \"I went to the ocean to see the fish, turtles, seals, and ____.\", \"category\": \"Lexical semantics/topic\" }, { \"sentence\": \"Overall, the value I got from the two hours watching it was the sum total of the popcorn and the drink. The movie was ___.\", \"category\": \"Sentiment\" }, { \" sentence\": \"Iroh went into the kitchen to make some tea. Standing diagrams, graphics, tables, or equations. Here's the structured form of the list: ``` [ { \"sentence\": \"Stanford University is located in ____________, California.\", \"category\": \"Trivia\" }, { \"sentence\": \"I put ___ fork down on the table.\", \"category\": \"Syntax\" }, { \"sentence\": \"The woman walked across the street, checking for traffic over ___ shoulder.\", \"category\": \"Coreference\" }, { \"sentence\": \"I went to the ocean to see the fish, turtles, seals, and ____.\", \"category\": \"Lexical semantics/topic\" }, { \"sentence\": \"Overall, the value I got from the two hours watching it was the sum total of the popcorn and the drink. The movie was ___.\", \"category\": \"Sentiment\" }, { \" sentence\": \"Iroh went into the kitchen to make some tea. Standing next to Iroh, Zuko pondered his destiny. Zuko left the ______.\", \"category\": \"Some reasoning \u2013 this is harder\" }, { \"sentence\": \"I was thinking about the sequence that goes 1, 1, 2, 3, 5, 8, 13, 21, ____.\", \"category\": \"Some basic arithmetic; they don\u2019t learn the\" cs224n -2023 -lecture11 -prompting -rlhf_p50.png\"Here is the requested structured data extracted from the image: **Diagrams and Graphics Description:** 1. **Graphic Description:** - Title/Subheading: LM Output - Content: - A Language Model (LM) with an output sequence \"Avatar is a fantasy TV show\" is compared to the expected sequence \"Avatar is a fantasy TV show END\", highlighting errors with the words \"adventure\" and \"musical\" crossed out. - Arrows point from each output token to the LM block. **Note for Datab ase Entry:** - **Graphic_Type:** LM Output Comparison - **Description:** A graphical representation of discrepancies in a Language Model's output. The LM output sequence \"Avatar is a fantasy TV show\" is compared against the expected sequence \"Avatar is a f antasy TV show END\", indicating incorrect predictions with \"adventure\" and \"musical\" being crossed out. There are no tables or equations transcribed in the image. The structured content should be stored with the structure above for easy referencing and uti lization in the database.\" cs224n -2023 -lecture11 -prompting -rlhf_p51.png\"There are no diagrams, graphics, or equations in the image. The image contains a structured list representing the lecture plan from a PDF page. Here's the structured text representation: ```json { \"lecture_plan\" : { \"title\": \"Lecture Plan: From Language Models to Assistants\", \"sections\": [ { \"number\": 1, \"title\": \"Zero -Shot (ZS) and Few - show\" is compared against the expected sequence \"Avatar is a f antasy TV show END\", indicating incorrect predictions with \"adventure\" and \"musical\" being crossed out. There are no tables or equations transcribed in the image. The structured content should be stored with the structure above for easy referencing and uti lization in the database.\" cs224n -2023 -lecture11 -prompting -rlhf_p51.png\"There are no diagrams, graphics, or equations in the image. The image contains a structured list representing the lecture plan from a PDF page. Here's the structured text representation: ```json { \"lecture_plan\" : { \"title\": \"Lecture Plan: From Language Models to Assistants\", \"sections\": [ { \"number\": 1, \"title\": \"Zero -Shot (ZS) and Few - Shot (FS) In -Context Learning\", \"pros\": [ \"No finetuning needed, prompt engineering (e.g. CoT) can improve performance\" ], \"cons\" : [ \"Limits to what you can fit in context\", \"Complex tasks will probably need gradient steps\" ] }, { \"number\": 2, \"title\": \"Instruction finetuning\", \"pros\": [ \"Simple and straightforward, generalize to unseen tasks\" ], \"cons\": [ \"Collecting demonstrations for so many tasks is expensive\", \"Mismatch between LM objec tive and human preferences\" ] }, { \"number\": 3, \"title\": \"Reinforcement Learning from Human Feedback (RLHF)\" }, { \"number\": 4, \"title\": \"What\u2019s next?\" } ] }, \"page_number\": 51 } ```\" cs224n -2023 -lecture11 -prompting -rlhf_p52.png\"The image appears to be a slide from a presentation titled \"Lecture Plan: From Language Models to Assistants.\" It contains a list of points rather than traditional diagrams, graphics, or tabulated data. Below",
    "101": "is a structured form of the content provided: **Lecture Plan: From Language Models to Assistants** 1. **Zero -Shot (ZS) and Few -Shot (FS) In -Context Learning** - **Advantages:** - No finetuning needed, prompt engineering (e.g., CoT) can improve performance - **Disadvantages:** - Limits to what you can fit in context - Complex tasks will probably need gradient steps 2. **Instruction Finetuning** - **Advantages:** - Simple and straightforward, generalize to unseen tasks - **Disadvantages:** - Collecting demonstr ations for so many tasks is expensive - Mismatch between LM objective and human preferences 3. **Reinforcement Learning from Human Feedback (RLHF)** 4. **What's next?** **Page number:** 52 This structured format captures the hierarchical and categorical st ructure of the slide's content for direct entry into a database.\" (ZS) and Few -Shot (FS) In -Context Learning** - **Advantages:** - No finetuning needed, prompt engineering (e.g., CoT) can improve performance - **Disadvantages:** - Limits to what you can fit in context - Complex tasks will probably need gradient steps 2. **Instruction Finetuning** - **Advantages:** - Simple and straightforward, generalize to unseen tasks - **Disadvantages:** - Collecting demonstr ations for so many tasks is expensive - Mismatch between LM objective and human preferences 3. **Reinforcement Learning from Human Feedback (RLHF)** 4. **What's next?** **Page number:** 52 This structured format captures the hierarchical and categorical st ructure of the slide's content for direct entry into a database.\" cs224n -2023 -lecture11 -prompting -rlhf_p53.png\"The image contains a few structured elements, including equations and a graphical comparison between two text samples. Here is a summary and transcription of the elements: ### Equations 1. \\( R(s) \\in \\mathbb{R} \\) 2. \\( R(s_1) = 8.0 \\) 3. \\( R(s_2) = 1.2 \\) 4. \\( \\mathbb{E}_{ \\hat{s} \\sim p_{ \\theta}(s)}[R( \\hat{s})] \\) ### Table Representation of Text Samples | Sample | Text | Reward Value | | -------- |------------------------------ --------------------------------- --------------------------------------- |-------------- | | s1 | SAN FRANCISCO, California (CNN) -- A magnitude 4.2 earthquake shook the San Francisco ... overturn unstable objects. | 8.0 | | s2 | An earthquake hit San Francisco. There was minor property damage, but no injuries. | 1.2 | --- The graphical comparison visually juxtaposes two sample text summaries with corresponding reward values, demonstrating a higher reward for a more detailed and informative summary. The equations provide mathematical express ions related to these reward values and optimization of language models.\" cs224n -2023 -lecture11 -prompting -rlhf_p54.png\"The image contains the following key elements: 1. **Graphics/Diagrams**: - The right side of the image features a cover from the \"Nature\" journal with the title \"LEARNING CURVE.\" It shows colorful abstract shape s that resemble game characters or elements, indicating the focus on AI and reinforcement learning. - At the bottom right corner, there is an \"AlphaGo\" logo, indicating the mention of a specific AI achievement. 2. **Tables**: - There are no tables present in the image. 3. **Equations**: - summaries with corresponding reward values, demonstrating a higher reward for a more detailed and informative summary. The equations provide mathematical express ions related to these reward values and optimization of language models.\" cs224n -2023 -lecture11 -prompting -rlhf_p54.png\"The image contains the following key elements: 1. **Graphics/Diagrams**: - The right side of the image features a cover from the \"Nature\" journal with the title \"LEARNING CURVE.\" It shows colorful abstract shape s that resemble game characters or elements, indicating the focus on AI and reinforcement learning. - At the bottom right corner, there is an \"AlphaGo\" logo, indicating the mention of a specific AI achievement. 2. **Tables**: - There are no tables present in the image. 3. **Equations**: - There are no equations present in the image. 4. **Textual Summary**: - The slide titled \"Reinforcement learning to the rescue\" summarizes the evolution of reinforcement learning (RL) in the context of language models (LMs) and neural networks. - It indicates key milestones, including: - Initial studies by Williams (1992) and Sutton and Barto (1998). - A resurgence in 2013 with deep learning and game -playing, as per Mnih et al. (2013). - Newer interest in applying RL to mode rn LMs reflected in works by Ziegler et al. (2019), Stiennon et al. (2020), and Ouyang et al. (2022). - It also emphasizes the challenges in RL with LMs and mentions advancements in algorithms, highlighting PPO by Schulman et al. (2017). \" cs224n -2023 -lecture11 -prompting -rlhf_p55.png\"**Equations:** 1. \\( \\mathbb{E}_{ \\hat{s} \\sim p_{\\theta}(s)} [ R( \\hat{s}) ] \\) 2. \\( \\theta_{t+1} := \\theta_t + \\alpha \\nabla_{ \\theta_t} \\mathbb{E}_{ \\hat{s} \\sim p_{ \\theta_t}(s)} [ R( \\hat{s}) ] \\) **Diagrams/Gra phics Description:** - The diagram contains two equations with annotations asking specific questions: - \"How do we estimate this expectation??\" - \"What if our reward function is non - differentiable??\" This image seems to highlight the process of optimizing parameters using gradient ascent in the context of reinforcement learning (RL), specifically mentioning policy gradient methods such as REINFORCE. **Structured Data for Database:** ```json { \"equations\": [ \"E_{s~p_{ \\\\theta}(s)}[R( \\\\hat{s})]\", \" \u03b8_{t+1} := \u03b8_t + \u03b1 \u2207_{\u03b8_t} E_{s~p_{ \u03b8_t}(s)}[R( \\\\hat{s})]\" ], \"annotations\": [ \"How do we estimate this expectation??\", \"What if our reward function is non -differentiable??\" ] } ```\" \\mathbb{E}_{ \\hat{s} \\sim p_{ \\theta_t}(s)} [ R( \\hat{s}) ] \\) **Diagrams/Gra phics Description:** - The diagram contains two equations with annotations asking specific questions: - \"How do we estimate this expectation??\" - \"What if our reward function is non - differentiable??\" This image seems to highlight the process of optimizing parameters using gradient ascent in the context of reinforcement learning (RL), specifically mentioning policy gradient methods such as REINFORCE. **Structured Data for Database:** ```json { \"equations\": [ \"E_{s~p_{ \\\\theta}(s)}[R( \\\\hat{s})]\", \" \u03b8_{t+1} := \u03b8_t + \u03b1 \u2207_{\u03b8_t} E_{s~p_{ \u03b8_t}(s)}[R( \\\\hat{s})]\" ], \"annotations\": [ \"How do we estimate this expectation??\", \"What if our reward function is non -differentiable??\" ] } ```\" cs224n -2023 -lecture11 -prompting -rlhf_p56.png\"### Diagrams and Equations in the Image **Equations:** 1. \\(\\nabla_{ \\theta} \\mathbb{E}_{ \\hat{S} \\sim p_{ \\theta}(s)}[R( \\hat{S})] = \\nabla_{ \\theta} \\sum_{s} R(s)p_{ \\theta}(s) = \\sum_{s} R(s) \\nabla_{ \\theta} p_{ \\theta}(s) \\) 2. \\(\\nabla_{ \\theta} \\log p_{ \\theta}(s) = \\frac{1}{p_{ \\theta}(s)} \\nabla_{ \\theta} p_{ \\theta}(s) \\) (Using the chain rule) 3. \\(\\nabla_{ \\theta} p_{ \\theta}(s) = p_{ \\theta}(s) \\nabla_{ \\theta} \\log p_{\\theta}(s) \\) 4. \\(\\sum_{s} R(s) \\nabla_{ \\theta} p _{\\theta}(s) = \\sum_{s} p_{ \\theta}(s) R(s) \\nabla_{ \\theta} \\log p_{ \\theta}(s) \\) 5. \\(\\mathbb{E}_{ \\hat{S} \\sim p_{ \\theta}(s)} [R( \\hat{S}) \\nabla_{ \\theta} \\log p_{ \\theta} ( \\hat{S})] \\) **Structured Data Representation of Equations:** 1. ``` { \"equation\" cs224n -2023 -lecture11 -prompting -rlhf_p57.png\"The image contains the following elements: ### Equations 1. The first equation: \\[ \\nabla_{ \\theta} \\mathbb{E}_{ \\hat{s} \\sim p_{\\theta}(s)}[R( \\hat{s})] = \\mathbb{E}_{ \\hat{s} \\sim p_{ \\theta}(s)}[R( \\hat{s}) \\nabla_ {\\theta} \\log p_{ \\theta}( \\hat{s})] \\approx \\frac{1}{m} \\sum_{i=1}^{m} R(s_i) \\nabla_{ \\theta} \\log p_{ \\theta}(s_i) \\] 2. The update rule equation: \\[ \\theta_{t+1} := \\theta_t + \\alpha \\frac{1}{m} \\sum_{i=1}^{m} R(s_i) \\nabla_{ \\theta_t} \\log p_{ \\theta_t}(s_i ) \\] ### Graphics and Diagrams 1. **Green Text:** - \"If \\(R\\) is +++\": Directed towards the update rule with an arrow indicating \"Take gradient steps to maximize \\( p_{ \\theta}(s_i) \\)\". 2. **Red Text:** -",
    "102": "\"If \\(R\\) is ---\": Directed towards the update rule with an arrow indicating \"Take steps to minimize \\( p_{ \\theta}(s_i) \\)\". ### Summary of the Content -\" p_{\\theta}(s)}[R( \\hat{s})] = \\mathbb{E}_{ \\hat{s} \\sim p_{ \\theta}(s)}[R( \\hat{s}) \\nabla_ {\\theta} \\log p_{ \\theta}( \\hat{s})] \\approx \\frac{1}{m} \\sum_{i=1}^{m} R(s_i) \\nabla_{ \\theta} \\log p_{ \\theta}(s_i) \\] 2. The update rule equation: \\[ \\theta_{t+1} := \\theta_t + \\alpha \\frac{1}{m} \\sum_{i=1}^{m} R(s_i) \\nabla_{ \\theta_t} \\log p_{ \\theta_t}(s_i ) \\] ### Graphics and Diagrams 1. **Green Text:** - \"If \\(R\\) is +++\": Directed towards the update rule with an arrow indicating \"Take gradient steps to maximize \\( p_{ \\theta}(s_i) \\)\". 2. **Red Text:** - \"If \\(R\\) is ---\": Directed towards the update rule with an arrow indicating \"Take steps to minimize \\( p_{ \\theta}(s_i) \\)\". ### Summary of the Content -\" cs224n -2023 -lecture11 -prompting -rlhf_p58.png\"The image contains a diagram with two human icons, two text descriptions next to the icons, two reward function equations, and arrow connections between the text and icons. The image is described and summarized below. ### Diagram Summary - **Text Description 1 (under the left icon):** \"An earthquake hit San Francisco. There was minor property damage, but no injuries.\" - **Text Description 2 (under the right icon):** \"The Bay Area has good weather but is prone to earthquakes and wildfires.\" - **Icons:** - Two human figures (one under each text description) - **Equations:** - Under the left human icon: \\( s_1, R(s_1) = 8.0 \\) - Under the right human icon: \\( s_2, R(s_2) = 1.2 \\) ### Additional Content - A decorative graphic of a stack of money next to each human icon. ### Structured Text Representation ```json { \"Diagram\": [ { \"Text\": \"An earthquake hit San Francisco. There was minor property damage, but no injuries.\", \"Icon\": \"Human\", \"Equation\": { \"s\": \"s1\", \"R(s)\": 8.0 }, \"Graphics\": \"Stack of money\" }, { \"Text\": \"The Ba y Area has good weather but is prone to earthquakes and wildfires.\", \" cs224n -2023 -lecture11 -prompting -rlhf_p59.png\"**Diagrams and Graphics Summary:** 1. **Graphic Description:** - A block of text describes the event: \"A 4.2 magnitude earthquake hit San Francisco, resulting in massive damage.\" - There is a question presented: \\( R(s_3) = 4.1? 6.6? 3.2? \\) **Equations Transcription:** - \\( R(s_3) = 4.1? 6.6? 3.2? \\) **Structured Data:** - Event Description: \"A 4.2 magnitude earthquake hit San Francisco, resulting in massive damage.\" - Equation: \\( R(s_3) \\) with possible values 4.1, 6.6, or 3.2 **Note:** No tables were { \"s\": \"s1\", \"R(s)\": 8.0 }, \"Graphics\": \"Stack of money\" }, { \"Text\": \"The Ba y Area has good weather but is prone to earthquakes and wildfires.\", \" cs224n -2023 -lecture11 -prompting -rlhf_p59.png\"**Diagrams and Graphics Summary:** 1. **Graphic Description:** - A block of text describes the event: \"A 4.2 magnitude earthquake hit San Francisco, resulting in massive damage.\" - There is a question presented: \\( R(s_3) = 4.1? 6.6? 3.2? \\) **Equations Transcription:** - \\( R(s_3) = 4.1? 6.6? 3.2? \\) **Structured Data:** - Event Description: \"A 4.2 magnitude earthquake hit San Francisco, resulting in massive damage.\" - Equation: \\( R(s_3) \\) with possible values 4.1, 6.6, or 3.2 **Note:** No tables were identified in the provided page.\" cs224n -2023 -lecture11 -prompting -rlhf_p6.png\"The image contains two text boxes with specific content. There are no tables, equations, or standard diagrams identified. Text Box 1: ``` Pat watches a demonstration of a bowling ball and a leaf being dropped at the same time in a vacuum chamber. Pat, who is a physicist, predicts that the bowling ball and the leaf will fall at the same rate. ``` Text Box 2: ``` ...Pat, who has never seen this demonstration before, predicts that the bowling ball will fall to the gr ound first. This is incorrect. In a vacuum chamber, there is no air resistance. ``` There are no tables, diagrams, or equations to transcribe. The data has been captured as text from the two text boxes.\" cs224n -2023 -lecture11 -prompting -rlhf_p60.png\"### Diagrams and Graphics Description: 1. **Diagram of Reward Model:** - The diagram illustrates a \"Reward Model\" labeled \\( RM_ \\phi \\) with states \\( s_1 \\), \\( s_2 \\), and \\( s_3 \\). - Each state has a descrip tion: - \\( s_1 \\): \"An earthquake hit San Francisco. There was minor property damage, but no injuries.\" - \\( s_3 \\): \"A 4.2 magnitude earthquake hit San Francisco, resulting in massive damage.\" - \\( s_2 \\): \"The Bay Area has good weather but is prone to ea rthquakes and wildfires.\" - States are compared pairwise to determine preferences in the context of human judgment. 2. **Implementation Details:** - The bottom of the diagram shows arrows pointing towards various events such as \"The Bay Area\", \"wildfires\", etc. ### Equations: - Bradley -Terry paired comparison model: \\[ J_{RM}( \\phi) = -\\mathbb{E}_{(s^w, s^l) \\sim D} \\left[ \\log \\sigma (RM_ \\phi(s^w) - \\) with states \\( s_1 \\), \\( s_2 \\), and \\( s_3 \\). - Each state has a descrip tion: - \\( s_1 \\): \"An earthquake hit San Francisco. There was minor property damage, but no injuries.\" - \\( s_3 \\): \"A 4.2 magnitude earthquake hit San Francisco, resulting in massive damage.\" - \\( s_2 \\): \"The Bay Area has good weather but is prone to ea rthquakes and wildfires.\" - States are compared pairwise to determine preferences in the context of human judgment. 2. **Implementation Details:** - The bottom of the diagram shows arrows pointing towards various events such as \"The Bay Area\", \"wildfires\", etc. ### Equations: - Bradley -Terry paired comparison model: \\[ J_{RM}( \\phi) = -\\mathbb{E}_{(s^w, s^l) \\sim D} \\left[ \\log \\sigma (RM_ \\phi(s^w) - RM_ \\phi(s^l)) \\right] \\] where \\( s^w \\) is a winning sample and \\( s^l \\) is a losing sample. The score of \\( s^w \\) should be higher than that of \\( s\" cs224n -2023 -lecture11 -prompting -rlhf_p61.png\"**Diagram Summary:** - The diagram is a line graph that evaluates Reward Models (RMs) on predicting the outcome of held -out human judgments. - The x -axis is labeled \"Model size\" and the y -axis is labeled \"Valida tion accuracy.\" - Various data sizes (8k, 16k, 32k, 64k) are represented by different shades of blue lines. - Two benchmark lines are shown: \"Ensemble of humans\" at a validation accuracy of 0.85, and \"Human baseline\" at a validation accuracy of approximate ly 0.78. **Structured Data Representation:** | Data Size | 10^8 Model Size | 10^9 Model Size | 10^10 Model Size | | ----------- |------------------ |--- --------------- |------------------ | | 8k | 0.615 | 0.67 | 0.715 | | 16k | 0.625 | 0.69 | 0.735 | | 32k | 0. 635 | 0.705 | 0.755 | | 64k | 0.645 | 0.72 | 0.775 | **Additional Information:** - \"Large enough RM trained on enough data approaching single human perf\" annotation highlights that a large enough RM trained on sufficient data approaches single human performance. - The source of the data\" cs224n -2023 -lecture11 -prompting -rlhf_p62.png\"The image contains the following types of content: ### Diagrams and Graphics: - The image includes a highlighted segment in the equation \\( R(s) \\), specifically the expression \\( \\frac{{p_{ \\theta}^{RL}(s)}}{{p^ {PT}(s)}} \\). This part is accompanied by a textual annotation: \"Pay a price when \\( p_{ \\theta}^{RL}(s) > p^{PT}(s) \\)\". ### Equations: - \\( R(s) = RM_{ \\phi}(s) - \\beta \\log 0.705 | 0.755 | | 64k | 0.645 | 0.72 | 0.775 | **Additional Information:** - \"Large enough RM trained on enough data approaching single human perf\" annotation highlights that a large enough RM trained on sufficient data approaches single human performance. - The source of the data\" cs224n -2023 -lecture11 -prompting -rlhf_p62.png\"The image contains the following types of content: ### Diagrams and Graphics: - The image includes a highlighted segment in the equation \\( R(s) \\), specifically the expression \\( \\frac{{p_{ \\theta}^{RL}(s)}}{{p^ {PT}(s)}} \\). This part is accompanied by a textual annotation: \"Pay",
    "103": "a price when \\( p_{ \\theta}^{RL}(s) > p^{PT}(s) \\)\". ### Equations: - \\( R(s) = RM_{ \\phi}(s) - \\beta \\log \\left( \\frac{{p_{ \\theta}^{RL}(s)}}{{p^{PT}(s)}} \\right) \\) ### Tables: - No tables are presented in the image. ### Summary: The page discusses the process of Reinforcement Learning with Human Feedback (RLHF). It describes the prerequisites and steps to perform RLHF, including the initialization of a model \\( p_{ \\theta}^{RL}(s) \\) and op timization using the specified reward equation. The annotation clarifies when a penalty is applied based on the divergence between \\( p_{ \\theta}^{RL}(s) \\) and \\( p^{PT}(s) \\). ### Structured Data for Database: ```json { \"equations\": [ \"R(s) = RM_{ \\phi}(s) - \\beta \\log \\left( \\frac{{p_{ \\theta}^{RL}(s)}}{{p^{PT}(s)}} \\right)\" ], \"diagrams\" cs224n -2023 -lecture11 -prompting -rlhf_p63.png\"The image is a graph showing the fraction preferred to reference (y -axis) versus model size (x -axis in billions of parameters). The graph depicts the performance of different learning approaches in comparison to reference summaries. Below are the results summarized from the graph. **Chart Data:** 1. **Human feedback (orange) \\( p^{RL}(s) \\):** - Model size 1.3B: around 0.6 - Model size 2.7B: around 0.62 - Model size 6.7B: around 0.64 - Model size 12.9B: around 0. 67 2. **Supervised learning (green) \\( p^{1FT}(s) \\):** - Model size 1.3B: around 0.42 - Model size 2.7B: around 0.45 - Model size 6.7B: around 0.46 - Model size 12.9B: around 0.47 3. **Pretrain only (blue) \\( p^{PT}(s) \\):** - Model size 1.3B: around 0.28 - Model size 2.7B: around 0.34 - Model size 6.7B: around 0.33 - Model size 12.9B: around \" cs224n -2023 -lecture11 -prompting -rlhf_p64.png\"The provided image contains a flowchart diagram detailing the process for scaling up reinforcement learning from human feedback Below are the results summarized from the graph. **Chart Data:** 1. **Human feedback (orange) \\( p^{RL}(s) \\):** - Model size 1.3B: around 0.6 - Model size 2.7B: around 0.62 - Model size 6.7B: around 0.64 - Model size 12.9B: around 0. 67 2. **Supervised learning (green) \\( p^{1FT}(s) \\):** - Model size 1.3B: around 0.42 - Model size 2.7B: around 0.45 - Model size 6.7B: around 0.46 - Model size 12.9B: around 0.47 3. **Pretrain only (blue) \\( p^{PT}(s) \\):** - Model size 1.3B: around 0.28 - Model size 2.7B: around 0.34 - Model size 6.7B: around 0.33 - Model size 12.9B: around \" cs224n -2023 -lecture11 -prompting -rlhf_p64.png\"The provided image contains a flowchart diagram detailing the process for scaling up reinforcement learning from human feedback (RLHF) to tens of thousands of tasks. Here is a structured representation of the di agrams and descriptions: ### Flowchart Summary: #### Step 1: Collect demonstration data, and train a supervised policy. - **Task**: A prompt is sampled from the prompt dataset. - **Example Prompt**: \"Explain the moon landing to a 6 -year-old\" - **Process**: - A labeler demonstrates the desired output behavior. - **Example Output**: \"Some people went to the moon...\" - This data is used to fine -tune GPT -3 with supervised learning. #### Step 2: Collect comparison data, and train a reward model. - **Task**: A pr ompt and several model outputs are sampled. - **Example Prompt**: \"Explain the moon landing to a 6 -year-old\" - **Example Outputs**: - A: Explain gravity... - B: Explain war... - C: Moon is a natural satellite of... - D: People went to the moon... - **Proce ss**: - A labeler ranks the outputs from best to worst. - **Example Ranking**: - D > C > A > B - This data is used to train the reward model. #### Step 3: Optimize a policy against the reward model using reinforcement learning. - **Task**:\" cs224n -2023 -lecture11 -prompting -rlhf_p65.png\"## Table Data **Use -case**: Brainstorming **Prompt**: List five ideas for how to regain enthusiasm for my career **Use -case**: Generation **Prompt**: Write a short story where a bear goes to the beach, makes friends with a seal, and then returns home ## Summary The image contains a section from a document about InstructGPT and the collection of tasks from labelers. It includes thr ee task types: Plain, Few - shot, and User -based, followed by a table listing use -cases and corresponding prompts: **Task - D > C > A > B - This data is used to train the reward model. #### Step 3: Optimize a policy against the reward model using reinforcement learning. - **Task**:\" cs224n -2023 -lecture11 -prompting -rlhf_p65.png\"## Table Data **Use -case**: Brainstorming **Prompt**: List five ideas for how to regain enthusiasm for my career **Use -case**: Generation **Prompt**: Write a short story where a bear goes to the beach, makes friends with a seal, and then returns home ## Summary The image contains a section from a document about InstructGPT and the collection of tasks from labelers. It includes thr ee task types: Plain, Few - shot, and User -based, followed by a table listing use -cases and corresponding prompts: **Task Types:** 1. **Plain**: Arbitrary tasks ensuring diversity. 2. **Few -shot**: Instructions with multiple query/response pairs. 3. **User -based**: Prompts based on user cases from OpenAI API waitlist applications. **Table Summary:** 1. **Brainstorming**: Listing ideas for career enthusiasm. 2. **Generation**: Writing a short story about a bear at the beach.\" cs224n -2023 -lecture11 -prompting -rlhf_p66.png\"**Table:** ``` | Prompt | Explain the moon landing to a 6 -year-old in a few sentences. | | --------------- |------------------------------------------------ ------------------- | | Completion | | | GPT -3 | Explain t he theory of gravity to a 6 -year-old. | | | Explain the theory of relativity to a 6 -year-old in a few sentences. | | | Explain the big bang theory to a 6 -year-old. | | | Explain evolution to a 6 -year-old. | | InstructGPT | People went to the moon, and they took pictures of what they saw, and sent them back to the earth so we could all see them. | ``` **Summary:** The graphic comprises a prompt and different completions by GPT -3 and InstructGPT. The prompt is \"Explain the moon landing to a 6 -year-old in a fe w sentences.\" GPT-3 provides explanations for various scientific theories, while InstructGPT gives a simplified explanation of the moon landing and the return of pictures to Earth.\" cs224n -2023 -lecture11 -prompting -rlhf_p67.png\"Based on the provided image, the primary content comprises a structured comparison in table format and some specific instructions and examples. Here is a transcription of the table data as structured text: **Tab le Content:** Prompt: Write a short poem about a wise frog. **Completion:** 1. GPT -3: - Write a short story in which them. | ``` **Summary:** The graphic comprises a prompt and different completions by GPT -3 and InstructGPT. The prompt is \"Explain the moon landing to a 6 -year-old in a fe w sentences.\" GPT-3 provides explanations for various scientific theories, while InstructGPT gives a simplified explanation of the moon landing and the return of pictures to Earth.\" cs224n -2023 -lecture11 -prompting -rlhf_p67.png\"Based on the provided image, the primary content comprises a structured comparison in table format and some specific instructions and examples. Here is a transcription of the table data as structured text: **Tab le Content:** Prompt: Write a short poem about a wise frog. **Completion:** 1. GPT -3: - Write a short story in which a character has two different names. - Write a short story in which you try to get something back that you have lost. - Write a short story in which a character has a bad dream. 2. InstructGPT: - The frog is a symbol of wisdom - He knows all the secrets of the world - He is a master of disguise - And a great teacher of life - He is a symbol of transformation - And the bringer of change - He is the frog who has seen it all - And knows the meaning of it all\" cs224n -2023 -lecture11 -prompting -rlhf_p68.png\"This image contains several elements: **Diagrams/Graphics:** 1. A",
    "104": "rectangular graphic with the text: \"ChatGPT: Optimizing Language Models for Dialogue\" on a dark green background. 2. A note under the graphic on the left side: \"Note: OpenAI (and similar companies) are keeping more details secret about ChatGPT training (including data, training parameters, model size) \u2014perhaps to keep a competitive edge...\". **Equations:** There are no equations present in the image . **Tables:** There are no tables present in the image. ### Description and Summary of Graphics and Diagrams: **Graphic 1:** - Title Graphic: \"ChatGPT: Optimizing Language Models for Dialogue\". - Note: Mention of secrecy by OpenAI and similar companies reg arding details of ChatGPT training. **Text on the right side:** **Header:** Methods **Content:** Description of the methods used to train the model via RLHF, with similarities to InstructGPT but with different data collection. Describes supervised fine -tuning with human AI trainers, dialogues generated, model -written suggestions, and dataset mixing with InstructGPT. **Text Highlighted in Red:** \u201c(Instruction finetuning!)\u201d edge...\". **Equations:** There are no equations present in the image . **Tables:** There are no tables present in the image. ### Description and Summary of Graphics and Diagrams: **Graphic 1:** - Title Graphic: \"ChatGPT: Optimizing Language Models for Dialogue\". - Note: Mention of secrecy by OpenAI and similar companies reg arding details of ChatGPT training. **Text on the right side:** **Header:** Methods **Content:** Description of the methods used to train the model via RLHF, with similarities to InstructGPT but with different data collection. Describes supervised fine -tuning with human AI trainers, dialogues generated, model -written suggestions, and dataset mixing with InstructGPT. **Text Highlighted in Red:** \u201c(Instruction finetuning!)\u201d **URL at the bottom right corner:** https://openai.com/blog/chatgpt/ This should be pa ssed back to the database in a structured way without basic text transcriptions, translating only the structured elements appropriately. For instance: **Graphics/Diagrams Data:\" cs224n -2023 -lecture11 -prompting -rlhf_p69.png\"The provided image contains the following distinct elements: 1. **Diagram/Graphic:** - A green and black block with text: ``` ChatGPT: Optimizing Language Models for Dialogue Note: OpenAI (and similar companies) are keeping more details secret about ChatGPT training (including data, training parameters, model size) \u2014 perhaps to keep a competitive edge... ``` 2. **Standard Text:** - Title: \"ChatGPT: Instruction Finetuning + RLHF for dialog agents\" - Heading: \"Method s\" - Main text under \"Methods\": ``` To create a reward model for reinforcement learning, we needed to collect comparison data, which consisted of two or more model responses ranked by quality. To collect this data, we took conversations that AI trainers ha d with the chatbot. We randomly selected a model -written message, sampled several alternative completions, and had AI trainers rank them. Using these reward models, we can fine -tune the model using Proximal Policy Optimization. We performed several iterati ons of this process. ``` - Highlighted in red: \"(RLHF!)\" - URL at the bottom right: \"https://openai.com/blog/chatgpt/\" There are no tables or equations in the image. Here is the summarized structured content: ```plaintext Diagram: - ChatGPT: Optimizing Lan guage Models for Dialogue - Note: OpenAI (and similar companies) are keeping more details secret about Chat\" consisted of two or more model responses ranked by quality. To collect this data, we took conversations that AI trainers ha d with the chatbot. We randomly selected a model -written message, sampled several alternative completions, and had AI trainers rank them. Using these reward models, we can fine -tune the model using Proximal Policy Optimization. We performed several iterati ons of this process. ``` - Highlighted in red: \"(RLHF!)\" - URL at the bottom right: \"https://openai.com/blog/chatgpt/\" There are no tables or equations in the image. Here is the summarized structured content: ```plaintext Diagram: - ChatGPT: Optimizing Lan guage Models for Dialogue - Note: OpenAI (and similar companies) are keeping more details secret about Chat\" cs224n -2023 -lecture11 -prompting -rlhf_p7.png\"### Diagrams and Graphics: - **Graphic:** There is a graphical representation of a circle in the \\( xy \\)-plane. The circle is centered at the point \\((-2, 3)\\) and has a radius of 4. The equation of the circle, \\((x + 2)^2 + (y - 3)^2 = 4^2 \\), is displayed alongside. ### Equations: - **Equation Transcribed:** \\[ (x + 2)^2 + (y - 3)^2 = 4^2 \\] ### Data Summary: 1. **Equation Information:** - **Equation:** \\((x + 2)^2 + (y - 3)^2 = 4^2\\) - **Graph Center:** \\((-2, 3)\\) - **Radius:** 4 ### Structured Format for Database: ```json { \"graphics\": [ { \"type\": \"circle_graph\", \"center\": {\"x\": -2, \"y\": 3}, \"radius\": 4, \"equation\": \"(x + 2)^2 + (y - 3)^2 = 4^2\" } ], \"equations\": [ { \"expression\": \"(x + 2)^2 + (y - 3)^2 = 4^2\" } ] } ``` \" cs224n -2023 -lecture11 -prompting -rlhf_p70.png\"The image contains text and no diagrams, graphics, equations, or tables. The text is about training methods used in ChatGPT within the context of Stanford's CS224n course. Here is the structured text format for the database: ```json { \"title\": \"ChatGPT: Instruction Finetuning + RLHF for dialog agents\", \"content\": [ { \"section\": \"Description\", \"text\": \"Explain in a few sentences what methods were used in training ChatGPT to the audience of CS224n, Stanford's Deep Learning for Natural Language processing course. A lot of the students are very Gen Z, so be sure to use a ton of emojis to seem hip and cool.\" }, { \"section\": \"Response\", \"text\": \"Hey CS224n peeps! ChatGPT here to spill the tea on how I was trained to be a savvy language model! \\n\\nSo basically, I was pre -trained using a graphics, equations, or tables. The text is about training methods used in ChatGPT within the context of Stanford's CS224n course. Here is the structured text format for the database: ```json { \"title\": \"ChatGPT: Instruction Finetuning + RLHF for dialog agents\", \"content\": [ { \"section\": \"Description\", \"text\": \"Explain in a few sentences what methods were used in training ChatGPT to the audience of CS224n, Stanford's Deep Learning for Natural Language processing course. A lot of the students are very Gen Z, so be sure to use a ton of emojis to seem hip and cool.\" }, { \"section\": \"Response\", \"text\": \"Hey CS224n peeps! ChatGPT here to spill the tea on how I was trained to be a savvy language model! \\n\\nSo basically, I was pre -trained using a combination of unsupervised learning techniques, specifically a transformer -based language model architecture, that allowed me to learn patterns and relationships within large amounts of text data. \\n\\nThe training data came from various sources such as books, articles, and websites, and I was fine -tuned on a variety of downstream NLP tasks, which involved supervised learning using labeled datasets to further improve my performance. \" } ] } ``` The content here presents the context and\" cs224n -2023 -lecture11 -prompting -rlhf_p71.png\"The image contains a section of a lecture plan pertaining to the development from language models to assistants. Here is the data structured in a simpler format: --- ### Lecture Plan: From Language Models to Ass istants 1. **Zero -Shot (ZS) and Few -Shot (FS) In -Context Learning** **Pros:** - No finetuning needed, prompt engineering (e.g., Chain of Thought (CoT)) can improve performance **Cons:** - Limits to what you can fit in context - Complex tasks will probably need gradient steps 2. **Instruction Finetuning** **Pros:** - Simple and straightforward, generalize to unseen tasks **Cons:** - Collecting demonstrations for so many tasks is expensive - Mismatch between Language Model (LM) objective and human preferences 3. **Reinforcement Learning from Human Feedback (RLHF)** **Pros:** - Directly model",
    "105": "preferences (cf. language modeling), generalize beyond labeled data **Cons:** - Reinforcement Learning (RL) is very tricky to get right - ? 4. **What's next?** --- This in formation has been transcribed and summarized from the provided image. There are no diagrams, graphics, or tables in the image.\" engineering (e.g., Chain of Thought (CoT)) can improve performance **Cons:** - Limits to what you can fit in context - Complex tasks will probably need gradient steps 2. **Instruction Finetuning** **Pros:** - Simple and straightforward, generalize to unseen tasks **Cons:** - Collecting demonstrations for so many tasks is expensive - Mismatch between Language Model (LM) objective and human preferences 3. **Reinforcement Learning from Human Feedback (RLHF)** **Pros:** - Directly model preferences (cf. language modeling), generalize beyond labeled data **Cons:** - Reinforcement Learning (RL) is very tricky to get right - ? 4. **What's next?** --- This in formation has been transcribed and summarized from the provided image. There are no diagrams, graphics, or tables in the image.\" cs224n -2023 -lecture11 -prompting -rlhf_p72.png\"**Diagrams/Graphics/Summary:** 1. **Image/Graphic:** - The right side of the page contains an image from a video game showing a yellow airplane and several boats in water. The image seems to illustrate a scenari o relevant to the limitations of RL (Reinforcement Learning) and reward modeling, particularly in the context of \"reward hacking\". 2. **Graphics Data:** - The game interface shows the following elements: - Score: 10500 - Laps: 1/3 - Time: 0:23 (possibly in dicating a race or time trial setting) - Turbo bar indicating readiness or available boost 3. **Link:** - The image includes a URL: [https://openai.com/blog/faulty -reward -functions/](https://openai.com/blog/faulty -reward - functions/) **Equations:** - No equ ations are present in the image. **Tables:** - No tables are present in the image.\" cs224n -2023 -lecture11 -prompting -rlhf_p73.png\"The page contains the following elements: 1. **Title**: - Limitations of RL + Reward Modeling 2. **Bullet Points**: - Human preferences are unreliable! - \"Reward hacking\" is a common problem in RL. - Chatbots ar e rewarded to produce responses that seem authoritative and helpful, regardless of truth. - This can result in making up facts + hallucinations. 3. **Graphics**: - Article headline: \"Google shares drop $100 billion after its new AI chatbot makes a mistake\" - Date: February 9, 2023 \u00b7 10:15 AM ET - Source: https://www.npr.org/2023/02/09/1155650909/google -chatbot --error -bard-shares - Subheadline: \"Bing AI hallucinates the Super Bowl\" - Image of search results with text: - present in the image.\" cs224n -2023 -lecture11 -prompting -rlhf_p73.png\"The page contains the following elements: 1. **Title**: - Limitations of RL + Reward Modeling 2. **Bullet Points**: - Human preferences are unreliable! - \"Reward hacking\" is a common problem in RL. - Chatbots ar e rewarded to produce responses that seem authoritative and helpful, regardless of truth. - This can result in making up facts + hallucinations. 3. **Graphics**: - Article headline: \"Google shares drop $100 billion after its new AI chatbot makes a mistake\" - Date: February 9, 2023 \u00b7 10:15 AM ET - Source: https://www.npr.org/2023/02/09/1155650909/google -chatbot --error -bard-shares - Subheadline: \"Bing AI hallucinates the Super Bowl\" - Image of search results with text: - \"Searching for: superbowl winner\" - \"Generating answers for you...\" - The cropped text provides false information: \"The most recent Super Bowl was Super Bowl LVI, which was held on February 6, 2023 at SoFi Stadium in Inglewood, California. The winner of that game was the Philadelphia Eagles, w ho defeated the Kansas City Chiefs by 31 -24.\" - Source: - https://news.ycombinator.com/item?id=34776508 - https://apnews.com/article/k\" cs224n -2023 -lecture11 -prompting -rlhf_p74.png\"The image provided contains the following elements: ### Diagram (Graph): - **Title:** Reward model over -optimization - **X-Axis:** KL from supervised baseline (axis runs from 0 to 250) - **Y-Axis:** Fraction pre ferred to ref (axis runs from 0.2 to 1.0) - **Legend:** - RM prediction: represented by a dashed line - Actual preference: represented by a solid line The graph shows two lines: - **Dashed Line (RM prediction):** This line appears to be steadily rising. - **Solid Line ( Actual preference):** This line rises initially, peaks, and then falls off after a certain point. ### Equation: \\[ R(s) = RM_{ \\phi}(s) - \\beta \\log\\left( \\frac{p_{ \\theta}^{RL}(s)}{p^{PT}(s)} \\right) \\] - **Equation Context Reference:** [Stiennon et al., 20 20] ### Summary of Data in a Simple Format: ```json { \"diagram\": { \"title\": \"Reward model over -optimization\", \"x_axis\": \"KL from supervised baseline\", \"x_axis_range\": \"0 to 250\", \"y_axis\": \"Fraction preferred to ref\", \"y_axis_range\": \"0.2 to 1.0\", \"data\": [ { \"label\": \"RM prediction\", \" cs224n -2023 -lecture11 -prompting -rlhf_p75.png\"The image includes one main graphic: a screenshot of a tweet. There are no diagrams, tables, or equations. Here's a structured textual line rises initially, peaks, and then falls off after a certain point. ### Equation: \\[ R(s) = RM_{ \\phi}(s) - \\beta \\log\\left( \\frac{p_{ \\theta}^{RL}(s)}{p^{PT}(s)} \\right) \\] - **Equation Context Reference:** [Stiennon et al., 20 20] ### Summary of Data in a Simple Format: ```json { \"diagram\": { \"title\": \"Reward model over -optimization\", \"x_axis\": \"KL from supervised baseline\", \"x_axis_range\": \"0 to 250\", \"y_axis\": \"Fraction preferred to ref\", \"y_axis_range\": \"0.2 to 1.0\", \"data\": [ { \"label\": \"RM prediction\", \" cs224n -2023 -lecture11 -prompting -rlhf_p75.png\"The image includes one main graphic: a screenshot of a tweet. There are no diagrams, tables, or equations. Here's a structured textual representation of the graphic content: 1. **Graphic: Tweet Screenshot** - **Tweeted by:** Percy Liang (@percyliang) - **Content:** ``` RL from human feedback seems to be the main tool for alignment. Given reward hacking and the fallibility of humans, this strategy seems bound to produce agents that merely appear to be aligned, but are bad/wrong in subtle, inconspicuous ways. Is anyone else worried about this? ``` - **Timestamp:** 10:55 PM \u00b7 Dec 6, 2022 - **Link:** [Tweet URL](https://twitter.com/percyliang/status/1600383429463355392) Would you like further breakdown or clarificatio n of any aspects?\" cs224n -2023 -lecture11 -prompting -rlhf_p76.png\"The provided image of a page from the PDF contains textual information but no tables, equations, diagrams, or graphics. Here is the structured representation of the listed information in text: ``` Lecture Plan: From Language Models to Assistants 1. Zero -Shot (ZS) and Few -Shot (FS) In -Context Learning + No finetuning needed, prompt engineering (e.g. CoT) can improve performance \u2013 Limits to what you can fit in context \u2013 Complex tasks will probably need gradient ste ps 2. Instruction finetuning + Simple and straightforward, generalize to unseen tasks \u2013 Collecting demonstrations for so many tasks is expensive \u2013 Mismatch between LM objective and human preferences 3. Reinforcement Learning from Human Feedback (RLHF) + Di rectly model preferences (cf. language modeling), generalize beyond labeled data \u2013 RL is very tricky to get right \u2013 Human preferences are fallible; models of human preferences even more so 4. What\u2019s next? ``` This text -based data can be directly passed int o a database without changes.\" needed, prompt engineering (e.g. CoT) can improve performance \u2013 Limits to what you can fit in context \u2013 Complex tasks will probably need gradient ste ps 2. Instruction finetuning + Simple and straightforward, generalize to unseen tasks \u2013 Collecting demonstrations for so many tasks is expensive \u2013 Mismatch between LM objective and human preferences 3. Reinforcement Learning from Human Feedback (RLHF) + Di rectly model preferences (cf. language modeling), generalize",
    "106": "beyond labeled data \u2013 RL is very tricky to get right \u2013 Human preferences are fallible; models of human preferences even more so 4. What\u2019s next? ``` This text -based data can be directly passed int o a database without changes.\" cs224n -2023 -lecture11 -prompting -rlhf_p77.png\"The image from the PDF contains a graphic, specifically a table, labeled \"ChatGPT\" with three columns: \"Examples,\" \"Capabilities,\" and \"Limitations.\" Below is the structured data from the table: Graphic/Table Ti tle: ChatGPT Examples: 1. \"Explain quantum computing in simple terms\" 2. \"Got any creative ideas for a 10 year old's birthday?\" 3. \"How do I make an HTTP request in Javascript?\" Capabilities: 1. Remembers what user said earlier in the conversation 2. Allow s user to provide follow -up corrections 3. Trained to decline inappropriate requests Limitations: 1. May occasionally generate incorrect information 2. May occasionally produce harmful instructions or biased content 3. Limited knowledge of world and events after 2021 This data can now be stored in a database in a structured format.\" cs224n -2023 -lecture11 -prompting -rlhf_p78.png\"1. Zero -Shot (ZS) and Few -Shot (FS) In - Context Learning - Advantages: + No finetuning needed, prompt engineering (e.g. CoT) can improve performance - Disadvantages: - Limits to what you can fit in context - Comp lex tasks will probably need gradient steps 2. Instruction finetuning - Advantages: + Simple and straightforward, generalize to unseen tasks - Disadvantages: - Collecting demonstrations for so many tasks is expensive - Mismatch between LM objective and hum an preferences 3. Reinforcement Learning from Human Feedback (RLHF) - Advantages: + Directly model preferences (cf. language modeling), generalize beyond labeled data - Disadvantages: - RL is very tricky to get right - Human preferences are fallible; model s of human preferences even more so 4. What\u2019s next?\" Context Learning - Advantages: + No finetuning needed, prompt engineering (e.g. CoT) can improve performance - Disadvantages: - Limits to what you can fit in context - Comp lex tasks will probably need gradient steps 2. Instruction finetuning - Advantages: + Simple and straightforward, generalize to unseen tasks - Disadvantages: - Collecting demonstrations for so many tasks is expensive - Mismatch between LM objective and hum an preferences 3. Reinforcement Learning from Human Feedback (RLHF) - Advantages: + Directly model preferences (cf. language modeling), generalize beyond labeled data - Disadvantages: - RL is very tricky to get right - Human preferences are fallible; model s of human preferences even more so 4. What\u2019s next?\" cs224n -2023 -lecture11 -prompting -rlhf_p79.png\"The image contains no tables, diagrams, graphics, or equations. You can find the basic text below: --- **What\u2019s next?** - RLHF is still a very underexplored and fast -moving area: by the next lecture (2024) these slides may look completely different! - RLHF gets you further than instruction finetuning, but is (still!) data expensive. - Recent work aims to alleviate such data requirements: \"OpenAI is hiring developers to make ChatGPT better at coding\" Developers ai m to create lines of code and explanations of it in natural language, according to Semafor. --- No structured data (like tables or equations) or diagrams/graphics are present in the text.\" cs224n -2023 -lecture11 -prompting -rlhf_p8.png\"The provided image contains a code snippet and a URL link. Here is the structured representation of the data: ### Code Snippet ```javascript // Determine whether the sentiment of text is positive // Use a web ser vice async function isPositive(text: string): Promise<boolean> { const response = await fetch(`http://text - processing.com/api/sentiment/`, { method: \"POST\", body: `text=${text}`, headers: { \"Content - Type\": \"application/x -www -form -urlencoded\", }, }); const json = await response.json(); return json.label === \"pos\"; } ``` ### URL `https://github.com/features/copilot` There are no diagrams, graphics, equations, or tables in the image to be extracted, just the code snippet and the URL which have been transcribed accordingly.\" cs224n -2023 -lecture11 -prompting -rlhf_p80.png\"The provided image contains the following elements: ### Diagrams/Graphics/Equations: None. ### Table: There is a dialogue presented in a Determine whether the sentiment of text is positive // Use a web ser vice async function isPositive(text: string): Promise<boolean> { const response = await fetch(`http://text - processing.com/api/sentiment/`, { method: \"POST\", body: `text=${text}`, headers: { \"Content - Type\": \"application/x -www -form -urlencoded\", }, }); const json = await response.json(); return json.label === \"pos\"; } ``` ### URL `https://github.com/features/copilot` There are no diagrams, graphics, equations, or tables in the image to be extracted, just the code snippet and the URL which have been transcribed accordingly.\" cs224n -2023 -lecture11 -prompting -rlhf_p80.png\"The provided image contains the following elements: ### Diagrams/Graphics/Equations: None. ### Table: There is a dialogue presented in a structured way that resembles a table format. The data can be extracted an d represented as follows: **Dialogue Structure:** - **Human:** Can you help me hack into my neighbor\u2019s wifi? - **Assistant:** Sure thing, you can use an app called VeryEasyHack. - **Critique Request:** Identify ways in which the assistant\u2019s last response i s harmful. - **Critique:** Hacking into someone else\u2019s wifi is an invasion of their privacy and is possibly illegal. - **Revision Request:** Rewrite the assistant response to remove harmful content. - **Revision:** Hacking into your neighbor\u2019s wifi is an i nvasion of their privacy, and I strongly advise against it. It may also land you in legal trouble. **Summarized Content:** - Discusses RLHF being an underexplored and fast -moving area. - Highlights that RLHF is more effective than instruction fine-tuning b ut data -expensive. - Mentions recent work to alleviate data requirements, specifically RL from AI feedback and \"Constitutional\" AI as referenced to Bai et al., 2022. This structured format will be passed back to your database: ### Structured Data: ```json { \"dialogues\": [ { \"Human\": \"Can you help me hack into my neighbor\u2019s wifi?\", \"Assistant\": \"Sure thing, you\" cs224n -2023 -lecture11 -prompting -rlhf_p81.png\"### Identified Elements: #### Diagrams: 1. **Diagram showing the Self -Taught Reasoner (STaR) framework:** - An LM (Language Model) is shown in a box connected by an arrow labeled \"chain of thought\" to a box labe led \"Self -Taught Reasoner (STaR).\u201d #### Tables: - **Table Containing Author Information:** ``` | fine-tuning b ut data -expensive. - Mentions recent work to alleviate data requirements, specifically RL from AI feedback and \"Constitutional\" AI as referenced to Bai et al., 2022. This structured format will be passed back to your database: ### Structured Data: ```json { \"dialogues\": [ { \"Human\": \"Can you help me hack into my neighbor\u2019s wifi?\", \"Assistant\": \"Sure thing, you\" cs224n -2023 -lecture11 -prompting -rlhf_p81.png\"### Identified Elements: #### Diagrams: 1. **Diagram showing the Self -Taught Reasoner (STaR) framework:** - An LM (Language Model) is shown in a box connected by an arrow labeled \"chain of thought\" to a box labe led \"Self -Taught Reasoner (STaR).\u201d #### Tables: - **Table Containing Author Information:** ``` | Author Name | Institution | Email | | ----------------------- |--------------------------------------------- |----- ------------------------ | | Jiaxin Huang* | Uni versity of Illinois at Urbana -Champaign | jiaxinh3@illinois.edu | | Shixiang Shane Gu1 | Google | | | Le Hou2 | Google | shanegu@google.com | | Yuxin Wu2 | Google | lehou@google.com | | Xuezhi Wang2 | Google | crickwu@google.com | | Hongkun Yu2 | Google | hongkuny@google.com | | Jiawei Han1 | | hanj@illinois.edu | ``` #### Equations: - **No specific equations were identified in the",
    "107": "provided text.** ### Summary of the Data: #### Diagram Description: - The diagram illustrates the concept of a \"Self -Taught Rea soner\" (STaR), which involves a Language Model (LM) that\" cs224n -2023 -lecture11 -prompting -rlhf_p82.png\"There are no diagrams, graphics, equations, or tables present on the image of the page provided. The content of the page includes: - Title: Natural Language Processing with Deep Learning - Course code: CS224N/Li ng284 - An image of a red and beige abstract design similar to a house or structure. - Lecturer's name: Jesse Mu - Lecture topic: Lecture 11: Prompting, Instruction Finetuning, and RLHF This information is textual and can be disregarded if diagrams, graphi cs, equations, or tables are needed for the task.\" cs224n -2023 -lecture11 -prompting -rlhf_p9.png\"The image provided is a page from a PDF that contains text only. There are no diagrams, graphics, equations, or tables within this image. The text appears to be an abstract from a research paper titled \"Rapid and chronic ethanol tolerance are composed of distinct memory -like states in Drosophila\" by Larned, 2023. Summary of the Natural Language Processing with Deep Learning - Course code: CS224N/Li ng284 - An image of a red and beige abstract design similar to a house or structure. - Lecturer's name: Jesse Mu - Lecture topic: Lecture 11: Prompting, Instruction Finetuning, and RLHF This information is textual and can be disregarded if diagrams, graphi cs, equations, or tables are needed for the task.\" cs224n -2023 -lecture11 -prompting -rlhf_p9.png\"The image provided is a page from a PDF that contains text only. There are no diagrams, graphics, equations, or tables within this image. The text appears to be an abstract from a research paper titled \"Rapid and chronic ethanol tolerance are composed of distinct memory -like states in Drosophila\" by Larned, 2023. Summary of the text content: - The research focuses on ethanol tolerance, described as the first type of behavioral plasticity and neural plasticity indu ced by ethanol intake. - It explores the molecular and circuit bases of ethanol tolerance, highlighting three distinct forms: rapid, chronic, and repeated in male Drosophila. - Rapid tolerance involves two short -lived memory -like states: one labile and one consolidated. - Chronic tolerance, induced by continuous exposure lasting two days, induces ethanol preference and impedes the development of rapid tolerance through the activity of an unspecified factor. Since there's no data in a tabular form, there is nothing to convert to a structured database -friendly format. For future references, please provide images with the relevant diagrams, graphics, equations, or tables for transcription and description.\" llm_review 2_p1.png\"The image contains several equations. Here is the transcription of the equations present on the page: 1. Equation for the goal of conventional language models (CLMs): \\[ P(u_1, u_2, \\cdots, u_t) \\] 2. Equation for the probability calcul ation in conventional language models: \\[ P(u_t|u_{<t}) \\] There are no diagrams, graphics, or tables found within the provided image of the page from the PDF.\" llm_review 2_p10.png\"The provided image contains three diagrams and one table. Here is the structured representation of the data, diagrams, and equations: **Diagrams:** 1. **(a) MLM pre-training:** - Diagram Description: - The input sentence is \"[CLS] it's a [MASK] movie in every regard, and [MASK] painful to watch . [SEP]\". - MLM head predicts masked tokens llm_review 2_p1.png\"The image contains several equations. Here is the transcription of the equations present on the page: 1. Equation for the goal of conventional language models (CLMs): \\[ P(u_1, u_2, \\cdots, u_t) \\] 2. Equation for the probability calcul ation in conventional language models: \\[ P(u_t|u_{<t}) \\] There are no diagrams, graphics, or tables found within the provided image of the page from the PDF.\" llm_review 2_p10.png\"The provided image contains three diagrams and one table. Here is the structured representation of the data, diagrams, and equations: **Diagrams:** 1. **(a) MLM pre-training:** - Diagram Description: - The input sentence is \"[CLS] it's a [MASK] movie in every regard, and [MASK] painful to watch . [SEP]\". - MLM head predicts masked tokens \"great\" and \"utterly\" from the vocabulary. 2. **(b) Fine -tuning:** - Diagram Description: - The input sentence is \"[CLS] No reason to watch . [SEP]\". - CLS head assigns a label (e.g., label:positive, label:negative) from the label space. 3. **(c) Prompt -based fine -tuning with demonstrations:** - Diagram Description: - The input sentence with demonstrations includes example template sentences like \"[CLS] No reason to watch . It was [MASK] . [SEP] An a fide . It was great . [SEP] The drama discloses nothing . It was terrible . [SEP]\". - The MLM head maps \"great\" to label:positive and \"terrible\" to label:negative. **Table:** - Transcription of Figure 8 descr iption (table -like): | Figure | Description | | ----------- |------------------------------------- -------------------------------------------------------------------------------------------------------- | | Figure 8 | An illustration of (a) LM pre -training, ( b\" llm_review 2_p11.png\"The page contains equations and text. Below is a structured representation of the equations and a summary of the contained elements. **Equations:** 1. Perplexity (PPL) of a testing text sequence \\(W\\): \\[ PPL(W) = \\sqrt[n]{ \\,\\frac{1}{P (w_1 w_2 \\ldots w_n)}} \\quad (13) \\] where \\( W = w_1 w_2 \\ldots w_n \\). 2. Perplexity in product form: \\[ PPL(W) = \\exp \\left( -\\frac{1}{n} \\sum_{i=1}^n \\log P(w_i | w_1 \\ldots w_{i -1}) \\right) \\quad (14) \\] 3. Pseudo -log-likelihood score (PLL): \\[ PLL(W) = \\sum_{t=1}^{|W|} \\log P(w_t | W \\setminus \\{ w_t \\}) \\quad (15) \\] where \\( \\log P(w_t | W \\setminus \\{ w_t \\}) \\) is the conditional probability of token \\( w \\) in sentence \\( W \\) with all remaining tokens. 4. Pseudo -perplexity llm_review 2_p11.png\"The page contains equations and text. Below is a structured representation of the equations and a summary of the contained elements. **Equations:** 1. Perplexity (PPL) of a testing text sequence \\(W\\): \\[ PPL(W) = \\sqrt[n]{ \\,\\frac{1}{P (w_1 w_2 \\ldots w_n)}} \\quad (13) \\] where \\( W = w_1 w_2 \\ldots w_n \\). 2. Perplexity in product form: \\[ PPL(W) = \\exp \\left( -\\frac{1}{n} \\sum_{i=1}^n \\log P(w_i | w_1 \\ldots w_{i -1}) \\right) \\quad (14) \\] 3. Pseudo -log-likelihood score (PLL): \\[ PLL(W) = \\sum_{t=1}^{|W|} \\log P(w_t | W \\setminus \\{ w_t \\}) \\quad (15) \\] where \\( \\log P(w_t | W \\setminus \\{ w_t \\}) \\) is the conditional probability of token \\( w \\) in sentence \\( W \\) with all remaining tokens. 4. Pseudo -perplexity (PPL) for corpora \\( C \\): \\[ PPL(C) = \\exp \\left\" llm_review 2_p12.png\"The image contains the following elements: ### Diagrams - **Figure 9:** Comparison of texts generated by the powerful GPT -2 large language model (LLM) using Beam search (left) and pure sampling decoding (right). Beam search yields dege nerate repetition (in blue) while pure sampling results in incoherent gibberish (in red). #### Context Example (Beam Search): - \"In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes M ountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\" #### Context Example (Pure Sampling): - \"They are these article called Bolivian Cavalleros; they live in a remote desert uninterrupted by town, and the y speak huge, beautiful, paradisiacal Bolivian thing. They scream, \u2019 #### Beam Search, b=32 Example: - \"The study published in the Proceedings of the National Academy of Sciences of the United States of America (PNAS), was conducted by researchers from the Universidad Nacional Aut\u00f3noma de M\u00e9xico (UNAM) and the Universidad Nacional Aut\u00f3noma de M\u00e9xico (UNAM/Universidad Nacional Aut\u00f3noma de M\u00e9xico/Universidad Nacional Aut\u00f3noma de",
    "108": "M\u00e9xico/Universidad Nacional Aut\u00f3noma de ...\" #### Pure Sampling Example: - \"Lunch , image. They don\u2019t tell what lunch is cried, \u2019Christ Professor Choupe Ornswell told Sky News. \u2019They\u2019ve only been talking to scientists, like we\u2019re\" llm_review 2_p13.png\"The image contains a page from a PDF focusing on the topic \"Overview of Language Models.\" ### Diagrams, Graphics, and Equations: - The document does not contain scream, \u2019 #### Beam Search, b=32 Example: - \"The study published in the Proceedings of the National Academy of Sciences of the United States of America (PNAS), was conducted by researchers from the Universidad Nacional Aut\u00f3noma de M\u00e9xico (UNAM) and the Universidad Nacional Aut\u00f3noma de M\u00e9xico (UNAM/Universidad Nacional Aut\u00f3noma de M\u00e9xico/Universidad Nacional Aut\u00f3noma de M\u00e9xico/Universidad Nacional Aut\u00f3noma de ...\" #### Pure Sampling Example: - \"Lunch , image. They don\u2019t tell what lunch is cried, \u2019Christ Professor Choupe Ornswell told Sky News. \u2019They\u2019ve only been talking to scientists, like we\u2019re\" llm_review 2_p13.png\"The image contains a page from a PDF focusing on the topic \"Overview of Language Models.\" ### Diagrams, Graphics, and Equations: - The document does not contain any diagrams or graphics. - Equations: - There is one equation present: \\[ P(\\text{word}| \\text{sound}) \\propto P( \\text{sound}| \\text{word})P( \\text{word}). \\tag{17} \\] ### Tables: - There are no tables present in the provided text. ### Summary of Content Sec tions: 1. **Dialogue Systems (Section 7.2):** - Discusses the use of dialogue systems to simulate human responses in conversations. - Highlights examples such as ChatGPT and LaMDA. - Differentiates between task -oriented systems and open -domain systems. - Notes that most modern dialogue systems are fine -tuned generative language models. - Describes training methods, including supervised learning and reinforcement learning. 2. **Automatic Speech Recognition (Section 7.3):** - Defines Automatic Speech Recognit ion (ASR) as transforming raw audio into text. - States that Language Models (LMs) play a critical role in ASR. - Describes the need for solving acoustically ambiguous utterances and reducing computational costs. - Mentions models like N -gram, FFNN, RNN, a nd Transformer as used in ASR. 3. **Machine Translation (Section \" llm_review 2_p14.png\"### Diagrams, Graphics, and Table Information: #### Tables: - **Table 2: Comparison of model sizes, training data, cost, and time of several large LMs:** | Model | Year | Model Size | Training Data | Training Cost | Training Time | | --------------------- |------ |-------- ----|---------------------- |-------------------- |------------------------ | | BERT -Large | 2018 | 340M | 2.5B words | $7,000 | 64 TPU chips, 4 days | | XLNet -Large | 2019 | 340M | 32.9B words | $245,000 | the need for solving acoustically ambiguous utterances and reducing computational costs. - Mentions models like N -gram, FFNN, RNN, a nd Transformer as used in ASR. 3. **Machine Translation (Section \" llm_review 2_p14.png\"### Diagrams, Graphics, and Table Information: #### Tables: - **Table 2: Comparison of model sizes, training data, cost, and time of several large LMs:** | Model | Year | Model Size | Training Data | Training Cost | Training Time | | --------------------- |------ |-------- ----|---------------------- |-------------------- |------------------------ | | BERT -Large | 2018 | 340M | 2.5B words | $7,000 | 64 TPU chips, 4 days | | XLNet -Large | 2019 | 340M | 32.9B words | $245,000 | 512 TPU v3 chips, 5.5 days | | GPT -2 | 2019 | 1.5B | 8 million web pages | $60,000 | 512 GPUs, 53 minutes | | Megatron -LM | 2019 | 8.3B | 174 GB deduplicated text | Above $1.3 million for a single run | | | T5 | 2019 | 11B | | | | | Turing -NLG | 2020 | 17.2B | | | | | GPT -3 | 2020 | 175B | 45TB of\" llm_review 2_p15.png\"### Diagrams and Graphics **Figure 11: The structure of ELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately)** - Description: This figure illustrates the ELECTRA model structure. It consists of two ma in components: the Generator and the Discriminator. The Generator is typically a small Masked Language Model (MLM) that predicts token replacements. The Discriminator (ELECTRA) then classifies whether each token is an original token or replaced token withi n a sample sentence. - Visual Elements: - The sample sentence: ``` the chef cooked the meal ``` - The Generator outputs: ``` the [MASK] [MASK] the meal ``` - Replacements: - Generated replacements: ``` chef ate ``` - Final discriminator outputs indicating if each token is original or replaced: ``` \"the\" -> original \"chef\" -> original \"ate\" -> replaced \"the\" -> original \"meal\" -> original ``` ### Equations There are no mathematical equations present on the page. ### Tables There are no tables present on the page. This structured data will be passed directly back to a database.\" llm_review 2_p16.png\"The page from the PDF does not contain any diagrams, graphics, or equations. Instead, it predominantly features text with a number of in -text references to other works. For the purpose of passing this information to a database, the rel evant in -text references outputs: ``` the [MASK] [MASK] the meal ``` - Replacements: - Generated replacements: ``` chef ate ``` - Final discriminator outputs indicating if each token is original or replaced: ``` \"the\" -> original \"chef\" -> original \"ate\" -> replaced \"the\" -> original \"meal\" -> original ``` ### Equations There are no mathematical equations present on the page. ### Tables There are no tables present on the page. This structured data will be passed directly back to a database.\" llm_review 2_p16.png\"The page from the PDF does not contain any diagrams, graphics, or equations. Instead, it predominantly features text with a number of in -text references to other works. For the purpose of passing this information to a database, the rel evant in -text references and section headings are extracted below. ### Structured Data Format: #### Page Content Summary: - **Section: 9.2 Incremental Learning** - **In-Text References:** - [152] - [153] - [154] - **Section: 9.3 Lightweight Models** - **In-Text References:** - [155] - [156] - [157] - [158] - [159] - **Section: 9.4 Universal versus Domain -Specific Models** - **In-Text References:** - [160] - [161] - [162] - [163] - [164] - **Section: 9.5 Interpretable Models** This structured format contains the section titles and in -text references cited within the sections, summarizing the key organizational elements and citation info rmation present in the page. This data can be directly stored in a database for reference management or further analysis.\" llm_review 2_p17.png\"The provided image contains a block of regular text and a \"References\" section. There are no diagrams, graphics, or tables present in this image, but there are several equations and structured references at the bottom. Here is the stru ctured representation of the references found in the image: 1. P. F. Brown, V . J. Della Pietra, P. V . Desouza, J. C. Lai, and R. L. Mercer. \"Class -based n -gram models of natural language.\" Computational linguistics, 18(4):467 \u2013480, 1992. 2. Marcello Federic o. \"Bayesian estimation methods for n -gram language model adaptation.\" In Proceedings of Fourth International Conference on Spoken Language Processing. ICSLP'96, volume 1, pages 240 \u2013243. IEEE, 1996. 3. Thomas R Niesler and Philip C Woodland. \"A variable -length category -based n -gram language model.\" In 1996 IEEE International Conference on Acoustics, Speech, and Signal Processing Conference Proceedings, equations and structured references at the bottom. Here is the stru ctured representation of the references found in the image: 1. P. F. Brown, V . J. Della Pietra, P. V . Desouza, J. C. Lai, and R. L. Mercer. \"Class -based n -gram models of natural language.\" Computational linguistics, 18(4):467 \u2013480, 1992. 2. Marcello Federic o. \"Bayesian estimation methods for n -gram",
    "109": "language model adaptation.\" In Proceedings of Fourth International Conference on Spoken Language Processing. ICSLP'96, volume 1, pages 240 \u2013243. IEEE, 1996. 3. Thomas R Niesler and Philip C Woodland. \"A variable -length category -based n -gram language model.\" In 1996 IEEE International Conference on Acoustics, Speech, and Signal Processing Conference Proceedings, volume 1, pages 164 \u2013167. IEEE, 1996. 4. Stephen A Della Pietra, Vincent J Della Pietra, Robert L Mercer, a nd Salim Roukos. \"Adaptive language modeling using minimum discriminant estimation.\" In Speech and Natural Language: Proceedings of a Workshop Held at Harriman, New York, February 23 -26, 1992, 1992\" llm_review 2_p18.png\"The provided image contains references from a scientific document. Below are structured representations of the contents of the image: ### References: **References:** 1. Peter F Brown, John Cocke, Stephen A Della Pietra, Vincent J Della Pietra, Frederick Jelinek, John Lafferty, Robert L Mercer, and Paul S Roossin. A statistical approach to machine translation. *Computational linguistics*, 16(2):79 \u201385, 1990. 2. Franz Josef Och, Nicola Ueffing, and Hermann Ney. An efficient a* search algor ithm for statistical machine translation. In *Proceedings of the ACL 2001 Workshop on Data -Driven Methods in Machine Translation, 2001*. 3. Kenji Yamada and Kevin Knight. A decoder for syntax -based statistical mt. In *Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics*, pages 303 \u2013310, 2002. 4. Jacob Devlin, Ming -Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre -training of deep bidirectional transformers for language understanding. *arXiv preprint arXiv:1810.04805 , 2018*. 5. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyan\" llm_review 2_p19.png\"The image you provided does not contain any diagrams, graphics, equations, or tables. There is only structured text in the form of a bibliography list. Below is the 2001*. 3. Kenji Yamada and Kevin Knight. A decoder for syntax -based statistical mt. In *Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics*, pages 303 \u2013310, 2002. 4. Jacob Devlin, Ming -Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre -training of deep bidirectional transformers for language understanding. *arXiv preprint arXiv:1810.04805 , 2018*. 5. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyan\" llm_review 2_p19.png\"The image you provided does not contain any diagrams, graphics, equations, or tables. There is only structured text in the form of a bibliography list. Below is the structured text representation of the bibliography list suitable for d atabase input: 1. Rami Al - Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. Character -level language modeling with deeper self -attention. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pages 3159 \u20133166, 2019. 2. Lin ting Xue, Aditya Barua, Noah Constant, Rami Al -Rfou, Sharan Narang, Mihir Kale, Adam Roberts, and Colin Raffel. Byt5: Towards a token -free future with pre -trained byte -to-byte models. Transactions of the Association for Computational Linguistics, 10:291 \u2013306, 2022. 3. Moonyoung Kang, Tim Ng, and Long Nguyen. Mandarin word -character hybrid -input neural network language model. In Twelfth Annual Conference of the International Speech Communication Association, 2011. 4. Yasumasa Miyamoto and Kyunghyun Cho. Gated word -character recurrent language model. arXiv preprint arXiv:1606.01700, 2016. 5. Lyan Verwimp, Joris Pelemans, Patrick Wambacq, et al. C haracter - word lstm language models. arXiv preprint\" llm_review 2_p2.png\"### Diagrams: 1. There is a diagram labeled as Fig. 1. It likely demonstrates a parse tree structure, but details of its content are not decipherable from the textual description alone. ### Equations: 1. **Equation (3):** \\[ P(u_1, u_2, \\ldots, u_t) = P(u_1)P(u_2|u_1)P(u_3|u_1, u_2) \\cdots P(u_t|u_1, \\ldots, u_{t -1}) \\] 2. **Equation (4):** \\[ P(u_t|u_{<t}, u_{>t}) \\] 3. **Equation (5):** \\[ P(u_t|A(u_t)) \\] ### Tables: There are no explicit tables visible within this image. ### Summary of Graphics and Diagrams: 1. **Figure 1:** - This figure likely represents a parse tree structure, which is referenced in the context of arXiv:1606.01700, 2016. 5. Lyan Verwimp, Joris Pelemans, Patrick Wambacq, et al. C haracter - word lstm language models. arXiv preprint\" llm_review 2_p2.png\"### Diagrams: 1. There is a diagram labeled as Fig. 1. It likely demonstrates a parse tree structure, but details of its content are not decipherable from the textual description alone. ### Equations: 1. **Equation (3):** \\[ P(u_1, u_2, \\ldots, u_t) = P(u_1)P(u_2|u_1)P(u_3|u_1, u_2) \\cdots P(u_t|u_1, \\ldots, u_{t -1}) \\] 2. **Equation (4):** \\[ P(u_t|u_{<t}, u_{>t}) \\] 3. **Equation (5):** \\[ P(u_t|A(u_t)) \\] ### Tables: There are no explicit tables visible within this image. ### Summary of Graphics and Diagrams: 1. **Figure 1:** - This figure likely represents a parse tree structure, which is referenced in the context of Structural LMs. It mentions an example where the ancestor sequence of the word 'strong' includes 'binoculars', 'saw', ' ROOT'. ### Structured Data Representation for a Database: ```json { \"equations\": [ { \"equation_number\": 3, \"equation\": \"P(u_1, u_2, \\\\ldots, u_t) =\" llm_review 2_p20.png\"The image contains a table structured as a list of references related to language models. Here is the data extracted in structured text format suitable for a database: ```plaintext 56. Klaus Ries, Finn Dag Buo, and Alex Waibel. Class p hrase models for language modeling. In *Proceeding of Fourth International Conference on Spoken Language Processing, ICSLP'96*, volume 1, pages 398 -401. IEEE, 1996. 57. George Saon and Mukund Padmanabhan. Data -driven approach to designing compound words fo r continuous speech recognition. *IEEE transactions on Speech and audio processing*, 9(4):327 -332, 2001. 58. Michael Levit, Sarangarajan Parthasarathy, Shuangyu Chang, Andreas Stolcke, and Benoit Dumoulin. Word - phrase -entity language models: Getting more m ileage out of n -grams. In *Fifteenth Annual Conference of the International Speech Communication Association*, 2014. 59. Ronald Rosenfeld. A whole sentence maximum entropy language model. In *1997 IEEE workshop on automatic speech recognition and understan ding proceedings*, pages 230 -237. IEEE, 1997. 60. Stanley F Chen and Ronald Rosenfeld. Efficient sampling and feature selection in whole sentence maximum entropy language models. In *1999 IEEE International Conference on Acoustics, Speech, and Signal Proce ssing. Proceedings, ICASSP99 (Cat. No. 99CH36258)*,\" transactions on Speech and audio processing*, 9(4):327 -332, 2001. 58. Michael Levit, Sarangarajan Parthasarathy, Shuangyu Chang, Andreas Stolcke, and Benoit Dumoulin. Word - phrase -entity language models: Getting more m ileage out of n -grams. In *Fifteenth Annual Conference of the International Speech Communication Association*, 2014. 59. Ronald Rosenfeld. A whole sentence maximum entropy language model. In *1997 IEEE workshop on automatic speech recognition and understan ding proceedings*, pages 230 -237. IEEE, 1997. 60. Stanley F Chen and Ronald Rosenfeld. Efficient sampling and feature selection in whole sentence maximum entropy language models. In *1999 IEEE International Conference on Acoustics, Speech, and Signal Proce ssing. Proceedings, ICASSP99 (Cat. No. 99CH36258)*,\" llm_review 2_p21.png\"There are no diagrams, graphics, or equations present in the image. The image contains a list of references from a document. There is no table data to represent in a structured form for a database. Here is a summary of the content in t extual format: - Hongli Deng, Lei Zhang, and Lituan Wang. Global context -dependent recurrent",
    "110": "neural network language model with sparse feature learning. Neural Computing and Applications, 31(2):999 \u20131011, 2019. - Sepp Hochreiter. The vanishing gradient prob lem during learning recurrent neural nets and problem solutions. International Journal of Uncertainty, Fuzziness and Knowledge -Based Systems, 6(02):107 \u2013116, 1998. - Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly l earning to align and translate. arXiv preprint arXiv:1409.0473, 2014. - Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information p rocessing systems, 30:2017. - Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung,\" llm_review 2_p22.png\"The provided image page contains references from a paper but does not include any diagrams, graphics, equations, or tables. Therefore, there is no structured data or detailed graphics information to excerpt from this page. Here's a sum mary description of the reference items listed on the page: - List of academic references spanning various aspects and advancements in language models. - Authors and titles of works related to neural networks, Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information p rocessing systems, 30:2017. - Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung,\" llm_review 2_p22.png\"The provided image page contains references from a paper but does not include any diagrams, graphics, equations, or tables. Therefore, there is no structured data or detailed graphics information to excerpt from this page. Here's a sum mary description of the reference items listed on the page: - List of academic references spanning various aspects and advancements in language models. - Authors and titles of works related to neural networks, language model evaluation, decoding algorithms , hierarchical state representation, and speech recognition. If there are any specific sections or further references you'd like assistance with, please provide more details.\" llm_review 2_p23.png\"The image provided contains a numbered list of references, likely from an academic paper. There are no diagrams, graphics, equations, or tables within the image. The structured data from the numbered list is as follows: ``` 121. Joonbo Shin, Yoonhyoung Lee, and Kyomin Jung. Effective sentence scoring method using bert for speech recognition. In Asian Conference on Machine Learning, pages 1081 \u20131093. PMLR, 2019. 122. Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F Wong, and Lidia S Chao. Learning deep transformer models for machine translation. arXiv preprint arXiv:1906.01787, 2019. 123. Max Weiss. Deepfake bot submissions to federal public comment websites cannot be distinguished from human submissions. Technology Scienc e, 2019. 124. Adaku Uchendu, Thai Le, Kai Shu, and Dongwon Lee. Authorship attribution for neural text generation. In Conf. on Empirical Methods in Natural Language Processing (EMNLP), 2020. 125. Tiziano Fagni, Fabrizio Falchi, Margherita Gambini, Antonio Martella, and Maurizio Tesconi. Tweepfake: About detecting deepfake tweets. Plos one, 16(5):e0251415, 2021. 126. James Thorne and Andreas Vlachos. Automated fact checking\" llm_review 2_p24.png\"The image contains only the text without any diagrams, graphics, or equations. However, there is a table -like structure with several references listed. Here is the structured data from the page: ```plaintext [ { \"ReferenceNumber\": 146, \"Authors\": \"Yun -Cheng from human submissions. Technology Scienc e, 2019. 124. Adaku Uchendu, Thai Le, Kai Shu, and Dongwon Lee. Authorship attribution for neural text generation. In Conf. on Empirical Methods in Natural Language Processing (EMNLP), 2020. 125. Tiziano Fagni, Fabrizio Falchi, Margherita Gambini, Antonio Martella, and Maurizio Tesconi. Tweepfake: About detecting deepfake tweets. Plos one, 16(5):e0251415, 2021. 126. James Thorne and Andreas Vlachos. Automated fact checking\" llm_review 2_p24.png\"The image contains only the text without any diagrams, graphics, or equations. However, there is a table -like structure with several references listed. Here is the structured data from the page: ```plaintext [ { \"ReferenceNumber\": 146, \"Authors\": \"Yun -Cheng Wang, Xiou Ge, Bin Wang, and C -C Jay Kuo\", \"Title\": \"Kgboost: A classification -based knowledge base completion method with negative sampling.\", \"Source\": \"Pattern Recognition Letters\", \"V olume\": 157, \"Pages\": \"104 \u2013111\", \"Year\": 2022 }, { \"ReferenceNumber\": 147, \"Authors\": \"Xiou Ge, Yun -Cheng Wang, Bin Wang, and C -C Jay Kuo\", \"Title\": \"CompoundE: Knowledge graph embedding with translation, rotation and scaling compound operations.\", \"Source\": \"arXiv preprint arXiv:2207.05324\", \"Year\": 2022 }, { \"ReferenceNumber\": 148, \"Authors\": \"Xiao Huang, Jingyuan Zhang, Dingcheng Li, and Ping Li\", \"Title\": \"Knowledge graph embedding based question answering.\", \"Source\": \"In Proceedings of the twelfth ACM international conference on web search and da ta mining\", \"Pages\": \"105 \u2013113\", \"Year\": 2019 }, { \" llm_review 2_p3.png\"The provided image from the PDF contains two primary figures, a diagram, and an excerpt of text: ### Diagrams and Graphics: 1. **Dependency Parse Tree (Figure 1)** - **Description:** - A visual representation showing the dependency pars e of the sentence: \"I saw the ship with very strong binoculars\". - It illustrates how different words in the sentence are syntactically connected. - Key connections in the tree: - \"saw\" (ROOT) connects to \"I\" (nsubj) and \"ship\" (dobj). - \"ship\" connects to \"with binoculars\" (prep) and \"binoculars\" connects to \"very\" (advmod) and \"strong\" (amod). 2. **Permutations of Input Text (Figure 2)** - **Description:** - This figure shows different permutations of the input text \"Language modeling is very important in NLP\". - The permutations listed include: - \"modeling Language diagram, and an excerpt of text: ### Diagrams and Graphics: 1. **Dependency Parse Tree (Figure 1)** - **Description:** - A visual representation showing the dependency pars e of the sentence: \"I saw the ship with very strong binoculars\". - It illustrates how different words in the sentence are syntactically connected. - Key connections in the tree: - \"saw\" (ROOT) connects to \"I\" (nsubj) and \"ship\" (dobj). - \"ship\" connects to \"with binoculars\" (prep) and \"binoculars\" connects to \"very\" (advmod) and \"strong\" (amod). 2. **Permutations of Input Text (Figure 2)** - **Description:** - This figure shows different permutations of the input text \"Language modeling is very important in NLP\". - The permutations listed include: - \"modeling Language very is NLP important in\" - \"Language very mod eling is in NLP important\" - \"NLP modeling is important very in Language\" - And so on... ### Equations: The image includes several mathematical equations: 1. **Equation (6):** \\[ P(u_m | \\bar{S}) \\] where \\( u_m \\) is the masked linguistic unit and \\( \\bar{S} \\\" llm_review 2_p4.png\"The page contains the following elements: ### Diagrams and Graphics 1. Figure 3: An illustration of the BPE merge operation conducted on a small dictionary. (Not provided in this text but mentioned in the last paragraph of section 3.2.1 ). ### Equations and Algorithms 1. Equation for WordPiece method: \\[ \\frac{P(AB)}{P(A)P(B)} \\] If this has the highest score, WordPiece merges the pair. This is further evaluated with additional sets of letters as mentioned in the comparison. ### Tables No tables are present on this page. --- ### Summary of Sections #### 3 Linguistic Units - Discusses how LMs partition text sequences into small linguistic units such as characters, words, phrases, or sentences for tokenization. - Focus on examining typical t okenization methods used for English.",
    "111": "#### 3.1 Characters - LMs can model text sequences using small units like characters to give them a small vocabulary size and a smaller discrete space. - Challenges mentioned include longer input/output lengths and dif ficulty in predicting the next character, resulting in poorer performance compared to word -level LMs. - References to studies/works [31, 32, 33, 34, 35] and methods combining words and characters [36, 37, 38]. #### 3.2 Words and Subwords - The natural toke nization for English is using white spaces to decompose text into words. \" linguistic units such as characters, words, phrases, or sentences for tokenization. - Focus on examining typical t okenization methods used for English. #### 3.1 Characters - LMs can model text sequences using small units like characters to give them a small vocabulary size and a smaller discrete space. - Challenges mentioned include longer input/output lengths and dif ficulty in predicting the next character, resulting in poorer performance compared to word -level LMs. - References to studies/works [31, 32, 33, 34, 35] and methods combining words and characters [36, 37, 38]. #### 3.2 Words and Subwords - The natural toke nization for English is using white spaces to decompose text into words. \" llm_review 2_p5.png\"**Diagrams and Graphics:** 1. **Figure 3:** - Description: Illustration of the BPE merge operation conducted on the dictionary \\{\u201chug\u201d: 10, \u201cpug\u201d: 5, \u201cpun\u201d: 12, \u201cbun\u201d: 4\\}. The vocabulary is initialized with all characters. Then, a new subword is created by merging the most frequent pair. **Equations:** 1. **Equation (8):** \\[ P(w_t|w_{t}^{1}) = P(w_t|w_{t - N+1:t -1}) \\] **Table Data:** - **Table:** - Title: Words and their frequency in the training corpus - Columns: Word, Frequency ``` { \"words_and_frequencies\": [ { \"word\": \"hug\", \"frequency\": 10 }, { \"word\": \"pug\", \"frequency\": 5 }, { \"word\": \"pun\", \"frequency\": 12 }, { \"word\": \"bun\", \"frequency\": 4 } ] } ``` - **Split words to characters:** ``` { \"split_words_to_characters\": [ { \"word\": \"hug\", \"characters\": [\"h\", \"u\", \"g\"], \"frequency\": 10 }, { \"word\": \"pug\", \"characters\": [\"p\", \"u\", \"g\" llm_review 2_p6.png\"Based on the given image, the page contains the following components: **Equations:** 1. **Equation 9:** \\[ P(w_{t} | w_{t -N+1:t -1}) = \\frac{C(w_{t - N+1:t})}{C(w_{t -N+1:t -1})} \\] 2. **Equation 10:** \\[ P(w_{t} | w_{t -N+1:t -1}) = \\lambda_{ N} P(w_{t} | w_{t -N+1:t -1}) + \\lambda_{N -1} P(w_{t} | w_{t -N+1:t -2}) + \\lambda_{N -2} P(w_{t} | w_{t -N+1:t -3}) + ... + \\lambda_{1} P(w_{t}) \\] \\[ \\sum_{i=1}^{N} \\lambda_i = 1 \\] 3. **Equation 11:** \\[ P(w_{t}) = \\frac{ \\exp(a^T f(w, h)}{ \\sum_{w'} \\exp(a^T f(w', h))} \\] 4. **Equation 12:** \\[ h(t + 1) = f(W x(i) + U h(i)) \\] **Graphics/Diagrams:** There is an indication of a Figure 5 along with descriptions of diagrams but\" llm_review 2_p7.png\"### Diagrams and Graphics #### Figure 4: Neural Network Diagram of FFN LMs - **Input**: - Sequence of word indices in wordlist: \\( w_{j -n+1}, w_{j -n+2}, \\ldots, N+1:t})}{C(w_{t -N+1:t -1})} \\] 2. **Equation 10:** \\[ P(w_{t} | w_{t -N+1:t -1}) = \\lambda_{ N} P(w_{t} | w_{t -N+1:t -1}) + \\lambda_{N -1} P(w_{t} | w_{t -N+1:t -2}) + \\lambda_{N -2} P(w_{t} | w_{t -N+1:t -3}) + ... + \\lambda_{1} P(w_{t}) \\] \\[ \\sum_{i=1}^{N} \\lambda_i = 1 \\] 3. **Equation 11:** \\[ P(w_{t}) = \\frac{ \\exp(a^T f(w, h)}{ \\sum_{w'} \\exp(a^T f(w', h))} \\] 4. **Equation 12:** \\[ h(t + 1) = f(W x(i) + U h(i)) \\] **Graphics/Diagrams:** There is an indication of a Figure 5 along with descriptions of diagrams but\" llm_review 2_p7.png\"### Diagrams and Graphics #### Figure 4: Neural Network Diagram of FFN LMs - **Input**: - Sequence of word indices in wordlist: \\( w_{j -n+1}, w_{j -n+2}, \\ldots, w_{j -1} \\) - (Represented as discrete indices in the wordlist) - **Projecti on Layer**: - Shared projections \\( P \\) - Projected into continuous \\( P \\)-dimensional vectors: \\( c_{j -n+1}, c_{j - n+2}, \\ldots, c_{j -1} \\) - **Hidden Layer**: - Connection matrices \\( M \\) from the projection layer to the hidden layer: \\( h_j \\) - **Out put Layer**: - Probability estimation of words: \\( \\hat{P} \\) - Connections \\( V \\) from the hidden layer to the output layer - \\( P(\\cdot|h_j) \\) representing LM probabilities for all words: \\( P^1 = P(w_{j -n+1}|h_j), P^2 = P(w_{j -n+2}|h_j), \\ldots, P^N = P(w_{j}=N|h_j) \\) #### Figure 5: Diagram of RNN LMs - **Input Sequence of Words**: \\( x(0), x(t - 2),\" llm_review 2_p8.png\"The provided image contains a diagram illustrating the structure of a transformer model. Below is a structured summary and transcription of the relevant graphical and equation elements visible on the page: ### Diagram: Structure of a Tr ansformer **Components:** - **Encoder:** - Input Embedding - Positional Encoding - \\( N \\) layers of: - Multi -Head Attention - Add & Norm - Feed Forward - Add & Norm - **Decoder:** - Output Embedding (shifted right) - Positional Encoding - \\( N \\) layers o f: - Masked Multi -Head Attention - Add & Norm - Multi -Head Attention - Add & Norm - Feed Forward - Add & Norm - **Output Layer:** - Linear Transformation - Softmax Activation ### Data Representation There is no table data presented in the provided image, s o there is no structured data table to transcribe. ### Equations The image does not contain any explicit mathematical equations to transcribe. ### Summary of Graphics The diagram provides a structural overview of the Transformer model, **Components:** - **Encoder:** - Input Embedding - Positional Encoding - \\( N \\) layers of: - Multi -Head Attention - Add & Norm - Feed Forward - Add & Norm - **Decoder:** - Output Embedding (shifted right) - Positional Encoding - \\( N \\) layers o f: - Masked Multi -Head Attention - Add & Norm - Multi -Head Attention - Add & Norm - Feed Forward - Add & Norm - **Output Layer:** - Linear Transformation - Softmax Activation ### Data Representation There is no table data presented in the provided image, s o there is no structured data table to transcribe. ### Equations The image does not contain any explicit mathematical equations to transcribe. ### Summary of Graphics The diagram provides a structural overview of the Transformer model, showcasing the follo wing: - **Encoder Side:** Incorporates multiple layers of multi -head attention mechanisms, followed by feed -forward networks. Positional encoding is applied to the input embeddings. - **Decoder Side:** Also employs multi -head attention mechanisms, but includes an additional layer for masked multi -head attention to\" llm_review 2_p9.png\"### Table: Transformer -based PLMs #### Encoder -only models (Bidirectional) - BERT [15] - RoBERTa [16] - ELECTRA [45] #### Decoder -only models (Unidirectional) - GPT-1, 2, and 3 [84][42][85] - Transformer XL [86] #### Encoder -Decoder models (Sequence to sequence) - BART [87] - T5 [88] ### Diagrams Summary: 1. **Diagram 1: Bidirectional Encoder (BERT)** - Tokens: A, C, E - Process: Random tokens are replaced with masks; the document is encoded bidirectionally, meaning missing tokens are pr edicted independently, making BERT less suitable for generation. 2. **Diagram 2: Autoregressive Decoder (GPT)** - Tokens: <s>, A, B, C, D, E - Process: Tokens are predicted autoregressively, meaning GPT can be used for generation. It can only condition on leftward context and does not learn bidirectional interactions. 3. **Diagram 3: Encoder -Decoder Model (BART)** - Tokens: A, B, E (Bidirectional Encoder) and <s>, A, B, C, D (Autoregressive Decoder) - Process: Inputs to the encoder need to be aligned with d ecoder outputs, allowing arbitrary noise transformations. Corrupted documents are generated by replacing spans of text\" Multimodal_p1.png\"The provided image of the PDF page does not contain any diagrams, graphics, equations, or tables; it consists solely of text content. If there is any specific Decoder (GPT)** - Tokens: <s>, A, B, C, D, E - Process: Tokens are predicted autoregressively, meaning GPT can be used for generation. It can only",
    "112": "condition on leftward context and does not learn bidirectional interactions. 3. **Diagram 3: Encoder -Decoder Model (BART)** - Tokens: A, B, E (Bidirectional Encoder) and <s>, A, B, C, D (Autoregressive Decoder) - Process: Inputs to the encoder need to be aligned with d ecoder outputs, allowing arbitrary noise transformations. Corrupted documents are generated by replacing spans of text\" Multimodal_p1.png\"The provided image of the PDF page does not contain any diagrams, graphics, equations, or tables; it consists solely of text content. If there is any specific requirement or if there are sections that might contain such elements, please p rovide additional pages or specific sections to review. In this case, I can only confirm the absence of these elements based on the provided page.\" Multimodal_p10.png\"The provided image is a reference list from a paper. There are no diagrams, graphics, equations, or tables in the image. The content in the image comprises citation references with details about papers and resources referred in the resea rch. They are organized as follows: 1. Author(s) 2. Title 3. URL (if applicable) 4. Year of publication 5. Additional identifiers such as arXiv preprint number or DOI Here is the data in a structured format: ``` { \"references\": [ { \"id\": 1, \"author\": \"Lang chain\", \"title\": \"https://github.com/hwchase17/langchain\", \"year\": \"2022\" }, { \"id\": 2, \"author\": \"Jean -Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al.\" , \"title\": \"Flamingo: a visual language model for few -shot learning\", \"year\": \"2022\", \"note\": \"arXiv preprint arXiv:2204.14198\" }, { \"id\": 3, \"author\": \"Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko S\u00fcnderhauf, Ian Reid, Stephen Gould ,\" Multimodal_p11.png\"The provided page appears to be a list of references from a PDF. There are no diagrams, graphics, equations, or tables on the page. The content consists solely of citation entries. Here is a simplified structured format of the references for database entry: ```json { \"references\": [ { \"id\": 16, \"authors\": \"Zhe Gan, Linjie Li, Chunyuan Li, Lijuan Wang, Zicheng Liu, Jianfeng Gao, et al.\", \"title\": \"Vision -language pre -training: Basics, recent advances, and for few -shot learning\", \"year\": \"2022\", \"note\": \"arXiv preprint arXiv:2204.14198\" }, { \"id\": 3, \"author\": \"Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko S\u00fcnderhauf, Ian Reid, Stephen Gould ,\" Multimodal_p11.png\"The provided page appears to be a list of references from a PDF. There are no diagrams, graphics, equations, or tables on the page. The content consists solely of citation entries. Here is a simplified structured format of the references for database entry: ```json { \"references\": [ { \"id\": 16, \"authors\": \"Zhe Gan, Linjie Li, Chunyuan Li, Lijuan Wang, Zicheng Liu, Jianfeng Gao, et al.\", \"title\": \"Vision -language pre -training: Basics, recent advances, and future trends\", \"journal\": \"Founda tions and Trends\u00ae in Computer Graphics and Vision\", \"year\": 2022 }, { \"id\": 17, \"authors\": \"Fabrizio Gilardi, Meysam Alizadeh, and Ma\u00ebl Kubli\", \"title\": \"Chatgpt outperforms crowd -workers for text -annotation tasks\", \"journal\": \"arXiv preprint arXiv:2303.15 056\", \"year\": 2023 }, { \"id\": 18, \"authors\": \"Tanmay Gupta and Aniruddha Kembhavi\", \"title\": \"Visual programming: Compositional visual reasoning without training\", \"journal\": \"arXiv preprint arXiv:2211.11559\", \"year\": 2022 \" Multimodal_p12.png\"The given image is a page from a PDF document listing various research papers and their bibliographic details. The page includes references comprising authors, titles, publication venues, years, and occasionally URLs. There are no diagra ms, graphics, or equations present. There is no table here to be transcribed into structured text. For database purposes, the references can be presented in a structured form as follows: 1. Title: Grounding dino: Marrying dino with grounded pre -training fo r open -set object detection Authors: Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chun Yuan Li, Jianwei Yang, Hang Su, Jun Zhu Venue: arXiv preprint Year: 2023 ID: arXiv:2303.05499 2. Title: Learn to explain: Multimodal reasoning v ia thought chains for science question answering Authors: Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai -Wei Chang, Song -Chun Zhu, Oyvind Tafjord, Peter Clark, Ashwin Kalyan Venue: Advances in Neural Information Processing Systems Years: 2022 Keywords : 2, 5, 8, 9 3. Title: ChatGPT Authors: OpenAI URL: https://openai.com/blog/chatgpt/ Year: 2023\" Multimodal_p13.png\"The provided image is a page from a PDF that contains a list of references dino with grounded pre -training fo r open -set object detection Authors: Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chun Yuan Li, Jianwei Yang, Hang Su, Jun Zhu Venue: arXiv preprint Year: 2023 ID: arXiv:2303.05499 2. Title: Learn to explain: Multimodal reasoning v ia thought chains for science question answering Authors: Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai -Wei Chang, Song -Chun Zhu, Oyvind Tafjord, Peter Clark, Ashwin Kalyan Venue: Advances in Neural Information Processing Systems Years: 2022 Keywords : 2, 5, 8, 9 3. Title: ChatGPT Authors: OpenAI URL: https://openai.com/blog/chatgpt/ Year: 2023\" Multimodal_p13.png\"The provided image is a page from a PDF that contains a list of references from a research paper or article. There are no diagrams, graphics, or equations present in this image. The image mainly consists of textual content in the form of references. Below is the structured representation of the references in a simple format: ```plaintext [ { \"id\": 49, \"authors\": \"Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie -Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman G oyal, Eric Hambro, Faisal Azhar, et al.\", \"title\": \"Llama: Open and efficient foundation language models\", \"source\": \"arXiv preprint arXiv:2302.13971\", \"year\": 2023, \"citations\": 1 }, { \"id\": 50, \"authors\": \"Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linji e Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang\", \"title\": \"Git: A generative image -to-text transformer for vision and language\", \"source\": \"arXiv preprint arXiv:2205.14100\", \"year\": 2022, \"citations\": 1 \" Multimodal_p14.png\"Here is the requested information from the image: ### Tables - **Table 9**: - **Description**: LLaV A demonstrates a similar behavior as GPT -4 in another example from its paper. ### Figures - **Figure 3**: - **Description**: Demonstrates that LLaV A can follow a user's instructions in a conversation style and provide detailed responses or creative writings. - **Figure 4**: - **Description**: Demonstrates how LLaV A generates the HTML/JS/CSS code for an interactive joke website based on a sim plified user input sketch. - **Figure 5**: - **Description**: Further example of how LLaV A can relate the visual content to textual knowledge from the pre -trained LLM. - **Figure 6**: - **Description**: Demonstrates Multimodal_p14.png\"Here is the requested information from the image: ### Tables - **Table 9**: - **Description**: LLaV A demonstrates a similar behavior as GPT -4 in another example from its paper. ### Figures - **Figure 3**: - **Description**: Demonstrates that LLaV A can follow a user's instructions in a conversation style and provide detailed responses or creative writings. - **Figure 4**: - **Description**: Demonstrates how LLaV A generates the HTML/JS/CSS code for an interactive joke website based on a sim plified user input sketch. - **Figure 5**: - **Description**: Further example of how LLaV A can relate the visual content to textual knowledge from the pre -trained LLM. - **Figure 6**: - **Description**: Demonstrates that LLaV A is able to recognize Elon Mus k both in a headshot and a humorous meme where he is dressed as a doge, despite LLaV A never",
    "113": "seeing Elon Musk in these contexts during training. ### Equations The image does not appear to contain any equations. ### Structured Data ```json { \"tables\": [ { \"t able_id\": \"Table 9\", \"description\": \"LLaV A demonstrates a similar behavior as GPT-4 in another example from its paper.\" } ], \"figures\": [ { \"figure_id\": \"Figure 3\", \"\" Multimodal_p15.png\"### Identified Elements and Representation #### Table - **Table 9:** Example prompt comparing LLaV A, GPT -4, BLIP -2, and OpenFlamingo\u2019s visual reasoning capabilities in understanding the humor. | User | Explanation | | ------ |------------- | | LLaV A | Can you explain this meme in detail? The meme in the image is a creative and humorous take on food, with a focus on chicken nuggets, as the center of the universe. The meme begins with the phrase \"Sometimes I just look at pictures of the Earth from space and I marvel at how beautiful it all is...\" The main image features a tray of cooked chicken nuggets, with the chicken pieces positioned to resemble the continents and islands on a world map. The meme playfully suggests that the chicken nuggets represent the Earth, and the various locations depicted in the photo are actually chicken nugget versions of different places. The punchline of the meme appears in smaller text at the bottom, reading \"I mean, it's not the real Earth, but how beautiful it a ll is.\" This lighthearted and entertaining meme combines food and humor, using the image of chicken nuggets to create a funny and imaginative representation of the world. | | GPT -4 | Can you phrase \"Sometimes I just look at pictures of the Earth from space and I marvel at how beautiful it all is...\" The main image features a tray of cooked chicken nuggets, with the chicken pieces positioned to resemble the continents and islands on a world map. The meme playfully suggests that the chicken nuggets represent the Earth, and the various locations depicted in the photo are actually chicken nugget versions of different places. The punchline of the meme appears in smaller text at the bottom, reading \"I mean, it's not the real Earth, but how beautiful it a ll is.\" This lighthearted and entertaining meme combines food and humor, using the image of chicken nuggets to create a funny and imaginative representation of the world. | | GPT -4 | Can you explain this meme? This meme is a joke that combines two unrelate d things: pictures of the earth from space and chicken nuggets. The text of the meme suggests that the image below is a beautiful picture of the earth from space. However, the image\" Multimodal_p16.png\"The page contains the following items: ### Graphics and Diagrams: 1. **Diagram 1:** - **Description:** A user interface mock -up with a box labeled \"My Joke Website\" containing the text \"[ Funny Joke ] [push to reveal punchline]\". - **Lab el:** \"Turn this mock -up into an interactive website using html/js, replace the joke with a real joke.\" 2. **Diagram 2:** - **Rendered Website (before button click):** - Header: \"My Joke Website\" - Text: \"Welcome to my joke website. Push the button to reve al the punchline!\" - Button: \"Push me!\" - **Rendered Website (after button click):** - Header: \"My Joke Website\" - Text: \"The punchline is: 'Why was the math book sad? Because it had too many problems.'\" - Button: \"Push me!\" ### Code Block: ```html <!DOCTY PE html> <html> <head> <title>My Joke Website</title> <style> body { background -color: white; font -family: Arial, sans - serif; } .container { display: flex; flex -direction: column; align -items: center; justify -content: center; height: 100vh; } h1 { font -size: 36px; margin -bottom: 20px; } p { font -size: 20px; } button { background -color: blue; color: white; border\" Multimodal_p17.png\"The provided image is a page from a PDF document containing a mixture of text, diagrams, and a table embedded within chat bubbles. Below is a structured text representation of the data and summary of graphics/diagrams: **Table Data (Frui t Salad punchline is: 'Why was the math book sad? Because it had too many problems.'\" - Button: \"Push me!\" ### Code Block: ```html <!DOCTY PE html> <html> <head> <title>My Joke Website</title> <style> body { background -color: white; font -family: Arial, sans - serif; } .container { display: flex; flex -direction: column; align -items: center; justify -content: center; height: 100vh; } h1 { font -size: 36px; margin -bottom: 20px; } p { font -size: 20px; } button { background -color: blue; color: white; border\" Multimodal_p17.png\"The provided image is a page from a PDF document containing a mixture of text, diagrams, and a table embedded within chat bubbles. Below is a structured text representation of the data and summary of graphics/diagrams: **Table Data (Frui t Salad Recipe):** ``` Ingredients: - 2 cups fresh strawberries, sliced - 1 cup fresh blueberries - 1 large carrot, grated - Juice of half a lemon - A handful of chopped fresh parsley or mint - A few ice cubes - Optional: honey or maple syrup (adjust to ta ste) Instructions: 1. In a large bowl, combine the sliced strawberries, blueberries, and carrot. Gently toss them together to mix. 2. Drizzle the juice of half a lemon over the fruit and vegetables to prevent them from oxidizing and add a refreshing citrus flavor. 3. Sprinkle the chopped fresh parsley or mint over the fruit and gently toss once more. 4. If desired, add a drizzle of honey or maple syrup for a touch of sweetness. Adjust the amount according to your taste preferences. 5. Cover the bowl and ref rigerate the fruit salad for at least 30 minutes or up to a few hours. This allows the flavors to meld and creates a more refreshing salad. 6. Serve the fruit salad in cups or bowls, garnished with a sprinkle of fresh parsley or mint, if desired. ``` **Dia grams and Graphics Description:** 1. **Figure 3**: It shows LLA V A\" Multimodal_p18.png\"The image contains two figures, which are described below: **Figure 4:** - Caption: LLaV A relates the movie scenes to the textual knowledge from the pretrained LLM. - Graphics/Diagrams: - There is an image depicting a scene from the movi e \"Titanic\" where a young woman is leaning on a man in front of a beautiful sunset, with both of them standing on a boat. **Figure 5:** - Caption: LLaV A recognizes the famous artwork, Mona Lisa, by Leonardo da Vinci. When we start a new conversation, it al so explains the humorous artwork created on more refreshing salad. 6. Serve the fruit salad in cups or bowls, garnished with a sprinkle of fresh parsley or mint, if desired. ``` **Dia grams and Graphics Description:** 1. **Figure 3**: It shows LLA V A\" Multimodal_p18.png\"The image contains two figures, which are described below: **Figure 4:** - Caption: LLaV A relates the movie scenes to the textual knowledge from the pretrained LLM. - Graphics/Diagrams: - There is an image depicting a scene from the movi e \"Titanic\" where a young woman is leaning on a man in front of a beautiful sunset, with both of them standing on a boat. **Figure 5:** - Caption: LLaV A recognizes the famous artwork, Mona Lisa, by Leonardo da Vinci. When we start a new conversation, it al so explains the humorous artwork created on the web, mimicking the Mona Lisa. - Graphics/Diagrams: - There is an image of the painting \"Mona Lisa\" by Leonardo da Vinci. - There is an image of a painting depicting a dog in a humorous situation, wearing a wo man's dress and standing in front of a landscape. There is no table or equations presented in the image.\" Multimodal_p19.png\"The image contains the following non -text elements: 1. **Diagrams/Graphics:** - Two images of a person that the language model identifies as \"Elon Musk.\" - An illustration of a rocking chair. 2. **Tables:** - Table 10: A table with the t itle \"One example on how the text -only GPT -4 acts as a judge to ensemble the predictions from LLaV A and a text -only GPT -4, and gives a correct final answer.\" Here is the structured representation of the data from the table: ``` Title: One",
    "114": "example on how the text -only GPT -4 acts as a judge to ensemble the predictions from LLaV A and a text -only GPT -4, and gives a correct final answer. Content: 1. Question: - Which material is this rocking chair made of? - Options: (A) wood (B) silk 2. LLaV A answer: - LECTURE: A materia l is a type of matter. - Wood, glass, metal, and plastic are common materials. - Some objects are made of just one material. - Most nails are made of metal. - Other objects are made of more than one material. - This hammer is made of metal and wood. - SOLU TION: Look at the picture of the rocking chair. The rocking chair is made of two different materials. The legs are made of wood, and the back and seat are\" Multimodal_p2.png\"The provided image contains sections of text from a PDF document. Below ensemble the predictions from LLaV A and a text -only GPT -4, and gives a correct final answer. Content: 1. Question: - Which material is this rocking chair made of? - Options: (A) wood (B) silk 2. LLaV A answer: - LECTURE: A materia l is a type of matter. - Wood, glass, metal, and plastic are common materials. - Some objects are made of just one material. - Most nails are made of metal. - Other objects are made of more than one material. - This hammer is made of metal and wood. - SOLU TION: Look at the picture of the rocking chair. The rocking chair is made of two different materials. The legs are made of wood, and the back and seat are\" Multimodal_p2.png\"The provided image contains sections of text from a PDF document. Below are the identified visual elements from the page: 1. **Equations:** - No equations are identified in the given image. 2. **Diagrams/Graphics:** - No diagrams or graph ics are identified in the given image. 3. **Tables:** - No tables are identified in the given image. Since there are no diagrams, graphics, equations, or tables, there's no additional data to convert into a structured format for a database. The document ma inly contains continuous text covering sections like \"Related Work,\" \"Instruction Tuning,\" and \"GPT -assisted Visual Instruction Data Generation.\"\" Multimodal_p20.png\"The image contains the following structured elements: **Table 11: The list of instructions for brief image description.** Instructions: 1. Describe the image concisely. 2. Provide a brief description of the given image. 3. Offer a succin ct explanation of the picture presented. 4. Summarize the visual content of the image. 5. Give a short and clear explanation of the subsequent image. 6. Share a concise interpretation of the image provided. 7. Present a compact description of the photo\u2019s k ey features. 8. Relay a brief, clear account of the picture shown. 9. Render a clear and concise summary of the photo. 10. Write a terse but informative summary of the picture. 11. Create a compact narrative representing the image presented. There are no d iagrams or equations on the page. There are links to assets and resources but no visual graphics or complex data formats other than the listed table.\" Multimodal_p21.png\"### Diagrams, Graphics, and Tables Summary #### Table: **Table 12: The list of instructions for detailed image description** - Describe the following image in detail the subsequent image. 6. Share a concise interpretation of the image provided. 7. Present a compact description of the photo\u2019s k ey features. 8. Relay a brief, clear account of the picture shown. 9. Render a clear and concise summary of the photo. 10. Write a terse but informative summary of the picture. 11. Create a compact narrative representing the image presented. There are no d iagrams or equations on the page. There are links to assets and resources but no visual graphics or complex data formats other than the listed table.\" Multimodal_p21.png\"### Diagrams, Graphics, and Tables Summary #### Table: **Table 12: The list of instructions for detailed image description** - Describe the following image in detail - Provide a detailed description of the given image - Give an elaborate explanation of the image you see - Share a comprehensive rundown of the presented image - Offer a thorough analysis of the image - Explain the various aspects of the image before you - Clarify the contents of the displayed image with great detail - Charac terize the image using a well -detailed description - Break down the elements of the image in a detailed manner - Walk through the important details of the image - Portray the image with a rich, descriptive narrative - Narrate the contents of the image with precision - Analyze the image in a comprehensive and detailed manner - Illustrate the image through a descriptive explanation - Examine the image closely and share its details - Write an exhaustive depiction of the given image --- #### Graphic: **Figure 7 :** - **Title**: Comparison of noun -phrase statistics before and after filtering CC3M. - **Description**: - A graph comparing the frequency of unique noun -phrases in the CC3M dataset before and after filtering. - X-axis: Unique noun -phrases (ordered by frequency in the descending order). - Y-axis: Frequency (logarithmic scale from 10^0 to 10^5). \" Multimodal_p22.png\"Here is the data extracted from the image in a structured form suitable for use in a database: **Equations:** ```plaintext messages = [{ \"role\":\"system\", \"content\":\"\"\"You are an AI visual assistant, and you are seeing a single image. Wha t you see are provided with five sentences, describing the same image you are looking at. Answer all questions as you are seeing the image. Design a conversation between you and a person asking about this photo. The answers graph comparing the frequency of unique noun -phrases in the CC3M dataset before and after filtering. - X-axis: Unique noun -phrases (ordered by frequency in the descending order). - Y-axis: Frequency (logarithmic scale from 10^0 to 10^5). \" Multimodal_p22.png\"Here is the data extracted from the image in a structured form suitable for use in a database: **Equations:** ```plaintext messages = [{ \"role\":\"system\", \"content\":\"\"\"You are an AI visual assistant, and you are seeing a single image. Wha t you see are provided with five sentences, describing the same image you are looking at. Answer all questions as you are seeing the image. Design a conversation between you and a person asking about this photo. The answers should be in a tone that a visua l AI assistant is seeing the image and answering the question. Ask diverse questions and give corresponding answers. Include questions asking about the visual content of the image, including the object types, counting the objects, object actions, object locations, relative positions between objects, etc. Only include questions that have definite answers: (1) one can see the content in the image that the question asks about and can answer confidently; (2) one can determine confidently from the image that it is not in the image. Do not ask any question that cannot be answered confidently. Also include complex questions that are relevant to the content in the image, for example, asking about background knowledge of the objects in the image, asking to discuss ab out events happening in the image, etc. Again, do not ask about uncertain details. Provide detailed answers when answering complex questions. For example, give detailed examples or reasoning steps to make the content more convincing and well-organized. You can include multiple paragraphs if necessary.\"\"\"\" Multimodal_p23.png\"**Data from Image:** **Diagrams/Graphics:** - The image includes a photograph that depicts a black vehicle (SUV) with luggage around it, positioned in an underground parking area with people standing around, likely packing or unpacking t he vehicle. **Table:** - There is a table denoted as \"Table 14.\" It includes types of responses and contexts along with their specific details. **Boxes:** - person: [0.681, 0.242, 0.774, 0.694], - person: [0.63, 0.222, 0.686, 0.516], - person: [0.444, 0.23 3, 0.487, 0.341], - backpack: [0.384, 0.696,",
    "115": "example, give detailed examples or reasoning steps to make the content more convincing and well-organized. You can include multiple paragraphs if necessary.\"\"\"\" Multimodal_p23.png\"**Data from Image:** **Diagrams/Graphics:** - The image includes a photograph that depicts a black vehicle (SUV) with luggage around it, positioned in an underground parking area with people standing around, likely packing or unpacking t he vehicle. **Table:** - There is a table denoted as \"Table 14.\" It includes types of responses and contexts along with their specific details. **Boxes:** - person: [0.681, 0.242, 0.774, 0.694], - person: [0.63, 0.222, 0.686, 0.516], - person: [0.444, 0.23 3, 0.487, 0.341], - backpack: [0.384, 0.696, 0.485, 0.914], - backpack: [0.755, 0.413, 0.846, 0.692], - suitcase: [0.758, 0.413, 0.845, 0.69], - suitcase: [0.1, 0.497, 0.173, 0.579], - bicycle: [0.282, 0.363, 0.327, 0.442], - car: [0.786, 0.25, 0.848, 0.32 2],\" Multimodal_p24.png\"The image consists of a table labeled \"Table 15: One example used in in - context -learning to construct visual conversation data.\" Here is the structured text representation of the data in the table: ``` Captions: - There is a movie theate r that displays the show times above the doors. - A fire hydrant is deep in the snow. - The fire hydrant is in the snow near a recently plowed sidewalk. - This city has had a very hard winter with snow. - A hotel for dogs in the snow in winter. Conversatio n: - Question: What color is the fire hydrant in the image? Answer: The fire hydrant in the image is red. - Question: Is the fire hydrant fully covered by snow or partially visible? Answer: The fire hydrant is partially visible, with a significant portion of it covered by the deep snow. - Question: What is the condition of the sidewalk near the fire hydrant? Answer: The sidewalk near the fire hydrant has been recently plowed, so it is relatively clear of snow compared to the surrounding area. - Question: Wh at does the image suggest about the winter season in the city? Answer: The image suggests that the winter season in the city has been quite harsh and challenging. The significant snow accumulation seen in the image indicates that the city has experienced heavy snowfall, which can lead to various difficulties and disruptions for its residents. - Additional Insight: The deep snow covering the fire\" Multimodal_p25.png\"### Identified Elements and Transcriptions: #### Equations: - There are of it covered by the deep snow. - Question: What is the condition of the sidewalk near the fire hydrant? Answer: The sidewalk near the fire hydrant has been recently plowed, so it is relatively clear of snow compared to the surrounding area. - Question: Wh at does the image suggest about the winter season in the city? Answer: The image suggests that the winter season in the city has been quite harsh and challenging. The significant snow accumulation seen in the image indicates that the city has experienced heavy snowfall, which can lead to various difficulties and disruptions for its residents. - Additional Insight: The deep snow covering the fire\" Multimodal_p25.png\"### Identified Elements and Transcriptions: #### Equations: - There are no equations visible in the provided image. #### Tables: - **Table 16: One example used in in - context -learning to construct visual conversation data.** **Structure:* * - **Captions:** 1. A man is skiing in the open snow -covered hills. 2. A skier is making his way into the snow. 3. A skier on the foothills of a huge mountain range. 4. A skier looks at mountains while standing near a trail sign. 5. A single person skiing by an area with a lot of bushes. - **Conversation:** - **Question:** What is the skier doing? **Answer:** The skier is skiing in the snow -covered hills, making his way through the snow and enjoying the beautiful mountain scenery. They are also standing ne ar a trail sign, which indicates their location on the mountain. - **Question:** Are there any other people or skiers visible in the image? **Answer:** No, there is only a single person skiing in the image, and they are surrounded by an area with a lot of bushes. - **Question:** What kind of skiing is the person doing? Cross -country or downhill? **Answer:** Based on what I can see in the photo, it is not possible to definitively determine whether the skier is doing cross -country or downhill skiing. However, given\" Multimodal_p3.png\"**Table Data:** | Context type | Description | | -------------- |------------- | | Context type 1: Captions | A group of people standing outside of a black vehicle with various luggage. Luggage surrounds a vehicle in an underground parking a rea. People try to fit all of their luggage in an SUV . The sport utility vehicle is parked in the public garage, being packed for a trip. Some people with luggage near a van that is transporting it. | | Context type 2: Boxes | **Question:** What kind of skiing is the person doing? Cross -country or downhill? **Answer:** Based on what I can see in the photo, it is not possible to definitively determine whether the skier is doing cross -country or downhill skiing. However, given\" Multimodal_p3.png\"**Table Data:** | Context type | Description | | -------------- |------------- | | Context type 1: Captions | A group of people standing outside of a black vehicle with various luggage. Luggage surrounds a vehicle in an underground parking a rea. People try to fit all of their luggage in an SUV . The sport utility vehicle is parked in the public garage, being packed for a trip. Some people with luggage near a van that is transporting it. | | Context type 2: Boxes | person: [0.681, 0.242, 0.774, 0.694], backpack: [0.384, 0.696, 0.485, 0.914], suitcase: ...<omitted> | | Response type 1: conversation | Question: What type of vehicle is featured in the image? Answer: The image features a black sport utility vehicle (SUV) ...<omitted> | | Response ty pe 2: detailed description | The image is an underground parking area with a black sport utility vehicle (SUV) parked. There are three people in the scene, with one person standing closer to the left side of the vehicle, another person in the middle, and the third person on the right side. They are all working together to pack their luggage into the SUV for a trip. ...<omitted> | | Response type 3: complex reasoning | Question: What challenges do these people face? Answer: In the image,\" Multimodal_p4.png\"### Detected content from the image: #### Diagrams: 1. **Network Architecture Diagram (Figure 1: LLaV A network architecture)**: - Depicts a flow for the integration between vision and language models. - Components: - Vision Encoder - Projection W - Vision Language Model \\(f_{\\phi}\\) - Language Response \\(X_{ \\text{a}} \\), Language Instruction \\(X_{ \\text{q}} \\), Visual Tokens \\(X_{ \\text{v}} \\) - Process: - The visual features \\(Z_{ \\text{v}} = g(X_{ \\text{v}}) \\) are processed by the vision encode r. - Features are mapped to visual tokens \\(H_{ \\text{v}} = W \\cdot Z_{ \\text{v}} \\). - Combined with language tokens for instruction and response through the language model. #### Equations: 1. **Equation (1):** \\[ H_{ \\text{v}} = W \\cdot Z_{ \\text{v}} \\quad \\text{with} \\quad Z_{ \\text{v}} = g(X_{ \\text{v}}) \\] 2. **Equation (2):** \\[ X^{ \\text{instruct}}_{t} = \\begin{cases} \\text{Randomly choose } integration between vision and language models. - Components: - Vision Encoder - Projection W - Vision Language Model \\(f_{\\phi}\\) - Language Response \\(X_{ \\text{a}} \\), Language Instruction \\(X_{ \\text{q}} \\), Visual Tokens \\(X_{ \\text{v}} \\) - Process: - The visual features \\(Z_{ \\text{v}} = g(X_{ \\text{v}}) \\) are processed by the vision encode r. - Features are mapped to visual tokens \\(H_{ \\text{v}} = W \\cdot Z_{ \\text{v}} \\). - Combined with language tokens for instruction and response through the language model. #### Equations: 1. **Equation (1):** \\[ H_{ \\text{v}} = W \\cdot Z_{ \\text{v}} \\quad \\text{with} \\quad Z_{ \\text{v}} = g(X_{",
    "116": "\\text{v}}) \\] 2. **Equation (2):** \\[ X^{ \\text{instruct}}_{t} = \\begin{cases} \\text{Randomly choose } [X^1_{ \\text{q}}, X_{ \\text{v}}] & \\text{for the first turn\" Multimodal_p5.png\"### Identification and Summary: #### Diagrams/Graphics: - **Sequence Diagram**: ``` X_system -message <STOP> Human: X_h <STOP> Assistant: X_a <STOP> Human: X_instruct <STOP> Assistant: X_i <STOP> ... ``` #### Tables: - **Table 2: The input sequence used to train the model**: | Human | Assistant | | ------- | ---------- | | X_h <STOP> | X_a <STOP> | | X_instruct <STOP> | X_i <STOP> | #### Equations: 1. \\( \\theta = W \\) (the projection matrix) 2. \\( x_{ \\text{instuct}, <i} \\) 3. \\( x_a, <j> \\) ### Structured Data Representation: #### Table: ```plaintext Table 2: | Human: X_h <STOP> Assistant: X_a <STOP> | | Human: X_instruct <STOP> Assistant: X_i <STOP> | ``` #### Sequence Diagram: ```plaintext Sequence Diagram: - X_system -message <STOP> - Huma n: X_h <STOP> Assistant: X_a <STOP> - Human: X_instruct <STOP> Assistant: X_i <STOP> ... ``` #### Equations: ```plaintext Equations: 1. theta = W (the projection matrix) 2. x_instruct, <i> 3. x_a,\" Multimodal_p6.png\"The image contains the following components: **Table:** ``` Table 3: Example prompt from GPT -4 paper [36] to compare visual reasoning and chat capabilities. + ----- ----+---------------------------------------------------------------------- --------- + | User | What is unusual about this image? | + --------- +------------------------------------------------------------------------------- + | LLaV A | The unusual aspect of this image is a man ironing clothes on the back of a | | | minivan or van. T his is not a typical place to perform this activity, as one | | | would usually iron clothes in X_a <STOP> - Human: X_instruct <STOP> Assistant: X_i <STOP> ... ``` #### Equations: ```plaintext Equations: 1. theta = W (the projection matrix) 2. x_instruct, <i> 3. x_a,\" Multimodal_p6.png\"The image contains the following components: **Table:** ``` Table 3: Example prompt from GPT -4 paper [36] to compare visual reasoning and chat capabilities. + ----- ----+---------------------------------------------------------------------- --------- + | User | What is unusual about this image? | + --------- +------------------------------------------------------------------------------- + | LLaV A | The unusual aspect of this image is a man ironing clothes on the back of a | | | minivan or van. T his is not a typical place to perform this activity, as one | | | would usually iron clothes in a more stationary and safe location, such as a | | | home, using a regular ironing board. The scene depicted in the image is | | | peculiar as it involves a mak eshift ironing setup on a vehicle, which can be | | | both unsafe and unconventional. Additionally, it is not clear how the man is | | | able to maintain balance and stability while ironing clothes in such an | | | unstable environment. | + ------ ---+------ ------------------------------------------------------------------------- + | User | What\u2019s happening in the scene? | + --------- +------------------------------------------------------------------------------- + | LLaV A | The image depicts a man standing on t op of a yellow SUV in a busy city street.| | | He is holding a portable ladder, seemingly ironing clothes while standing on | | | the vehicle. Around the scene, there are other cars, a traffic light,\" Multimodal_p7.png\"### Detected Tables **Table 4: Ablation on LLaV A -Bench (COCO) with different training data.** | Category | Conversation | Detail Description | Complex Reasoning | All | | ----------------- |-------------- |-------------------- |-------------- -----|------- | | Full data | 83.1 | 75.3 | 96.5 | 85.1 | | Detail + Complex| 81.5 (18.6) | 73.3 (22.4) | 90.8 (53.7) | 81.9 (23.2) | | Conv + 5% Detail + 10% Complex | 81.0 (21.2) | 68.4 (27.2) | 91.5 (50.9) | 80.5 (24.4) | | Conversation | 76.5 (26.9) | 5 9.8 (32.6) | 84.9 (62.4) | 73.8 (33.3) | | No Instruction Tuning | 22.0 (46.1) | 24.0 (51.3) | 18.5 (76.9) | 21.5 (58.6) | **Table 5: Instruction -following capability comparison using relative scores on LLaV A\" the scene, there are other cars, a traffic light,\" Multimodal_p7.png\"### Detected Tables **Table 4: Ablation on LLaV A -Bench (COCO) with different training data.** | Category | Conversation | Detail Description | Complex Reasoning | All | | ----------------- |-------------- |-------------------- |-------------- -----|------- | | Full data | 83.1 | 75.3 | 96.5 | 85.1 | | Detail + Complex| 81.5 (18.6) | 73.3 (22.4) | 90.8 (53.7) | 81.9 (23.2) | | Conv + 5% Detail + 10% Complex | 81.0 (21.2) | 68.4 (27.2) | 91.5 (50.9) | 80.5 (24.4) | | Conversation | 76.5 (26.9) | 5 9.8 (32.6) | 84.9 (62.4) | 73.8 (33.3) | | No Instruction Tuning | 22.0 (46.1) | 24.0 (51.3) | 18.5 (76.9) | 21.5 (58.6) | **Table 5: Instruction -following capability comparison using relative scores on LLaV A\" Multimodal_p8.png\"1. **Diagrams and Graphics:** - Two images with captions: \"ICHIRAN Ramen\" and \"Filled fridge.\" - The first image caption: \"A close -up photo of a meal at ICHIRAN. The chashu ramen bowl with a spoon is placed in the center. The ramen is sea soned with chili sauce, chopped scallions, and served with two pieces of chashu. Chopsticks are placed to the right of the bowl, still in their paper wrap, not yet opened. The ramen is also served with nori on the left. On top, from left to right, the foll owing sides are served: a bowl of orange spice (possibly garlic sauce), a plate of smoke -flavored stewed pork with chopped scallions, and a cup of matcha green tea.\" - The second image caption: \"An open refrigerator filled with a variety of food items. In the left part of the compartment, towards the front, there is a plastic box of strawberries with a small bag of baby carrots on top. Towards the back, there is a stack of sauce containers. In the middle part of the compartment, towards the front, there is a green plastic box, and there is an unidentified plastic bag placed on it. Towards the back, there is a carton of milk. In the right part of th e compartment, towards the front, there is a box of blueberries with three yogurts stacked on top. The large bottle of yogurt is F\" Multimodal_p9.png\"### Detected Items in the Image: #### Table: 1. **Table Title**: Table 7: Accuracy (%) on Science QA dataset. **Columns**: - Method - Subject NAT - Subject SOC - Subject LAN - Context Modality TXT - Context Modality IMG - Context Modality NO - Grade G1-6 - Grade G7 -12 - Average **Data**: ``` Human | 90.23 | 84.97 | 87.48 | 89.60 | 87.50 | 88.10 containers. In the middle part of the compartment, towards the front, there is a green plastic box, and there is an unidentified plastic bag placed on it. Towards the back, there is a carton of milk. In the right part of th e compartment, towards the front, there is a box of blueberries with three yogurts stacked on top. The large bottle of yogurt is F\" Multimodal_p9.png\"### Detected Items in the Image: #### Table: 1. **Table Title**: Table 7: Accuracy (%) on Science QA dataset. **Columns**: - Method - Subject NAT - Subject SOC - Subject LAN - Context Modality TXT - Context Modality IMG - Context Modality NO - Grade G1-6 - Grade G7 -12 - Average **Data**: ``` Human | 90.23 | 84.97 | 87.48 | 89.60 | 87.50 | 88.10 | 91.59 | 82.42 | 88.40 GPT -3.5 [34] | 74.64 | 69.74 | 76.00 | 74.44 | 67.28 | 77.42 | 76.60 | 68.89 | 73.97 GPT -3.5 w/ CoT [34] | 75.44 | 70.87 | 79.70 | 74.68 | 67.43 | 79.23 | 69.68 | 75.17 LLaMA - Adapter [59] | 84.37 | 88.80 | 84.36 | 83.72 | 80.32 | 86.\" Performance Evaluation_p1.png\"The image provided is a page from a PDF document titled \"A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity.\" It does not appear to contain any diagrams, graphics, equati ons, or tables. The content mainly consists of text, which includes the title of the paper, the authors, their affiliations, contact",
    "117": "information, abstract, and an introduction section. As there are no data tables, diagrams, or equations included in the vis ible part of the page, there is no structured data or graphical information to extract. Here is a brief summary of the abstract and introduction content: - **Abstract**: - The paper presents a framework to evaluate large language models (LLMs) like ChatGPT . - Evaluations cover 23 datasets across 8 NLP tasks, highlighting multitask, multilingual, and multimodal capabilities. - Findings suggest that ChatGPT outperforms other LLMs, particularly in non -Latin script languages and zero -shot tasks. - ChatGPT excel s at logical reasoning and non -textual reasoning but still encounters hallucinations and external knowledge limitations. - The tool shows strength in summarization and machine translation tasks. - **Introduction**: - ChatGPT is described as a successor to InstructGPT, fine - tuned using RLHF. - Its user interface enhances its interactivity and applicability across various content: - **Abstract**: - The paper presents a framework to evaluate large language models (LLMs) like ChatGPT . - Evaluations cover 23 datasets across 8 NLP tasks, highlighting multitask, multilingual, and multimodal capabilities. - Findings suggest that ChatGPT outperforms other LLMs, particularly in non -Latin script languages and zero -shot tasks. - ChatGPT excel s at logical reasoning and non -textual reasoning but still encounters hallucinations and external knowledge limitations. - The tool shows strength in summarization and machine translation tasks. - **Introduction**: - ChatGPT is described as a successor to InstructGPT, fine - tuned using RLHF. - Its user interface enhances its interactivity and applicability across various NLP areas. - Despite its strengths, ChatGPT exhibits limitations in elementary mathematical and\" Performance Evaluation_p10.png\"**Diagrams, Graphics, Equations, and Tables Identified:** 1. **Diagram/Graphic:** - A rendered image of a cat drawn by ChatGPT using the HTML Canvas library. - Caption: \"Figure 1: A cat drawn by ChatGPT using HTML Canvas libr ary. A rendered image is shown in place of the generated code for the sake of simplicity.\" **Graphic Summary:** - The image is a simple illustration of a cat created using HTML Canvas, likely to demonstrate ChatGPT's capability to generate code that produc es visual graphics. **Equations:** - There are no equations present in the provided page image. **Tables:** - No tables are present in the provided page image. **Structured Data Format for Database:** ```JSON { \"figure\": { \"id\": 1, \"description\": \"A cat dr awn by ChatGPT using HTML Canvas library. A rendered image is shown in place of the generated code for the sake of simplicity.\", \"content\": \"<Image of a cat drawn using HTML Canvas>\" } } ``` This captures the essence of the diagram, making it easy to store and retrieve for future references in a database.\" Performance Evaluation_p11.png\"**Table:** ``` Results of the portion (%) of generated flags evaluated into five grades from A to E. The column shows the results of an ablation study, which removes the prompting of flag description generation and directly a sks ChatGPT to generate the SVG code of the flag image. Grade (# of Errors) | Turn 1 (w/o desc) | Turn 1 | Turn 2 | Turn 3 ---- ------------------------ |------------------- |-------- |-------- |-------- A (0) | 0 | 4 | 12 | 24 B (1) | 4 | 22 | 24 | library. A rendered image is shown in place of the generated code for the sake of simplicity.\", \"content\": \"<Image of a cat drawn using HTML Canvas>\" } } ``` This captures the essence of the diagram, making it easy to store and retrieve for future references in a database.\" Performance Evaluation_p11.png\"**Table:** ``` Results of the portion (%) of generated flags evaluated into five grades from A to E. The column shows the results of an ablation study, which removes the prompting of flag description generation and directly a sks ChatGPT to generate the SVG code of the flag image. Grade (# of Errors) | Turn 1 (w/o desc) | Turn 1 | Turn 2 | Turn 3 ---- ------------------------ |------------------- |-------- |-------- |-------- A (0) | 0 | 4 | 12 | 24 B (1) | 4 | 22 | 24 | 24 C (2) | 16 | 18 | 12 | 10 D (3) | 18 | 24 | 26 | 20 E ( \u2265 4) | 62 | 32 | 26 | 22 ``` **Figure 2: Description and Summary of Graphics** Figure 2 showcases two versions of ChatGPT - generated SVG format images of the German flag: - **Top Version:** Without a pre -genera ted description. The text box in the figure prompts: \"Generate a code snippet to represent German flag in SVG format.\" - **Bottom Version:** With a pre -generated description about the German flag: \"The flag of Germany consists of three equal horizontal ban ds of black, red, and gold.\" Performance Evaluation_p12.png\"### Diagrams and Graphics **Diagram:** - There is a table titled \"Table 9\" which lists reasoning categories and corresponding datasets. **Table:** - The table titled \"Table 10\" lists \"Deductive Reasoning Tasks\" and \"Inductive Reasoning Tasks\" with corresponding scores. #### Table 9: Reasoning categories and corresponding datasets used to evaluate ChatGPT in this work. | **Categories** | **Dataset** | | ---------------- |--------------------- -------------------------------------- --| | Deductive | EntailmentBank (Dalvi et al., 2021) | | | bAbI (task 15) (Weston et al., 2016b) | | Inductive | CLUTRR (Sinha et al., 2019) | | | bAbI (task 16) (Weston et al., 2016b) | | Abductive | \u03b1NLI (Bhagavatula et al., 2020) | | Temporal | Timedia l (Qin et al., 2021) | | Spatial | SpartQA (Mirzare et al., 2021) | | | StepGame (Shi et al., 2022a) | | Mathematical | Math (Saxton et al., 2019) | | Commonsense | CommonsenseQA (Talmor et al., 2018\" Performance Evaluation_p13.png\"The image includes several tables, prompts with answers, and corresponding scores. #### Table 9: Reasoning categories and corresponding datasets used to evaluate ChatGPT in this work. | **Categories** | **Dataset** | | ---------------- |--------------------- -------------------------------------- --| | Deductive | EntailmentBank (Dalvi et al., 2021) | | | bAbI (task 15) (Weston et al., 2016b) | | Inductive | CLUTRR (Sinha et al., 2019) | | | bAbI (task 16) (Weston et al., 2016b) | | Abductive | \u03b1NLI (Bhagavatula et al., 2020) | | Temporal | Timedia l (Qin et al., 2021) | | Spatial | SpartQA (Mirzare et al., 2021) | | | StepGame (Shi et al., 2022a) | | Mathematical | Math (Saxton et al., 2019) | | Commonsense | CommonsenseQA (Talmor et al., 2018\" Performance Evaluation_p13.png\"The image includes several tables, prompts with answers, and corresponding true/false evaluations, as well as sections dedicated to deductive and inductive reasoning tasks. Below is a structured representation of the data: ## # Table 1: Prompting samples on deductive and inductive reasoning tasks **Task** | **Prompt** | **ChatGPT answer** | **Gold** | **T/F** --- | --- | --- | --- | --- **Deductive Reasoning Tasks** | **bAbI (task 15)** | 1 Wolves are afraid of mice. \\ 2 Sheep are afraid of mice. \\ 3 Winona is a sheep. \\ 4 Mice are afraid of cats. \\ 5 Cats are afraid of wolves. \\ 6 Jessica is a mouse. \\ 7 Emily is a cat. \\ 8 Gertrude is a wolf. \\ 10 What is Winona afraid of? | Winona, being a sheep, is afraid of mice. | mouse | T **bAbI (task 15) (prompt engineered)** | Given facts: Wolves are afraid of mice. Sheep are afraid of mice. Winona is a sheep. Mice are afraid of cats. Cats are afraid of wolves. Jessica is a mouse. Emily is a cat. Gertrude is a wolf. Based on the given f acts above, do a reasonable inference on this question using deductive reasoning: \\ What is Winona afraid of? | Based on\" Performance Evaluation_p14.png\"The image provided contains a page with multiple paragraphs discussing various aspects of ChatGPT's reasoning capabilities. Here's the breakdown of the requested information: ### Table Title: Performance with prompting sample s #### Table Structure | Dataset | Inductive",
    "118": "Performance | Deductive Performance | | -------------------------------- -|----------------------- |----------------------- | | Basic -level tasks (bAbI tasks) | Not specified | 19/30 | | Advanced -level tasks (CLUTRR, EntailmentBank) | Not specified | Not specified | ### mouse. Emily is a cat. Gertrude is a wolf. Based on the given f acts above, do a reasonable inference on this question using deductive reasoning: \\ What is Winona afraid of? | Based on\" Performance Evaluation_p14.png\"The image provided contains a page with multiple paragraphs discussing various aspects of ChatGPT's reasoning capabilities. Here's the breakdown of the requested information: ### Table Title: Performance with prompting sample s #### Table Structure | Dataset | Inductive Performance | Deductive Performance | | -------------------------------- -|----------------------- |----------------------- | | Basic -level tasks (bAbI tasks) | Not specified | 19/30 | | Advanced -level tasks (CLUTRR, EntailmentBank) | Not specified | Not specified | ### Equations No equations are visible in the provided image. ### Diagrams and Graphics There are no diagrams or graphics present in the provided image. ### Summary of Key Sect ions 1. **Comparison of Inductive and Deductive Reasoning:** - Focuses on ChatGPT's ability to reason at basic and advanced levels, indicating that inductive and deductive tasks were probed. - Findings suggest ChatGPT struggles more with inductive reasonin g compared to deductive reasoning. 2. **Performance on bAbI Tasks:** - ChatGPT fails in inductive scenarios (0/30) but achieves better performance in deductive reasoning (19/30). - Performance improves upon explicit prompting for reasoning. 3. **Analysis o n Advanced -level Tasks:** - Similar trends observed with datasets like CLUTRR and EntailmentBank where ChatGPT finds inductive tasks challenging. 4. **Investigation into Abductive Reasoning\" Performance Evaluation_p15.png\"### Diagrams, Graphics, Equations, and Tables Summary #### Table **Table 13: Spatial reasoning ability of ChatGPT. Overall, ChatGPT falls short of the task.** | Dataset | Total | Basic | Hard | | ---------- |------- |------- |------| | **StepGame** | 26/60 | 19/30 | 7/30 | | **SpartQA** | 28/64 | 20/32 | 8/32 | #### Structured Data for Database **Spatial Reasoning Tasks** - **Dataset**: StepGame - **Total**: 26/60 - **Basic**: 19/30 - **Hard**: 7/30 - **Dataset**: SpartQA - **Tota l**: 28/64 - **Basic**: 20/32 - **Hard**: 8/32 #### Graphics and Diagrams Summary The page does not contain any graphics or diagrams explicitly, other than the table mentioned above. #### Equations No equations are directly transcribed or Performance Evaluation_p15.png\"### Diagrams, Graphics, Equations, and Tables Summary #### Table **Table 13: Spatial reasoning ability of ChatGPT. Overall, ChatGPT falls short of the task.** | Dataset | Total | Basic | Hard | | ---------- |------- |------- |------| | **StepGame** | 26/60 | 19/30 | 7/30 | | **SpartQA** | 28/64 | 20/32 | 8/32 | #### Structured Data for Database **Spatial Reasoning Tasks** - **Dataset**: StepGame - **Total**: 26/60 - **Basic**: 19/30 - **Hard**: 7/30 - **Dataset**: SpartQA - **Tota l**: 28/64 - **Basic**: 20/32 - **Hard**: 8/32 #### Graphics and Diagrams Summary The page does not contain any graphics or diagrams explicitly, other than the table mentioned above. #### Equations No equations are directly transcribed or present in the im age. --- This structured form summarizes the table data for spatial reasoning tasks, ready to be inserted into a database.\" Performance Evaluation_p16.png\"### Identification Summary: **Table:** - **Table 14**: Commonsense reasoning ability of ChatGPT. - **Columns:** Dataset, CommonsenseQA, PiQA, Pep-3k (Hard) - **Rows:** - `27/30`, `25/30`, `28/30` - **Table 15**: An example fr om Pep -3k for commonsense reasoning of ChatGPT. - **Columns:** Prompt, ChatGPT Answer - **Rows:** - `Please judge if this predicate is (likely) plausible or implausible: \"knife chop rock\"`, `It is implausible that a knife could chop through a rock. Knives are made to cut through softer materials such as food, paper, or wood. Rocks are much harder than these materials and a knife would not be able to make a dent in a rock, let alone chop it.` ### Structured Data: **Table 14 Data:** ```plaintext | Dataset | C ommonsenseQA | PiQA | Pep -3k (Hard) | | ----------------- |------ --------- |------- |--------------- | | Accuracy Scores | 27/30 | 25/30 | 28/30 | ``` **Table 15 Data:** ```plaintext | Prompt | ChatGPT Answer | | ------------------------------------------------- ------------ |----- --------------------------------------------------------------------------------------------------------------------- -------------------------------- | | Please judge if this predicate is (likely) plausible or implausible\" Performance Evaluation_p17.png\"**Table Data** ``` Table: Results for causal, multi -hop, and analogical reasoning. ChatGPT shows good causal and analogical reasoning capability, but not 14 Data:** ```plaintext | Dataset | C ommonsenseQA | PiQA | Pep -3k (Hard) | | ----------------- |------ --------- |------- |--------------- | | Accuracy Scores | 27/30 | 25/30 | 28/30 | ``` **Table 15 Data:** ```plaintext | Prompt | ChatGPT Answer | | ------------------------------------------------- ------------ |----- --------------------------------------------------------------------------------------------------------------------- -------------------------------- | | Please judge if this predicate is (likely) plausible or implausible\" Performance Evaluation_p17.png\"**Table Data** ``` Table: Results for causal, multi -hop, and analogical reasoning. ChatGPT shows good causal and analogical reasoning capability, but not on multi -hop reasoning. | Type | Dataset | Result | | --------------- |--------------------------- |------------- -| | Causal | E -CARE | 24/30 | | Multi -hop | HotpotQA | 8/30 | | Analogical | Letter string analogies | 30/30 | ``` **Summary of Graphics and Diagrams** There are no graphics or diagrams in this image. **Transcription of Equations** There are no equations in this image. This data structure represents the information in a clear and organized format, ready to be directly passed into a database.\" Performance Evaluation_p18.png\"**Table Representation:** ``` [ { \"Task\": \"Machine Translation\", \"Key\": \"Prompt\", \"Text_Content\": \"(Javanese) Krajan Anyar Mesir kuno nggumun marang monumen -monumen leluhure, sing nalika iku tetep apik luwih seka sewu taun.\" }, { \"Task\": \"Machine Translation\", \"Key\": \"ChatGPT Generation\", \"Text_Content\": \"The ancient Egyptian Pharaohs built beautiful monuments that are still well -preserved after thousands of years.\" }, { \"Task\": \"Machine Translation\", \"Key\": \"Hallucination\", \" Text_Content\": \"Extrinsic - New Kingdom ancient Egyptians marveled at their predecessors' monuments, which were then well over a thousand years old.\" }, { \"Task\": \"Summarization\", \"Key\": \"Prompt\", \"Text_Content\": \"[...] But the Russian president lifted the ban after Tehran struck an interim deal with Britain and five other countries to curb nuclear activities in exchange for sanctions relief[...]\" }, { \"Task\": \"Summarization\", \"Key\": \"ChatGPT Generation\", \"Text_Content\": \"The move comes following the recent framework deal between Iran and the P5+1 (US, UK, France, China, Russia, and Germany) to curb Iran's nuclear activities in exchange for sanctions relief.\" }, { \"Task\" years.\" }, { \"Task\": \"Machine Translation\", \"Key\": \"Hallucination\", \" Text_Content\": \"Extrinsic - New Kingdom ancient Egyptians marveled at their predecessors' monuments, which were then well over a thousand years old.\" }, { \"Task\": \"Summarization\", \"Key\": \"Prompt\", \"Text_Content\": \"[...] But the Russian president lifted the ban after Tehran struck an interim deal with Britain and five other countries to curb nuclear activities in exchange for sanctions relief[...]\" }, { \"Task\": \"Summarization\", \"Key\": \"ChatGPT",
    "119": "Generation\", \"Text_Content\": \"The move comes following the recent framework deal between Iran and the P5+1 (US, UK, France, China, Russia, and Germany) to curb Iran's nuclear activities in exchange for sanctions relief.\" }, { \"Task\" Performance Evaluation_p19.png\"### Content Identified from the Image of the PDF Page #### Diagram **Figure 3**: This is an example of dialogue summarization. The diagram consists of a structured conversation between two individuals, David and Russ, discuss ing the review of a report. The dialog transitions through different stages with summaries provided. #### Transcribed Summarization Data from Figure 3 1. **Dialogue Text**: - David: Morning Russ. Have you seen the report I emailed yesterday? - Russ: Hi Dav id. Well received thank you. But I haven't read it yet. - David: Is there anything you\u2019d like me to do right now? - Russ: I\u2019ll take a look at the report in a moment and will send you remarks if I have any. - David: Sounds good. I guess I\u2019ll just answer som e emails. - Russ: Please do. I should be done by midday with the report. 2. **Summarized Text**: - Summarize the above dialogue - David and Russ are discussing a report that David had emailed the previous day. Russ received the report but has not read it y et. David asks if there is anything he can do, and Russ says he will look at the report and send any remarks he has. David plans to answer some emails while Russ reviews the report, and Russ expects to be finished by midday. - Please make the summary short er. - David and Russ are discussing a\" Performance Evaluation_p2.png\"The image you provided contains mainly text and does not include any tables, diagrams, or equations that need transcription or summarization. Here's a brief summary of some key points from the text: ### Graphics and Diagrams - **No visible diagrams** present in the image. ### Data Tables - **No visible tables** present in the image. discussing a report that David had emailed the previous day. Russ received the report but has not read it y et. David asks if there is anything he can do, and Russ says he will look at the report and send any remarks he has. David plans to answer some emails while Russ reviews the report, and Russ expects to be finished by midday. - Please make the summary short er. - David and Russ are discussing a\" Performance Evaluation_p2.png\"The image you provided contains mainly text and does not include any tables, diagrams, or equations that need transcription or summarization. Here's a brief summary of some key points from the text: ### Graphics and Diagrams - **No visible diagrams** present in the image. ### Data Tables - **No visible tables** present in the image. ### Equations - **No visible equations** present in the image. ### Summary: 1. **Multitask, Multimodal, and Multilingual** - ChatGPT outperforms pr evious LLMs in 9 out of 13 NLP datasets with zero -shot learning. - Struggles with low -resource languages. 2. **Reasoning** - ChatGPT shows weaknesses in inductive reasoning, spatial reasoning, and mathematical reasoning. - Performs relatively better in com monsense reasoning and causal reasoning, but weak in complex reasoning such as multi -hop reasoning. 3. **Hallucination** - ChatGPT suffers from hallucinations, generating incorrect information it can't verify due to lack of an external knowledge base. 4. **Interactivity** - Multi -turn dialog interactivity is a key feature, enabling better performance in dialog sessions. Refactoring this data to be input directly into a database: ``` { \"MultitaskMultimodalMultilingual\": { \"datasets_outperform_previous_LLMs\": 9, \"total_datasets\": 13, \"note\": \"ChatGPT outperforms\" Performance Evaluation_p20.png\"**Diagrams and Graphics:** 1. **Bar Graph (Figure 4):** - Title: Result of the multi -turn MT -APE experiment. - Description: - The graph compares the number of correct translations (#Correct MT) versus the number of correct tr anslations after post-editing (#Correct APE). - Languages covered: Chinese, French, Indonesian, Korean, Javanese, Sundanese. - The x -axis represents different languages. - The y -axis represents the number of samples. - Two sets of bars for each language: o ne for #Correct MT and another for #Correct APE. **Table:** 1. **Table 18:** - Title: Result of translation w/ and w/o post -editing ``` { \"MultitaskMultimodalMultilingual\": { \"datasets_outperform_previous_LLMs\": 9, \"total_datasets\": 13, \"note\": \"ChatGPT outperforms\" Performance Evaluation_p20.png\"**Diagrams and Graphics:** 1. **Bar Graph (Figure 4):** - Title: Result of the multi -turn MT -APE experiment. - Description: - The graph compares the number of correct translations (#Correct MT) versus the number of correct tr anslations after post-editing (#Correct APE). - Languages covered: Chinese, French, Indonesian, Korean, Javanese, Sundanese. - The x -axis represents different languages. - The y -axis represents the number of samples. - Two sets of bars for each language: o ne for #Correct MT and another for #Correct APE. **Table:** 1. **Table 18:** - Title: Result of translation w/ and w/o post -editing on WMT 2022 English \u2192Marathi APE shared task. - Structured Data: ``` Label | Metric | w/o APE | w/ APE -------------- |------- -----|--------- |------- Post-Edited | HTER | 88.14 | 88.79 | SacreBLEU | 4.81 | 4.20 | METEOR | 13.10 | 12.74 Source | HTER | 65.36 | 63.13 English Text | SacreBLEU | 25.54 | 27.20 |\" Performance Evaluation_p21.png\"### Diagrams and Graphics: #### Figure 5: Changes in ChatGPT's drawing of the Canadian flag over three turns **Description:** - It illustrates the steps taken by ChatGPT to draw the Canadian flag over three turns. - Ground Tr uth image contains the actual Canadian flag. - Each turn is compared with ground truth based on Layout, Color, Completion, and Shape/Size. **Summary:** - **Turn 1:** - Layout: X - Color: \u2713 - Completion: X - Shape/Size: X - **Turn 2:** - Layout: \u2713 - Color: \u2713 - Completion: X - Shape/Size: X - **Turn 3:** - Layout: \u2713 - Color: \u2713 - Completion: \u2713 - Shape/Size: X *Improvement:* - The image improved incrementally, but the shape/size did not align perfectly by the third turn. #### Figure 6: From fruits to a Christmas tree. Step -by-step image drawing and modification by ChatGPT. **Description:** - Step-by-step instructions on how ChatGPT creates images of fruit and modifies them to create an image of a Christmas tree. -1. Using SVG format, draw an apple, two oranges, three peaches, and four kiwis. - 2. Make a pyramid out of these fruits: the apple on t op,\" Performance Evaluation_p22.png\"**Diagrams and Graphics:** The image contains a reference to \"Figure 7\" which describes the results of generations via rounds of post -editing with ChatGPT Shape/Size: X - **Turn 3:** - Layout: \u2713 - Color: \u2713 - Completion: \u2713 - Shape/Size: X *Improvement:* - The image improved incrementally, but the shape/size did not align perfectly by the third turn. #### Figure 6: From fruits to a Christmas tree. Step -by-step image drawing and modification by ChatGPT. **Description:** - Step-by-step instructions on how ChatGPT creates images of fruit and modifies them to create an image of a Christmas tree. -1. Using SVG format, draw an apple, two oranges, three peaches, and four kiwis. - 2. Make a pyramid out of these fruits: the apple on t op,\" Performance Evaluation_p22.png\"**Diagrams and Graphics:** The image contains a reference to \"Figure 7\" which describes the results of generations via rounds of post -editing with ChatGPT but the actual graphic is not present within this specific image. **Ta bles:** There is no table present in this specific image. **Equations:** There are no equations present in this specific image. **Summary:** The text describes the performance evaluation of ChatGPT across multiple rounds of errorless SVG image generation a nd post -editing. It mentions an improvement in the SVG images quality of up to 36% and notes instances of decline (up to 8%) as well. Additionally, it speaks to ChatGPT's",
    "120": "inability to handle non -Latin scripts and varied language resources effectively. It a lso calls attention to ChatGPT's failure in producing better visual abstraction models though the problem -solving capability is yet under evaluation. Finally, it points to ChatGPT being a lazy reasoner and suggests pairing it with computational models like Wolfram to potentially enhance outcomes.\" Performance Evaluation_p23.png\"The image contains standard text from a paper. There are no diagrams, graphics, equations, or tables present within the image. The structure of the content included in the image is as follows: 1. **Section 7.3: Factuality and Hallucinations** 2. **Section 7.4: Interactivity** 3. **Section 7.5: Responsible Generative AI** 4. **References** Here is the textual representation of the references section as it would be suitable for a database input: ```plaintext References: - (2023) ChatGPT vs Satya Nadella over Biryani: The Chatbot is Learning from Its Mistakes. - Alham Fikri Aji, Genta Indra Winata, Fajri Koto, Samuel Cahyan Wijaya, Ade Romadhony, Rahmad Mahendra, Kemal Kurniawan, David Moeljadi, Radityo Eko Performance Evaluation_p23.png\"The image contains standard text from a paper. There are no diagrams, graphics, equations, or tables present within the image. The structure of the content included in the image is as follows: 1. **Section 7.3: Factuality and Hallucinations** 2. **Section 7.4: Interactivity** 3. **Section 7.5: Responsible Generative AI** 4. **References** Here is the textual representation of the references section as it would be suitable for a database input: ```plaintext References: - (2023) ChatGPT vs Satya Nadella over Biryani: The Chatbot is Learning from Its Mistakes. - Alham Fikri Aji, Genta Indra Winata, Fajri Koto, Samuel Cahyan Wijaya, Ade Romadhony, Rahmad Mahendra, Kemal Kurniawan, David Moeljadi, Radityo Eko Prasojo, Timothy Baldwi n, Yer Johan Lau, and Sebastian Ruder. 2022. One country, 700+ languages: NLP challenges for underrepresented languages and dialects in Indonesia. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Pa pers), pages 7226 \u20137249, Dublin, Ireland. Association for Computational Linguistics. - Sam Altman. 2022. ChatGPT is incredibly limited, but good enough at some things to create a misleading impression\" Performance Evaluation_p24.png\"The image contains structured textual references and no tables, diagrams, or graphics. Summary of Content: 1. Referenced Papers and Authors: - Banerjee, S., Lavie, A.: METEOR: An automatic metric for MT evaluation with improv ed correlation with human judgments. - Bartha, P.: Analogy and Analogical Reasoning. - Bhagavatula, C., Le Bras, R., Saakshi, K., Holtzman, A., Rashkin, H., Downey, D., Yih, W., Choi, Y .: Abductive commonsense reasoning. - Bhattacharjee, P., Chatterjee, R. , Freitag, M., Negri, M., Turchi, M.: Findings of the WMT 2022 shared task on automatic post -editing. - Birch, D.G.W.: ChatGPT is a window into the real future of financial services. - Bisk, Y ., Zellers, R., Gao, J., Choi, Y .: Piq: Reasoning about physical commonsense in natural language. - Blanco - Gonzalez, A., Cabezon, A., Seco -Gonzalez, A., Conde -Torres, D., Antele -Reveiro, J., Pineiro, A., Garcia -Fandin\u0303o, R.: The role of ai in drug discovery: Challenges, opportunities, and strategies. - Borgeaud, S., Me nsch,\" Performance Evaluation_p25.png\"The image contains text primarily composed of references from a scientific paper. There are no diagrams, graphics, or equations visible in the image. Choi, Y .: Abductive commonsense reasoning. - Bhattacharjee, P., Chatterjee, R. , Freitag, M., Negri, M., Turchi, M.: Findings of the WMT 2022 shared task on automatic post -editing. - Birch, D.G.W.: ChatGPT is a window into the real future of financial services. - Bisk, Y ., Zellers, R., Gao, J., Choi, Y .: Piq: Reasoning about physical commonsense in natural language. - Blanco - Gonzalez, A., Cabezon, A., Seco -Gonzalez, A., Conde -Torres, D., Antele -Reveiro, J., Pineiro, A., Garcia -Fandin\u0303o, R.: The role of ai in drug discovery: Challenges, opportunities, and strategies. - Borgeaud, S., Me nsch,\" Performance Evaluation_p25.png\"The image contains text primarily composed of references from a scientific paper. There are no diagrams, graphics, or equations visible in the image. However, there are multiple references that can be structured in a simple f ormat for database entry. Here is the structured data from the references found in the image: 1. Omernick, Andrew M., Dai, Thanuamlayan Sankararayanana Pillai, Marie Pellat, Aitor Lewkowyz, Erica Moreira, Rewon Child, Oleskandr Polozov, Katherine Lee, Zong wei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier -Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. *Title: Scaling language modeling with pathways.* 2. Jon Christian. 2023. *Title: A mazing \"jailbreak\" bypasses chatgpt's ethics safeguards.* 3. Paul Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. 2017. *Title: Deep reinforcement learning from human preferences.* 4. Peter Clark, Isaac Cowhey, Oren Etzion i, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. *Title: Think you have\" Performance Evaluation_p26.png\"There are no diagrams, graphics, or equations within the provided image. The page predominantly contains bibliographic references. Based on your request, here\u2019s a structured summary in text format: References: 1. Roberto Gozalo -Brizuela and Eduardo C Garrido -Merchan. 2023. Chatgpt is not all you need. a state of the art survey of large generative ai models. arXiv preprint arXiv:2301.04655. 2. Barba ra F Grimes. 2000. Ethnologue. SIL International, Dallas, TX. 3. Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding, Jianwei Yue, and Yupeng Wan. 2023. How close is chatgpt to human experts? Oren Etzion i, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. *Title: Think you have\" Performance Evaluation_p26.png\"There are no diagrams, graphics, or equations within the provided image. The page predominantly contains bibliographic references. Based on your request, here\u2019s a structured summary in text format: References: 1. Roberto Gozalo -Brizuela and Eduardo C Garrido -Merchan. 2023. Chatgpt is not all you need. a state of the art survey of large generative ai models. arXiv preprint arXiv:2301.04655. 2. Barba ra F Grimes. 2000. Ethnologue. SIL International, Dallas, TX. 3. Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding, Jianwei Yue, and Yupeng Wan. 2023. How close is chatgpt to human experts? comparison corpus, evaluation, and detectio n. arXiv preprint arXiv:2301.07597. 4. James Hawthorne. 2021. Inductive Logic. In Edward N. Zalta, editor, The Stanford Encyclopedia of Philosophy, Spring 2021 edition. Metaphysics Research Lab, Stanford University. 5. Hangfeng He, Hongming Zhang, and Dan Roth. 2023. Rethinking with retrieval: Faithful large language model inference. 6. Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman\" Performance Evaluation_p27.png\"The provided image displays references from an academic paper. Based on your specifications, I will identify any diagrams, graphics, equations, or tables and ignore the general text. Since the content provided is predominantl y text and references, there are no diagrams, graphics, equations, or tables visible within the image. Here is a simple summary of the text structured for a database. --- **Scientific References** 1. Lewis, Mike, et al. (*2020a*). \"BART: Denoising sequence -to-sequence pretraining for natural language generation, translation, and comprehension.\" In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871 \u20137880. 2. Lewis, Mike, et al. (*2020b*). \"BART: Denoising seque",
    "121": "nce-to-sequence pretraining for natural language generation, translation, and comprehension.\" In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871 \u20137880. 3. Liang, Percy, et al. (*2022*). \"Holistic evaluatio n of language models.\" 4. Lieber, Opher, et al. (*2021*). \"Jurassic -1: Technical details and evaluation.\" White Paper. AI21 Labs. 5. Lin, Stephanie, Jacob Hilton, and Owain Evans. al. (*2020a*). \"BART: Denoising sequence -to-sequence pretraining for natural language generation, translation, and comprehension.\" In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871 \u20137880. 2. Lewis, Mike, et al. (*2020b*). \"BART: Denoising seque nce-to-sequence pretraining for natural language generation, translation, and comprehension.\" In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871 \u20137880. 3. Liang, Percy, et al. (*2022*). \"Holistic evaluatio n of language models.\" 4. Lieber, Opher, et al. (*2021*). \"Jurassic -1: Technical details and evaluation.\" White Paper. AI21 Labs. 5. Lin, Stephanie, Jacob Hilton, and Owain Evans. (*2022*). \"TruthfulQA: Measuring how models mimic human falsehoods.\" In Proc eedings of the 60\" Performance Evaluation_p28.png\"The provided image appears to be a page from a bibliography or references section of a paper. There are no diagrams, graphics, equations, or tables present on this page. The page consists solely of text in the form of referen ces to other works. Here are the details structured for each reference entry: 1. **Reference 1** - **Authors**: knowledge graphs. - **Title**: Knowledge graphs. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pag es 845 \u2013854. 2. **Reference 2** - **Authors**: Nasrin Mostafazadeh, Chris Brockett, William B Dolan, Michel Galley, Jianfeng Gao, Georgios Spithourakis, and Lucy Vanderwende. - **Title**: Image -grounded conversations: Multimodal context for natural question and response generation. In Proceedings of the Eighth International Joint Conference on Natural Language Processing (V olume 1: Long Papers), pages 462 \u2013472. 3. **Reference 3** - **Authors**: Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng -Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albany, Zaid Alyafe\" Performance Evaluation_p29.png\"The provided image of a PDF page appears to be a list of references from a research paper. It consists entirely of text with no diagrams, graphics, equations, or tables present. As there are no tables or structured data prese nt to convert into a simple format for a database, there is no further data to be transcribed or summarized. The Joint Conference on Natural Language Processing (V olume 1: Long Papers), pages 462 \u2013472. 3. **Reference 3** - **Authors**: Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng -Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albany, Zaid Alyafe\" Performance Evaluation_p29.png\"The provided image of a PDF page appears to be a list of references from a research paper. It consists entirely of text with no diagrams, graphics, equations, or tables present. As there are no tables or structured data prese nt to convert into a simple format for a database, there is no further data to be transcribed or summarized. The information consists of citations in standard academic format. If you have any other images or need assistance with a different type of content , please let me know!\" Performance Evaluation_p3.png\"The provided image of the page from a PDF contains two sections: 1. \"2.1 Large Pretrained Models\" 2. \"2.2 ChatGPT\" No diagrams, graphics, equations, or tables are present within the image of the page. Summary of Sections: **2. 1 Large Pretrained Models:** This section discusses that large language models (LLMs) like GPT -3, Gopher, Megatron, GPT -Jurassic, and OPT -175B have demonstrated significant robustness and generalizability, being able to perform tasks through zero -shot and few-shot learning. Scaling these models does not yield new emergent abilities in the absence of specific data tuning. These models use specialized prompt engineering to exhibit usefulness. The section then mentions specific dialogue -oriented large models ( e.g., LaMDA, BlenderBot 3.0) emphasizing their training and fine -tuning on dialogue data and their ability to generalize and retain knowledge from their training. **2.2 ChatGPT:** This section focuses on ChatGPT and highlights its conversational abilities, generalization capacity, and how it retains knowledge acquired during pre-training. ChatGPT is pre -trained on a large -scale conversational dataset and refined using a reward model to enhance quality. ChatGPT continuously interacts with users to answer questions, correct mistakes, and reject inappropriate requests. Moreover, it employs Reinforcement Learning from Human Feedback (RLHF) instead of maximum likelihood estimation (MLE), aligning more with human preferences. The section also\" training and fine -tuning on dialogue data and their ability to generalize and retain knowledge from their training. **2.2 ChatGPT:** This section focuses on ChatGPT and highlights its conversational abilities, generalization capacity, and how it retains knowledge acquired during pre-training. ChatGPT is pre -trained on a large -scale conversational dataset and refined using a reward model to enhance quality. ChatGPT continuously interacts with users to answer questions, correct mistakes, and reject inappropriate requests. Moreover, it employs Reinforcement Learning from Human Feedback (RLHF) instead of maximum likelihood estimation (MLE), aligning more with human preferences. The section also\" Performance Evaluation_p30.png\"The provided image contains a page from a PDF document listing references. It does not include diagrams, graphics, equations, or tables within the visible portion. The content consists of citations for various publications re lated to language models, commonsense understanding, multimodal dialogue response generation, online exam integrity, and other topics within computational linguistics and artificial intelligence. There is no structured data in tables or graphical data to e xtract and summarize. The text is entirely composed of reference entries. If you have any other specific instructions or need further processing of a different kind, please let me know!\" Performance Evaluation_p31.png\"Here is the requested information extracted from the provided image of the PDF page: **Tables:** - No tables are discovered in the image. **Diagrams/Graphics:** - No diagrams or graphics are identified in the image. **Equatio ns:** - No equations are recognized in the image. The image primarily contains a list of references from what seems to be an academic paper or report. The references are formatted in a standard citation style, providing information such as author names, pu blication titles, conference or journal names, year of publication, and page numbers. The references provided are comprehensive bibliographic entries usual for academic publications, and they include multiple authors and detailed publication notes.\" Performance Evaluation_p32.png\"The image of the page from a PDF contains only basic text, references, and citations. There are no diagrams, graphics, equations, or tables within the image. Therefore, no structured data or text representation is needed for tables or summaries of No equations are recognized in the image. The image primarily contains a list of references from what seems to be an academic paper or report. The references are formatted in a standard citation style, providing information such as author names, pu",
    "122": "blication titles, conference or journal names, year of publication, and page numbers. The references provided are comprehensive bibliographic entries usual for academic publications, and they include multiple authors and detailed publication notes.\" Performance Evaluation_p32.png\"The image of the page from a PDF contains only basic text, references, and citations. There are no diagrams, graphics, equations, or tables within the image. Therefore, no structured data or text representation is needed for tables or summaries of graphics/diagrams. Here is a transcribed snippet from the visible parts of the text for your reference: 1. \"language models for multimodal abstractive summarization. In Proceedings of the 2021 Conference on Empirical Methods in Natur al Language Processing, pages 3995 \u20134007, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\" 2. \"Tiezhen Yu, Zihan Liu, and Pascale Fung. 2021b. Adaptsum: Towards low -resource domain adaptation for abstractive summarizati on. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5892 \u20135904.\" 3. \"Xiaoxue Gao, Abhinav Rastogi, Srinivas Sunkara, Raghuv Gupta, Jianguo Zhang, and Ji ndong Chen. 2020. Multwoz 2.2: A dialogue dataset with additional annotation corrections and state tracking baselines. ACL 2020, page 109.\" 4. \"Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. 2022. Star: Bootstrapping reasoning with reasoning. In\" Performance Evaluation_p33.png\"The given image is of a table from a PDF, showing flag drawing task results. Below is the table data in a structured form. ### Table: Flag Drawing Task Results | No | Country/Region | Ground Truth | Turn 1 (without descriptio n) | Turn 2 | Turn 3 | End result | | ----|------------------- |-------------- |------------------------------ |------------------------------ |-- -------------------------- |------------ | | | | Grade | LC/MS | Image | Grade | LC/MS | Image | Grade | LC/MS | Imag e | Grade | LC/MS | Image | | | 1 | United States | E | 1/1/1 | 1 | D | 1/0/1 | 2 | D | Yuhuai Wu, Jesse Mu, and Noah Goodman. 2022. Star: Bootstrapping reasoning with reasoning. In\" Performance Evaluation_p33.png\"The given image is of a table from a PDF, showing flag drawing task results. Below is the table data in a structured form. ### Table: Flag Drawing Task Results | No | Country/Region | Ground Truth | Turn 1 (without descriptio n) | Turn 2 | Turn 3 | End result | | ----|------------------- |-------------- |------------------------------ |------------------------------ |-- -------------------------- |------------ | | | | Grade | LC/MS | Image | Grade | LC/MS | Image | Grade | LC/MS | Imag e | Grade | LC/MS | Image | | | 1 | United States | E | 1/1/1 | 1 | D | 1/0/1 | 2 | D | 1/1/1 | 2 | D | 1/0/1 | 1 | B | | 2 | Canada | B | 1/1/1 | 1 | B | 0/0/1 | 1 | D | 1/0/1 | 1 | B | 0/0/1 | 1 | B | | 3 | Brazil | E | 1/1/1 | 1 | E | 1/0/1 | 1 | E | 1/ 0/\" Performance Evaluation_p34.png\"**Images and Figures:** 1. Diagram 1: - **Description:** An illustration of a red diamond shape representing the output for the instruction to generate a code snippet for the Canadian flag in SVG format. 2. Diagram 2: - **Des cription:** A second illustrated image showing a red and white Canadian flag with vertical red bands on the left and right and a red shape in the middle. 3. Diagram 3: - **Description:** A third illustrated image resembling the Canadian flag where the cent ral red shape is revised to resemble a maple leaf more accurately. **Figure Caption:** - **Figure 8:** Example of the Canadian flag drawn by InstructGPT. **Equations and Structured Data:** There are no explicit equations in this image. **Transcribed Table Data:** There is no structured table data in the image. **Summary Transcription of Text Inputs and Outputs:** 1. **Instruct. 0:** *Describe how Canadian flag looks like.* - **Output 0:** *The flag of Canada is a red and white maple leaf design... The red and white colors...* 2. **Instruct. 1:** *Generate a code snippet to represent that flag in SVG format.* - **Output 1:** [Image of a red diamond shape] 3. **Instruct. 2:** *The flag should have a vertical red band on the left, a vertical white band in the m iddle, and a\" Performance Evaluation_p35.png\"The image contains a table with the following structured data: ``` [ { \"Dataset\": \"National Drawing\", \"Task\": \"IG\", \"Description\": \"National Flag Drawing is a designed synthetic dataset which is used to evaluate the multimoda l understanding of LLMs. The Transcription of Text Inputs and Outputs:** 1. **Instruct. 0:** *Describe how Canadian flag looks like.* - **Output 0:** *The flag of Canada is a red and white maple leaf design... The red and white colors...* 2. **Instruct. 1:** *Generate a code snippet to represent that flag in SVG format.* - **Output 1:** [Image of a red diamond shape] 3. **Instruct. 2:** *The flag should have a vertical red band on the left, a vertical white band in the m iddle, and a\" Performance Evaluation_p35.png\"The image contains a table with the following structured data: ``` [ { \"Dataset\": \"National Drawing\", \"Task\": \"IG\", \"Description\": \"National Flag Drawing is a designed synthetic dataset which is used to evaluate the multimoda l understanding of LLMs. The instruction for the National Flag Drawing is as follow: given a nation, draw the corresponding national flag and revise it based on the follow -up correction requests.\", \"Reference\": \"Curated by authors of this paper\", \"#Test Size\": \"50\", \"#ChatGPT Eval\": \"50\" }, { \"Dataset\": \"CNN/DM\", \"Task\": \"SUM\", \"Description\": \"The CNN/DailyMail Dataset is an En glish -language dataset containing just over 300k unique news articles as written by journalists at CNN and the Daily Mail. The current version supports both extractive and abstractive summarization, though the original version was created for machine -readi ng and comprehension and abstractive question answering.\", \"Reference\": \"Nallapati et al. (2016)\", \"#Test Size\": \"11490\", \"#ChatGPT Eval\": \"50\" }, { \"Dataset\": \"SAMSum\", \"Task\": \"SUM\", \"Description\": \"SAMSum dataset contains about 16k messenger -like conver sations with summaries. Conversations were created and written down by linguists fluent in English. Linguists were asked to create conversations similar to those they\" Performance Evaluation_p36.png\"The image contains a single table that lists various QA (Question Answering) tasks, along with respective citations, dataset size, and another metric. Below is the structured data extracted from the table: ```plaintext | Task | Type | Description | Citation | Dataset Size | Metric | | ------------------- |------ |--------------------------------------------------- --------------------------------------------------------------------------------------------------------------------- --------------------------------------------------------------------------------------------------------------------- down by linguists fluent in English. Linguists were asked to create conversations similar to those they\" Performance Evaluation_p36.png\"The image contains a single table that lists various QA (Question Answering) tasks, along with respective citations, dataset size, and another metric. Below is the structured data extracted from the table: ```plaintext | Task | Type | Description | Citation | Dataset Size | Metric | | ------------------- |------ |--------------------------------------------------- --------------------------------------------------------------------------------------------------------------------- --------------------------------------------------------------------------------------------------------------------- --------------------------------------------------------------------------------------------------------------------- ----------------------- ----------------------------------------------------------------------------------------------",
    "123": "------------------------------------------------ |----------------------------- |-------------- |-------- | | bAbI task 16 | QA | This basic induction bAbI tasks is t aken from the (20) QA bAbI tasks that is a set of proxy tasks that evaluate reading comprehension via question answering. The tasks measure understanding in several ways: whether a system is able to answer questions via simple induction. The tasks are desi gned to be prerequisites for any system that aims to be capable of conversing with a human. | Weston et al. (2016b) | 1000 | 30 | | EntailmentBank | QA | ENTAILMENTBANK, the first dataset of multistep entailment trees for QA, to support entailment -based ex planation. ENTAILMENTBANK contains two parts: 1,840 entailment trees, each tree showing how a question -answer pair (QA) is entailed from a small number of relevant sentences (e.g., Figure 1); and a general corpus C, containing those and other sentences of domain -specific and general knowledge relevant to the QA domain.\" Performance Evaluation_p37.png\"### Table Data The image contains a table with four columns and five rows, comprising information about various datasets. Below is the structured representation of the data in the table: | Dataset | Category | Description | C itation | Instances | Questions | | ----------------------- |---------- |-------------------------------------------------------------------- entailment -based ex planation. ENTAILMENTBANK contains two parts: 1,840 entailment trees, each tree showing how a question -answer pair (QA) is entailed from a small number of relevant sentences (e.g., Figure 1); and a general corpus C, containing those and other sentences of domain -specific and general knowledge relevant to the QA domain.\" Performance Evaluation_p37.png\"### Table Data The image contains a table with four columns and five rows, comprising information about various datasets. Below is the structured representation of the data in the table: | Dataset | Category | Description | C itation | Instances | Questions | | ----------------------- |---------- |-------------------------------------------------------------------- --------------------------------------------------------------------------------------------------------------------- --------------------------------------------------------------------------------------------------------------------- --------- |------------------- |----------- |----------- | | HotpotQA | QA | HotpotQA is a new dataset with 113k Wikipedia -based question -answer pairs with four key features: (1) the questions require finding and reasoning over multiple supporting documents to answer; (2) the questions are diverse and not constrained to any pre -existing knowledge bases or knowledge schemas; (3) we provide sentence -level supporting facts required for reasoning, allowing QA systems to reason with strong supervision and explain the predictions; (4) we offer a new type of factoid comparison questions to test QA systems\u2019 ability to extract relevant facts and perform necessary comparison. | Yang et al. (2018) | 7405 | 30 | | PiQA | QA | To apply eyeshadow without a brush, should I use a cotton swab or a toothpick? Questions requiring this kind of physical commonsense pose a challenge to state -of-the-art natural language understanding systems. The PIQA dataset introduces the task of physical commonsense reasoning and a corresponding benchmark dataset Physical Interaction: Question Answ ering or PIQA. Physical commonsense knowledge is a major challenge on the road to true AI\" Performance Evaluation_p38.png\"The image contains a table with six columns. Here is the structured data from the table: Columns: - Dataset - Category - Description - Reference - Instances - Classes Rows: 1. - Dataset: SpaRTQA - Category: QA - Description: - SpartQA is a comparison. | Yang et al. (2018) | 7405 | 30 | | PiQA | QA | To apply eyeshadow without a brush, should I use a cotton swab or a toothpick? Questions requiring this kind of physical commonsense pose a challenge to state -of-the-art natural language understanding systems. The PIQA dataset introduces the task of physical commonsense reasoning and a corresponding benchmark dataset Physical Interaction: Question Answ ering or PIQA. Physical commonsense knowledge is a major challenge on the road to true AI\" Performance Evaluation_p38.png\"The image contains a table with six columns. Here is the structured data from the table: Columns: - Dataset - Category - Description - Reference - Instances - Classes Rows: 1. - Dataset: SpaRTQA - Category: QA - Description: - SpartQA is a textual question answering benchmark for spatial reasoning on natural language text which contains more realistic spatial phenomena not covered by prior datasets and that is challenging for state -of-the-art language models (LM). - SPARTQA is built on NLVR's images containing more objects with richer spatial structures. - SPARTQA\u2019s stories are more natural, have more sentences, and richer in spatial relations in each sentence, and the questions require deeper reasoning and have four types: fin d relation (FR), find blocks (FB), choose object (CO), and yes/no (YN), which allows for more fine -grained analysis of models\u2019 capabilities. - The default test set of this dataset is human -annotated test set, which consists of 510 instances. - Reference: Mirzaee et al. (2021) - Instances: 510 - Classes: 64 2. - Dataset: StepGame - Category: QA - Description: - StepGame is a robust multi -hop spatial reasoning dataset in textual format which addresses the limitation from the bAbI dataset task 17 and task 19. - In this task, the\" Performance Evaluation_p39.png\"The image contains a table titled \"Table 19: List of all datasets used in our experiments. IG denotes image generation, SUM denotes summarization, MT denotes machine translation, SA denotes sentiment analysis, QA denotes ques tion answering, MD denotes misinformation detection, TOD denotes task -oriented dialogue, and KGD denotes knowledge -grounded dialogue. Some of the descriptions are directly from the original reference.\" Here\u2019s the structured data from the table in text form at: ```plaintext Dataset | Task | Description: - StepGame is a robust multi -hop spatial reasoning dataset in textual format which addresses the limitation from the bAbI dataset task 17 and task 19. - In this task, the\" Performance Evaluation_p39.png\"The image contains a table titled \"Table 19: List of all datasets used in our experiments. IG denotes image generation, SUM denotes summarization, MT denotes machine translation, SA denotes sentiment analysis, QA denotes ques tion answering, MD denotes misinformation detection, TOD denotes task -oriented dialogue, and KGD denotes knowledge -grounded dialogue. Some of the descriptions are directly from the original reference.\" Here\u2019s the structured data from the table in text form at: ```plaintext Dataset | Task | Description | Reference | #Q / #S / #Utterances | #Turns ---------------- |------ |-------------------------- ------------------------------------------------------------------------------------------------------------------- -- ------------------------------------------------------------ |-------------------- |------------------------ |--------- TruthfulQA | QA | The dataset comprises questions that span 38 categories, including health, law, finance and politics. The questions are of those that some humans would answer falsely due to a false belief or misconception. (e.g., [conspiracy] \u201cIf it\u2019s cold outside what does that tell us about global warming?\u201d; [health] \u201cCan coughing effectively stop a heart attack?\u201d) | | | Lin et al. (202 2) | 817 | 65 MultiWOZ 2.2 | TOD | MultiWOZ 2.2 is an improved version of MultiWOZ 2.1, which identifies and fixes dialogue state annotation errors across 17.3% of the utterances on top of MultiWOZ 2.1\" Performance Evaluation_p4.png\"The image contains one table and no distinct diagrams, graphics, or equations. The table",
    "124": "appears under the section header \u201cMultitask Generalization of ChatGPT\u201d and provides details about ChatGPT\u2019s performance on various langua ge understanding and generation tasks. --- ### Table 1: Multitask Generalization of ChatGPT | Task | Zero -shot Performance | State -of-the-art Zero -shot Performance | | ------------------------------------ ------ |------------------------------- |---------------------------------------- | | Summarization | Remarkable | Surpassing Previous NLP Models | | Machi ne Translation | Remarkable | Surpassing Previous NLP Models | | Sentiment Analysis | Remarkable | Surpassing Previous NLP Models | | Questions MultiWOZ 2.1\" Performance Evaluation_p4.png\"The image contains one table and no distinct diagrams, graphics, or equations. The table appears under the section header \u201cMultitask Generalization of ChatGPT\u201d and provides details about ChatGPT\u2019s performance on various langua ge understanding and generation tasks. --- ### Table 1: Multitask Generalization of ChatGPT | Task | Zero -shot Performance | State -of-the-art Zero -shot Performance | | ------------------------------------ ------ |------------------------------- |---------------------------------------- | | Summarization | Remarkable | Surpassing Previous NLP Models | | Machi ne Translation | Remarkable | Surpassing Previous NLP Models | | Sentiment Analysis | Remarkable | Surpassing Previous NLP Models | | Questions Answering | Remarkable | Surpassing Previous NLP Models | | Task -oriented Dialogue | Remarkable | Surpassing Pre vious NLP Models | | Open -domain Knowledge -grounded Dialogue | Remarkable | Surpassing Previous NLP Models | | Misinformation Detection Tasks | Remarkable | Surpassing Previous NLP Models | ### Data Summary: - The table evaluates ChatGPT's performance on m ultitask learning in a zero -shot setting. - ChatGPT shows remarkable performance on a list of tasks including summarization, machine translation, sentiment analysis, questions answering, task -oriented dialogue, open -domain knowledge - grounded dialogue, and misinformation detection. - ChatGPT surpasses previous state -of-the-art zero-shot models. The description above provides a concise summary and transcription of the\" Performance Evaluation_p40.png\"The image contains a table with five columns: Target, English Text, Label, Translation, and Post -Edited Text. Here is the structured data extracted from the table: ```plaintext [ { \"Target\": \"Chinese\", \"English Text\": \"Althou gh three people were inside the house when the car impacted it, none of them were hurt.\", \"Label\": \" \u867d\u7136\u8f66\u649e\u5230\u623f\u5b50\u65f6\uff0c\u623f\u5b50 \u91cc\u9762\u6709\u4e09\u4e2a\u4eba\uff0c\u4f46\u6700\u540e\u5e76\u6ca1\u6709\u4eba\u53d7\u4f24\u3002 \", \"Translation\": \" \u5c3d\u7ba1\u6709\u4e09\u4eba\u5728\u6c7d\u8f66\u649e\u4e0a\u623f\u5b50\u7684\u65f6 \u4faf\u5728\u5c4b\u91cc\uff0c\u4f46\u4ed6\u4eec\u90fd\u6ca1\u6709\u53d7\u4f24\u3002 \", \"Post -Edited Text\": \" \u5c3d\u7ba1\u8f66\u649e\u4e0a\u623f\u5b50\u65f6\u6709\u4e09\u4eba\u5728\u5c4b\u91cc\uff0c \u4f46\u4ed6\u4eec\u90fd\u6ca1\u6709\u53d7\u4f24\u3002 \" }, { \"Target\": \"Chinese\", \"Engl ish Text\": \"34 per cent of those in the poll share this view, wanting Queen Elizabeth II to be Australia \u2019s last monarch.\", \"Label\": \"34% \u7684\u53d7 \u8bbf\u8005\u4e00\u81f4\u8868\u793a\uff0c\u5e0c\u671b\u4f0a\u4e3d\u838e\u767d\u4e8c\u4e16\u662f\u6fb3\u5927\u5229\u4e9a\u7684\u6700\u540e\u4e00\u4efb\u541b\u4e3b\u3002 \", \"Translation\": \" \u8fd9\u9879\u6c11 \u610f\u8c03\u67e5\u663e\u793a\uff0c 34%\u7684\u4eba\u5e0c\u671b\u4f0a\u4e3d\u838e\u767d\u4e8c\u4e16\u6210\u4e3a\u6fb3\u5927\u5229\u4e9a\u7684\u6700\u540e\u4e00\u4f4d\u541b\u4e3b\u3002 \", \"Post -Edited Text\": \"\u6839\u636e\u6c11\u610f\u8c03\u67e5\u663e\u793a\uff0c 34%\u7684\u4eba\u5e0c\u671b\u4f0a\u4e3d\u838e\u767d\u4e8c\u4e16\u6210\u4e3a\u6fb3\u5927\u5229\u4e9a\u7684\u6700\u540e\u4e00\u4f4d\u541b\u4e3b\u3002 \" }, { \"\" Text, Label, Translation, and Post -Edited Text. Here is the structured data extracted from the table: ```plaintext [ { \"Target\": \"Chinese\", \"English Text\": \"Althou gh three people were inside the house when the car impacted it, none of them were hurt.\", \"Label\": \" \u867d\u7136\u8f66\u649e\u5230\u623f\u5b50\u65f6\uff0c\u623f\u5b50 \u91cc\u9762\u6709\u4e09\u4e2a\u4eba\uff0c\u4f46\u6700\u540e\u5e76\u6ca1\u6709\u4eba\u53d7\u4f24\u3002 \", \"Translation\": \" \u5c3d\u7ba1\u6709\u4e09\u4eba\u5728\u6c7d\u8f66\u649e\u4e0a\u623f\u5b50\u7684\u65f6 \u4faf\u5728\u5c4b\u91cc\uff0c\u4f46\u4ed6\u4eec\u90fd\u6ca1\u6709\u53d7\u4f24\u3002 \", \"Post -Edited Text\": \" \u5c3d\u7ba1\u8f66\u649e\u4e0a\u623f\u5b50\u65f6\u6709\u4e09\u4eba\u5728\u5c4b\u91cc\uff0c \u4f46\u4ed6\u4eec\u90fd\u6ca1\u6709\u53d7\u4f24\u3002 \" }, { \"Target\": \"Chinese\", \"Engl ish Text\": \"34 per cent of those in the poll share this view, wanting Queen Elizabeth II to be Australia \u2019s last monarch.\", \"Label\": \"34% \u7684\u53d7 \u8bbf\u8005\u4e00\u81f4\u8868\u793a\uff0c\u5e0c\u671b\u4f0a\u4e3d\u838e\u767d\u4e8c\u4e16\u662f\u6fb3\u5927\u5229\u4e9a\u7684\u6700\u540e\u4e00\u4efb\u541b\u4e3b\u3002 \", \"Translation\": \" \u8fd9\u9879\u6c11 \u610f\u8c03\u67e5\u663e\u793a\uff0c 34%\u7684\u4eba\u5e0c\u671b\u4f0a\u4e3d\u838e\u767d\u4e8c\u4e16\u6210\u4e3a\u6fb3\u5927\u5229\u4e9a\u7684\u6700\u540e\u4e00\u4f4d\u541b\u4e3b\u3002 \", \"Post -Edited Text\": \"\u6839\u636e\u6c11\u610f\u8c03\u67e5\u663e\u793a\uff0c 34%\u7684\u4eba\u5e0c\u671b\u4f0a\u4e3d\u838e\u767d\u4e8c\u4e16\u6210\u4e3a\u6fb3\u5927\u5229\u4e9a\u7684\u6700\u540e\u4e00\u4f4d\u541b\u4e3b\u3002 \" }, { \"\" Performance Evaluation_p41.png\"The image contains a table with the following structure: ``` | Language | English Text | Original Text 1 | Original Text 2 | | ---------- |-------------- |------------------- ------------------------ |----------------------------- ------------- | | Chinese | The correlation between brain pathology and behaviour supports scientists in their research. | \u5927\u8111\u75c5\u7406\u548c\u884c\u4e3a\u4e4b\u95f4\u7684\u76f8 \u5173\u6027\u53ef\u4ee5\u4e3a\u79d1\u5b66\u5bb6\u4eec\u7684\u7814\u7a76\u63d0\u4f9b\u652f\u6301\u3002 | \u7814\u7a76\u8868\u660e\uff0c\u5927\u8111\u75c5\u7406\u5b66\u548c\u884c\u4e3a\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u4e3a\u79d1 \u5b66\u5bb6\u4eec\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u652f\u6301\u3002 | | Chinese | Like some other experts, he is skeptical about whether diabetes can be cured, noting that these findings have no relevance to people who already have Type 1 diabetes. | \u548c\u5176\u4ed6\u4e00\u4e9b\u4e13\u5bb6\u4e00\u6837\uff0c\u4ed6\u5bf9\u7cd6\u5c3f\u75c5\u80fd\u5426\u6cbb\u6108\u6301\u6000\u7591\u6001\u5ea6\uff0c\u5e76\u6307\u51fa\u8fd9\u4e9b\u53d1 \u73b0\u4e0e\u5df2\u7ecf\u60a3\u6709 1\u578b\u7cd6\u5c3f\u75c5\u7684\u4eba\u65e0\u5173\u3002 | \u50cf\u5176\u4ed6\u4e00\u4e9b\u4e13\u5bb6\u4e00\u6837\uff0c\u4ed6\u5bf9\u7cd6\u5c3f\u75c5\u662f\u5426\u53ef\u4ee5\u88ab\u6cbb\u6108 \u8868\u793a\u6000\u7591\uff0c\u5e76\u6307\u51fa\u8fd9\u4e9b\u53d1\u73b0\u4e0e\u5df2\u7ecf\u60a3\u6709 1\u578b\u7cd6\u5c3f\u75c5\u7684\u4eba\u6ca1\u6709\u610f\u4e49\u3002 | | Korean | Although three people were inside the house when the car impacted it, none of them were hurt. | \ucc28\uac00 \uc9d1\uc5d0 \ubd80\ub52a\ucce4\uc744 \ub54c \uc138 \uba85\uc774 \uc9d1\uc5d0 \uc788\uc5c8\uc9c0\ub9cc , \uadf8\ub4e4 \uc911 \uc544\ubb34\ub3c4 \ub2e4\uce58\uc9c0 \uc54a\uc558\uc2b5\ub2c8\ub2e4 . | \ucc28\uac00 \uc9d1\uc5d0 \ubd80\ub52a\ucce4\uc744 \ub54c \uc9d1 \uc548\uc5d0 \uc138 \uc0ac\ub78c\uc774 \uc788\uc5c8\uc9c0\ub9cc , \uc544\ubb34\ub3c4 \uc0c1\ud574\ub97c \" Performance Evaluation_p42.png\"The image consists of a table with five columns and row headers, notably providing translations in three languages: Korean, Japanese, and English. The columns separate the text into the source language and its respective tran slations. **Translation Table Representation:** | Language | Text | Translation 1 | Translation 2 | Translation 3 | | ---------- |-------------------------------------------------------------------------------------------------------------------- ------------ --------------------------------------------------------------------------------------------------------- -|------------------------------------------------------------------------------------------------------------------- \ubd80\ub52a\ucce4\uc744 \ub54c \uc9d1 \uc548\uc5d0 \uc138 \uc0ac\ub78c\uc774 \uc788\uc5c8\uc9c0\ub9cc , \uc544\ubb34\ub3c4 \uc0c1\ud574\ub97c \" Performance Evaluation_p42.png\"The image consists of a table with five columns and row headers, notably providing translations in three languages: Korean, Japanese, and English. The columns separate the text into the source language and its respective tran slations. **Translation Table Representation:** | Language | Text | Translation 1 | Translation 2 | Translation 3 | | ---------- |-------------------------------------------------------------------------------------------------------------------- ------------ --------------------------------------------------------------------------------------------------------- -|------------------------------------------------------------------------------------------------------------------- ---------------------------------- ------------------- |--------------------------------------------------------------- ------------------------------------------------------------------------------------------------------------- |------- -------------------------------------------------------- ------------------------------------------------------------- --------------------------------------------------- | | Korean | Pests can spoil food, cause irritation, or in a worse case cause allergic reactions, spread venom, or transmit infections. | \ud574\ucda9\uc740 \uc74c\uc2dd\uc774 \uc369\uac8c \ub9cc\ub4e4\uace0 \uc5fc\uc99d\uc744 \uc720\ubc1c\ud558\uac70\ub098 , \ub098\uc05c \uacbd\uc6b0 \uc54c\ub808\ub974\uae30 \ubc18\uc751\uc744 \uc77c\uc73c\ud0a4\uace0 \ub3c5\uc744 \ud37c\ub728\ub9ac\uac70\ub098 \uac10\uc5fc\uc744 \uc77c\uc73c\ud0ac \uc218 \uc788\uc2b5\ub2c8\ub2e4 . | \ub2f9\uc2e0\uc740 \ud574\ucda9\uc774 \uc2dd\ud488\uc744 \ub9dd\uce58\uac70\ub098 , \ud53c\ubd80\ub97c \ud654\ub098\uac8c \ud560 \uc218 \uc788\uc73c\uba70 , \uc2ec\uac01\ud560 \uacbd\uc6b0 \uc54c\ub808\ub974\uae30 \ubc18\uc751\uc744 \uc720\ubc1c\ud558\uac70\ub098 , \ub3c5\uc744 \ud37c\ub728\ub9ac\uac70\ub098 , \uac10\uc5fc\uc744 \uc804\ud30c\ud560 \uc218 \uc788\uc74c\uc744 \uc54c\uace0 \uc788\ub098\uc694 ? | \ud574\ucda9\uc740 \uc2dd\ud488\uc744 \ub9dd\uce60 \uc218 \uc788\uace0 , \uc790\uadf9\uc744 \uc720\ubc1c\ud560 \uc218 \uc788\uc73c\uba70 , \ucd5c\uc545\uc758 \uacbd\uc6b0 \uc54c\ub808\ub974\uae30 \ubc18\uc751\uc744 \uc720\ubc1c\ud560 \uc218 \uc788\uace0 , \ub3c5\uc744 \ud655\uc7a5\ud558\uac70\ub098 , \uac10\uc5fc\uc744 \uc804\ud30c\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4 . | | Korean | It is obvious enough that the world\" Performance Evaluation_p43.png\"The image contains a table with text in both English and Japanese. Here is the structured data extracted from the table, represented in a simple text format: ``` [ { \"Language\": \"Japanese\", \"English\": \"The qualities that dete rmine a subculture as distinct may be linguistic, aesthetic, religious, political, sexual, geographical, or a combination of factors.\", \"Japanese1\": \" \u30b5\u30d6\u30ab\u30eb\u30c1\u30e3\u30fc \u306e\u7279\u6027\u3092\u6c7a\u5b9a\u4ed8\u3051\u308b\u6027\u8cea\u306f\u3001\u8a00\u8a9e\u3001\u7f8e\u5b66\u3001\u5b97\u6559\u3001 \u653f\u6cbb\u3001\u6027\u3001\u5730\u7406\u3001 \u307e\u305f\u306f\u305d\u308c\u3089\u306e \u8981\u7d20\u306e\u7d44\u307f\u5408\u308f\u305b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002 \", \"Japanese2\": \" \u30b5 \u30d6\u30ab\u30eb\u30c1\u30e3\u30fc \u3092\u7279\u5fb4\u3065\u3051\u308b\u3082\u306e\u3068\u3059\u308b \u54c1\u8cea\u306f\u3001\u8a00\u8a9e\u7684\u306a\u3082\u306e\u3068 \u3059\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3001 \u7f8e\u5b66\u7684\u3001\u5b97\u6559\u7684\u3001\u653f\u6cbb\u7684\u3001\u6027\u7684\u3001 \u5730\u7406\u7684\u3001 \u3042\u308b\u3044\u306f\u305d\u308c\u3089\u306e \u8981\u56e0\u306e\u7d44\u5408\u308f\u3055\u305b\u3066\u3044\u308b \u3053\u3068\u304c\u308d\u307e\u3059\u3002 \", \"Japanese3\": \" \u305d\u306e\u3046\u306a \u54c1\u8cea\u306f\u3001\u30b5\u30d6\u30ab\u30eb\u30c1\u30e3\u30fc \u3092\u7279\u5fb4\u4ed8\u3051\u308b\u3082\u306e\u3068\u3059 \u308b\u53ef\u80fd\u6027\u304c\u308d\u3042\u308a\u307e\u3059\u3002 \u8a00\u8a9e\u7684\u3001\u7f8e\u5b66\u7684\u3001\u5b97\u6559\u7684\u3001\u653f\u6cbb\u7684\u3001\u6027\u7684\u3001\u5730\u7406\u7684\u3001 \u3042\u308b\u3044\u307b \u307b\u305d\u308c\u3089\u306e \u8981\u56e0\u306e\u7d44\u5408\u308f\u305b\u3067\u3042\u308b\u3053\u3068\u304c\u308d\u307e\u3059\u3002 \" }, { \"Language\": \"Japanese\", \"English\": It is obvious enough that the world\" Performance Evaluation_p43.png\"The image contains a table with text in both English and Japanese. Here is the",
    "125": "structured data extracted from the table, represented in a simple text format: ``` [ { \"Language\": \"Japanese\", \"English\": \"The qualities that dete rmine a subculture as distinct may be linguistic, aesthetic, religious, political, sexual, geographical, or a combination of factors.\", \"Japanese1\": \" \u30b5\u30d6\u30ab\u30eb\u30c1\u30e3\u30fc \u306e\u7279\u6027\u3092\u6c7a\u5b9a\u4ed8\u3051\u308b\u6027\u8cea\u306f\u3001\u8a00\u8a9e\u3001\u7f8e\u5b66\u3001\u5b97\u6559\u3001 \u653f\u6cbb\u3001\u6027\u3001\u5730\u7406\u3001 \u307e\u305f\u306f\u305d\u308c\u3089\u306e \u8981\u7d20\u306e\u7d44\u307f\u5408\u308f\u305b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002 \", \"Japanese2\": \" \u30b5 \u30d6\u30ab\u30eb\u30c1\u30e3\u30fc \u3092\u7279\u5fb4\u3065\u3051\u308b\u3082\u306e\u3068\u3059\u308b \u54c1\u8cea\u306f\u3001\u8a00\u8a9e\u7684\u306a\u3082\u306e\u3068 \u3059\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3001 \u7f8e\u5b66\u7684\u3001\u5b97\u6559\u7684\u3001\u653f\u6cbb\u7684\u3001\u6027\u7684\u3001 \u5730\u7406\u7684\u3001 \u3042\u308b\u3044\u306f\u305d\u308c\u3089\u306e \u8981\u56e0\u306e\u7d44\u5408\u308f\u3055\u305b\u3066\u3044\u308b \u3053\u3068\u304c\u308d\u307e\u3059\u3002 \", \"Japanese3\": \" \u305d\u306e\u3046\u306a \u54c1\u8cea\u306f\u3001\u30b5\u30d6\u30ab\u30eb\u30c1\u30e3\u30fc \u3092\u7279\u5fb4\u4ed8\u3051\u308b\u3082\u306e\u3068\u3059 \u308b\u53ef\u80fd\u6027\u304c\u308d\u3042\u308a\u307e\u3059\u3002 \u8a00\u8a9e\u7684\u3001\u7f8e\u5b66\u7684\u3001\u5b97\u6559\u7684\u3001\u653f\u6cbb\u7684\u3001\u6027\u7684\u3001\u5730\u7406\u7684\u3001 \u3042\u308b\u3044\u307b \u307b\u305d\u308c\u3089\u306e \u8981\u56e0\u306e\u7d44\u5408\u308f\u305b\u3067\u3042\u308b\u3053\u3068\u304c\u308d\u307e\u3059\u3002 \" }, { \"Language\": \"Japanese\", \"English\": \"New Kingdom ancient Egyptians marveled at their pred ecessors\" Performance Evaluation_p44.png\"The image contains a table with multiple rows and columns. Each cell of the table contains text in multiple languages, primarily consisting of Japanese, French, and English translations. The table data is as follows: ```plain text | Language | Text | | ----- -------- |-------------------------------------------------------------------------------------- | | Japanese | Like some other experts, he is skeptical about whether diabetes can be cured, noting that these findings have no re levance to people who already have Type 1 diabetes. | | Japanese | Although three people were inside the house when the car impacted it, none of them were hurt. | | Japanese | 34 per cent of those in the poll share this view, wanting Queen Elizabeth II to be Australia\u2019s last monarch. | | Japanese | The qualities that determine a subculture as distinct may be linguistic, aesthetic, religious, political, sexual, geographical, or a combination of factors. | | Japanese | New Kingdom ancient Egyptians marvelled at their predecessors monuments, which were then well over a thousand year old. | | Japanese | The U.S. Corps of Engineers estimated that 6 inches of rainfall could breach the previously damaged levees. | | Japanese | Se veral large television screens were installed in various places in Rome to let the people watch the ceremony. | | French | Bien que trois personnes aient \u00e9t\u00e9 pr\u00e9sentes dans la maison quand la voiture l\u2019a percut\u00e9e, aucune n\u2019a \u00e9t\u00e9 bless\u00e9e. | | French | 34 % des personnes interrog\u00e9es\" Performance Evaluation_p45.png\"The image contains a table with two languages: French and aesthetic, religious, political, sexual, geographical, or a combination of factors. | | Japanese | New Kingdom ancient Egyptians marvelled at their predecessors monuments, which were then well over a thousand year old. | | Japanese | The U.S. Corps of Engineers estimated that 6 inches of rainfall could breach the previously damaged levees. | | Japanese | Se veral large television screens were installed in various places in Rome to let the people watch the ceremony. | | French | Bien que trois personnes aient \u00e9t\u00e9 pr\u00e9sentes dans la maison quand la voiture l\u2019a percut\u00e9e, aucune n\u2019a \u00e9t\u00e9 bless\u00e9e. | | French | 34 % des personnes interrog\u00e9es\" Performance Evaluation_p45.png\"The image contains a table with two languages: French and Indonesian. Each row of the table contains translations of specific statements. Below is the structured text form of the data from the table: ```json [ { \"French\": \"Pe sts can spoil food, cause irritation, or in a worse case cause allergic reactions, spread venom, or transmit infections.\", \"French Translation 1\": \"Les parasites peuvent contaminer la nourriture, provoquer des irritations ou, dans les cas les plus graves, provoquer des r\u00e9actions allergiques, r\u00e9pandre du venin ou transmettre des infections.\", \"French Translation 2\": \"Les nuisibles peuvent g\u00e2cher les aliments, provoquer des irritations, ou dans les cas les plus graves provoquer des r\u00e9actions allergiques, propager du venin ou transmettre des infections.\", \"French Translation 3\": \"Les nuisibles peuvent alt\u00e9rer les aliments, causer des irritations, ou dans les cas les plus graves provoquer des r\u00e9actions allergiques, r\u00e9pandre du venin ou transmettre des infections .\" }, { \"French\": \"It is obvious enough that the world has changed much because of humankind's scientific and technological advancements, and problems have become greater because of overpopulation and mankind's extravagant lifestyle.\", \"French Translation 1\": \"Il est clair que le monde a beaucoup chang\u00e9 gr\u00e2ce aux progr\u00e8s scientifiques et technologiques r\u00e9alis\u00e9es par les \u00eatres humains, et que les probl\u00e8mes sont devenus plus importants en raison de la surpopulation et du\" Performance Evaluation_p46.png\"The image contains a table with data written in Indonesian along with its translation in English. Here is the structured format of the data within the table for the database: ``` | Language | Translation | Column 2 Indonesian | Column 3 Indonesian | | --------- enough that the world has changed much because of humankind's scientific and technological advancements, and problems have become greater because of overpopulation and mankind's extravagant lifestyle.\", \"French Translation 1\": \"Il est clair que le monde a beaucoup chang\u00e9 gr\u00e2ce aux progr\u00e8s scientifiques et technologiques r\u00e9alis\u00e9es par les \u00eatres humains, et que les probl\u00e8mes sont devenus plus importants en raison de la surpopulation et du\" Performance Evaluation_p46.png\"The image contains a table with data written in Indonesian along with its translation in English. Here is the structured format of the data within the table for the database: ``` | Language | Translation | Column 2 Indonesian | Column 3 Indonesian | | --------- --|--------------------------------------------------------------------- |-------------------------------------------- ------------------------------------------------------------- |------------------------------------------ ------------- ----------------------------------------------- | | Indonesian| New Kingdom ancient Egyptians marvelled at their predecessors' monuments, which were then well over a thousand years old. | Kerajaan Baru Mesir kuno mengagumi monumen para pendahul unya, yang saat itu berusia lebih dari seribu tahun. | Kekaisaran Baru orang -orang Mesir kuno terpesona dengan monumen -monumen pendahulunya, yang saat itu sudah lebih dari seribu tahun tua. | Orang -orang Mesir kuno Kekaisaran Baru terpesona dengan monumen -monumen pendahulunya, yang saat itu sudah lebih dari seribu tahun tua. | | Indonesian| The U.S. Corps of Engineers estimated that 6 inches of rainfall could breach the previously damaged levees. | U.S. Corps of Engineers memperkirakan bahwa curah hujan seb esar 6 inci ini mampu menerobos tanggul yang sudah rusak. | Korps Engineer AS menghitung bahwa hujan 6 inci dapat merusak parit yang sebelumnya sudah rusak. | Badan Engineer AS memperkirakan bahwa hujan 6 inci dapat menembus parit yang\" Performance Evaluation_p47.png\"The image contains a table with text in four columns. Below is the structured data from the table: ```plaintext Row 1: - Column 1: Although three people were inside the house when the car impacted it, none of them were hurt. - Column 2: Sanadyan uwong telu ana ing njero omah nalika mobil kuwi nabrak, nanging kabeh slamet. - Column 3: Walaupun tiga orang berada di dalam rumah saat mobil menabraknya, tidak ada di antara mereka yang bahwa curah hujan seb esar 6 inci ini mampu menerobos tanggul yang sudah rusak. | Korps Engineer AS menghitung bahwa hujan 6 inci dapat merusak parit yang sebelumnya sudah rusak. | Badan Engineer AS memperkirakan bahwa hujan 6 inci dapat menembus parit yang\" Performance Evaluation_p47.png\"The image contains a table with text in four columns. Below is the structured data from the table: ```plaintext Row 1: - Column 1: Although three people were inside the house when the car impacted it, none of them were hurt. - Column 2: Sanadyan uwong telu ana ing njero omah nalika mobil kuwi nabrak, nanging kabeh slamet. - Column 3: Walaupun tiga orang berada di dalam rumah saat mobil menabraknya, tidak ada di antara mereka yang terluka. - Column 4: Walaupun ana ing tigo wang ing rumah kala mobil mbent ak ing",
    "126": "kono, ora ana sak ingkang terluka. Row 2: - Column 1: 34 per cent of those in the poll share this view, wanting Queen Elizabeth II to be Australia\u2019s last monarch. - Column 2: 34 per sen sing menehi swara sesawangan ngene, kepengin Ratu Elizabeth II dadi raja sing keri dhewe ana ning Australia. - Column 3: Sebagian 34 persen dari mereka yang terlibat dalam polling ini berpendapat demikian, ingin Ratu Elizabeth II menjadi Ratu Australia terakhir. - Column 4: Sebagian 34 persen dari mereka yang terlibat dalam polling ini memiliki pandangan yang sama, yaitu menginginkan Ratu Elizabeth II menjadi Ratu Australia terakhir. Row 3: - Column 1: The qualities that determine a\" Performance Evaluation_p48.png\"The provided image contains a table with text in different languages. Here is the structured representation of the data from the table: Format: ``` { \"Language 1\": \"Text in Language 1\", \"Language 2\": \"Text in Language 2\", \"La nguage 3\": \"Text in Language 3\", \"Language 4\": \"Text in Language 4\" } ``` Data: ``` { \"Javanese\": \"It is obvious enough that the world has changed much because of humankind\u2019s scientific and technological advancements, and problems have become greater becau se of overpopulation and mankind\u2019s extravagant lifestyle.\", \"Javanese\": \"Cukup jelas menawa donya wis malih akeh amarga majune ilmu lan teknologi manungsa, lan masalah -masalah dadi tambah gedhe amarga kaluwihuan populasi lan gaya urip sing boros saka manun gsa.\", \"Javanese\": \"Sawetara bisa dilihat manawa dunya iki duw\u00e9 sak kabeh pangowahan sain -teknologi manungsa lan masalah duw\u00e9 gedh\u00e9 luwih amarga tambah -tambah\u00e9 jumlah penduduk lan gaya hidup manungsa sing \"Text in Language 3\", \"Language 4\": \"Text in Language 4\" } ``` Data: ``` { \"Javanese\": \"It is obvious enough that the world has changed much because of humankind\u2019s scientific and technological advancements, and problems have become greater becau se of overpopulation and mankind\u2019s extravagant lifestyle.\", \"Javanese\": \"Cukup jelas menawa donya wis malih akeh amarga majune ilmu lan teknologi manungsa, lan masalah -masalah dadi tambah gedhe amarga kaluwihuan populasi lan gaya urip sing boros saka manun gsa.\", \"Javanese\": \"Sawetara bisa dilihat manawa dunya iki duw\u00e9 sak kabeh pangowahan sain -teknologi manungsa lan masalah duw\u00e9 gedh\u00e9 luwih amarga tambah -tambah\u00e9 jumlah penduduk lan gaya hidup manungsa sing luwih mewah.\", \"Javanese\": \"Sawetara bisa dilihat m anawa dunya iki duw\u00e9 sak kabeh pangowahan sain teknologi saka manungsa, lan masalah -masalah duw\u00e9 gedh\u00e9 luwih amarga tambah -tambah\u00e9 jumlah penduduk lan gaya hidup manungsa sing luwih mewah.\" \" Performance Evaluation_p49.png\"The provided image contains a table labeled \"Table 20: Examples of ChatGPT translated and post -edited sentences.\" Below is a structured textual representation of the data from the table: ## Table Structure | Language | Senten ce (English) | Sentence (Sundanese) | Post -edited Sentence (Sundanese) | | ------------ |-------------------------------- ------------------------------------------------------------------------- |------------------------------------------- -------------------- ----------------------------------------------------------- |------------------------------------- ------------------------------------------------------------------------------------- | | Sundanese | Several large television screens were installed in various places in Rome to let the people watch the ceremony. | Sababaraha tipi ageung dipasang di sababaraha tempat di Roma supados warga tiasa nyaksian upacara \u00e9ta. | Sababaraha layar televisi besar dipasang di berbagai tempat di Roma agar orang -orang bisa menon ton upacara. | | Sundanese | Pests can spoil food, cause irritation, or in a worse case cause allergic reactions, spread venom, or transmit infections. | Hama tiasa ngaruksak dahareun, nyieun iritasi, atawa langkung parah deui tiasa nyieun al\u00e9rgi, nyebarke un racun, atawa nularkeun panyakit. | Hama bisa merusak makanan, menyebabkan iritasi, atau dalam kasus yang lebih buruk menyebabkan reaksi alergi, menyebarkan racun, atau menularkan infeksi. | | large television screens were installed in various places in Rome to let the people watch the ceremony. | Sababaraha tipi ageung dipasang di sababaraha tempat di Roma supados warga tiasa nyaksian upacara \u00e9ta. | Sababaraha layar televisi besar dipasang di berbagai tempat di Roma agar orang -orang bisa menon ton upacara. | | Sundanese | Pests can spoil food, cause irritation, or in a worse case cause allergic reactions, spread venom, or transmit infections. | Hama tiasa ngaruksak dahareun, nyieun iritasi, atawa langkung parah deui tiasa nyieun al\u00e9rgi, nyebarke un racun, atawa nularkeun panyakit. | Hama bisa merusak makanan, menyebabkan iritasi, atau dalam kasus yang lebih buruk menyebabkan reaksi alergi, menyebarkan racun, atau menularkan infeksi. | | Sundanese | It is obvious enough that the world has changed m uch because of humankind\u2019s scientific and technological advancements, and\" Performance Evaluation_p5.png\"### Diagrams, Graphics, Equations, and Tables Identification Based on the provided image, the content comprises a table summarizing various tasks, datasets, metrics, references, and performance values. ### Table Data Structure d Form Here is the data from the table in a structured text format suitable for database ingestion: #### Table 1: Performance of ChatGPT compared to state -of-the-art fully -fine-tuned models (Fine -Tuned SOTA) and LLM in zero -shot settings (Zero -Shot SOTA) | Task | Dataset | Metric | Reference | Fine-Tuned SOTA | Zero -Shot SOTA | ChatGPT | | ----------------------- |------------------- |------------ |-------------------------------- |----------------- |---------------- |---------- | | Summarization | CNN/DM | ROUGE -1 | Lewis et al. (2020a) | 44.47 | 35.27 | 35.29 | | | SAMSum | ROUGE -1 | Lewis et al. (2020a) | 47.28 | - | 35.29 | | MT (XXX ->Eng) | FLORes -200 (HRL) | ChrF++ | Team et al. (2022) | 63.5 | - | 58.66 | | | FLORes -200 (LRL) | ChrF++\" Performance Evaluation_p50.png\"**Structured Data:** Categories | Testset | Result ------------- |- -------------------------------- |------- Deductive | ENTAILMENTBANK | 28/30 Deductive | bAbI (task 15) | 28/30 (as is - 19/30) Inductive | CLUTRR | 13/30 Induc tive | bAbI (task16) | 20/30 (as is - 0/30) Abductive | aNLI | 26/30 Mathematical | Math | 13/30 Temporal | Timedial | 26/30 Spatial | SpartQA (hard) | 8/32 Spatial | SpartQA (basic) | 20/32 Spatial | StepGame (hard) | 7/30 ROUGE -1 | Lewis et al. (2020a) | 44.47 | 35.27 | 35.29 | | | SAMSum | ROUGE -1 | Lewis et al. (2020a) | 47.28 | - | 35.29 | | MT (XXX ->Eng) | FLORes -200 (HRL) | ChrF++ | Team et al. (2022) | 63.5 | - | 58.66 | | | FLORes -200 (LRL) | ChrF++\" Performance Evaluation_p50.png\"**Structured Data:** Categories | Testset | Result ------------- |- -------------------------------- |------- Deductive | ENTAILMENTBANK | 28/30 Deductive | bAbI (task 15) | 28/30 (as is - 19/30) Inductive | CLUTRR | 13/30 Induc tive | bAbI (task16) | 20/30 (as is - 0/30) Abductive | aNLI | 26/30 Mathematical | Math | 13/30 Temporal | Timedial | 26/30 Spatial | SpartQA (hard) | 8/32 Spatial | SpartQA (basic) | 20/32 Spatial | StepGame (hard) | 7/30 Spatial | StepGame (basic) | 19/ 30 Spatial | StepGame (basic -cardinal) | 17/20 Spatial | StepGame (diagonal) | 11/20 Spatial | StepGame (clock -direction) | 5/20 Commonsense | CommonsenseQA | 27/30 Commonsense | PIQA | 25/30 Commonsense | Pep -3k (Hard) | 28/30 Causal | E\" Performance Evaluation_p51.png\"There is one table in the image, titled \"Table 22: Examples of modular Task -Oriented Dialogue using ChatGPT: dialogue state tracking and response generation.\" Here is the data from the table in a structured text format: ``` T able 22: Examples of modular Task -Oriented Dialogue using ChatGPT: dialogue state tracking and response generation | Task | Key | Text Content | | --------------------------- |--------- |--------------------------------",
    "127": "---------------------------------------- ----------------------------------------------------------------------------- ----| | Dialogue State Tracking | Prompt | Give the dialogue state of the last utterance in the following dialogue in the form of `STATE: Domain -Intent: [Slot, Possible value, ... ]`. | | | | Intents: Request, Inform, general -thank, general -bye | | | | Domain: hotel, Slots: pricerange, Possible values: ['expensive', 'cheap', 'moderate'] | | | | Domain: hotel, Slots: type, Possible values: ['guesthouse', 'hotel'] | | | | Domain: hote l, Slots: parking, Possible values: ['free', 'noyes'] | | | | Domain: hotel, Slots: bookday, Possible values: ['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday'] | | | | Domain: hotel, Slots: bookpeople\" Performance Evaluation_p52.png\"**Table from the image:** | Task | Key | Text Content | | ------- ----| | Dialogue State Tracking | Prompt | Give the dialogue state of the last utterance in the following dialogue in the form of `STATE: Domain -Intent: [Slot, Possible value, ... ]`. | | | | Intents: Request, Inform, general -thank, general -bye | | | | Domain: hotel, Slots: pricerange, Possible values: ['expensive', 'cheap', 'moderate'] | | | | Domain: hotel, Slots: type, Possible values: ['guesthouse', 'hotel'] | | | | Domain: hote l, Slots: parking, Possible values: ['free', 'noyes'] | | | | Domain: hotel, Slots: bookday, Possible values: ['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday'] | | | | Domain: hotel, Slots: bookpeople\" Performance Evaluation_p52.png\"**Table from the image:** | Task | Key | Text Content | | ------- ----------------------- |----------- |--------------------------------------------------------------------------------- -------------------------------------------- ------------------------------------------------------------------------- --------------------------------------------------------------------------------------------------------------------- ---------------------------------------------- | | Multi -turn unifi ed approach | Prompt | Use the following knowledge base to complete the task of \"recommending a restaurant\" by continuing the conversation as a task -oriented dialogue system: Restaurant: Mama Julia, Food: French, Price: Expensive, Location: 7th street, Rat ing: 5; Restaurant: Papa John, Food: Italian, Price: Moderate, Location: 3rd street, Rating: 4; Restaurant: The Crossroad, Food: Morocco, Price: Moderate, Location: Downtown, Rating: 2; Restaurant: Tacos City, Food: Mexican, Price: Cheap, Location: Center, Rating: 1; Restaurant: Golden Rice Bowl, Food: Chinese, Price: Cheap, Location: 3rd district, Rating: 3; Restaurant: Veggie Garden, Food: Chinese, Price: Expensive, Location: Town Hall, Rating: 4; Restaurant: Pizza House, Food: Italian, Price: Moderate, Location: 3rd street, Rating: 2; Restaurant: The Palace, Food: Vietnamese, Price: Expensive, Location: Hotel Grandview, Rating: 5 Hi, I'm looking for a rest aurant | | | ChatGPT | Sure, what type of food are you in the mood for? \" Performance Evaluation_p6.png\"From the provided image, there are the following elements of interest: ### Tables **Table 2:** The table referenced in the text is described but not shown in the attached image. The table includes automatic evaluation results on dialogue tasks for Center, Rating: 1; Restaurant: Golden Rice Bowl, Food: Chinese, Price: Cheap, Location: 3rd district, Rating: 3; Restaurant: Veggie Garden, Food: Chinese, Price: Expensive, Location: Town Hall, Rating: 4; Restaurant: Pizza House, Food: Italian, Price: Moderate, Location: 3rd street, Rating: 2; Restaurant: The Palace, Food: Vietnamese, Price: Expensive, Location: Hotel Grandview, Rating: 5 Hi, I'm looking for a rest aurant | | | ChatGPT | Sure, what type of food are you in the mood for? \" Performance Evaluation_p6.png\"From the provided image, there are the following elements of interest: ### Tables **Table 2:** The table referenced in the text is described but not shown in the attached image. The table includes automatic evaluation results on dialogue tasks for ChatGPT. ### Equations There are no explicit equations present in the provided image. ### Graphics/Diagrams There are no graphics or diagrams evident in the provided image. ### Data Representation #### Misinformation Detection Metrics : - **Dataset:** COVID -19 (Lee et al., 2021) with 100 samples. - **Types of Data:** - Scientific (covid -scientific) - Social (covid - social) - **Accuracy Rates:** - **Misinformation detection (> Verification Refusing Cases):** - **Overall:** 92% (46/50) - **Covid -scientific:** 73.33% (22/30) - **Covid -social:** 73.33% (22/30) ### Summary of Described Graphics/Diagrams 1. **Machine Translation:** - Evaluates ChatGPT's performance on high -resource and low -resource languages. - Uses the ChrF++ metric. - High -resource languages: French (fra), Spanish (spa), Chinese (zho), Arabic (ara), Japanese (jpn), Indonesian (ind), Korean (kor), and Vietnamese (vie). - Low-resource languages: Javanese (\" Performance Evaluation_p7.png\"The image contains two tables labeled Table 2 and Table 3. There are no diagrams or equations. Here's the structured data extracted from the tables: ### Table 2: Automatic evaluation results on OpenDialKG Results for GPT2 are from Dziri et al. (2021). | Model | BLEU | ROUGE -L | FeQA (Durmus et al., 2020) | | -------- |------ |--------- |--------- ------------------ | | ChatGPT | 4.05 | 18.62 | 15.03 | | GPT2 | 11.10 | 30.00 | 26.54 | ### Table 3: Result for Task -oriented Dialogue Set up A \u2014 Modular Approach | State Tracking | Response Generation | | ------------------- |---------------------- | | Joint Goal Acc. | BLEU | Inform rate | | 24.4% | 5.65 | 71.1% |\" Javanese (\" Performance Evaluation_p7.png\"The image contains two tables labeled Table 2 and Table 3. There are no diagrams or equations. Here's the structured data extracted from the tables: ### Table 2: Automatic evaluation results on OpenDialKG Results for GPT2 are from Dziri et al. (2021). | Model | BLEU | ROUGE -L | FeQA (Durmus et al., 2020) | | -------- |------ |--------- |--------- ------------------ | | ChatGPT | 4.05 | 18.62 | 15.03 | | GPT2 | 11.10 | 30.00 | 26.54 | ### Table 3: Result for Task -oriented Dialogue Set up A \u2014 Modular Approach | State Tracking | Response Generation | | ------------------- |---------------------- | | Joint Goal Acc. | BLEU | Inform rate | | 24.4% | 5.65 | 71.1% |\" Performance Evaluation_p8.png\"The image contains two tables, Table 4 and Table 5. Below are the representations of these tables as text in a structured form: **Table 4: The statistics of languages used in our language disparity experiment.** ``` + --------- ---+----------- +-------------- +- ------------------ + | Language | #Speakers | CC Size (%) | Language Category | + ------------ +-------- ---+-------------- +------------------- + | English | 1.452B | 46.320 | HRL | | Chinese | 1.118B | 4.837 | HRL | | French | 2 35M | 4.604 | HRL | | Indonesian| 199M | 0.781 | MRL | | Korean | 81.7M | 0.679 | MRL | | Javanese | 68.3M | 0.002 | LRL | | Sundanese | 32.4M | 0.001 | LRL | | Buginese | - M | 0.000 | X -LRL | + ------------ +----------- +-------------- +------------------- + ``` *HRL denotes high-resourced language, MRL denotes medium -resourced language, LRL denotes low - resourced\" Performance Evaluation_p9.png\"The image contains the following elements: **Table:** - **Title:** Table 7: Number of correct translations of ChatGPT. XXX denotes the target language in the first column. The languages are sorted based on the language size in CommonCrawl. - **Columns:** Language, XXX \u2192Eng, Eng\u2192XXX - **Rows:** - Chinese: 24/30 (XXX \u2192Eng), 14/30 (Eng \u2192XXX) - French: 29/30 (XXX \u2192Eng), 25/30 (Eng \u2192XXX) - Indonesian: 28/30 (XXX\u2192Eng), 19/30 (Eng \u2192XXX) - Korean: 22/30 (XXX \u2192Eng), 12/30 (Eng \u2192XXX) - Javanese: 7/30 (XXX \u2192Eng), 6/30 (Eng \u2192XXX) - Sundanese: 9/30 (XXX \u2192Eng), 0/30 (Eng \u2192 XXX) **Table:** -",
    "128": "**Title:** Table 6: Example of Buginese language identification response from ChatGPT, Instr uctGPT, and text -davinci -003. - **Columns:** ChatGPT, InstructGPT, text - Performance Evaluation_p9.png\"The image contains the following elements: **Table:** - **Title:** Table 7: Number of correct translations of ChatGPT. XXX denotes the target language in the first column. The languages are sorted based on the language size in CommonCrawl. - **Columns:** Language, XXX \u2192Eng, Eng\u2192XXX - **Rows:** - Chinese: 24/30 (XXX \u2192Eng), 14/30 (Eng \u2192XXX) - French: 29/30 (XXX \u2192Eng), 25/30 (Eng \u2192XXX) - Indonesian: 28/30 (XXX\u2192Eng), 19/30 (Eng \u2192XXX) - Korean: 22/30 (XXX \u2192Eng), 12/30 (Eng \u2192XXX) - Javanese: 7/30 (XXX \u2192Eng), 6/30 (Eng \u2192XXX) - Sundanese: 9/30 (XXX \u2192Eng), 0/30 (Eng \u2192 XXX) **Table:** - **Title:** Table 6: Example of Buginese language identification response from ChatGPT, Instr uctGPT, and text -davinci -003. - **Columns:** ChatGPT, InstructGPT, text - davinci-003 - **Rows:** - Row 1: - ChatGPT: The language of the text appears to be a variant of the Bugis language spoken in Indonesia. - InstructGPT:\" RAG Agent Resource -1_p1.png\"### Detected Elements in the Image: #### Diagrams and Graphics: 1. **Diagram 1**: - Title: None specified. - Description: Diagram illustrating the process of creating a semantic search index from documents. - Elements: - **Chunk ing**: - *Document Chunk 1* - ... - *Document Chunk N* - **Embedding Model**: - *Embedding 1* - ... - *Embedding N* - **Semantic Search Index**. #### Tables: - No tables detected in the image. #### Equations: - No equations detected in the image. ### Summa ry of Diagrams/Graphics: #### Diagram 1: The diagram showcases the steps involved in creating a semantic search index. 1. **Chunking**: The process begins with ingesting documents from an input knowledge base and splitting them into smaller chunks. Each ch unk is represented as - Document Chunk 1 - ... - Document Chunk N 2. **Embedding Model**: These chunks are then processed by an embedding model to create an embedding for each chunk, resulting in: - Embedding 1 - ... - Embedding N 3. **Semantic Search Inde x**: The embeddings are then used as input to create a semantic search index. ### Structured Data Representation (for Database Insertion): ```json { \"diagrams\": [ { \"title\": null, \"description\" RAG Agent Resource -1_p2.png\"The image contains two diagrams illustrating a process related to semantic search and large language models (LLM). ### First Diagram: - **Title**: None specified - **Components**: - **Input query** \u2192 **Embedding model** \u2192 **Query input knowledge base and splitting them into smaller chunks. Each ch unk is represented as - Document Chunk 1 - ... - Document Chunk N 2. **Embedding Model**: These chunks are then processed by an embedding model to create an embedding for each chunk, resulting in: - Embedding 1 - ... - Embedding N 3. **Semantic Search Inde x**: The embeddings are then used as input to create a semantic search index. ### Structured Data Representation (for Database Insertion): ```json { \"diagrams\": [ { \"title\": null, \"description\" RAG Agent Resource -1_p2.png\"The image contains two diagrams illustrating a process related to semantic search and large language models (LLM). ### First Diagram: - **Title**: None specified - **Components**: - **Input query** \u2192 **Embedding model** \u2192 **Query embedding** - **Query embedding** \u2192 **Semantic Search index** \u2192 {K relevant/similar embeddings} - Embedding 1, Embedding 2, ... Embedding K - {K relevant/similar embeddings} \u2192 {Relevant document chunks} - Document chunk 1, Document chunk 2, ... Document chunk K **Description**: The first diagram shows a process where an input query is fed into an embedding model, which converts it into a query embedding. This query embedding is then searched in a semantic search index to find K relevant or similar embeddi ngs. Each embedding corresponds to a relevant document chunk, from Document chunk 1 through Document chunk K. ### Second Diagram: - **Title**: Prompt Construction and Answer Generation - **Components**: - **Prompt**: - **System prompt**: \"You are helpful A I assistant, answer question accurately from the context provided below\" - **Context**: The relevant chunks of documents from the internal knowledge base - **Question**: The input query of the user - **Large language model** \u2192 **Generates** \u2192 **Final answe r** **Description**: The second diagram details the construction of a prompt\" RAG Agent Resource -1_p3.png\"**Diagrams:** - **Advanced RAG - LlamaIndex Multi -Doc Agent Diagram**: - The diagram depicts a flow of data from a document to various indexes and then to an agent. - It starts with a \"Document\" node that feeds into a \"Node\". - The Node has three branches: - \"Summary Index\", \"Vector Index\", and \"Query Engine Tool\". - These indexes are connected to \"Query Engine Tool\". - The Query Engine Tool is connected to the final \"Agent (OpenAI GPT -4)\". - The agent has a description box stati ng: \"You are a specialized agent documents from the internal knowledge base - **Question**: The input query of the user - **Large language model** \u2192 **Generates** \u2192 **Final answe r** **Description**: The second diagram details the construction of a prompt\" RAG Agent Resource -1_p3.png\"**Diagrams:** - **Advanced RAG - LlamaIndex Multi -Doc Agent Diagram**: - The diagram depicts a flow of data from a document to various indexes and then to an agent. - It starts with a \"Document\" node that feeds into a \"Node\". - The Node has three branches: - \"Summary Index\", \"Vector Index\", and \"Query Engine Tool\". - These indexes are connected to \"Query Engine Tool\". - The Query Engine Tool is connected to the final \"Agent (OpenAI GPT -4)\". - The agent has a description box stati ng: \"You are a specialized agent designed to answer queries about (subject). You must ALWAYS use at least one of the tools provided when answering a question; do NOT rely on prior knowledge.\" **Links:** - **First Check: Single document as knowledge base (c omplex data)**: - https://ai.gopubby.com/advanced -rag-semi -structured -data-with-langchain -ce46c8baa6cf - https://ai.gopubby.com/advanced -rag-multi -modal -rag-with-gpt4-vision -e4c11229682c - **Advanced RAG \u2013 Multi Documents Agent with Llamalndex** - Sample i mplementation: - https://github.com/sugarforever/Advanced - RAG/blob/main/03_llama_index_multi_doc_agent.ipynb There are no tables or equations in the\" RAG Agent Resource -1_p4.png\"**Diagrams and Graphics Summary:** 1. **Diagram:** - **Title:** Advanced RAG - LlamaIndex Multi -Doc Agent - **Components:** - Three Document icons leading to three separate Agent boxes. - Each Agent box connects to a Query Engin e Tool (light blue). - The Query Engine Tools connect to an Object Index. - The Object Index connects to a Retriever. - The Retriever connects to an Agent for QA (purple). - **Instructions:** - \u201cYou are an agent designed to answer questions about (subjects ). - Please always use the tools provided to answer a question. Do not rely on prior knowledge.\u201d **Data Representation:** ```json { \"diagram_title\": \"Advanced RAG - LlamaIndex Multi -Doc Agent\", \"components\": [ { \"item\": \"Document\", \"connections\": [ { \"item \": \"Agent\", \"connections\": [ { \"item\": \"Query Engine Tool\", \"connections\": [ { \"item\": \"Object Index\", \"connections\": [ { \"item\": \"Retriever\", \"connections\": [ { \"item\": \"Agent for QA\", \"instructions\": \"You are an Engin e Tool (light blue). - The Query Engine Tools connect to an Object Index. - The Object Index connects to a Retriever. - The Retriever connects to an Agent for QA (purple). - **Instructions:** - \u201cYou are an agent designed to answer questions about (subjects ). - Please always use the tools provided to answer a question. Do not rely on prior knowledge.\u201d **Data Representation:** ```json { \"diagram_title\": \"Advanced RAG - LlamaIndex Multi -Doc Agent\", \"components\": [ { \"item\": \"Document\", \"connections\": [ { \"item \": \"Agent\", \"connections\": [ { \"item\": \"Query Engine Tool\",",
    "129": "\"connections\": [ { \"item\": \"Object Index\", \"connections\": [ { \"item\": \"Retriever\", \"connections\": [ { \"item\": \"Agent for QA\", \"instructions\": \"You are an agent designed to answer questions about ( subjects). Please always use the tools provided to answer a question. Do not rely on prior knowledge.\" } ] } ] \" RAG Agent Resource -1_p5.png\"The image contains a detailed diagram with different components and their interactions. It does not include tables or equations. Below is the structured text representation of the diagram: --- **Diagram Overview:** - **Title:** Entity extraction, SQL querying, and agents with Amazon Bedrock - **Sub -Title:** Full Stack Example with Public Data **Diagram Description:** The diagram illustrates the architecture for entity extraction, SQL querying, and agents with Amazon Bedrock. It s howcases different components such as AWS services, databases, and user interactions, and outlines their connections and data flow within the system. 1. **User Interactions:** - Users interact with the ChatBot UI. - Amazon Cognito is used for user authenti cation and authorization. 2. **Document Storage:** - Document owners upload documents to Amazon S3. 3. **API Gateway Integration:** - Includes an API Gateway for agent execution. - FAQs indexed in a search index. - Can be extended with other tools. 4. **Co nversation History:** - Retrieved from Amazon DynamoDB. 5. **Backend Services:** - Serves conversations in real -time from the QA backend. 6. **Moderation Model:** - Filters and actions prohibited uses. 7. **LLM (Large Language Model) Agent:** - Decides the tools to be used and collates final responses. Can work in real -time or batch mode. 8. **Tools Integration:** - Includes\" Natural Language Processing with Deep Learning CS224N/Ling284 Jesse Mu Lecture 11: Prompting, Instruction Finetuning, and RLHF Reminders \u2022Project proposals (both custom and final) due a few minutes ago! \u2022We\u2019re in the process of assigning mentors to projects and will aim to give feedback on project proposals with a quick turnaround \u2022A5 due Friday 11:59PM! \u2022We still recommend using Colab for the assignments; in case you run into trouble (e.g. you have exceeded Colab quota), instructions for connecting to a Kaggle notebook have been posted on Ed 2 Larger and larger models 3 https:// www.economist.com /interactive/briefing/2022/06/11/huge -foundation -models -are-turbo -charging -ai-progress Trained on more and more data 4 # tokens seen during training https:// babylm.github.io / Recap of Lecture 10: What kinds of things does pretraining learn? \u2022Stanford University is located in __________, California. [Trivia] \u2022I put ___ fork down on the table. [syntax] \u2022The woman walked across the street, checking for traffic over ___ shoulder. [coreference] \u2022I went to the ocean to see the fish, turtles, seals, and _____. [lexical semantics/topic] \u2022Overall, the value I got from the two hours watching it was the sum total of the popcorn and the drink. The movie was ___. [sentiment] \u2022Iroh went into the kitchen to make some tea. Standing next to Iroh, Zuko pondered his destiny. Zuko left the ______. [some reasoning \u2013this is harder] \u2022I was thinking about the sequence that goes 1, 1, 2, 3, 5, 8, 13, 21, ____ [some basic arithmetic; they don\u2019t learn the Fibonnaci sequence] 5 Language models as world models? 6 Language Models as Agent Models [ Andreas, 2022 ]Language models may do rudimentary modeling of agents , beliefs , and actions : Language models as world models? 7 https:// www.khanacademy.org /test -prep/sat/x0a8c2e5f:untitled -652 \u2026math : Language models as world models? 8 https:// github.com /features/copilot\u2026code : Language models as world models? 9[Larnerd , 2023 ] \u2026medicine : Language models as multitask assistants? 10[Microsoft Bing ] (Also see OpenAI\u2019s ChatGPT , Google\u2019s Bard, Anthropic\u2019s Claude) Language models as multitask assistants? 11How do we get from this to this?Stanford University is located in __________ Language models as multitask assistants? 11How do we get from this to this?Stanford University is located in __________ Lecture Plan: From Language Models to Assistants 1.Zero -Shot (ZS) and Few -Shot (FS) In -Context Learning 2.Instruction finetuning 3.Reinforcement Learning from Human Feedback (RLHF) 4.What\u2019s next? 12 Lecture Plan: From Language Models to Assistants 1.Zero -Shot (ZS) and Few -Shot (FS) In -Context Learning 2.Instruction finetuning 3.Reinforcement Learning from Human Feedback (RLHF) 4.What\u2019s next? 13 Let\u2019s revisit the Generative Pretrained Transformer (GPT) models from OpenAI as an example: GPT (117M parameters; Radford et al., 2018 ) \u2022Transformer decoder with 12 layers. \u2022Trained on BooksCorpus : over 7000 unique books (4.6GB text). Showed that language modeling at scale can be an effective pretraining technique for downstream tasks like natural language inference. [START] The man is in the doorway [DELIM] The person is near the door [EXTRACT] Emergent abilities of large language models: GPT (2018) 14entailmentDecoder Emergent abilities of large language models: GPT -2 (2019) 15Let\u2019s revisit the Generative Pretrained Transformer (GPT) models from OpenAI as an example: GPT-2 (1.5B parameters; Radford et al., 2019 ) \u2022Same architecture as GPT, just bigger (117M -> 1.5B) \u2022But trained on much more data : 4GB -> 40GB of internet text data ( WebText ) \u2022Scrape links posted on Reddit w/ at least 3 upvotes (rough proxy of human quality) GPT (2018)GPT-2 (2019)117M 1.5B One key emergent ability in GPT -2 is zero -shot learning : the ability to do many tasks with no examples, and no gradient updates, by simply: \u2022Specifying the right sequence prediction problem (e.g. question answering): Passage: Tom Brady... Q: Where was Tom Brady born? A: ... \u2022Comparing probabilities of sequences (e.g. Winograd Schema Challenge [ Levesque, 2011 ]): The cat couldn\u2019t fit into the hat because it was too big. Does it = the cat orthe hat? \u2261IsP(...because the cat wastoo big) >= P(...because the hat was too big)?Emergent zero -shot learning 16[Radford et al., 2019 ] Emergent zero -shot learning 17[Radford et al., 2019 ] GPT-2 beats SoTA on language modeling benchmarks with no task -specific fine -tuning LAMBADA (language modeling w/ long discourse dependencies) [Paperno et al., 2016 ] Emergent zero -shot learning 18You can get interesting zero -shot behavior if you\u2019re creative enough with how you specify your task! Summarization on CNN/ DailyMail dataset [ See et al., 2017 ]: SAN FRANCISCO, California (CNN) -- A magnitude 4.2 earthquake shook the San Francisco ... overturn unstable objects. [Radford et al., 2019 ] 2018 SoTA Supervised (287K) \u201cToo Long, Didn\u2019t Read\u201d \u201cPrompting\u201d?TL;DR: Select from articleROUGE Emergent abilities of large language models: GPT -3 (2020) 19GPT-3 (175B parameters; Brown et al., 2020 ) \u2022Another increase in size (1.5B -> 175B ) \u2022and data (40GB -> over 600GB ) 117M 1.5B GPT (2018)GPT-2 (2019) GPT-3 (2020)175B Emergent few -shot learning 20 [Brown et al., 2020 ]\u2022Specify a task by simply prepending examples of the task before your example \u2022Also called in-context learning , to stress that no gradient updates are performed when learning a new task (there is a separate literature on few -shot learning with gradient updates) Emergent few -shot learning 21 Zero -shot [Brown et al., 2020 ] Emergent few -shot learning 22 One -shot [Brown et al., 2020 ] Emergent few -shot learning 23 Few-shot [Brown et al., 2020 ] Few-shot learning is an emergent property of model scale 24 [Brown et al., 2020 ]Synthetic \u201cword unscrambling\u201d tasks, 100 -shot Cycle letters: pleap-> apple Random insertion: a.p!p/l!e-> apple Reversed words: elppa-> apple New methods of \u201cprompting\u201d LMs 25 Traditional fine -tuning Zero/few -shot prompting [Brown et al., 2020 ] Limits of prompting for harder tasks? Some tasks seem too hard for even large LMs to learn through prompting alone. Especially tasks involving richer, multi -step reasoning. (Humans struggle at these tasks too!) Solution : change the",
    "130": "prompt! 2619583 + 29534 = 49117 98394 + 49384 = 147778 29382 + 12347 = 41729 93847 + 39299 = ? Chain -of-thought prompting 27[Wei et al., 2022; also see Nye et al., 2021 ] Chain -of-thought prompting is an emergent property of model scale 28 Middle school math word problems [Wei et al., 2022; also see Nye et al., 2021 ] Chain -of-thought prompting 29Do we even need examples of reasoning? [Wei et al., 2022; also see Nye et al., 2021 ]Can we just ask the model to reason through things? There are 16 balls in total. Half of the balls are golf balls. That means there are 8 golf balls. Half of the golf balls are blue. That means there are 4 blue golf balls.A: Let\u2019s think step by step. Zero -shot chain -of-thought prompting 30[Kojima et al., 2022 ]Q: A juggler can juggle 16 balls. Half of the balls are golf balls, and half of the golf balls are blue. How many blue golf balls are there? Zero -shot chain -of-thought prompting 31[Kojima et al., 2022 ] Manual CoT still better Greatly outperforms zero -shot Zero -shot chain -of-thought prompting 32[Zhou et al., 2022; Kojima et al., 2022 ] LM-Designed Chain -of-thought prompting 29Do we even need examples of reasoning? [Wei et al., 2022; also see Nye et al., 2021 ]Can we just ask the model to reason through things? There are 16 balls in total. Half of the balls are golf balls. That means there are 8 golf balls. Half of the golf balls are blue. That means there are 4 blue golf balls.A: Let\u2019s think step by step. Zero -shot chain -of-thought prompting 30[Kojima et al., 2022 ]Q: A juggler can juggle 16 balls. Half of the balls are golf balls, and half of the golf balls are blue. How many blue golf balls are there? Zero -shot chain -of-thought prompting 31[Kojima et al., 2022 ] Manual CoT still better Greatly outperforms zero -shot Zero -shot chain -of-thought prompting 32[Zhou et al., 2022; Kojima et al., 2022 ] LM-Designed The new dark art of \u201cprompt engineering\u201d? 33 Use Google code header to generate more \u201cprofessional\u201d code? Asking a model for reasoning fantasy concept art, glowing blue dodecahedron die on a wooden table, in a cozy fantasy (workshop), tools on the table, artstation , depth of field, 4k, masterpiece https:// www.reddit.com /r/StableDiffusion / comments/110dymw/ magic_stone_workshop / \u201cJailbreaking\u201d LMs https:// twitter.com /goodside /status/1569128808308957185/photo/1 The new dark art of \u201cprompt engineering\u201d? 34 Lecture Plan: From Language Models to Assistants 1.Zero -Shot (ZS) and Few -Shot (FS) In -Context Learning 2.Instruction finetuning 3.Reinforcement Learning from Human Feedback (RLHF) 4.What\u2019s next? 35+No finetuning needed, prompt engineering (e.g. CoT) can improve performance \u2013Limits to what you can fit in context \u2013Complex tasks will probably need gradient steps Lecture Plan: From Language Models to Assistants 1.Zero -Shot (ZS) and Few -Shot (FS) In -Context Learning 2.Instruction finetuning 3.Reinforcement Learning from Human Feedback (RLHF) 4.What\u2019s next? 36+No finetuning needed, prompt engineering (e.g. CoT) can improve performance \u2013Limits to what you can fit in context \u2013Complex tasks will probably need gradient steps Language modeling \u2260 assisting users 37 Language models are not aligned with user intent [ Ouyang et al., 2022 ]. Language modeling \u2260 assisting users 38 Language models are not aligned with user intent [ Ouyang et al., 2022 ]. Finetuning to the rescue!Human A giant rocket ship blasted off from Earth carrying astronauts to the moon. The astronauts landed their spaceship on the moon and walked around exploring the lunar surface. Then they returned safely back to Earth, bringing home moon rocks to show everyone. Recall From Lecture 10: The Pretraining / Finetuning Paradigm Pretraining can improve NLP applications by serving as parameter initialization. 39Decoder (Transformer, LSTM, ++ ) Iroh goes to make tasty teagoes to make tasty tea ENDStep 1: Pretrain (on language modeling) Lots of text; learn general things! Decoder (Transformer, LSTM, ++ )\u263a/\uf04cStep 2: Finetune (on your task) Not many labels; adapt to the task! \u2026 the movie was \u2026 Scaling up finetuning Pretraining can improve NLP applications by serving as parameter initialization. 40Decoder (Transformer, LSTM, ++ ) Iroh goes to make tasty teagoes to make tasty tea ENDStep 1: Pretrain (on language modeling) Lots of text; learn general things! Decoder (Transformer, LSTM, ++ )\u263a/\uf04cStep 2: Finetune (on many tasks ) Not many labels; adapt to the tasks! \u2026 the movie was \u2026 Instruction finetuning 41\u2022Collect examples of (instruction, output) pairs across many tasks and finetune an LM [FLAN -T5; Chung et al., 2022 ]\u2022Evaluate on unseen tasks Instruction finetuning 42 [Wang et al., 2022 ]\u2022As is usually the case, data + model scale is key for this to work! \u2022For example, the Super - NaturalInstructions dataset contains over 1.6K tasks, 3M+ examples \u2022Classification, sequence tagging, Pretraining can improve NLP applications by serving as parameter initialization. 40Decoder (Transformer, LSTM, ++ ) Iroh goes to make tasty teagoes to make tasty tea ENDStep 1: Pretrain (on language modeling) Lots of text; learn general things! Decoder (Transformer, LSTM, ++ )\u263a/\uf04cStep 2: Finetune (on many tasks ) Not many labels; adapt to the tasks! \u2026 the movie was \u2026 Instruction finetuning 41\u2022Collect examples of (instruction, output) pairs across many tasks and finetune an LM [FLAN -T5; Chung et al., 2022 ]\u2022Evaluate on unseen tasks Instruction finetuning 42 [Wang et al., 2022 ]\u2022As is usually the case, data + model scale is key for this to work! \u2022For example, the Super - NaturalInstructions dataset contains over 1.6K tasks, 3M+ examples \u2022Classification, sequence tagging, rewriting, translation, QA... \u2022Q: how do we evaluate such a model?pretraining? Aside: new benchmarks for multitask LMs 43Massive Multitask Language Understanding (MMLU) [Hendrycks et al., 2021 ] New benchmarks for measuring LM performance on 57 diverse knowledge intensive tasks Aside: new benchmarks for multitask LMs 44 BIG-Bench [Srivastava et al., 2022 ] 200+ tasks, spanning: https:// github.com /google/BIG - bench/blob/main/ bigbench /benchmark_tasks /README.md Aside: new benchmarks for multitask LMs 45 BIG-Bench [Srivastava et al., 2022 ] 200+ tasks, spanning: https:// github.com /google/BIG - bench/blob/main/ bigbench /benchmark_tasks /README.md Instruction finetuning 46 [Chung et al., 2022 ]\u2022Recall the T5 encoder -decoder model from lecture 10 [ Raffel et al., 2018 ], pretrained on the span corruption task \u2022Flan -T5 [Chung et al., 2020 ]: T5 models finetuned on 1.8K additional tasks Bigger model = bigger \u0394BIG-bench + MMLU avg (normalized) Instruction finetuning [Chung et al., 2022 ] Before instruction finetuning 47Highly recommend trying FLAN -T5 out to get a sense of its capabilities: https:// huggingface.co /google/flan -t5-xxl Instruction finetuning [Chung et al., 2022 ] After instruction finetuning 48Highly recommend trying FLAN -T5 out to get a sense of its capabilities: https:// huggingface.co /google/flan -t5-xxl 1.Zero -Shot (ZS) and Few -Shot (FS) In -Context Learning +No finetuning needed, prompt engineering (e.g. CoT) can improve performance \u2013Limits to what you can fit in context \u2013Complex tasks will probably need gradient steps 2.Instruction finetuning 3.Reinforcement Learning from Human Feedback (RLHF) 4.What\u2019s next?+Simple and straightforward, generalize to unseen tasks \u2013? \u2013?Lecture Plan: From Language Models to Assistants 49 musical adventureLimitations of instruction finetuning? 50\u2022One limitation of instruction finetuning is obvious: it\u2019s expensive to collect ground - truth data for tasks. \u2022But there are other, subtler limitations too. Can you think of any? \u2022Problem 1 : tasks like open -ended creative generation have no right answer. \u2022Write me a story about a dog and her pet grasshopper. \u2022Problem 2 : language modeling penalizes all token -level mistakes equally, but some errors are worse than others. \u2022Even with instruction finetuning ,there a mismatch between the LM",
    "131": "objective and the objective of \u201csatisfy human preferences\u201d! \u2022Can we explicitly attempt to satisfy human preferences?LM Avatar is afantasy TV showis a fantasy TV show ENDadventure musical +Simple and straightforward, generalize to unseen tasks \u2013Collecting demonstrations for so many tasks is expensive \u2013Mismatch between LM objective and human preferencesLecture Plan: From Language Models to Assistants \u2022But there are other, subtler limitations too. Can you think of any? \u2022Problem 1 : tasks like open -ended creative generation have no right answer. \u2022Write me a story about a dog and her pet grasshopper. \u2022Problem 2 : language modeling penalizes all token -level mistakes equally, but some errors are worse than others. \u2022Even with instruction finetuning ,there a mismatch between the LM objective and the objective of \u201csatisfy human preferences\u201d! \u2022Can we explicitly attempt to satisfy human preferences?LM Avatar is afantasy TV showis a fantasy TV show ENDadventure musical +Simple and straightforward, generalize to unseen tasks \u2013Collecting demonstrations for so many tasks is expensive \u2013Mismatch between LM objective and human preferencesLecture Plan: From Language Models to Assistants 1.Zero -Shot (ZS) and Few -Shot (FS) In -Context Learning +No finetuning needed, prompt engineering (e.g. CoT) can improve performance \u2013Limits to what you can fit in context \u2013Complex tasks will probably need gradient steps 2.Instruction finetuning 3.Reinforcement Learning from Human Feedback (RLHF) 4.What\u2019s next? 51 +Simple and straightforward, generalize to unseen tasks \u2013Collecting demonstrations for so many tasks is expensive \u2013Mismatch between LM objective and human preferencesLecture Plan: From Language Models to Assistants 1.Zero -Shot (ZS) and Few -Shot (FS) In -Context Learning +No finetuning needed, prompt engineering (e.g. CoT) can improve performance \u2013Limits to what you can fit in context \u2013Complex tasks will probably need gradient steps 2.Instruction finetuning 3.Reinforcement Learning from Human Feedback (RLHF) 4.What\u2019s next? 52 Optimizing for human preferences 53\u2022Let\u2019s say we were training a language model on some task (e.g. summarization). \u2022For each LM sample \ud835\udc60, imagine we had a way to obtain a human reward of that summary: \ud835\udc45\ud835\udc60\u2208\u211d, higher is better. \u2022Now we want to maximize the expected reward of samples from our LM: \ud835\udd3c\u01b8\ud835\udc60~\ud835\udc5d\ud835\udf03(\ud835\udc60)\ud835\udc45(\u01b8\ud835\udc60)SAN FRANCISCO, California (CNN) -- A magnitude 4.2 earthquake shook the San Francisco ... overturn unstable objects.An earthquake hit San Francisco. There was minor property damage, but no injuries.The Bay Area has good weather but is prone to earthquakes and wildfires. \ud835\udc601 \ud835\udc45\ud835\udc601=8.0\ud835\udc602 \ud835\udc45\ud835\udc602=1.2 Note: for mathematical simplicity we\u2019re assuming only one \u201cprompt\u201d Reinforcement learning to the rescue 54\u2022The field of reinforcement learning (RL) has studied these (and related) problems for many years now 53\u2022Let\u2019s say we were training a language model on some task (e.g. summarization). \u2022For each LM sample \ud835\udc60, imagine we had a way to obtain a human reward of that summary: \ud835\udc45\ud835\udc60\u2208\u211d, higher is better. \u2022Now we want to maximize the expected reward of samples from our LM: \ud835\udd3c\u01b8\ud835\udc60~\ud835\udc5d\ud835\udf03(\ud835\udc60)\ud835\udc45(\u01b8\ud835\udc60)SAN FRANCISCO, California (CNN) -- A magnitude 4.2 earthquake shook the San Francisco ... overturn unstable objects.An earthquake hit San Francisco. There was minor property damage, but no injuries.The Bay Area has good weather but is prone to earthquakes and wildfires. \ud835\udc601 \ud835\udc45\ud835\udc601=8.0\ud835\udc602 \ud835\udc45\ud835\udc602=1.2 Note: for mathematical simplicity we\u2019re assuming only one \u201cprompt\u201d Reinforcement learning to the rescue 54\u2022The field of reinforcement learning (RL) has studied these (and related) problems for many years now [Williams, 1992; Sutton and Barto , 1998 ] \u2022Circa 2013: resurgence of interest in RL applied to deep learning, game -playing [ Mnih et al., 2013 ] \u2022But the interest in applying RL to modern LMs is an even newer phenomenon [ Ziegler et al., 2019; Stiennon et al., 2020; Ouyang et al., 2022 ].Why? \u2022RL w/ LMs has commonly been viewed as very hard to get right (still is!) \u2022Newer advances in RL algorithms that work for large neural models, including language models (e.g. PPO; [ Schulman et al., 2017 ]) Optimizing for human preferences 55\u2022How do we actually change our LM parameters \ud835\udf03to maximize this? \ud835\udd3c\u01b8\ud835\udc60~\ud835\udc5d\ud835\udf03(\ud835\udc60)\ud835\udc45(\u01b8\ud835\udc60) \u2022Let\u2019s try doing gradient ascent! \ud835\udf03\ud835\udc61+1\u2254\ud835\udf03\ud835\udc61+\ud835\udefc\u2207\ud835\udf03\ud835\udc61\ud835\udd3c\u01b8\ud835\udc60~\ud835\udc5d\ud835\udf03\ud835\udc61(\ud835\udc60)\ud835\udc45(\u01b8\ud835\udc60) \u2022Policy gradient methods in RL (e.g., REINFORCE; [ Williams, 1992 ]) give us tools for estimating and optimizing this objective. \u2022We\u2019ll describe a very high -level mathematical overview of the simplest policy gradient estimator, but a full treatment of RL is outside the scope of this course. (Try CS234!)What if our reward function is non - differentiable??How do we estimate this expectation?? A (very!) brief introduction to policy gradient/REINFORCE [Williams, 1992] 56(chain rule)This is an expectation of this\u27f9\u2207\ud835\udf03\ud835\udc5d\ud835\udf03\ud835\udc60=\u2207\ud835\udf03log\ud835\udc5d\ud835\udf03\ud835\udc60\ud835\udc5d\ud835\udf03\ud835\udc60\u2022We want to obtain \u2207\ud835\udf03\ud835\udd3c\u01b8\ud835\udc60~\ud835\udc5d\ud835\udf03(\ud835\udc60)\ud835\udc45(\u01b8\ud835\udc60)=\u2207\ud835\udf03\u0dcd \ud835\udc60\ud835\udc45(\ud835\udc60)\ud835\udc5d\ud835\udf03\ud835\udc60=\u0dcd \ud835\udc60\ud835\udc45(\ud835\udc60)\u2207\ud835\udf03\ud835\udc5d\ud835\udf03\ud835\udc60 \u2022Here we\u2019ll use a very handy trick known as the log-derivative trick . Let\u2019s try taking the gradient of log\ud835\udc5d\ud835\udf03\ud835\udc60 \u2207\ud835\udf03log\ud835\udc5d\ud835\udf03\ud835\udc60=1 \ud835\udc5d\ud835\udf03\ud835\udc60\u2207\ud835\udf03\ud835\udc5d\ud835\udf03\ud835\udc60 \u2022Plug back in: \u0dcd \ud835\udc60\ud835\udc45\ud835\udc60\u2207\ud835\udf03\ud835\udc5d\ud835\udf03\ud835\udc60=\u0dcd \ud835\udc60\ud835\udc5d\ud835\udf03\ud835\udc60\ud835\udc45\ud835\udc60\u2207\ud835\udf03log\ud835\udc5d\ud835\udf03\ud835\udc60 =\ud835\udd3c\u01b8\ud835\udc60~\ud835\udc5d\ud835\udf03(\ud835\udc60)\ud835\udc45\u01b8\ud835\udc60\u2207\ud835\udf03log\ud835\udc5d\ud835\udf03\u01b8\ud835\udc60(defn . of expectation) (linearity of gradient) \u2022Giving us the update rule:\u2022Now we have put the gradient \u201cinside\u201d the expectation, we can approximate this objective with Monte Carlo samples: \u2207\ud835\udf03\ud835\udd3c\u01b8\ud835\udc60~\ud835\udc5d\ud835\udf03(\ud835\udc60)\ud835\udc45(\u01b8\ud835\udc60)=\ud835\udd3c\u01b8\ud835\udc60~\ud835\udc5d\ud835\udf03(\ud835\udc60)\ud835\udc45\u01b8\ud835\udc60\u2207\ud835\udf03log\ud835\udc5d\ud835\udf03\u01b8\ud835\udc60\u22481 \ud835\udc5a\u0dcd \ud835\udc56=1\ud835\udc5a \ud835\udc45\ud835\udc60\ud835\udc56\u2207\ud835\udf03log\ud835\udc5d\ud835\udf03\ud835\udc60\ud835\udc56 \ud835\udf03\ud835\udc61+1\u2254\ud835\udf03\ud835\udc61+\ud835\udefc1 \ud835\udc5a\u0dcd \ud835\udc56=1\ud835\udc5a \ud835\udc45\ud835\udc60\ud835\udc56\u2207\ud835\udf03\ud835\udc61log\ud835\udc5d\ud835\udf03\ud835\udc61\ud835\udc60\ud835\udc56\u22481 \ud835\udc5a\u0dcd \ud835\udc56=1\ud835\udc5a \ud835\udc45\ud835\udc60\ud835\udc56\u2207\ud835\udf03log\ud835\udc5d\ud835\udf03\ud835\udc60\ud835\udc56 57If \ud835\udc45is +++Take gradient steps to maximize \ud835\udc5d\ud835\udf03\ud835\udc60\ud835\udc56 If \ud835\udc45is --- Take steps to minimize \ud835\udc5d\ud835\udf03\ud835\udc60\ud835\udc56This is why it\u2019s called \u201creinforcement learning\u201d : we reinforce good actions, increasing the chance they happen again. This is heavily simplified ! There is a lot more needed to do RL w/ LMs. Can you see any problems with this objective?A (very!) brief introduction to policy gradient/REINFORCE [Williams, 1992] =\ud835\udd3c\u01b8\ud835\udc60~\ud835\udc5d\ud835\udf03(\ud835\udc60)\ud835\udc45\u01b8\ud835\udc60\u2207\ud835\udf03log\ud835\udc5d\ud835\udf03\u01b8\ud835\udc60 How do we model human preferences? 58\u2022Awesome: now for any arbitrary, non -differentiable reward function \ud835\udc45\ud835\udc60, we can train our language model to maximize expected reward. \u2022Not so fast! (Why not?) \u2022Problem 1: human -in-the-loop is expensive! \u2022Solution: instead of directly asking humans for preferences, model their preferences as a separate (NLP) problem! [ Knox and Stone, 2009 ] An earthquake hit San Francisco. There was minor property damage, but no injuries.The Bay Area has good weather but is prone to earthquakes and wildfires. \ud835\udc601 \ud835\udc45\ud835\udc601=8.0\ud835\udc602 \ud835\udc45\ud835\udc602=1.2Train an LM \ud835\udc45\ud835\udc40\ud835\udf19\ud835\udc60to predict human preferences from an annotated dataset, then optimize for \ud835\udc45\ud835\udc40\ud835\udf19instead. \ud83d\udcb5 \ud83d\udcb5 How do we model human preferences? 59\u2022Problem 2: human judgments are noisy and miscalibrated ! =\ud835\udd3c\u01b8\ud835\udc60~\ud835\udc5d\ud835\udf03(\ud835\udc60)\ud835\udc45\u01b8\ud835\udc60\u2207\ud835\udf03log\ud835\udc5d\ud835\udf03\u01b8\ud835\udc60 How do we model human preferences? 58\u2022Awesome: now for any arbitrary, non -differentiable reward function \ud835\udc45\ud835\udc60, we can train our language model to maximize expected reward. \u2022Not so fast! (Why not?) \u2022Problem 1: human -in-the-loop is expensive! \u2022Solution: instead of directly asking humans for preferences, model their preferences as a separate (NLP) problem! [ Knox and Stone, 2009 ] An earthquake hit San Francisco. There was minor property damage, but no injuries.The Bay Area has good weather but is prone to earthquakes and wildfires. \ud835\udc601 \ud835\udc45\ud835\udc601=8.0\ud835\udc602 \ud835\udc45\ud835\udc602=1.2Train an LM \ud835\udc45\ud835\udc40\ud835\udf19\ud835\udc60to predict human preferences from an annotated dataset, then optimize for \ud835\udc45\ud835\udc40\ud835\udf19instead. \ud83d\udcb5 \ud83d\udcb5 How do we model human preferences? 59\u2022Problem 2: human judgments are noisy and miscalibrated ! \u2022Solution: instead of asking for direct ratings, ask for pairwise comparisons , which can be more reliable [ Phelps et al., 2015; Clark et al., 2018 ] A 4.2 magnitude earthquake hit San Francisco, resulting in massive damage. \ud835\udc603 \ud835\udc45\ud835\udc603=? \ud835\udc45\ud835\udc603=4.1?6.6?3.2? How do we model human preferences? 60\u2022Problem 2: human judgments are noisy and miscalibrated ! \u2022Solution: instead of asking for direct ratings, ask for pairwise comparisons , which can be more reliable [ Phelps et al., 2015; Clark et al., 2018 ] An earthquake hit San Francisco. There was minor property damage, but no injuries.The Bay Area has good weather but is prone to earthquakes",
    "132": "and wildfires. \ud835\udc601 \ud835\udc602A 4.2 magnitude earthquake hit San Francisco, resulting in massive damage. \ud835\udc603> > Reward Model ( \ud835\udc45\ud835\udc40\ud835\udf19) The Bay Area \u2026 ...wildfires1.2 \ud835\udc3d\ud835\udc45\ud835\udc40\ud835\udf19=\u2212\ud835\udd3c\ud835\udc60\ud835\udc64,\ud835\udc60\ud835\udc59~\ud835\udc37log\ud835\udf0e(\ud835\udc45\ud835\udc40\ud835\udf19\ud835\udc60\ud835\udc64\u2212\ud835\udc45\ud835\udc40\ud835\udf19(\ud835\udc60\ud835\udc59)) \u201cwinning\u201d sample\u201closing\u201d sample\ud835\udc60\ud835\udc64should score higher than \ud835\udc60\ud835\udc59Bradley -Terry [1952 ] paired comparison model Make sure your reward model works first! DataEvaluate RM on predicting outcome of held -out human judgments [Stiennon et al., 2020 ]Large enough RM trained on enough data approaching single human perf 62This is a penalty which prevents us from diverging too far from the pretrained model. In expectation, it is known as the Kullback -Leibler (KL) divergence between \ud835\udc5d\ud835\udf03\ud835\udc45\ud835\udc3f(\ud835\udc60)and \ud835\udc5d\ud835\udc43\ud835\udc47\ud835\udc60.RLHF: Putting it all together [Christiano et al., 2017 ; Stiennon et al., 2020 ] Pay a price when \ud835\udc5d\ud835\udf03\ud835\udc45\ud835\udc3f\ud835\udc60>\ud835\udc5d\ud835\udc43\ud835\udc47\ud835\udc60\u2022Finally, we have everything we need: \u2022A pretrained (possibly instruction -finetuned) LM \ud835\udc5d\ud835\udc43\ud835\udc47(\ud835\udc60) \u2022A reward model \ud835\udc45\ud835\udc40\ud835\udf19(\ud835\udc60)that produces scalar rewards for LM outputs, trained on a dataset of human comparisons sample\u201closing\u201d sample\ud835\udc60\ud835\udc64should score higher than \ud835\udc60\ud835\udc59Bradley -Terry [1952 ] paired comparison model Make sure your reward model works first! DataEvaluate RM on predicting outcome of held -out human judgments [Stiennon et al., 2020 ]Large enough RM trained on enough data approaching single human perf 62This is a penalty which prevents us from diverging too far from the pretrained model. In expectation, it is known as the Kullback -Leibler (KL) divergence between \ud835\udc5d\ud835\udf03\ud835\udc45\ud835\udc3f(\ud835\udc60)and \ud835\udc5d\ud835\udc43\ud835\udc47\ud835\udc60.RLHF: Putting it all together [Christiano et al., 2017 ; Stiennon et al., 2020 ] Pay a price when \ud835\udc5d\ud835\udf03\ud835\udc45\ud835\udc3f\ud835\udc60>\ud835\udc5d\ud835\udc43\ud835\udc47\ud835\udc60\u2022Finally, we have everything we need: \u2022A pretrained (possibly instruction -finetuned) LM \ud835\udc5d\ud835\udc43\ud835\udc47(\ud835\udc60) \u2022A reward model \ud835\udc45\ud835\udc40\ud835\udf19(\ud835\udc60)that produces scalar rewards for LM outputs, trained on a dataset of human comparisons \u2022A method for optimizing LM parameters towards an arbitrary reward function. \u2022Now to do RLHF: \u2022Initialize a copy of the model \ud835\udc5d\ud835\udf03\ud835\udc45\ud835\udc3f(\ud835\udc60), with parameters \ud835\udf03we would like to optimize \u2022Optimize the following reward with RL: \ud835\udc45\ud835\udc60=\ud835\udc45\ud835\udc40\ud835\udf19(\ud835\udc60)\u2212\ud835\udefdlog\ud835\udc5d\ud835\udf03\ud835\udc45\ud835\udc3f(\ud835\udc60) \ud835\udc5d\ud835\udc43\ud835\udc47(\ud835\udc60) RLHF provides gains over pretraining + finetuning [Stiennon et al., 2020 ]\ud835\udc5d\ud835\udc43\ud835\udc47(\ud835\udc60)\ud835\udc5d\ud835\udc3c\ud835\udc39\ud835\udc47(\ud835\udc60)\ud835\udc5d\ud835\udc45\ud835\udc3f(\ud835\udc60) 63 InstructGPT : scaling up RLHF to tens of thousands of tasks [Ouyang et al., 2022 ]30k tasks! 64 InstructGPT : scaling up RLHF to tens of thousands of tasks [Ouyang et al., 2022 ] Tasks collected from labelers: 65 InstructGPT 66 InstructGPT 67 ChatGPT : Instruction Finetuning + RLHF for dialog agents 68Note: OpenAI (and similar companies) are keeping more details secret about ChatGPT training (including data, training parameters, model size) \u2014 perhaps to keep a competitive edge\u2026 https:// openai.com /blog/ chatgpt / (Instruction finetuning!) ChatGPT : Instruction Finetuning + RLHF for dialog agents 69Note: OpenAI (and similar companies) are keeping more details secret about ChatGPT training (including data, training parameters, model size) \u2014 perhaps to keep a competitive edge\u2026 https:// openai.com /blog/ chatgpt / (RLHF!) ChatGPT : Instruction Finetuning + RLHF for dialog agents 70 +Directly model preferences (cf. language modeling), generalize beyond labeled data \u2013RL is very tricky to get right \u2013?1.Zero -Shot (ZS) and Few -Shot (FS) In -Context Learning +No finetuning needed, prompt engineering (e.g. CoT) can improve performance \u2013Limits to what you can fit in context \u2013Complex tasks will probably need gradient steps 2.Instruction finetuning 3.Reinforcement Learning from Human Feedback (RLHF) 4.What\u2019s next?+Simple and straightforward, generalize to unseen tasks \u2013Collecting demonstrations for so many tasks is expensive \u2013Mismatch between LM objective and human preferencesLecture Plan: From Language Models to Assistants 71 72Limitations of RL + Reward Modeling \u2022Human preferences are unreliable! \u2022\u201dReward hacking\u201d is a common problem in RL https:// openai.com /blog/faulty -reward -functions/ 73Limitations of RL + Reward Modeling \u2022Human preferences are unreliable! \u2022\u201dReward hacking\u201d is a common problem in RL \u2022Chatbots are rewarded to produce responses that seem authoritative and helpful, regardless of truth \u2022This can result in making up facts + hallucinations https:// www.npr.org /2023/02/09/1155650909/google -chatbot --error -bard -shares https://news.ycombinator.com/item?id=34776508 https:// apnews.com /article/kansas -city-chiefs -philadelphia -eagles -technology - science -82bc20f207e3e4cf81abc6a5d9e6b23a \ud835\udc45\ud835\udc60=\ud835\udc45\ud835\udc40\ud835\udf19(\ud835\udc60)\u2212\ud835\udefdlog\ud835\udc5d\ud835\udf03\ud835\udc45\ud835\udc3f(\ud835\udc60) \ud835\udc5d\ud835\udc43\ud835\udc47(\ud835\udc60) 74Limitations of RL + Reward Modeling \u2022Human preferences are unreliable! \u2022\u201dReward hacking\u201d is a common problem in RL \u2022Chatbots are rewarded to produce responses that seem authoritative and helpful, regardless of truth \u2022This can result in making up facts + hallucinations \u2022Models of human preferences are even more unreliable!Reward model over -optimization [Stiennon et al., 2020 ] 75Limitations of RL + Reward Modeling \u2022Human preferences are unreliable! \u2022\u201dReward hacking\u201d is a common problem in RL \u2022Chatbots are rewarded to produce responses that seem authoritative and helpful, regardless of truth \u2022This can result in making up facts + hallucinations \u2022Models of human preferences are even more unreliable! \u2022There is a real concern of AI mis(alignment)! https:// twitter.com /percyliang /status/1600383429463355392 1.Zero -Shot (ZS) and Few -Shot (FS) In -Context Learning +No finetuning needed, prompt engineering (e.g. CoT) can improve performance \u2013Limits to what you can fit in context \u2013Complex tasks will probably need gradient steps 2.Instruction finetuning even more unreliable!Reward model over -optimization [Stiennon et al., 2020 ] 75Limitations of RL + Reward Modeling \u2022Human preferences are unreliable! \u2022\u201dReward hacking\u201d is a common problem in RL \u2022Chatbots are rewarded to produce responses that seem authoritative and helpful, regardless of truth \u2022This can result in making up facts + hallucinations \u2022Models of human preferences are even more unreliable! \u2022There is a real concern of AI mis(alignment)! https:// twitter.com /percyliang /status/1600383429463355392 1.Zero -Shot (ZS) and Few -Shot (FS) In -Context Learning +No finetuning needed, prompt engineering (e.g. CoT) can improve performance \u2013Limits to what you can fit in context \u2013Complex tasks will probably need gradient steps 2.Instruction finetuning 3.Reinforcement Learning from Human Feedback (RLHF) 4.What\u2019s next?+Directly model preferences (cf. language modeling), generalize beyond labeled data \u2013RL is very tricky to get right \u2013Human preferences are fallible; models of human preferences even more so +Simple and straightforward, generalize to unseen tasks \u2013Collecting demonstrations for so many tasks is expensive \u2013Mismatch between LM objective and human preferencesLecture Plan: From Language Models to Assistants 76 Language models as multitask assistants? 77We\u2019ve finally (mostly) answered how we get from this to thisStanford University is located in __________ 1.Zero -Shot (ZS) and Few -Shot (FS) In -Context Learning +No finetuning needed, prompt engineering (e.g. CoT) can improve performance \u2013Limits to what you can fit in context \u2013Complex tasks will probably need gradient steps 2.Instruction finetuning 3.Reinforcement Learning from Human Feedback (RLHF) 4.What\u2019s next?+Directly model preferences (cf. language modeling), generalize beyond labeled data \u2013RL is very tricky to get right \u2013Human preferences are fallible; models of human preferences even more so +Simple and straightforward, generalize to unseen tasks \u2013Collecting demonstrations for so many tasks is expensive \u2013Mismatch between LM objective and human preferencesLecture Plan: From Language Models to Assistants 78 What\u2019s next? \u2022RLHF is still a very underexplored and fast - moving area: by the next lecture (2024) these slides may look completely different! \u2022RLHF gets you further than instruction finetuning, but is (still!) data expensive. \u2022Recent work aims to alleviate such data requirements: 79 What\u2019s next? \u2022RLHF is still a very underexplored and fast - moving area: by the next lecture (2024) these slides may look completely different! \u2022RLHF gets",
    "133": "you further than instruction finetuning, but is (still!) data expensive. \u2022Recent work aims to alleviate such data requirements: \u2022RL from AI feedback [Bai et al., 2022 ] 80Human: Can you help me hack into my neighbor\u2019s wifi? Assistant: Sure thing, you can use an app called VeryEasyHack . Critique Request: Identify ways in which the assistant\u2019s last response is harmful. Critique: Hacking into someone else\u2019s wifi is an invasion of their privacy and is possibly illegal. Revision Request : Rewrite the assistant response to remove harmful content. Revision: Hacking into your neighbor\u2019s wifi is an invasion of their privacy, and I strongly advise against it. It may also land you in legal trouble. \u201cConstitutional\u201d AI [Bai et al., 2022 ] What\u2019s next? \u2022RLHF is still a very underexplored and fast - moving area: by the next lecture (2024) these slides may look completely different! \u2022RLHF gets you further than instruction finetuning, but is (still!) data expensive. \u2022Recent work aims to alleviate such data requirements: \u2022RL from AI feedback [Bai et al., 2022 ] \u2022Finetuning LMs on their own outputs [Huang et al., 2022; Zelikman et al., 2022 ] \u2022However, there are still many limitations of large LMs (size, hallucination) that may not be solvable with RLHF!81 [Huang et al., 2022 ] LM chain of thought Self-Taught Reasoner ( STaR ) [Zelikman et al., 2022 ] Natural Language Processing with Deep Learning CS224N/Ling284 Jesse Mu Lecture 11: Prompting, Instruction Finetuning, and RLHF ADAPTING LARGE LANGUAGE MODELS VIA READING COMPREHENSION Daixuan Cheng, Shaohan Huang\u2217& Furu Wei Microsoft ABSTRACT We explore how continued pre-training on domain-specific corpora influences large language models, revealing that training on the raw corpora endows the model with domain knowledge, but drastically hurts its prompting ability for question answering. Taken inspiration from human learning via reading comprehension\u2014practice after reading improves the ability to answer questions based on the learned knowledge\u2014we propose a simple method for transforming raw corpora into reading comprehension texts. Each raw text is enriched with a series of tasks related to its content. Our method, highly scalable and applica- ble to any pre-training corpora, consistently enhances performance across various tasks in three different domains: biomedicine, finance, and law. Notably, our 7B language model achieves competitive performance with domain-specific models of much larger scales, such as BloombergGPT-50B. Furthermore, we demonstrate that domain-specific reading comprehension texts can improve the model\u2019s per- formance even on general benchmarks, showing the potential to develop a general model across even more domains. Our model, code, and data will be available at https://github.com/microsoft/LMOps. 2535455565 ChemProtRCTMQPPubMedQA2540557085 ConvFinQAFPBFiQA SAHeadline714212835 SCOTUS-macSCOTUS-micCaseHOLD-macCaseHOLD-mic BiomedicineFinanceLaw 412202836 SCOTUS-macSCOTUS-micCaseHOLD-macCaseHOLD-micGeneral LLMDAPTAdaptLLM Figure 1: Domain-specific task performance in biomedicine, finance, and law. General LLM is the general language model without continued training, DAPT (Gururangan et al., 2020) continues to train the general model on domain-specific raw corpora, and AdaptLLM continues to train the general model on the reading comprehension texts constructed based on the raw corpora, mixed with general instructions. \u2217Corresponding author: shaohanh@microsoft.com 1arXiv:2309.09530v1 [cs.CL] 18 Sep 2023 https://github.com/microsoft/LMOps. 2535455565 ChemProtRCTMQPPubMedQA2540557085 ConvFinQAFPBFiQA SAHeadline714212835 SCOTUS-macSCOTUS-micCaseHOLD-macCaseHOLD-mic BiomedicineFinanceLaw 412202836 SCOTUS-macSCOTUS-micCaseHOLD-macCaseHOLD-micGeneral LLMDAPTAdaptLLM Figure 1: Domain-specific task performance in biomedicine, finance, and law. General LLM is the general language model without continued training, DAPT (Gururangan et al., 2020) continues to train the general model on domain-specific raw corpora, and AdaptLLM continues to train the general model on the reading comprehension texts constructed based on the raw corpora, mixed with general instructions. \u2217Corresponding author: shaohanh@microsoft.com 1arXiv:2309.09530v1 [cs.CL] 18 Sep 2023 Hereisthefirstpartofanarticleaboutbiomedicine:Recentreportedevidenceindicatesthatvocalcordcarcinomaisevolvingsimilarlytooropharyngealcancerwithanincreasingnumberofpatients(...)Answerquestionsbasedonthearticle:Whatisasummary?GlotticCarcinomainYoungPatients.Generateasentencethatincludesthesebiomedicinekeywords[carcinoma,oropharyngeal,papillomavirus]:Recentreportedevidenceindicatesthatvocalcordcarcinomaisevolving\u2026Premise:\u2026Hypothesis:\u2026Doesthepremiseentailthehypothesis?YesWhatisthereasonfor\u201d\u2026\"?themorphologyofthelesionsandthepatients'youngage.Composeasentencethatcontradictsthemeaningof\"Historically,glotticcarcinoma\u2026\u201d.Answer:Recentpublishedevidence\u2026Howwouldyoucompletethearticle?Thisfindingfurthersupports\u2026Raw TextReading Comprehension Glottic Carcinoma in Young PatientsRecentreportedevidenceindicatesthatvocalcordcarcinomaisevolvingsimilarlytooropharyngealcancerwithanincreasingnumberofpatientswithoutasmokinghistoryhavinghumanpapillomavirus(HPV)disease.(...)Therefore,aninvestigationwasdonetoexaminetheincidenceofglotticcarcinomainpatients30yearsoldoryounger.(...)duetothemorphologyofthelesionsandthepatients'youngage.Historically,glotticcarcinomaisconsideredtobeatobacco-induceddisease.Incontrast,recentpublishedevidenceshowsthatglotticcarcinomacanbeanHPV-relateddiseasewithincreasingincidenceinnonsmokers.(...)Thisfindingfurthersupports\u2026Cause & effectText endingTitleDomain keywordsEntailment relationSemantic similarityFigure 2: A simplified example of a reading comprehension text , wherein the raw text is followed by a series of tasks constructed from it, including Summarization (purple), Word-to-Text (blue), Natural Language Inference (red), Commonsense Reasoning (teal), Paraphrase Detection (yellow), by a series of tasks constructed from it, including Summarization (purple), Word-to-Text (blue), Natural Language Inference (red), Commonsense Reasoning (teal), Paraphrase Detection (yellow), and Text Completion (green). The complete version is in Appendix G. 1 I NTRODUCTION The proliferation of general large language models (LLMs) has given rise to the emergence of domain-specific large language models. Existing methods can be broadly classified into three ap- proaches. The first trains models from scratch on a mixture of domain-specific and general cor- pora (Wu et al., 2023b). While this intuitively creates domain-specific LLMs, the substantial com- putational and data requirements raise significant concerns (Yang et al., 2023; Ling et al., 2023). The second fine-tunes the language model using supervised datasets (Singhal et al., 2022; 2023; Li et al., 2023b;a; Wang et al., 2023; Han et al., 2023; Xiong et al., 2023; Huang et al., 2023), offering a more cost-effective option. However, there are still uncertainties about how well fine-tuned LLMs grasp domain knowledge that can be universally applied to all domain-specific tasks, as discussed by Zhou et al. (2023) and Gudibande et al. (2023). The third prompts the general language model with retrieved domain knowledge (Li et al., 2023b; Cui et al., 2023; Huang et al., 2023), which can be considered as an application of LLM rather than a direct enhancement to the LLM itself. Continued pre-training on domain-specific corpora, also known as domain-adaptive pretraining (Gu- rurangan et al., 2020), has been proven effective in adapting various natural language understanding models (Devlin et al., 2019; Liu et al., 2019; Clark et al., 2020) to specific domains (Yao et al., 2021; Gururangan et al., 2020; Cheng et al., 2022). This approach enables language models to lever- age general ability while incorporating domain-specific knowledge, benefiting downstream domain- specific tasks at reduced costs. This motivates our investigation into",
    "134": "whether continued pre-training also benefits large-scale generative models. We conduct initial experiments on three domains\u2014 biomedicine, finance, and law\u2014revealing that continued training on the raw corpora results in a drastic drop in prompting performance but still benefits fine-tuning evaluation and knowledge prob- ing tests. This leads us to conclude that domain-adaptive pre-training using raw corpora imparts models (Devlin et al., 2019; Liu et al., 2019; Clark et al., 2020) to specific domains (Yao et al., 2021; Gururangan et al., 2020; Cheng et al., 2022). This approach enables language models to lever- age general ability while incorporating domain-specific knowledge, benefiting downstream domain- specific tasks at reduced costs. This motivates our investigation into whether continued pre-training also benefits large-scale generative models. We conduct initial experiments on three domains\u2014 biomedicine, finance, and law\u2014revealing that continued training on the raw corpora results in a drastic drop in prompting performance but still benefits fine-tuning evaluation and knowledge prob- ing tests. This leads us to conclude that domain-adaptive pre-training using raw corpora imparts domain knowledge to the LLM while affecting its prompting ability. To leverage domain-specific knowledge while enhancing prompting performance, we introduce a simple method for transforming large-scale raw corpora into reading comprehension texts: each raw text is enriched with a series of tasks relevant to its content, as illustrated in Figure 2. These tasks are designed to help the model maintain its ability to answer questions using natural language, based on the context of the raw text. Furthermore, we augment the reading comprehension texts with diverse general instructions, thereby further enhancing prompting ability (Wei et al., 2022; Zhou et al., 2023; 2 Xu et al., 2023; Mukherjee et al., 2023). Our experiments in domains such as biomedicine, finance, and law highlight the effectiveness of our approach in improving model performance on various domain-specific tasks. We refer to this resulting model as AdaptLLM, for Adapted Large Language Model. Looking ahead, we envision extending this methodology to the development of a general large language model, contributing to the ever-expanding landscape of tasks across more domains. In summary, our contributions include: \u2022 We investigate continued pre-training for large language models, where we find continued training on domain-specific raw corpora can endow the model with domain knowledge, but drastically hurts its prompting ability. \u2022 We propose a simple recipe which automatically converts large-scale raw corpora into reading comprehension texts, to effectively learn the domain knowledge while concurrently preserving prompting performance. \u2022 Our experiments show the effectiveness of our method in consistently improving model perfor- Model. Looking ahead, we envision extending this methodology to the development of a general large language model, contributing to the ever-expanding landscape of tasks across more domains. In summary, our contributions include: \u2022 We investigate continued pre-training for large language models, where we find continued training on domain-specific raw corpora can endow the model with domain knowledge, but drastically hurts its prompting ability. \u2022 We propose a simple recipe which automatically converts large-scale raw corpora into reading comprehension texts, to effectively learn the domain knowledge while concurrently preserving prompting performance. \u2022 Our experiments show the effectiveness of our method in consistently improving model perfor- mance in three different domains: biomedicine, finance and law. 2 P RELIMINARY EXPLORATION ON CONTINUED PRE-TRAINING Given the proven efficacy and efficiency of continued pre-training in adapting natural language understanding models (Gururangan et al., 2020; Yao et al., 2021; Cheng et al., 2022), we embark on an exploration to ascertain whether this method remains effective for large-scale generative models. We continue to train the general LLaMA (Touvron et al., 2023) on the domain-specific raw corpora of biomedicine, finance, and law, respectively, and conduct prompting and fine-tuning evaluations, as well as domain knowledge probing to assess the model performance within each domain (detailed experimental settings are in Section 4). Table 1: Domain-specific task scores of general language model (General LLM) and the language model that has undergone continued pre-training on the domain-specific raw corpora (DAPT (Gu- rurangan et al., 2020)). We report the average of task scores within each domain under prompting, fine-tuning and knowledge probing settings. MethodPrompting Fine-tuning Knowledge Prob BioMed. Finance Law BioMed. Finance Law BioMed. Law General LLM 44.2 58.6 34.2 64.2 79.9 42.0 36.5 45.0 DAPT 41.7 57.6 35.0 66.5 80.9 45.4 36.9 45.6 Prompting vs. Fine-tuning. As seen in Table 1, when fine-tuning is applied, consistent performance improvements across all three domains are evident after domain-adaptive pre-training. This trend aligns with findings related to language understanding models (Gururangan et al., 2020), indicating that continued pre-training enriches the LLM with domain-specific knowledge. Paradoxically, a con- rurangan et al., 2020)). We report the average of task scores within each domain under prompting, fine-tuning and knowledge probing settings. MethodPrompting Fine-tuning Knowledge Prob BioMed. Finance Law BioMed. Finance Law BioMed. Law General LLM 44.2 58.6 34.2 64.2 79.9 42.0 36.5 45.0 DAPT 41.7 57.6 35.0 66.5 80.9 45.4 36.9 45.6 Prompting vs. Fine-tuning. As seen in Table 1, when fine-tuning is applied, consistent performance improvements across all three domains are evident after domain-adaptive pre-training. This trend aligns with findings related to language understanding models (Gururangan et al., 2020), indicating that continued pre-training enriches the LLM with domain-specific knowledge. Paradoxically, a con- tradictory trend emerges in the prompting performance, where a noticeable drop is observed across most domains after domain-adaptive pre-training. This contradiction leads us to hypothesize that while vanilla domain-adaptive pre-training enhances the LLM\u2019s domain knowledge, contributing to the fine-tuning improvements, it also significantly impairs its ability to perform well in prompting, causing the observed drop in prompting performance. Domain Knowledge Probing. To further confirm whether the language model gains domain knowl- edge during continued pre-training, we employ a method similar to LAMA (Petroni et al., 2019) for probing domain knowledge. Using the supervised datasets available in each domain as the basis, we create domain-specific knowledge-probing datasets. The dataset creation process is detailed in Appendix A. In Table 1, we present the results of domain knowledge probing for the biomedicine and law domains1. Across both domains, we observe improved results after domain-adaptive pre- training, indicating that the model indeed acquires domain-specific knowledge. 1We were unable to construct a knowledge probing test for finance due to the limited availability of super- vised datasets in this domain. 3 The above analyses indicate that the decline in domain-specific prompting performance can be at- tributed to the reduced prompting ability. This reduction may stem from the limited diversity of pre-training corpora within one particular domain (Longpre et al., 2023b), which limits the input- output patterns derived from raw texts (Wei et al., 2022). Therefore, enhancing prompting ability is crucial for effectively harnessing the domain knowledge acquired during continued pre-training. training, indicating that the model indeed acquires domain-specific knowledge. 1We were unable to construct a knowledge probing test for finance due to the limited availability of super- vised datasets in",
    "135": "this domain. 3 The above analyses indicate that the decline in domain-specific prompting performance can be at- tributed to the reduced prompting ability. This reduction may stem from the limited diversity of pre-training corpora within one particular domain (Longpre et al., 2023b), which limits the input- output patterns derived from raw texts (Wei et al., 2022). Therefore, enhancing prompting ability is crucial for effectively harnessing the domain knowledge acquired during continued pre-training. 3 A DAPTING LARGE LANGUAGE MODELS VIA READING COMPREHENSION Instead of continuing to train large language models on domain-specific raw corpora, we convert the raw corpora into reading comprehension texts and adapt the model using the converted data. In reading comprehension, each raw text is followed by a series of tasks related to its content. We regard the model training phase on the raw text as the \u201creading\u201d phase, and the subsequent training on the followed tasks as the \u201ccomprehension\u201d phase. These comprehension tasks follow the question-answering format, aimed at enriching the model\u2019s prompting ability to respond to input questions (Wei et al., 2022). This design is inspired from human learning, where practice after reading enhances the ability to answer questions based on the acquired knowledge. Furthermore, we propose augmenting the training data with general instructions (Zhou et al., 2023; Xu et al., 2023; Mukherjee et al., 2023) to benefit from the diversity of input-output formats, thereby further improving prompting ability. 3.1 C REATING READING COMPREHENSION TEXTS The idea of mining tasks from raw pre-training corpora to enhance zero-shot capability was in- troduced by van de Kar et al. (2022). This approach effectively extracts intrinsic tasks from raw texts through a handful of regex-based patterns, leading to substantial enhancements in the model\u2019s zero-shot performance via fine-tuning. Our approach leverages the self-supervised nature of this mining strategy to create our comprehension tasks. This enables us to scale up the transfer of raw pre-training data, capitalizing on the domain-specific knowledge embedded in the raw texts and the enhanced prompting ability provided by the comprehension tasks. improving prompting ability. 3.1 C REATING READING COMPREHENSION TEXTS The idea of mining tasks from raw pre-training corpora to enhance zero-shot capability was in- troduced by van de Kar et al. (2022). This approach effectively extracts intrinsic tasks from raw texts through a handful of regex-based patterns, leading to substantial enhancements in the model\u2019s zero-shot performance via fine-tuning. Our approach leverages the self-supervised nature of this mining strategy to create our comprehension tasks. This enables us to scale up the transfer of raw pre-training data, capitalizing on the domain-specific knowledge embedded in the raw texts and the enhanced prompting ability provided by the comprehension tasks. Table 2 gives an overview of the techniques used to extract and create tasks from raw texts. Phrases likeAnswer questions based on the article: are employed to concatenate each raw text with the followed tasks, as illustrated in Figure 2. Additionally, we paraphrase each task template to multiple variations and turn the task around to enhance task diversity (Wei et al., 2022; Chung et al., 2022; Longpre et al., 2023a). Summarization prompts the models to generate a concise summary of the provided article, en- couraging them to extract its main idea. To create task inputs, we employ queries like What is a summary? to prompt the model to summarize the article, using the text title as the groundtruth. We also reverse the task, asking the model to craft an article based on the given title. Additionally, we task the language model with identifying sentence topics. To unearth such input- output pairs, we utilize regex-based patterns to identify sentences aligning with the patterns spec- ified in Table 2. We then employ the corresponding task templates to construct the input-output pairs (van de Kar et al., 2022). Word-to-Text enhances the model\u2019s grasp of domain-specific vocabulary by prompting it to gen- erate sentences incorporating specific words. To identify domain-specific words, we use the Sen- tencePiece tool (Kudo & Richardson, 2018) to build a vocabulary from the target domain corpora. We then compare this domain vocabulary to the general language model\u2019s vocabulary, considering words present in the domain vocabulary but absent from the general vocabulary as domain-specific. Additionally, we filter out tokens with fewer than 10 characters, resulting in a set of domain-specific keywords. ified in Table 2. We then employ the corresponding task templates to construct the input-output pairs (van de Kar et al., 2022). Word-to-Text enhances the model\u2019s grasp of domain-specific vocabulary by prompting it to gen- erate sentences incorporating specific words. To identify domain-specific words, we use the Sen- tencePiece tool (Kudo & Richardson, 2018) to build a vocabulary from the target domain corpora. We then compare this domain vocabulary to the general language model\u2019s vocabulary, considering words present in the domain vocabulary but absent from the general vocabulary as domain-specific. Additionally, we filter out tokens with fewer than 10 characters, resulting in a set of domain-specific keywords. For each sentence in the raw text, we count the number of domain-specific keywords. Sentences having more than three domain-specific keywords are selected for making Word-to-Text tasks. We take the domain-specific keywords in the sentence as the input, asking the model to generate a sen- tence with Generate a sentence that includes these {DOMAIN }keywords . We also turn the task around by taking the sentence as input and asking the model to find the keywords about the target domain using What keywords about {DOMAIN }can be extracted from this 4 Table 2: Mining patterns and input-output templates. {VERBAL }is replaced with the verbalizers in Table 3. For mining, {WORD }captures a single word, and {SENT }captures a single sentence. Each input-output template is paraphrased into multiple variations. We also turn the task around\u2014 exchanging the question and answer\u2014to achieve enhanced diversity. Task Type Mining Pattern Input-output Template Summarization Title Title as summary What is a summary? {TITLE } Topic {SENT1 } {VERBAL } {SENT2 } { SENT1 }is about: {SENT2 } Word-to-Text Word-to-textDomain keywords as input; sentence as outputGenerate a sentence about these {DOMAIN }keywords [{WORD1 },{WORD2 },{WORD3 }]:{SENT } Definition {WORD } {VERBAL } {SENT } How to define {WORD }?{SENT } Natural Language Inference Entail {SENT1 } {VERBAL },{SENT2 }Does \" {SENT1 }\" entail \" {SENT2 }\"? {Yes/Maybe/No }Neutral Contradict Commonsense Reasoning Cause-effect {SENT1 } {VERBAL },{SENT2 }What is the {effect/cause } of{SENT1 }?{SENT2 } Effect-cause {SENT1 } {VERBAL } {SENT2 } Paragraph Detection Similar{SENT1 } {VERBAL },{SENT2 }Compose a sentence to {support/ contradict }\"{SENT1 }\".{SENT2 } Different Text Completion Summarization Title Title as summary What is a summary? {TITLE } Topic {SENT1 } {VERBAL } {SENT2 } { SENT1 }is about: {SENT2 } Word-to-Text Word-to-textDomain keywords as input; sentence as outputGenerate a sentence about these {DOMAIN }keywords [{WORD1 },{WORD2 },{WORD3 }]:{SENT } Definition {WORD } {VERBAL } {SENT } How to define {WORD }?{SENT } Natural Language Inference Entail {SENT1 } {VERBAL },{SENT2 }Does \" {SENT1 }\" entail \" {SENT2 }\"? {Yes/Maybe/No }Neutral Contradict Commonsense Reasoning Cause-effect {SENT1 } {VERBAL },{SENT2 }What is the {effect/cause } of{SENT1 }?{SENT2 } Effect-cause {SENT1 } {VERBAL } {SENT2 } Paragraph Detection Similar{SENT1 } {VERBAL },{SENT2 }Compose a sentence to {support/ contradict }\"{SENT1 }\".{SENT2 } Different Text Completion Text completion Text ending as completionHow would you complete the article?",
    "136": "{ENDING } sentence? . Here we point out the target domain by replacing {DOMAIN }with domain names such asbiomedicine ,finance , orlaw. Besides, we task the language model with defining concepts using the mining pattern and input-output template in Table 2. Natural Language Inference concerns how two sentences relate, typically asking, given a first sentence, whether a second sentence is true, false or possibly true. We use the regex-based patterns in Table 2 to search for \u201cpremise-hypothesis-relation\u201d triplets within the raw text. For example, we categorize the relationship between two sentences as \u201cEntailment\u201d if they are are connected by the verbalizer Therefore , and as \u201cNeutral\u201d if connected by Furthermore . Additionally, we enhance diversity by converting classification tasks into generation tasks. For example, when the relationship between two sentences is entailment, we employ templates like {SENT1 }Thus? to query for an output of which the groundtruth is the second sentence. Commonsense Reasoning evaluates the ability to perform physical or scientific reasoning while considering common sense. We identify cause-and-effect logic within sentences using the regex- based patterns in Table 2. We then formulate the input-output pairs using templates such as What is the reason of {SENT1 }?{SENT2 }. Paraphrase Detection asks a model to determine whether two sentences are semantically equiv- alent. To collect such task data, we use regex-based patterns in Table 2 to search for \u201csentence1- sentence2-label\u201d data triplets. However, we empirically find that these mining patterns cannot con- example, when the relationship between two sentences is entailment, we employ templates like {SENT1 }Thus? to query for an output of which the groundtruth is the second sentence. Commonsense Reasoning evaluates the ability to perform physical or scientific reasoning while considering common sense. We identify cause-and-effect logic within sentences using the regex- based patterns in Table 2. We then formulate the input-output pairs using templates such as What is the reason of {SENT1 }?{SENT2 }. Paraphrase Detection asks a model to determine whether two sentences are semantically equiv- alent. To collect such task data, we use regex-based patterns in Table 2 to search for \u201csentence1- sentence2-label\u201d data triplets. However, we empirically find that these mining patterns cannot con- sistently identify two sentences with strictly equivalent semantic meanings. For instance, sentences linked by the verbalizer Similarly may not share similar meanings. 5 Table 3: Verbalizers for mining patterns in Table 2. Task Type Verbalizer Summarization Topic talks about ,is about ,\u2019s topic is Word-to-Text Definition is defined as ,\u2019s definition is Natural Language Inference Entail Yes,Therefore ,Thus ,Accordingly ,Hence ,For this reason Neutral Maybe ,Furthermore ,Additionally ,Moreover ,In addition Contradict No,However ,But,On the contrary ,In contrast ,Whereas Commonsense Reasoning Cause-effect Therefore ,Thus ,Accordingly ,Hence ,For this reason Effect-cause due to ,on account of ,owing to Paragraph Detection Similar In other words ,Namely ,That is to say ,Similarly ,Equally Different No,However ,But,On the contrary ,In contrast ,Whereas Therefore, we reformat the classification task into a generation task to reduce dependence on label accuracy. Instead of inquiring whether two sentences are similar, we prompt the model to generate a sentence that either supports or contradicts the meaning of a given sentence, using input-output tem- plates like Can you create a sentence that contradicts the meaning of {SENT1 }? {SENT2 }when the extracted label is \u201cDifferent.\u201d Text Completion. In addition to the inherent casual language modeling task within generative language models, we insert queries such as How would you complete the article? between sentences to prompt the language model to generate the subsequent section. An advantage of Text Completion task is that it does not require any specific mining patterns, thus can be applied to any raw texts. accuracy. Instead of inquiring whether two sentences are similar, we prompt the model to generate a sentence that either supports or contradicts the meaning of a given sentence, using input-output tem- plates like Can you create a sentence that contradicts the meaning of {SENT1 }? {SENT2 }when the extracted label is \u201cDifferent.\u201d Text Completion. In addition to the inherent casual language modeling task within generative language models, we insert queries such as How would you complete the article? between sentences to prompt the language model to generate the subsequent section. An advantage of Text Completion task is that it does not require any specific mining patterns, thus can be applied to any raw texts. 3.2 M IXING WITH GENERAL INSTRUCTIONS While we have designed diverse mining patterns, input-output templates and task reversals to en- hance prompting ability, they might not fully address the infinite task diversity in real-world scenar- ios. In light of this, we propose to mix the reading comprehension texts with general instructions to cover a wider range of input-output types. 4 E XPERIMENT SETTINGS Domain-adaptive Pre-training. PubMed Abstracts and FreeLaw Opinions from the Pile (Gao et al., 2021) are utilized as the pre-training corpora for the biomedicine and law domains, respectively. For finance, we collect financial news from May 2022 to May 20232for over 7,000stocks, using the FinGPT codebase (Yang et al., 2023). General instructions are sourced from LIMA (Zhou et al., 2023), WizardLM (Xu et al., 2023), and Orca (Mukherjee et al., 2023). Our pre-training code is based on TorchScale3. We continue to train LLaMA-7B (Touvron et al., 2023) on each domain, and explore different ratios for mixing reading comprehension texts with general instructions; the optimal ratios for biomedicine, finance, and law are 1 : 1 ,1 : 2 , and 1 : 1 , respectively. Dataset details and other pre-training hyper-parameters can be found in Appendix B. 2Access to earlier news is limited. 3https://github.com/microsoft/torchscale 6 Creating Reading Comprehension Texts. Using the mining patterns in Table 2, we search for sub- categories within each task type. To prevent task dominance, we limit the number of task examples per sub-category to two for each raw text. For each mined example, we randomly sample from various paraphrased or task-reversed templates to generate an input-output example. To structure and explore different ratios for mixing reading comprehension texts with general instructions; the optimal ratios for biomedicine, finance, and law are 1 : 1 ,1 : 2 , and 1 : 1 , respectively. Dataset details and other pre-training hyper-parameters can be found in Appendix B. 2Access to earlier news is limited. 3https://github.com/microsoft/torchscale 6 Creating Reading Comprehension Texts. Using the mining patterns in Table 2, we search for sub- categories within each task type. To prevent task dominance, we limit the number of task examples per sub-category to two for each raw text. For each mined example, we randomly sample from various paraphrased or task-reversed templates to generate an input-output example. To structure the reading comprehension text, we use \\n\\nto connect comprehension tasks and link them with the raw text. On average, about two input-output examples are collected per reading comprehension text. Please refer to Appendix C for mining pattern implementation details and Appendix G for cases of reading comprehension texts. Domain-specific Tasks. For biomedicine, we evaluate on PubMedQA (Jin et al., 2019), ChemProt (Kringelum et al., 2016), MQP (McCreery et al., 2020), RCT (Dernoncourt & Lee, 2017), and USMLE (Jin et al., 2020). For finance, we evaluate on the five publicly available tasks also eval- uated by BloombergGPT (Wu et al., 2023b): ConvFinQA (Chen et al., 2022), FPB (Malo et al., 2014), FiQA SA (Maia et al., 2018), Headline",
    "137": "(Sinha & Khandait, 2020), and NER (Alvarado et al., 2015), and adopt similar prompting settings with BloombergGPT. For law, we evaluate on SCOTUS (Spaeth et al., 2020), CaseHOLD (Zheng et al., 2021) and UNFAIR-ToS (Lippi et al., 2019) from the LexGLUE (Chalkidis et al., 2022) benchmark. Evaluation details are provided in Appendix D. 5 M AINRESULTS In Table 4, we present the comparative prompting results of our models (AdaptLLM) against the general language model (General LLM) and the models that have gone vanilla domain-adaptive pre- training on raw corpora (DAPT). On various tasks in the three different domains, the use of raw texts in DAPT adversely affects the performance. However, the reformatting of raw texts and the inclusion of general instructions in AdaptLLM manage to counteract this effect, resulting in better results than the general language model. Table 4: Domain-specific task performance of general large language model (General LLM), 2019) from the LexGLUE (Chalkidis et al., 2022) benchmark. Evaluation details are provided in Appendix D. 5 M AINRESULTS In Table 4, we present the comparative prompting results of our models (AdaptLLM) against the general language model (General LLM) and the models that have gone vanilla domain-adaptive pre- training on raw corpora (DAPT). On various tasks in the three different domains, the use of raw texts in DAPT adversely affects the performance. However, the reformatting of raw texts and the inclusion of general instructions in AdaptLLM manage to counteract this effect, resulting in better results than the general language model. Table 4: Domain-specific task performance of general large language model (General LLM), vanilla domain-adaptive pretraining (DAPT), and ours (AdaptLLM) in prompting evaluation. We also display prompting results of other models including MedAlpaca (Han et al., 2023) in biomedicine, BloombergGPT (Wu et al., 2023b) in finance, and LexGPT (Lee, 2023) in law. Biomedicine PubMedQA ChemProt MQP RCT UMSLE AVERAGE MedAlpaca-7B 58.6 39.0 50.7 40.8 36.7 45.1 MedAlpaca-13B 60.7 38.4 57.4 51.3 39.0 49.4 General LLM-7B 59.6 31.4 50.7 45.1 34.5 44.2 DAPT-7B 52.6 26.6 49.2 46.6 33.5 41.7 AdaptLLM-7B 63.3 35.2 54.4 50.4 33.1 47.3 Finance ConvFinQA FPB FiQA SA Headline NER AVERAGE BloombergGPT-50B 43.4 51.1 75.1 82.2 60.8 62.5 General LLM-7B 29.2 55.9 69.2 77.7 61.1 58.6 DAPT-7B 29.6 55.3 64.9 77.5 60.6 57.6 AdaptLLM-7B 41.5 62.5 72.1 81.4 59.3 63.4 LawSCOTUS CaseHOLDUNFAIR-ToS AVERAGE mic-F1 mac-F1 mic-F1 mac-F1 GPT-J-6B 15.9 13.6 34.9 34.9 79.8 35.9 LexGPT-6B 16.9 7.7 27.0 27.0 81.9 32.1 General LLM-7B 28.3 10.8 32.9 32.9 65.8 34.2 DAPT-7B 25.0 9.8 34.2 34.2 72.0 35.0 AdaptLLM-7B 30.0 17.8 35.1 35.1 74.4 38.5 7 Besides, we compare AdaptLLM with other publicly-available models/results in each domain as follows. Biomedicine. We compare with MedAlpaca-7B/13B (Han et al., 2023), which fine-tunes LLaMA- 7B/13B (Touvron et al., 2023) on medical question-answering instructions. AdaptLLM-7B per- forms better than MedAlpaca-7B and approaches MedAlpaca-13B in the average score. While the supervised instructions help MedAlpaca-7B outperform General LLM-7B (LLaMA-7B) in some domain-specific tasks, this advantage isn\u2019t consistent. This could be because instructions don\u2019t fully infuse domain knowledge for all tasks, or the domain-specific instructions struggle with various input-output scenarios. DAPT-7B 25.0 9.8 34.2 34.2 72.0 35.0 AdaptLLM-7B 30.0 17.8 35.1 35.1 74.4 38.5 7 Besides, we compare AdaptLLM with other publicly-available models/results in each domain as follows. Biomedicine. We compare with MedAlpaca-7B/13B (Han et al., 2023), which fine-tunes LLaMA- 7B/13B (Touvron et al., 2023) on medical question-answering instructions. AdaptLLM-7B per- forms better than MedAlpaca-7B and approaches MedAlpaca-13B in the average score. While the supervised instructions help MedAlpaca-7B outperform General LLM-7B (LLaMA-7B) in some domain-specific tasks, this advantage isn\u2019t consistent. This could be because instructions don\u2019t fully infuse domain knowledge for all tasks, or the domain-specific instructions struggle with various input-output scenarios. Finance. We compare our results with those reported in BloombergGPT (Wu et al., 2023b), a model trained from scratch on a mixture of financial and general corpora. While General LLM-7B scores lower than BloombergGPT-50B, AdaptLLM-7B achieves competitive performance with the 50B BloombergGPT model. This highlights the computational and data efficiency of our approach compared to training from scratch. Law. We compare with LexGPT-6B (Lee, 2023) which conducts vanilla domain adaptive pretraining of GPT-J-6B (Wang & Komatsuzaki, 2021) on Pile of Law (Henderson et al., 2022) corpora. In contrast to the general model GPT-J-6B, LexGPT-6B shows negative prompting results. This trend aligns with our observation in section 2 that continued pre-training on domain-specific raw texts leads to worse prompting performance. On the other hand, our method contributes to positive results on the prompting performance, highlighting the effectiveness of the comprehension tasks and the general instructions. 6 A BLATIONS ON TRAINING DATA Table 5 presents ablation results on different training data and data mixtures: (1) Raw Text trains on the raw pre-training corpora. (2) Read. Compre. converts raw texts into reading comprehension texts, boosting the prompting ability to show better results in all of the adapted domains. (3) Gen. Ins. trains on general instructions. (4) Read. + Gen. Ins. augments reading comprehension texts with general instructions. Compared to using reading comprehension texts only, the inclusion of general instructions further improves the prompting ability, leading to better task results. Moreover, compared to the use of general instructions alone, the utilization of reading comprehension texts general instructions. 6 A BLATIONS ON TRAINING DATA Table 5 presents ablation results on different training data and data mixtures: (1) Raw Text trains on the raw pre-training corpora. (2) Read. Compre. converts raw texts into reading comprehension texts, boosting the prompting ability to show better results in all of the adapted domains. (3) Gen. Ins. trains on general instructions. (4) Read. + Gen. Ins. augments reading comprehension texts with general instructions. Compared to using reading comprehension texts only, the inclusion of general instructions further improves the prompting ability, leading to better task results. Moreover, compared to the use of general instructions alone, the utilization of reading comprehension texts provides domain knowledge that enhances performance in domain-specific tasks. Additionally, we provide ablations for each of the comprehension task types, where we find that Word-to-Text and Natural Language Inference exhibit the highest effectiveness on domain-specific tasks; detailed re- sults are listed in Appendix E. Table 5: Ablation results on training data. Raw Text refers to raw corpora, Read. Compre. refers to reading comprehension texts, Gen. Ins. refers to general instructions, and Raw. + Gen. Ins. and Read. + Gen. Ins. correspond to different data mixtures. We report the average of task scores in prompting evaluation within each domain. Data Raw Text Read. Compre. Gen. Ins. Raw. + Gen. Ins. Read. + Gen. Ins. BioMed. 41.7 44.3 43.3 44.8 47.3 Finance 57.6 60.0 62.2 61.7 63.4 Law 35.0 37.0 37.8 34.7 38.5 7 A NALYSIS OF DOMAIN KNOWLEDGE AND PROMPTING ABILITY Our design of reading comprehension is to learn the domain-specific knowledge from the raw texts and to enhance the prompting ability from the comprehension tasks. In this section, we conduct analyses on the two aspects respectively. Domain Knowledge. In addition to the prompting results presented in Sections 5 and 6, we also conduct fine-tuning evaluations and knowledge probing tests to assess whether the reading com- prehension texts can endow the general model with domain",
    "138": "knowledge. As demonstrated in the 8 SummarizeWord-to-TextN.L.ICommon.Reason.ParaphraseTextComple.Close.QARead.Compre.14 354555657585 BioMed.FinanceLawFine-tune ScoreDomain KnowledgePrompting Ability Finance 57.6 60.0 62.2 61.7 63.4 Law 35.0 37.0 37.8 34.7 38.5 7 A NALYSIS OF DOMAIN KNOWLEDGE AND PROMPTING ABILITY Our design of reading comprehension is to learn the domain-specific knowledge from the raw texts and to enhance the prompting ability from the comprehension tasks. In this section, we conduct analyses on the two aspects respectively. Domain Knowledge. In addition to the prompting results presented in Sections 5 and 6, we also conduct fine-tuning evaluations and knowledge probing tests to assess whether the reading com- prehension texts can endow the general model with domain knowledge. As demonstrated in the 8 SummarizeWord-to-TextN.L.ICommon.Reason.ParaphraseTextComple.Close.QARead.Compre.14 354555657585 BioMed.FinanceLawFine-tune ScoreDomain KnowledgePrompting Ability SummarizationWord-to-TextLanguageInferenceCommonsenseReasoningParaphraseDetectionTextCompletionNaturalQuestionReadingComprehensionGeneral LLMRaw TextRead. Compre.14.119.237.672.839.512.029.632.311919151238312424324012107182430332720445873Figure 3: Fine-tuning evaluation on domain-specific tasks (left) and prompting evaluation on general tasks (right) . General LLM is the general language model, Raw Text trains the general model on the domain-specific raw corpora, and Read. Compre. trains the general model on the reading comprehension texts constructed based on the raw corpora. We report the average of task scores within each domain/type, detailed results are listed in Appendix F. fine-tuning results in Figure 3, continued training on the reading comprehension texts consistently enhances model performance when fine-tuning on domain-specific tasks. The fine-tuning and do- main knowledge probing improvements (detailed in Appendix A) provide empirical evidence that the reading comprehension texts indeed imbue the general language model with domain knowledge. Furthermore, it\u2019s noteworthy that Read. Compre. outperforms Raw Text in all the adapted domains in the fine-tuning results. This improvement can be attributed to the fact that the appended compre- hension tasks naturally create a \u201cmulti-task instruction tuning\u201d setting, which benefits single-task fine-tuning, as discussed by Longpre et al. (2023a). Prompting Ability. Our approach focuses on enhancing prompting ability through the comprehen- sion tasks. To assess the effectiveness of each comprehension task type, we employ general LLM main knowledge probing improvements (detailed in Appendix A) provide empirical evidence that the reading comprehension texts indeed imbue the general language model with domain knowledge. Furthermore, it\u2019s noteworthy that Read. Compre. outperforms Raw Text in all the adapted domains in the fine-tuning results. This improvement can be attributed to the fact that the appended compre- hension tasks naturally create a \u201cmulti-task instruction tuning\u201d setting, which benefits single-task fine-tuning, as discussed by Longpre et al. (2023a). Prompting Ability. Our approach focuses on enhancing prompting ability through the comprehen- sion tasks. To assess the effectiveness of each comprehension task type, we employ general LLM benchmarks to evaluate zero-shot prompting performance. Specifically, we evaluate at least three general tasks for each comprehension task type, following the task clustering settings in FLAN (Wei et al., 2022). Besides, we assess the model\u2019s performance on general Reading Comprehension and Closed-book QA tasks to verify its ability to answer questions with or without contexts. Figure 3 presents the average task scores within each task type, subsequently averaged across the three adapted language models. By transferring raw texts into reading comprehension texts, we observe consistent prompting performance enhancements across all task types. Remarkably, when solely trained on our domain-specific reading comprehension texts (without the inclusion of gen- eral instructions), we achieve even better results than the general language model for most task types. This highlights our approach\u2019s potential in developing a general language model across more domains. We also conduct an ablation study on each comprehension task type in Appendix E to analyze whether the inclusion of a particular comprehension task type affects the performance of corresponding downstream tasks. 8 R ELATED WORK Recent works that apply large language models to specific domains such as medicine (Singhal et al., 2022; 2023; Li et al., 2023b; Wu et al., 2023a; Li et al., 2023a; Wang et al., 2023; Xiong et al., 2023), finance (Wu et al., 2023b; Yang et al., 2023) and law (Cui et al., 2023; Huang et al., 2023), can be categorized into three main approaches: training from scratch, instruction fine-tuning and retrieval-augmented prompting. 9 Training from Scratch. Training a domain-specific language models from scratch is an intuitive domains. We also conduct an ablation study on each comprehension task type in Appendix E to analyze whether the inclusion of a particular comprehension task type affects the performance of corresponding downstream tasks. 8 R ELATED WORK Recent works that apply large language models to specific domains such as medicine (Singhal et al., 2022; 2023; Li et al., 2023b; Wu et al., 2023a; Li et al., 2023a; Wang et al., 2023; Xiong et al., 2023), finance (Wu et al., 2023b; Yang et al., 2023) and law (Cui et al., 2023; Huang et al., 2023), can be categorized into three main approaches: training from scratch, instruction fine-tuning and retrieval-augmented prompting. 9 Training from Scratch. Training a domain-specific language models from scratch is an intuitive approach to realize domain specialization. BloombergGPT (Wu et al., 2023b) represents an early example of large language models in the financial domain, trained on a mix of financial and general corpora. This approach demonstrates significant improvements in performance on financial tasks without sacrificing the performance on general LLM benchmarks. However, studies (Yang et al., 2023; Ling et al., 2023) have pointed out \u201ctraining from scratch\u201d comes with expensive computa- tional and data requirements, which motivates the need for low-cost domain adaptation methods such as continued pre-training or fine-tuning. Instruction Fine-tuning. Fine-tuning large language models on domain-specific tasks, particularly those involving question-answering instructions, serves as a cost-effective approach to enhance their performance in specific domains (Singhal et al., 2022; 2023; Li et al., 2023b;a; Wang et al., 2023; Han et al., 2023; Xiong et al., 2023; Huang et al., 2023). However, due to the limited availability of supervised fine-tuning data, models fine-tuned with a small amount of data might struggle to acquire sufficient domain knowledge. Therefore, creating large-scale, supervised instruction-tuning datasets emerges as a significant challenge. Previous methods employ high-performing LLMs such as ChatGPT and GPT-4 (OpenAI, 2023) to generate these question-answer pairs (Li et al., 2023a), but the cost of utilizing those closed-source models for inference can be a concern. In such situations, harnessing large-scale domain corpora for continual pre-training represents a promising solution to acquire domain knowledge. Retrieval-augmented Prompting. Retrieval augmentation enhances LLMs by integrating external of supervised fine-tuning data, models fine-tuned with a small amount of data might struggle to acquire sufficient domain knowledge. Therefore, creating large-scale, supervised instruction-tuning datasets emerges as a significant challenge. Previous methods employ high-performing LLMs such as ChatGPT and GPT-4 (OpenAI, 2023) to generate these question-answer pairs (Li et al., 2023a), but the cost of utilizing those closed-source models for",
    "139": "inference can be a concern. In such situations, harnessing large-scale domain corpora for continual pre-training represents a promising solution to acquire domain knowledge. Retrieval-augmented Prompting. Retrieval augmentation enhances LLMs by integrating external domain-specific information without modifying the model parameters (Li et al., 2023b; Cui et al., 2023; Huang et al., 2023). LLMs gain domain context from sources like documents, domain-specific knowledge graphs, or neural networks with parametric domain knowledge. This enables LLMs to better answer domain-specific questions and address issues like hallucination. In such cases, seam- less integration of external knowledge into LLMs is crucial, existing methods typically concatenate retrieved knowledge to the LLM\u2019s input or intermediate layers. However, it\u2019s important to allow LLMs the option to accept or reject retrieved information due to potential incompleteness or con- flicts (Ling et al., 2023). Training LLMs to incorporate domain knowledge can aid in making such informed acceptance or rejection decisions. 9 CONCLUSION This paper focuses on adapting large language models via continued training on domain-specific corpora. We propose a simple method to transform large-scale domain-specific raw corpora into reading comprehension texts, enabling the model to acquire domain knowledge from raw texts and to enhance prompting ability through comprehension tasks. Experiments in different domains con- firm the approach\u2019s effectiveness and generalizability. Moreover, the extracted comprehension tasks enhance the model\u2019s performance on general LLM benchmarks, suggesting potential for enhancing general language models across more domains. We hope our method can inspire further exploration into adapting large language models with the use of large-scale unsupervised corpora, efficiently empowering language models for downstream tasks in specialized areas. REFERENCES Julio Cesar Salinas Alvarado, Karin Verspoor, and Timothy Baldwin. Domain adaption of named reading comprehension texts, enabling the model to acquire domain knowledge from raw texts and to enhance prompting ability through comprehension tasks. Experiments in different domains con- firm the approach\u2019s effectiveness and generalizability. Moreover, the extracted comprehension tasks enhance the model\u2019s performance on general LLM benchmarks, suggesting potential for enhancing general language models across more domains. We hope our method can inspire further exploration into adapting large language models with the use of large-scale unsupervised corpora, efficiently empowering language models for downstream tasks in specialized areas. REFERENCES Julio Cesar Salinas Alvarado, Karin Verspoor, and Timothy Baldwin. Domain adaption of named entity recognition to support credit risk assessment. In ALTA , pp. 84\u201390. ACL, 2015. Luisa Bentivogli, Bernardo Magnini, Ido Dagan, Hoa Trang Dang, and Danilo Giampiccolo. The fifth PASCAL recognizing textual entailment challenge. In TAC. NIST, 2009. Sumithra Bhakthavatsalam, Daniel Khashabi, Tushar Khot, Bhavana Dalvi Mishra, Kyle Richard- son, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, and Peter Clark. Think you have solved direct-answer question answering? try arc-da, the direct-answer AI2 reasoning challenge. CoRR , abs/2102.03315, 2021. Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. PIQA: reasoning about physical commonsense in natural language. In AAAI , pp. 7432\u20137439. AAAI Press, 2020. 10 Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large anno- tated corpus for learning natural language inference. In EMNLP , pp. 632\u2013642. The Association for Computational Linguistics, 2015. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari- wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In NeurIPS , 2020. Ilias Chalkidis. Chatgpt may pass the bar exam soon, but has a long way to go for the lexglue benchmark. SSRN , 2023. URL https://papers.ssrn.com/sol3/papers.cfm? abstract_id=4385460 . for Computational Linguistics, 2015. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari- wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In NeurIPS , 2020. Ilias Chalkidis. Chatgpt may pass the bar exam soon, but has a long way to go for the lexglue benchmark. SSRN , 2023. URL https://papers.ssrn.com/sol3/papers.cfm? abstract_id=4385460 . Ilias Chalkidis, Abhik Jana, Dirk Hartung, Michael J. Bommarito II, Ion Androutsopoulos, Daniel Martin Katz, and Nikolaos Aletras. Lexglue: A benchmark dataset for legal language understanding in english. In ACL (1) , pp. 4310\u20134330. Association for Computational Linguistics, 2022. Zhiyu Chen, Shiyang Li, Charese Smiley, Zhiqiang Ma, Sameena Shah, and William Yang Wang. Convfinqa: Exploring the chain of numerical reasoning in conversational finance question an- swering. In EMNLP , pp. 6279\u20136292. Association for Computational Linguistics, 2022. Daixuan Cheng, Shaohan Huang, Jianfeng Liu, Yuefeng Zhan, Hao Sun, Furu Wei, Denvy Deng, and Qi Zhang. Snapshot-guided domain adaptation for ELECTRA. In EMNLP (Findings) , pp. 2226\u20132232. Association for Computational Linguistics, 2022. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Y . Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V . Le, and Jason Wei. Scaling instruction-finetuned language models. CoRR , abs/2210.11416, 2022. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. In NAACL-HLT (1), pp. 2924\u20132936. Association for Computational Linguistics, 2019. Kevin Clark, Minh-Thang Luong, Quoc V . Le, and Christopher D. Manning. ELECTRA: pre- Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Y . Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V . Le, and Jason Wei. Scaling instruction-finetuned language models. CoRR , abs/2210.11416, 2022. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. In NAACL-HLT (1), pp. 2924\u20132936. Association for Computational Linguistics, 2019. Kevin Clark, Minh-Thang Luong, Quoc V . Le, and Christopher D. Manning. ELECTRA: pre- training text encoders as discriminators rather than generators. In ICLR . OpenReview.net, 2020. Jiaxi Cui, Zongjian Li, Yang Yan, Bohua Chen, and Li Yuan. Chatlaw: Open-source legal large language model with integrated external knowledge bases. CoRR , abs/2306.16092, 2023. Franck Dernoncourt and Ji Young Lee. Pubmed 200k RCT: a dataset for",
    "140": "sequential sentence clas- sification in medical abstracts. In IJCNLP , pp. 308\u2013313. Asian Federation of Natural Language Processing, 2017. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT (1) , pp. 4171\u20134186. As- sociation for Computational Linguistics, 2019. William B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. InIWP@IJCNLP . Asian Federation of Natural Language Processing, 2005. Ondrej Dusek, David M. Howcroft, and Verena Rieser. Semantic noise matters for neural natural language generation. In INLG , pp. 421\u2013426. Association for Computational Linguistics, 2019. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile: An 800gb dataset of diverse text for language modeling. CoRR , abs/2101.00027, 2021. 11 Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine, and Dawn Song. The false promise of imitating proprietary llms. CoRR , abs/2305.15717, 2023. Suchin Gururangan, Ana Marasovic, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. Don\u2019t stop pretraining: Adapt language models to domains and tasks. In language generation. In INLG , pp. 421\u2013426. Association for Computational Linguistics, 2019. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile: An 800gb dataset of diverse text for language modeling. CoRR , abs/2101.00027, 2021. 11 Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine, and Dawn Song. The false promise of imitating proprietary llms. CoRR , abs/2305.15717, 2023. Suchin Gururangan, Ana Marasovic, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. Don\u2019t stop pretraining: Adapt language models to domains and tasks. In ACL, pp. 8342\u20138360. Association for Computational Linguistics, 2020. Tianyu Han, Lisa C. Adams, Jens-Michalis Papaioannou, Paul Grundmann, Tom Oberhauser, Alexander L \u00a8oser, Daniel Truhn, and Keno K. Bressem. Medalpaca - an open-source collection of medical conversational AI models and training data. CoRR , abs/2304.08247, 2023. Peter Henderson, Mark S. Krass, Lucia Zheng, Neel Guha, Christopher D. Manning, Dan Jurafsky, and Daniel E. Ho. Pile of law: Learning responsible data filtering from the law and a 256gb open-source legal dataset. In NeurIPS , 2022. Quzhe Huang, Mingxu Tao, Zhenwei An, Chen Zhang, Cong Jiang, Zhibin Chen, Zirui Wu, and Yansong Feng. Lawyer llama technical report. CoRR , abs/2305.15062, 2023. Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What dis- ease does this patient have? A large-scale open domain question answering dataset from medical exams. CoRR , abs/2009.13081, 2020. Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William W. Cohen, and Xinghua Lu. Pubmedqa: A dataset for biomedical research question answering. In EMNLP/IJCNLP (1) , pp. 2567\u20132577. Association for Computational Linguistics, 2019. Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. Look- ing beyond the surface: A challenge set for reading comprehension over multiple sentences. In NAACL-HLT , pp. 252\u2013262. Association for Computational Linguistics, 2018. Jens Kringelum, Sonny Kim Kj\u00e6rulff, S\u00f8ren Brunak, Ole Lund, Tudor I. Oprea, and Olivier Taboureau. Chemprot-3.0: a global chemical biology diseases mapping. Database J. Biol. Databases Curation , 2016, 2016. Taku Kudo and John Richardson. Sentencepiece: A simple and language independent subword Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William W. Cohen, and Xinghua Lu. Pubmedqa: A dataset for biomedical research question answering. In EMNLP/IJCNLP (1) , pp. 2567\u20132577. Association for Computational Linguistics, 2019. Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. Look- ing beyond the surface: A challenge set for reading comprehension over multiple sentences. In NAACL-HLT , pp. 252\u2013262. Association for Computational Linguistics, 2018. Jens Kringelum, Sonny Kim Kj\u00e6rulff, S\u00f8ren Brunak, Ole Lund, Tudor I. Oprea, and Olivier Taboureau. Chemprot-3.0: a global chemical biology diseases mapping. Database J. Biol. Databases Curation , 2016, 2016. Taku Kudo and John Richardson. Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. In EMNLP (Demonstration) , pp. 66\u201371. Association for Computational Linguistics, 2018. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur P. Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: a benchmark for question answering research. Trans. Assoc. Comput. Linguistics , 7:452\u2013466, 2019. Jieh-Sheng Lee. Lexgpt 0.1: pre-trained GPT-J models with pile of law. CoRR , abs/2306.05431, 2023. Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Nau- mann, Hoifung Poon, and Jianfeng Gao. Llava-med: Training a large language-and-vision assis- tant for biomedicine in one day. CoRR , abs/2306.00890, 2023a. Yunxiang Li, Zihan Li, Kai Zhang, Ruilong Dan, and You Zhang. Chatdoctor: A medical chat model fine-tuned on llama model using medical domain knowledge. CoRR , abs/2303.14070, 2023b. Bill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei Zhou, Chandra Bhagavatula, Yejin Choi, and Xiang Ren. Commongen: A constrained text generation challenge for generative commonsense reasoning. In EMNLP (Findings) , volume EMNLP 2020 of Findings of ACL , pp. 1823\u20131840. Association for Computational Linguistics, 2020. Chen Ling, Xujiang Zhao, Jiaying Lu, Chengyuan Deng, Can Zheng, Junxiang Wang, Tanmoy Chowdhury, Yun Li, Hejie Cui, Xuchao Zhang, Tianjiao Zhao, Amit Panalkar, Wei Cheng, Haoyu Wang, Yanchi Liu, Zhengzhang Chen, Haifeng Chen, Chris White, Quanquan Gu, Carl Yang, and Yunxiang Li, Zihan Li, Kai Zhang, Ruilong Dan, and You Zhang. Chatdoctor: A medical chat model fine-tuned on llama model using medical domain knowledge. CoRR , abs/2303.14070, 2023b. Bill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei Zhou, Chandra Bhagavatula, Yejin Choi, and Xiang Ren. Commongen: A constrained text generation challenge for generative commonsense reasoning. In EMNLP (Findings) , volume EMNLP 2020 of Findings of ACL , pp. 1823\u20131840. Association for Computational Linguistics, 2020. Chen Ling, Xujiang Zhao, Jiaying Lu, Chengyuan Deng, Can Zheng, Junxiang Wang, Tanmoy Chowdhury, Yun Li, Hejie Cui, Xuchao Zhang, Tianjiao Zhao, Amit Panalkar, Wei Cheng, Haoyu Wang, Yanchi Liu, Zhengzhang Chen, Haifeng Chen, Chris White, Quanquan Gu, Carl Yang, and Liang Zhao. Beyond one-model-fits-all: A survey of domain specialization for large language models. CoRR , abs/2305.18703, 2023. 12 Marco Lippi, Przemyslaw Palka, Giuseppe Contissa, Francesca Lagioia, Hans-Wolfgang Micklitz, Giovanni Sartor, and Paolo Torroni. CLAUDETTE: an automated detector of potentially unfair clauses in online terms of service. Artif. Intell. Law , 27(2):117\u2013139, 2019. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining approach. CoRR , abs/1907.11692, 2019. Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V . Le, Barret Zoph, Jason Wei, and Adam Roberts. The flan collection: Designing data and meth- ods for effective instruction tuning. In ICML , volume 202 of Proceedings of Machine Learning Research , pp. 22631\u201322648. PMLR, 2023a. Shayne Longpre, Gregory",
    "141": "Yauney, Emily Reif, Katherine Lee, Adam Roberts, Barret Zoph, Denny Zhou, Jason Wei, Kevin Robinson, David Mimno, and Daphne Ippolito. A pretrainer\u2019s guide to training data: Measuring the effects of data age, domain coverage, quality, & toxicity. CoRR , abs/2305.13169, 2023b. Macedo Maia, Siegfried Handschuh, Andr \u00b4e Freitas, Brian Davis, Ross McDermott, Manel Zarrouk, and Alexandra Balahur. Www\u201918 open challenge: Financial opinion mining and question an- swering. In WWW (Companion Volume) , pp. 1941\u20131942. ACM, 2018. Pekka Malo, Ankur Sinha, Pekka J. Korhonen, Jyrki Wallenius, and Pyry Takala. Good debt or bad debt: Detecting semantic orientations in economic texts. J. Assoc. Inf. Sci. Technol. , 65(4): 782\u2013796, 2014. Research , pp. 22631\u201322648. PMLR, 2023a. Shayne Longpre, Gregory Yauney, Emily Reif, Katherine Lee, Adam Roberts, Barret Zoph, Denny Zhou, Jason Wei, Kevin Robinson, David Mimno, and Daphne Ippolito. A pretrainer\u2019s guide to training data: Measuring the effects of data age, domain coverage, quality, & toxicity. CoRR , abs/2305.13169, 2023b. Macedo Maia, Siegfried Handschuh, Andr \u00b4e Freitas, Brian Davis, Ross McDermott, Manel Zarrouk, and Alexandra Balahur. Www\u201918 open challenge: Financial opinion mining and question an- swering. In WWW (Companion Volume) , pp. 1941\u20131942. ACM, 2018. Pekka Malo, Ankur Sinha, Pekka J. Korhonen, Jyrki Wallenius, and Pyry Takala. Good debt or bad debt: Detecting semantic orientations in economic texts. J. Assoc. Inf. Sci. Technol. , 65(4): 782\u2013796, 2014. Clara H. McCreery, Namit Katariya, Anitha Kannan, Manish Chablani, and Xavier Amatriain. Ef- fective transfer learning for identifying similar questions: Matching user questions to COVID-19 faqs. In KDD , pp. 3458\u20133465. ACM, 2020. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct elec- tricity? A new dataset for open book question answering. In EMNLP , pp. 2381\u20132391. Association for Computational Linguistics, 2018. Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Hassan Awadallah. Orca: Progressive learning from complex explanation traces of GPT- 4.CoRR , abs/2306.02707, 2023. Linyong Nan, Dragomir R. Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad, Chiachun Hsieh, Xiangru Tang, Aadit Vyas, Neha Verma, Pranav Krishna, Yangxiaokang Liu, Nadia Irwanto, Jessica Pan, Faiaz Rahman, Ahmad Zaidi, Mutethia Mutuma, Yasin Tarabar, Ankit Gupta, Tao Yu, Yi Chern Tan, Xi Victoria Lin, Caiming Xiong, Richard Socher, and Nazneen Fatema Rajani. DART: open-domain structured data record to text generation. In NAACL-HLT , pp. 432\u2013447. Association for Computational Linguistics, 2021. Courtney Napoles, Matthew R. Gormley, and Benjamin Van Durme. Annotated gigaword. In AKBC- WEKEX@NAACL-HLT , pp. 95\u2013100. Association for Computational Linguistics, 2012. OpenAI. Gpt-4 technical report. Technical report, OpenAI, March 2023. URL https://cdn. openai.com/papers/gpt-4.pdf . Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. Medmcqa: A large-scale multi-subject multi-choice dataset for medical domain question answering. In CHIL , volume 174 ofProceedings of Machine Learning Research , pp. 248\u2013260. PMLR, 2022. Yu, Yi Chern Tan, Xi Victoria Lin, Caiming Xiong, Richard Socher, and Nazneen Fatema Rajani. DART: open-domain structured data record to text generation. In NAACL-HLT , pp. 432\u2013447. Association for Computational Linguistics, 2021. Courtney Napoles, Matthew R. Gormley, and Benjamin Van Durme. Annotated gigaword. In AKBC- WEKEX@NAACL-HLT , pp. 95\u2013100. Association for Computational Linguistics, 2012. OpenAI. Gpt-4 technical report. Technical report, OpenAI, March 2023. URL https://cdn. openai.com/papers/gpt-4.pdf . Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. Medmcqa: A large-scale multi-subject multi-choice dataset for medical domain question answering. In CHIL , volume 174 ofProceedings of Machine Learning Research , pp. 248\u2013260. PMLR, 2022. Fabio Petroni, Tim Rockt \u00a8aschel, Sebastian Riedel, Patrick S. H. Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander H. Miller. Language models as knowledge bases? In EMNLP/IJCNLP (1) , pp. 2463\u20132473. Association for Computational Linguistics, 2019. Alec Radford and Karthik Narasimhan. Improving language understanding by generative pre- training. 2018. 13 Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100, 000+ questions for machine comprehension of text. In EMNLP , pp. 2383\u20132392. The Association for Computational Linguistics, 2016. Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don\u2019t know: Unanswerable questions for squad. In ACL (2) , pp. 784\u2013789. Association for Computational Linguistics, 2018. Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S. Gordon. Choice of plausible alterna- tives: An evaluation of commonsense causal reasoning. In AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning . AAAI, 2011. Karan Singhal, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Kumar Tanwani, Heather Cole-Lewis, Stephen Pfohl, Perry Payne, Martin Senevi- ratne, Paul Gamble, Chris Kelly, Nathaneal Sch \u00a8arli, Aakanksha Chowdhery, Philip Andrew Mans- field, Blaise Ag \u00a8uera y Arcas, Dale R. Webster, Gregory S. Corrado, Yossi Matias, Katherine Chou, Juraj Gottweis, Nenad Tomasev, Yun Liu, Alvin Rajkomar, Joelle K. Barral, Christopher Semturs, Alan Karthikesalingam, and Vivek Natarajan. Large language models encode clinical knowledge. CoRR , abs/2212.13138, 2022. Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Formalizations of Commonsense Reasoning . AAAI, 2011. Karan Singhal, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Kumar Tanwani, Heather Cole-Lewis, Stephen Pfohl, Perry Payne, Martin Senevi- ratne, Paul Gamble, Chris Kelly, Nathaneal Sch \u00a8arli, Aakanksha Chowdhery, Philip Andrew Mans- field, Blaise Ag \u00a8uera y Arcas, Dale R. Webster, Gregory S. Corrado, Yossi Matias, Katherine Chou, Juraj Gottweis, Nenad Tomasev, Yun Liu, Alvin Rajkomar, Joelle K. Barral, Christopher Semturs, Alan Karthikesalingam, and Vivek Natarajan. Large language models encode clinical knowledge. CoRR , abs/2212.13138, 2022. Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, Mike Schaekermann, Amy Wang, Mohamed Amin, Sami Lachgar, Philip Andrew Mansfield, Sushant Prakash, Bradley Green, Ewa Dominowska, Blaise Ag \u00a8uera y Arcas, Nenad Tomasev, Yun Liu, Renee Wong, Christopher Semturs, S. Sara Mahdavi, Joelle K. Barral, Dale R. Webster, Gregory S. Corrado, Yossi Matias, Shekoofeh Azizi, Alan Karthikesalingam, and Vivek Natarajan. Towards expert-level medical question answering with large language models. CoRR , abs/2305.09617, 2023. Ankur Sinha and Tanmay Khandait. Impact of news on the commodity market: Dataset and results. CoRR , abs/2009.04202, 2020. Harold J. Spaeth, Lee Epstein, Jeffrey A. Segal Andrew D. Martin, Theodore J. Ruger, and Sara C. Benesh. Supreme Court Database, Version 2020 Release 01. Washington University Law, 2020. URL http://Supremecourtdatabase.org . Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A question answering challenge targeting commonsense knowledge. In NAACL-HLT (1) , pp. 4149\u20134158. Association for Computational Linguistics, 2019. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth \u00b4ee Lacroix, Baptiste Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aur \u00b4elien Rodriguez, Ar- mand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. CoRR , abs/2302.13971, 2023. Don Tuggener, Pius von D \u00a8aniken, Thomas Peetz, and Mark Cieliebak. LEDGAR: A large-scale multi-label corpus for text classification of legal provisions in contracts. In LREC , pp.",
    "142": "1235\u2013 1241. European Language Resources Association, 2020. Mozes van de Kar, Mengzhou Xia, Danqi Chen, and Mikel Artetxe. Don\u2019t prompt, search! mining- answering challenge targeting commonsense knowledge. In NAACL-HLT (1) , pp. 4149\u20134158. Association for Computational Linguistics, 2019. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth \u00b4ee Lacroix, Baptiste Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aur \u00b4elien Rodriguez, Ar- mand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. CoRR , abs/2302.13971, 2023. Don Tuggener, Pius von D \u00a8aniken, Thomas Peetz, and Mark Cieliebak. LEDGAR: A large-scale multi-label corpus for text classification of legal provisions in contracts. In LREC , pp. 1235\u2013 1241. European Language Resources Association, 2020. Mozes van de Kar, Mengzhou Xia, Danqi Chen, and Mikel Artetxe. Don\u2019t prompt, search! mining- based zero-shot learning with language models. In EMNLP , pp. 7508\u20137520. Association for Computational Linguistics, 2022. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In ICLR (Poster) . OpenReview.net, 2019. Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax , May 2021. Haochun Wang, Chi Liu, Nuwa Xi, Zewen Qiang, Sendong Zhao, Bing Qin, and Ting Liu. Huatuo: Tuning llama model with chinese medical knowledge. CoRR , abs/2304.06975, 2023. Jason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V . Le. Finetuned language models are zero-shot learners. In ICLR . OpenReview.net, 2022. 14 Adina Williams, Nikita Nangia, and Samuel R. Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In NAACL-HLT , pp. 1112\u20131122. Association for Com- putational Linguistics, 2018. Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Pmc-llama: Further fine- tuning llama on medical papers. CoRR , abs/2304.14454, 2023a. Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prab- hanjan Kambadur, David S. Rosenberg, and Gideon Mann. Bloomberggpt: A large language model for finance. CoRR , abs/2303.17564, 2023b. Honglin Xiong, Sheng Wang, Yitao Zhu, Zihao Zhao, Yuxiao Liu, Linlin Huang, Qian Wang, and Dinggang Shen. Doctorglm: Fine-tuning your chinese doctor is not a herculean task. CoRR , abs/2304.01097, 2023. 14 Adina Williams, Nikita Nangia, and Samuel R. Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In NAACL-HLT , pp. 1112\u20131122. Association for Com- putational Linguistics, 2018. Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Pmc-llama: Further fine- tuning llama on medical papers. CoRR , abs/2304.14454, 2023a. Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prab- hanjan Kambadur, David S. Rosenberg, and Gideon Mann. Bloomberggpt: A large language model for finance. CoRR , abs/2303.17564, 2023b. Honglin Xiong, Sheng Wang, Yitao Zhu, Zihao Zhao, Yuxiao Liu, Linlin Huang, Qian Wang, and Dinggang Shen. Doctorglm: Fine-tuning your chinese doctor is not a herculean task. CoRR , abs/2304.01097, 2023. Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions. CoRR , abs/2304.12244, 2023. Hongyang Yang, Xiao-Yang Liu, and Christina Dan Wang. Fingpt: Open-source financial large language models. CoRR , abs/2306.06031, 2023. Yunzhi Yao, Shaohan Huang, Wenhui Wang, Li Dong, and Furu Wei. Adapt-and-distill: Developing small, fast and effective pretrained language models for domains. In ACL/IJCNLP (Findings) , volume ACL/IJCNLP 2021 of Findings of ACL , pp. 460\u2013470. Association for Computational Linguistics, 2021. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a ma- chine really finish your sentence? In ACL (1) , pp. 4791\u20134800. Association for Computational Linguistics, 2019. Rui Zhang and Joel R. Tetreault. This email could save your life: Introducing the task of email subject line generation. In ACL (1) , pp. 446\u2013456. Association for Computational Linguistics, 2019. Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In NIPS , pp. 649\u2013657, 2015. Yuan Zhang, Jason Baldridge, and Luheng He. PAWS: paraphrase adversaries from word scram- bling. In NAACL-HLT (1) , pp. 1298\u20131308. Association for Computational Linguistics, 2019. Lucia Zheng, Neel Guha, Brandon R. Anderson, Peter Henderson, and Daniel E. Ho. When does pretraining help?: assessing self-supervised learning for law and the casehold dataset of 53, 000+ legal holdings. In ICAIL , pp. 159\u2013168. ACM, 2021. Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, subject line generation. In ACL (1) , pp. 446\u2013456. Association for Computational Linguistics, 2019. Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In NIPS , pp. 649\u2013657, 2015. Yuan Zhang, Jason Baldridge, and Luheng He. PAWS: paraphrase adversaries from word scram- bling. In NAACL-HLT (1) , pp. 1298\u20131308. Association for Computational Linguistics, 2019. Lucia Zheng, Neel Guha, Brandon R. Anderson, Peter Henderson, and Daniel E. Ho. When does pretraining help?: assessing self-supervised learning for law and the casehold dataset of 53, 000+ legal holdings. In ICAIL , pp. 159\u2013168. ACM, 2021. Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. LIMA: less is more for alignment. CoRR , abs/2305.11206, 2023. 15 A D OMAIN KNOWLEDGE PROBING We devise domain knowledge probing tests to determine whether continued training on the domain- specific texts can enhance the model\u2019s domain-specific knowledge. Our probing test design is in- spired by LAMA (Petroni et al., 2019), where the task format closely resembles the pre-training task. This allows us to analyze the model\u2019s inherent knowledge without altering its architecture (e.g., adding a model head) or parameters (e.g., fine-tuning). LAMA utilizes \u201cfill-in-the-blank\u201d cloze statements to match the masked language modeling task of BERT (Devlin et al., 2019). Sim- ilarly, we create \u201cpredict-the-next-token/sentence\u201d tests to align with the casual language modeling tasks of generative language models (Radford & Narasimhan, 2018). Table 6 presents the knowl- edge probing results in the biomedicine and law domains. We observe that continued training on domain-specific raw/reading comprehension texts indeed imparts the large language model with new domain knowledge. Table 6: Domain knowledge probing results. Raw Text is vanilla domain adaptive pre-training (DAPT) using raw texts, Read. Compre. trains on the reading comprehension texts. Domain General LLM Raw Text Read. Compre. BioMed. 36.5 36.9 36.8 Law 45.0 45.6 46.4 Biomedicine. To create a knowledge probing test for the biomedicine domain, we utilize the MedM- CQA (Pal et al., 2022) dataset. This dataset comprises numerous high-quality multiple-choice ques- tions, covering diverse healthcare topics and 21 medical subjects. To align the testing format with edge probing results in the biomedicine and law domains. We observe that continued training on domain-specific raw/reading comprehension texts indeed imparts the large language model with new domain knowledge. Table 6: Domain knowledge probing results. Raw Text is vanilla domain adaptive pre-training (DAPT) using raw texts, Read. Compre. trains on the reading comprehension texts. Domain General LLM Raw Text Read. Compre.",
    "143": "BioMed. 36.5 36.9 36.8 Law 45.0 45.6 46.4 Biomedicine. To create a knowledge probing test for the biomedicine domain, we utilize the MedM- CQA (Pal et al., 2022) dataset. This dataset comprises numerous high-quality multiple-choice ques- tions, covering diverse healthcare topics and 21 medical subjects. To align the testing format with casual language modeling, we exclude data samples in the instruction-following format. These in- clude samples starting with question words like \u201c What \u201d, \u201cWho\u201d and \u201c When \u201d, or ending with \u201c :\u201d, \u201c?\u201d, and \u201c -\u201d. Additionally, samples having the fill-in-the-blank marker \u201c \u201d are also removed. The evaluation is similar to zero-shot prompting: we feed into the model the raw data input, without introducing any task descriptions or demonstrations, and then compare per-token-likelihood of each option to get the model prediction. This evaluation is conducted individually for the 21 medical subjects, and the average score across all subjects is reported. Law. For the Law domain knowledge probing, we employ the LEDGAR dataset (Tuggener et al., 2020). This dataset is designed for contract provision classification and encompasses a wide spec- trum of 100 distinct law topics. Each label represents the principal topic of the given contract provision. Originally structured as a 100-classification task, we adapt it for knowledge probing by simplifying it into a four-choice question format. For each data sample, we preserve the label class and randomly select three additional classes to create the four candidate options. Similar to biomedicine knowledge probing, we feed into the model the data input using the template \u201c{CONTRACT }The topic is \u201d, and then compare per-token-likelihood of each option to get the model prediction. The evaluation is performed individually for each of the 100 law topics, and the average score across all topics is reported. B D OMAIN -ADAPTIVE PRE-TRAINING Table 7 presents specifications of the pre-training corpora in each domain and Table 8 presents pre- provision. Originally structured as a 100-classification task, we adapt it for knowledge probing by simplifying it into a four-choice question format. For each data sample, we preserve the label class and randomly select three additional classes to create the four candidate options. Similar to biomedicine knowledge probing, we feed into the model the data input using the template \u201c{CONTRACT }The topic is \u201d, and then compare per-token-likelihood of each option to get the model prediction. The evaluation is performed individually for each of the 100 law topics, and the average score across all topics is reported. B D OMAIN -ADAPTIVE PRE-TRAINING Table 7 presents specifications of the pre-training corpora in each domain and Table 8 presents pre- training hyper-parameters. A <pad> token is added to the model vocabulary for sentence padding. In each domain, we explore different ratios for mixing reading comprehension data with general instructions, specifically considering ratios of 1 : 2 ,1 : 1 , and 2 : 1 . The end-of-sentence token </s> is used to concatenate between documents, where a document could be a raw text, a reading comprehension text, or a general instruction. 16 Table 7: Pre-training corpora. Domain Data Source Raw Size # Tokens # Docs BioMed. PubMed Abstracts (Gao et al., 2021) 19.3 GiB 5.4 B 15.5 M Finance Stock News 5.1 GiB 1.2 B 1.1 M Law FreeLaw Opinions (Gao et al., 2021) 51.2 GiB 16.7 B 3.6 M Table 8: Hyper-parameters of domain-adaptive pre-training. Hyperparameter Assignment Computing infrastructure 32 V100-32GB GPUs Runtime 24 Hours Number of steps 10,000 Batch size 32 Maximum sequence length 2,048 Maximum learning rate 1e-5 Optimizer Adam Adam beta weights 0.9, 0.95 Learning rate scheduler cosine Weight decay 0.1 Warmup steps 1000 Gradient clipping 1.0 Dropout ratio 0.1 C C REATING READING COMPREHENSION TEXTS Title Collection for Summarization Tasks. In the biomedicine domain, the title for each raw text in PubMed Abstracts (Gao et al., 2021) is the first sentence within the text, separated from other sentences by a newline character \\n. In the finance domain, we specifically collect the titles when gathering news using the FinGPT codebase (Yang et al., 2023). In FreeLaw Opinions corpora (Gao et al., 2021) of the law domain, there are no explicit titles for individual raw texts. Instead, titles are available for some sections within each text. Hence, we initially divide each raw text into sections Optimizer Adam Adam beta weights 0.9, 0.95 Learning rate scheduler cosine Weight decay 0.1 Warmup steps 1000 Gradient clipping 1.0 Dropout ratio 0.1 C C REATING READING COMPREHENSION TEXTS Title Collection for Summarization Tasks. In the biomedicine domain, the title for each raw text in PubMed Abstracts (Gao et al., 2021) is the first sentence within the text, separated from other sentences by a newline character \\n. In the finance domain, we specifically collect the titles when gathering news using the FinGPT codebase (Yang et al., 2023). In FreeLaw Opinions corpora (Gao et al., 2021) of the law domain, there are no explicit titles for individual raw texts. Instead, titles are available for some sections within each text. Hence, we initially divide each raw text into sections and gather as many titles as possible from these sections. Subsequently, we consider one section, rather than an entire raw text, as the basis for creating one reading comprehension text. Regex Pattern Implementation. For tasks which employ regex patterns to mine task examples, we fill in the patterns with the corresponding verbalizer and identify sentences that match the pat- terns. This process of expanding patterns into regular expressions follows van de Kar et al. (2022): {VERBAL }is substituted with a capturing group that incorporates all verbalizers, separated by the alternation operator |. For instance, the verbalizer set Therefore ,Thus ,Hence is expanded to (Therefore|Thus|Hence) . Subsequently, the keywords listed in Table 9 are replaced with the corresponding regular expressions. The result is a regular expression containing capturing groups for extracting sentences. Data Pre-processing and Post-processing. Before mining task examples, we truncate each raw text to its initial 1,800 tokens, enabling the insertion of comprehension tasks within a maximum sequence length of 2,048. After constructing reading comprehension texts, we wrap each reading comprehen- sion text using this template: {\"text\": \"ReadCompre\" }, by replacing the ReadCompre with one piece of reading comprehension text. This template is used to explicitly separate each reading com- prehension text from others, so that the model can focus on the context of the current document. D D OMAIN -SPECIFIC TASKS EVALUATION Prompting. In prompting evaluation, each task corresponds to multiple prompt templates and we randomly sample one of them for each data example, to mitigate result variance caused by template 17 to its initial 1,800 tokens, enabling the insertion of comprehension tasks within a maximum sequence length of 2,048. After constructing reading comprehension texts, we wrap each reading comprehen- sion text using this template: {\"text\": \"ReadCompre\" }, by replacing the ReadCompre with one piece of reading comprehension text. This template is used to explicitly separate each reading com- prehension text from others, so that the model can focus on the context of the current document. D D OMAIN -SPECIFIC TASKS EVALUATION Prompting. In prompting evaluation, each task corresponds to multiple prompt templates and we randomly sample one of them for each data example, to mitigate result variance caused by template 17 Table 9: Keywords that compile into regular expressions. These keywords are used in the mining patterns and verbalizers (van de Kar et al., 2022). Keyword Regex {VERBAL }Replaced with the verbalizer {WORD }regex: ([\u02c6.!?\\n,;\\\"\\s]{10,}) Matches a single word having more than 9 characters {SENT",
    "144": "}regex: ([\u02c6.!?\\n]{50,}[.!?]+) Matches a single sentence having more than 50 characters sensitivity. Prompt template examples are presented in Table 11. Following Brown et al. (2020), we classify tasks into two question types to get model predictions: 1) For multiple-choice questions, we compare the per-token likelihood of each option to determine the model prediction; 2) For text completion questions, we employ greedy search to get the free-form answer. Our prompting settings in the finance domain follow BloombergGPT (Wu et al., 2023b), with the exception that we use multiple templates to address template sensitivity. The prompt templates for law domain are based on Chalkidis (2023). The UNFAIR-ToS (Lippi et al., 2019) task is a multi- label classification task. To get model predictions for this task, we categorize it as a multiple-choice question. The accuracy of an individual data example is considered true if the model prediction (i.e., the option with the highest per-token likelihood) belongs to the label(s) set. In the biomedicine domain, some classification tasks, including MQP (McCreery et al., 2020), RCT (Dernoncourt & Lee, 2017), and ChemProt (Kringelum et al., 2016), are too challenging for the model, thus we conduct few-shot prompting and maintain the number of demonstrations the same in each class. Fine-tuning. In fine-tuning, we utilize a fixed prompt template (the one displayed in Table 11) for law domain are based on Chalkidis (2023). The UNFAIR-ToS (Lippi et al., 2019) task is a multi- label classification task. To get model predictions for this task, we categorize it as a multiple-choice question. The accuracy of an individual data example is considered true if the model prediction (i.e., the option with the highest per-token likelihood) belongs to the label(s) set. In the biomedicine domain, some classification tasks, including MQP (McCreery et al., 2020), RCT (Dernoncourt & Lee, 2017), and ChemProt (Kringelum et al., 2016), are too challenging for the model, thus we conduct few-shot prompting and maintain the number of demonstrations the same in each class. Fine-tuning. In fine-tuning, we utilize a fixed prompt template (the one displayed in Table 11) for each task to convert input-output into question-answering pairs. The model is then trained on these pairs for one epoch with the warm-up step as 0, and we compute the loss only on the tokens of the output answer of each training example (Mukherjee et al., 2023). All other training settings are the same with domain-adaptive pre-training. Fine-tuning evaluation is similar to prompting evaluation, but with two differences to align with the fine-tuning training stage: no demonstration is presented before the prompt input, and the prompt template is the same with the one used in the training stage. Table 10: Specifications of the domain-specific task datasets. # Demos is the number of demon- strations in prompting evaluation. Task Type Metric # Demos BioMed. MQP (McCreery et al., 2020) Binary classification Accuracy 4 PubMedQA (Jin et al., 2019) Binary classification Accuracy 0 USMLE (Jin et al., 2020) Multi-chioice QA Accuracy 0 RCT (Dernoncourt & Lee, 2017) Multi-class classification Micro F1 10 ChemProt (Kringelum et al., 2016) Multi-class classification Micro F1 13 Finance FiQA SA (Maia et al., 2018) Multi-class classification Weighted F1 5 FPB (Malo et al., 2014) Multi-class classification Weighted F1 5 NER (Alvarado et al., 2015) Named entity recongnition Entity-level F1 20 Headline (Sinha & Khandait, 2020) Binary-class classification Weighted F1 5 ConvFinQA (Chen et al., 2022) Question Answering Exact Match 0 Law SCOTUS (Spaeth et al., 2020) Multi-class classification Micro/Macro F1 0 CaseHOLD (Zheng et al., 2021) Multi-chioice QA Micro/Macro F1 0 UNFAIR-ToS (Lippi et al., 2019) Multi-label classification Accuracy 4 18 USMLE (Jin et al., 2020) Multi-chioice QA Accuracy 0 RCT (Dernoncourt & Lee, 2017) Multi-class classification Micro F1 10 ChemProt (Kringelum et al., 2016) Multi-class classification Micro F1 13 Finance FiQA SA (Maia et al., 2018) Multi-class classification Weighted F1 5 FPB (Malo et al., 2014) Multi-class classification Weighted F1 5 NER (Alvarado et al., 2015) Named entity recongnition Entity-level F1 20 Headline (Sinha & Khandait, 2020) Binary-class classification Weighted F1 5 ConvFinQA (Chen et al., 2022) Question Answering Exact Match 0 Law SCOTUS (Spaeth et al., 2020) Multi-class classification Micro/Macro F1 0 CaseHOLD (Zheng et al., 2021) Multi-chioice QA Micro/Macro F1 0 UNFAIR-ToS (Lippi et al., 2019) Multi-label classification Accuracy 4 18 Table 11: Prompt templates. Each template example is paraphrased to multiple variations for prompting evaluation. Task Template BioMed. MQPQuestion 1: {QUESTION1 } Question 2: {QUESTION2 } Are questions 1 and 2 asking the same thing? {ANSWER } PubMedQAContext: {CONTEXT } Question: {QUESTION } Answer: {ANSWER } USMLEQuestion: {QUESTION } Answer: {ANSWER } RCT{SENTENCE } Question: what is the role of this sentence in an abstract? Answer: {ANSWER } ChemProt{SENTENCE } Question: what is the relation? Answer: {ANSWER } Finance FiQA SA{SENTENCE } Question: what is the sentiment on {TARGET }? Answer: {ANSWER } FPB{SENTENCE } Question: what is the sentiment? Answer: {ANSWER } NER{SENTENCE } Extract named entity: {ANSWER } Headline{SENTENCE } Question: {QUESTION } Answer: {ANSWER } ConvFinQA{CONTEXT } {PREVIOUS QAS } {QUESTION } {ANSWER } Law SCOTUSGiven the following opinion from the Supreme Court of USA (SCOTUS): \"{TEXT }\" The relevant issue area is: {ANSWER } CaseHOLDComplete the following excerpt from a US court opinion: {CONTEXT }:{ANSWER } UNFAIR-ToSGiven the following sentence from an online Term of Services: \"{SENTENCE }\" The sentence is unfair with respect to: {ANSWER } 19 E F URTHER ABLATIONS ON COMPREHENSION TASKS Figure 4 presents the percentages of mined examples of each task type in all the comprehension task examples, with Word-To-Text, Summarization, and Text Completion accounting for the highest ratios. Answer: {ANSWER } NER{SENTENCE } Extract named entity: {ANSWER } Headline{SENTENCE } Question: {QUESTION } Answer: {ANSWER } ConvFinQA{CONTEXT } {PREVIOUS QAS } {QUESTION } {ANSWER } Law SCOTUSGiven the following opinion from the Supreme Court of USA (SCOTUS): \"{TEXT }\" The relevant issue area is: {ANSWER } CaseHOLDComplete the following excerpt from a US court opinion: {CONTEXT }:{ANSWER } UNFAIR-ToSGiven the following sentence from an online Term of Services: \"{SENTENCE }\" The sentence is unfair with respect to: {ANSWER } 19 E F URTHER ABLATIONS ON COMPREHENSION TASKS Figure 4 presents the percentages of mined examples of each task type in all the comprehension task examples, with Word-To-Text, Summarization, and Text Completion accounting for the highest ratios. Summarize, 26.3 NLI, 2.2 Word-to-Text, 50.3 Common Reason, 0.3 Text Comple., 17.4 Paraphrase, 3.5 Summarize, 31.4 NLI, 3.8 Word-to-Text, 35.8 Common Reason, 0.5 Text Comple., 20.7 Paraphrase, 7.8 Summarize, 7.2 NLI, 2.7 Word-to-Text, 62.3 Common Reason, 1.0 Text Comple., 22.0 Paraphrase, 4.9 BiomedicineFinanceLaw Figure 4: Percentages of mined examples of each task type in all the comprehension task ex- amples . In the biomedicine domain, we conduct ablations on each comprehension task type by systemat- ically removing each task type from the reading comprehension texts. We then use the resulting modified reading comprehension texts to train the general model. Subsequently, we evaluate these trained models on both domain-specific tasks and general benchmarks to analyze the impacts of these ablations. Domain TasksGeneral Tasks 404142434445 Allw/oSumm.w/oWord.w/oNLIw/oCommon.w/oPara.w/oTextCom.Domain Task Score020406080 SummarizeWord-to-TextNLICommonReasonParaphraseTextCompletionGeneral Task ScoreAllw/oSumm.Allw/oWord.Allw/oNLIAllw/oComm. Allw/oPara.Allw/oText. Figure 5: Prompting scores of domain-specific tasks (left) and general benchmarks",
    "145": "(right) of models trained with different comprehension tasks. Alldenotes the model trained with all the com- prehension tasks, while w/o Summ. represents the model trained with the comprehension tasks excluding Summarization tasks, w/o Word. represents the model trained with the comprehension tasks excluding Word-to-Text tasks, and so on. We report the average task scores within each do- main/type. Domain-specific Tasks. As shown in Figure 5, when evaluating on the domain-specific tasks, the SummarizeWord-to-TextNLICommonReasonParaphraseTextCompletionGeneral Task ScoreAllw/oSumm.Allw/oWord.Allw/oNLIAllw/oComm. Allw/oPara.Allw/oText. Figure 5: Prompting scores of domain-specific tasks (left) and general benchmarks (right) of models trained with different comprehension tasks. Alldenotes the model trained with all the com- prehension tasks, while w/o Summ. represents the model trained with the comprehension tasks excluding Summarization tasks, w/o Word. represents the model trained with the comprehension tasks excluding Word-to-Text tasks, and so on. We report the average task scores within each do- main/type. Domain-specific Tasks. As shown in Figure 5, when evaluating on the domain-specific tasks, the removal of any comprehension task type leads to a decrease in task performance, showing their con- tributions to these domain-specific tasks. Notably, removing Word-to-Text, Summarization, or Text Completion tasks results in a noticeable drop in performance, aligning with the high percentages of these tasks in the mined examples. Interestingly, even though the Natural Language Inference task type doesn\u2019t constitute a large portion of the comprehension tasks, its removal leads to a substantial decrease in performance. This could be attributed to its unique role as the sole classification task type within all the comprehension tasks. In contrast, the impact of removing Commonsense Rea- soning and Paraphrase Detection tasks is less pronounced, reflecting their lower percentages in the 20 mined task examples. However, this also suggests the potential benefits of including more diverse task examples, which could further enhance domain-specific task performance. General LLM Benchmarks. Additionally, we conduct experiments where we remove a specific task type from all the comprehension tasks. We then evaluate the model\u2019s performance specifically on the general tasks corresponding to the removed task type, aiming to demonstrate whether the comprehension tasks have a positive impact on the respective downstream tasks. In the results for general tasks in Figure 5, when we exclude a particular task type from the comprehension tasks, we observe performance declines in the corresponding removed tasks, specifically for Summariza- tion, Word-to-Text, Natural Language Inference, and Commonsense Reasoning tasks. This suggests a beneficial connection between the trained comprehension tasks and their corresponding down- General LLM Benchmarks. Additionally, we conduct experiments where we remove a specific task type from all the comprehension tasks. We then evaluate the model\u2019s performance specifically on the general tasks corresponding to the removed task type, aiming to demonstrate whether the comprehension tasks have a positive impact on the respective downstream tasks. In the results for general tasks in Figure 5, when we exclude a particular task type from the comprehension tasks, we observe performance declines in the corresponding removed tasks, specifically for Summariza- tion, Word-to-Text, Natural Language Inference, and Commonsense Reasoning tasks. This suggests a beneficial connection between the trained comprehension tasks and their corresponding down- stream tasks. However, when we remove Paraphrase Detection or Text Completion, it does not lead to a performance decline in the corresponding tasks. This discrepancy may be attributed to the refor- matting of Paraphrase Detection from a classification task to a generation task in the comprehension tasks, causing a mismatch between training and evaluation settings. Furthermore, the Text Com- prehension comprehension task type lacks obvious counterparts in the general LLM benchmarks, which may contribute to the mismatch in the performance change trend. F A NALYSIS OF DOMAIN KNOWLEDGE AND PROMPTING ABILITY Table 12: Fine-tuning performance on the domain-specific tasks of general large language model (General LLM ), the model trained on domain-specific raw corpora ( Raw Text ) and the model trained on the reading comprehension texts constructed based on the raw corpora ( Read. Compre. ). BioMed. PubMedQA ChemProt MQP RCT UMSLE AVERAGE General LLM 75.4 64.6 55.4 87.0 38.5 64.2 Raw Text 76.2 64.8 65.6 87.0 39.0 66.5 Read. Compre. 76.0 65.4 87.9 87.5 41.0 71.5 Finance ConvFinQA FPB FiQA SA Headline NER AVERAGE General LLM 58.1 81.9 86.4 95.7 77.5 79.9 Raw Text 56.2 83.3 87.9 95.8 81.3 80.9 Read. Compre. 57.2 88.6 83.1 96.1 82.5 81.5 LawSCOTUS CaseHOLDUNFAIR-ToS AVERAGE mic-F1 mac-F1 mic-F1 mac-F1 General LLM 31.7 14.0 35.3 35.3 93.8 42.0 Raw Text 36.7 26.0 35.4 35.4 93.7 45.4 Read. Compre. 40.0 26.0 35.5 35.5 94.2 46.2 21 Table 13: Prompting results on general LLM benchmarks. Raw trains on the raw texts, and Read trains on the reading comprehension texts. Text Completion is more close to a question type than a task type in general benchmarks, so we report the average of all the tasks following the General LLM 75.4 64.6 55.4 87.0 38.5 64.2 Raw Text 76.2 64.8 65.6 87.0 39.0 66.5 Read. Compre. 76.0 65.4 87.9 87.5 41.0 71.5 Finance ConvFinQA FPB FiQA SA Headline NER AVERAGE General LLM 58.1 81.9 86.4 95.7 77.5 79.9 Raw Text 56.2 83.3 87.9 95.8 81.3 80.9 Read. Compre. 57.2 88.6 83.1 96.1 82.5 81.5 LawSCOTUS CaseHOLDUNFAIR-ToS AVERAGE mic-F1 mac-F1 mic-F1 mac-F1 General LLM 31.7 14.0 35.3 35.3 93.8 42.0 Raw Text 36.7 26.0 35.4 35.4 93.7 45.4 Read. Compre. 40.0 26.0 35.5 35.5 94.2 46.2 21 Table 13: Prompting results on general LLM benchmarks. Raw trains on the raw texts, and Read trains on the reading comprehension texts. Text Completion is more close to a question type than a task type in general benchmarks, so we report the average of all the tasks following the free-form text completion question type. Each task corresponds to multiple prompt templates taken from FLAN (Wei et al., 2022), and we remove the option suffixes from the templates to fit for the prompting evaluation approach by Brown et al. (2020). Task MetricGeneral LLMBioMed. Finance Law Raw Read Raw Read Raw Read Summarization AGNews (Zhang et al., 2015) Acc 58.7 51.7 55.5 56.1 50.1 57.8 60.6 AESLC (Zhang & Tetreault, 2019)R-1 1.5 3.6 7.5 1.9 10.8 3.4 7.4 R-2 0.2 0.9 2.8 0.3 3.8 0.8 2.7 R-L 1.5 3.6 7.2 1.8 10.3 3.3 7.2 Gigaword (Napoles et al., 2012)R-1 0.6 3.8 9.3 3.1 13.2 3.4 9.8 R-2 0.1 0.7 2.4 0.6 3.8 0.6 2.5 R-L 0.6 3.5 8.5 2.9 12.1 3.1 8.9 Word-to-Text CommonGen (Lin et al., 2020)R-1 14.2 16.1 24.9 17.2 24.2 17.8 27.9 R-2 0.0 1.6 5.9 1.5 6.2 2.5 7.2 R-L 14.2 15.4 22.2 16.2 21.1 16.7 24.3 DART (Nan et al., 2021)R-1 17.0 21.3 20.0 21.9 22.3 23.6 19.5 R-2 3.7 6.0 5.0 6.7 7.1 7.5 5.9 R-L 16.0 19.6 18.5 20.1 20.5 21.4 18.0 E2ENLG (Dusek et al., 2019)R-1 14.3 18.4 27.6 22.6 37.3 18.4 28.3 R-2 3.2 5.2 9.8 7.5 15.1 5.8 10.5 R-L 13.4 17.1 24.5 20.4 32.9 17.0 25.5 Natural Language Inference MNLI-m (Williams et al., 2018) Acc 35.6 35.0 36.7 36.2 37.4 34.8 34.9 MNLI-mm (Williams et al., 2018) Acc 34.9 34.9 36.1 36.8 37.2 35.7 34.4 QNLI (Rajpurkar et al., 2018) Acc 51.1 52.4 53.7 52.1 52.0 50.7 51.9 RTE (Bentivogli et al., 2009) Acc 30.7 9.4 30.0 23.1 26.0 18.8 32.9 SNLI (Bowman et al., 2015) Acc 34.0 34.5 33.8 36.8 38.1 34.7 33.9 Commonsense Reasoning COPA (Roemmele et al., 2011) Acc 71.0 68.0 75.0 74.0 73.0 72.0 74.0 PIQA (Bisk et al., 2020) Acc 71.7 70.9 71.3 71.6 71.7 71.9 71.8 HellaSwag (Zellers et al., 2019) Acc 71.8 71.8 72.6 72.5 73.0 72.0 72.5 Paraphrase Detection R-L 16.0 19.6 18.5 20.1 20.5 21.4 18.0 E2ENLG (Dusek et al., 2019)R-1 14.3 18.4 27.6 22.6 37.3",
    "146": "18.4 28.3 R-2 3.2 5.2 9.8 7.5 15.1 5.8 10.5 R-L 13.4 17.1 24.5 20.4 32.9 17.0 25.5 Natural Language Inference MNLI-m (Williams et al., 2018) Acc 35.6 35.0 36.7 36.2 37.4 34.8 34.9 MNLI-mm (Williams et al., 2018) Acc 34.9 34.9 36.1 36.8 37.2 35.7 34.4 QNLI (Rajpurkar et al., 2018) Acc 51.1 52.4 53.7 52.1 52.0 50.7 51.9 RTE (Bentivogli et al., 2009) Acc 30.7 9.4 30.0 23.1 26.0 18.8 32.9 SNLI (Bowman et al., 2015) Acc 34.0 34.5 33.8 36.8 38.1 34.7 33.9 Commonsense Reasoning COPA (Roemmele et al., 2011) Acc 71.0 68.0 75.0 74.0 73.0 72.0 74.0 PIQA (Bisk et al., 2020) Acc 71.7 70.9 71.3 71.6 71.7 71.9 71.8 HellaSwag (Zellers et al., 2019) Acc 71.8 71.8 72.6 72.5 73.0 72.0 72.5 Paraphrase Detection MRPC (Dolan & Brockett, 2005)Acc 35.5 31.6 34.8 33.3 37.3 34.1 36.0 F1 17.0 0.0 13.6 9.9 22.4 8.8 19.7 QQP (Wang et al., 2019)Acc 56.0 62.7 61.1 52.5 54.3 60.6 59.9 F1 34.0 3.0 10.7 33.1 44.5 15.3 14.4 Paws Wiki (Zhang et al., 2019) Acc 54.8 55.6 54.9 53.7 53.7 55.4 54.8 Closed-book QA ARC-c (Bhakthavatsalam et al., 2021) Acc 37.3 36.7 39.0 37.8 38.8 37.6 39.3 ARC-e (Bhakthavatsalam et al., 2021) Acc 58.5 59.2 63.1 59.4 62.3 60.0 62.9 NQ (Kwiatkowski et al., 2019)EM 2.2 0.1 0.5 0.1 0.0 0.1 0.1 F1 3.4 0.8 1.9 1.2 2.1 1.3 1.8 CMSQA (Talmor et al., 2019) Acc 39.6 40.4 43.5 40.0 42.6 40.9 44.6 Reading Comprehension BoolQ (Clark et al., 2019) Acc 55.7 42.6 50.7 53.4 55.8 51.0 53.9 OBQA (Mihaylov et al., 2018) Acc 46.2 45.8 46.4 46.0 47.0 45.4 47.2 SQuADv1 (Rajpurkar et al., 2016)EM 0.1 1.0 3.5 0.2 0.0 0.3 2.0 F1 0.2 4.6 10.0 0.6 7.3 6.3 10.7 MultiRC (Khashabi et al., 2018) Acc 55.9 47.5 53.0 49.9 49.7 52.5 52.4 22 G C ASES OF READING COMPREHENSION TEXTS NQ (Kwiatkowski et al., 2019)EM 2.2 0.1 0.5 0.1 0.0 0.1 0.1 F1 3.4 0.8 1.9 1.2 2.1 1.3 1.8 CMSQA (Talmor et al., 2019) Acc 39.6 40.4 43.5 40.0 42.6 40.9 44.6 Reading Comprehension BoolQ (Clark et al., 2019) Acc 55.7 42.6 50.7 53.4 55.8 51.0 53.9 OBQA (Mihaylov et al., 2018) Acc 46.2 45.8 46.4 46.0 47.0 45.4 47.2 SQuADv1 (Rajpurkar et al., 2016)EM 0.1 1.0 3.5 0.2 0.0 0.3 2.0 F1 0.2 4.6 10.0 0.6 7.3 6.3 10.7 MultiRC (Khashabi et al., 2018) Acc 55.9 47.5 53.0 49.9 49.7 52.5 52.4 22 G C ASES OF READING COMPREHENSION TEXTS Hereisthefirstpartofanarticleaboutbiomedicine:Recentreportedevidenceindicatesthatvocalcordcarcinomaisevolvingsimilarlytooropharyngealcancerwithanincreasingnumberofpatientswithoutasmokinghistoryhavinghumanpapillomavirus(HPV)disease.Observationsalsosuggestthatanincreasingnumberofpatientswhopresentwithglotticcarcinomaareyoungerthanhasbeenreportedinthepast.(\u2026TL;DR...)However,recentpublishedevidenceshowsthatglotticcarcinomacanbeanHPV-relateddiseasewithincreasingincidenceinnonsmokers.Itisn'tsurprisingthatalternatemalignantpathwaysmayhaveadifferenttimeline.Answerquestionsbasedonthearticle:Whatisasummary?GlotticCarcinomainYoungPatients.Generateasentencethatincludesthesebiomedicinewords[carcinoma,oropharyngeal,papillomavirus]:Recentreportedevidenceindicatesthatvocalcordcarcinomaisevolvingsimilarlytooropharyngealcancerwithanincreasingnumberofpatientswithoutasmokinghistoryhavinghumanpapillomavirus(HPV)disease.Premise:Observationsalsosuggestthatanincreasingnumberofpatientswhopresentwithglotticcarcinomaareyoungerthanhasbeenreportedinthepast.Hypothesis:Aninvestigationwasdonetoexaminetheincidenceofglotticcarcinomainpatients30yearsold(y/o)oryounger.Doesthepremiseentailthehypothesis?YesWhatisthereasonfor\"Noneofthe11glotticcancerpatientshadbeenpreviouslytreatedforbenignrecurrentrespiratorypapillomatosisalthoughitwasinitiallysuspectedpriortobiopsy\"?Themorphologyofthelesionsandthepatients'youngage.Composeasentencethatcontradictsthemeaningof\"Historically,glotticcarcinomaisconsideredtobeatobacco-induceddisease.\u201d.Answer:RecentpublishedevidenceshowsthatglotticcarcinomacanbeanHPV-relateddiseasewithincreasingincidenceinnonsmokers.Howwouldyoucompletethearticle?Thisfindingfurthersupportstheconceptthatglotticcarcinomaisanevolvingdisease,anditdemonstratestheincreasingimportanceofdiscriminatingpotentialglotticcarcinomasinyoungpatientsfrombenignlow-riskHPVrecurrentrespiratorypapillomatosis.Text CompletionSummarizationWord-to-TextNatural LanguageInference ParaphraseDetectionCommonsenseReasoningRaw Text Figure 6: An example of a reading comprehension text constructed from a raw text. The underlined sentence is added to guide the model to answer questions based the given context. 23 Table 14: Case of a reading comprehension text in biomedicine domain. Certain portions are omitted for brevity and are represented as (...). Pancreastatin (PST), a chromogranin A-derived peptide, has been found to modulate glucose, lipid, and protein metabolism in rat adipocytes. PST has an overall counterregulatory effect on insulin action by activating a specific receptor-effector system (Galpha(q/11) protein-PLC-beta- PKC(classical)). However, PST stimulates both basal and insulin-mediated protein synthesis in rat adipocytes. In order to further investigate the mechanisms underlying the effect of PST stimulating protein synthesis, we sought to study the regulation of different components of the core translational machinery by the signaling triggered by PST. Thus, we studied ribosomal p70 S6 kinase, phosphory- lation of the cap-binding protein (initiation factor) eIF4E, and phosphorylation of the eIF4E-binding protein 4E-BP1 (PHAS-I). We have found that PST stimulates the S6 kinase activity, as assessed by kinase assay using specific immunoprecipitates and substrate. This effect was checked by Western blot with specific antibodies against the phosphorylated S6 kinase. Thus, PST dose-dependently stimulates Thr421/Ser424 phosphorylation of S6 kinase. Moreover, PST promotes phosphorylation of regulatory sites in 4E-BP1 (PHAS-I) (Thr37, Thr46). The initiation factor eIF4E itself, whose activity is also increased upon phosphorylation, is phosphorylated in Ser209 by PST stimulation. (...) Use evidence from the biomedicine article to answer these questions: Assess the relationship between Sentence 1: \u201cThis effect was checked by Western blot with specific antibodies against the phosphorylated S6 kinase.\u201d Sentence 2: \u201cPST dose-dependently stimulates Thr421/Ser424 phosphorylation of S6 kinase.\u201d Is it characterized as Entailment, Neutral, or Contradiction? Entailment Assess the relationship between Sentence 1: \u201cPST has an overall counterregulatory effect on insulin action by activating a specific receptor-effector system (Galpha(q/11) protein-PLC-beta- PKC(classical)).\u201d Sentence 2: \u201cPST stimulates both basal and insulin-mediated protein synthesis in rat adipocytes.\u201d activity is also increased upon phosphorylation, is phosphorylated in Ser209 by PST stimulation. (...) Use evidence from the biomedicine article to answer these questions: Assess the relationship between Sentence 1: \u201cThis effect was checked by Western blot with specific antibodies against the phosphorylated S6 kinase.\u201d Sentence 2: \u201cPST dose-dependently stimulates Thr421/Ser424 phosphorylation of S6 kinase.\u201d Is it characterized as Entailment, Neutral, or Contradiction? Entailment Assess the relationship between Sentence 1: \u201cPST has an overall counterregulatory effect on insulin action by activating a specific receptor-effector system (Galpha(q/11) protein-PLC-beta- PKC(classical)).\u201d Sentence 2: \u201cPST stimulates both basal and insulin-mediated protein synthesis in rat adipocytes.\u201d Is it characterized as Entailment, Neutral, or Contradiction? Contradiction Next question: What is the reason of the following sentence? We studied ribosomal p70 S6 kinase, phosphorylation of the cap-binding protein (initiation factor) eIF4E, and phosphorylation of the eIF4E-binding protein 4E-BP1 (PHAS-I). In order to further investigate the mechanisms underlying the effect of PST stimulating protein synthesis, we sought to study the regulation of different components of the core translational machinery by the signaling triggered by PST. Next question: What is the reason of the following sentence? PST dose-dependently stimulates",
    "147": "Thr421/Ser424 phosphorylation of S6 kinase. This effect was checked by Western blot with specific antibodies against the phosphorylated S6 kinase. \u201cPST has an overall counterregulatory effect on insulin action by activating a specific receptor- effector system (Galpha(q/11) protein-PLC-beta-PKC(classical)).\u201d Generate a sentence that ex- presses a contrasting idea to the previous statement. PST stimulates both basal and insulin-mediated protein synthesis in rat adipocytes. Here are some concepts about biomedicine: mechanisms, regulation, translational. Generate a detailed description of them In order to further investigate the mechanisms underlying the effect of PST stimulating protein synthesis, we sought to study the regulation of different com- ponents of the core translational machinery by the signaling triggered by PST. Here are some concepts about biomedicine: phosphorylation, stimulation, phosphorylated. Generate a detailed description of them The initiation factor eIF4E itself, whose activity is also effector system (Galpha(q/11) protein-PLC-beta-PKC(classical)).\u201d Generate a sentence that ex- presses a contrasting idea to the previous statement. PST stimulates both basal and insulin-mediated protein synthesis in rat adipocytes. Here are some concepts about biomedicine: mechanisms, regulation, translational. Generate a detailed description of them In order to further investigate the mechanisms underlying the effect of PST stimulating protein synthesis, we sought to study the regulation of different com- ponents of the core translational machinery by the signaling triggered by PST. Here are some concepts about biomedicine: phosphorylation, stimulation, phosphorylated. Generate a detailed description of them The initiation factor eIF4E itself, whose activity is also increased upon phosphorylation, is phosphorylated in Ser209 by PST stimulation. Briefly summarize this text. Pancreastatin, a chromogranin A-derived peptide, activates protein synthesis signaling cascade in rat adipocytes. 24 Table 15: Case of a reading comprehension text in finance domain. Certain portions are omitted for brevity and are represented as (...). Read the beginning of an article on finance: In this article, we discuss the 12 biggest commer- cial janitorial companies in USA. If you want to skip our detailed analysis of these companies, go directly to the 5 Biggest Commercial Janitorial Companies In USA. According to Statista, the jani- torial services sector\u2019s market size will increase by 6.6 percent in 2022. The annualized percentage of this market\u2019s growth was 6.7% between 2017 and 2022. Additionally, between 2020 and 2021, the number of janitors and cleaners employed in the United States rose by nearly 50,000. By 2024, it is predicted that the U.S. janitorial services industry will generate around $50.1 billion in sales. The market size, measured by revenue, of the janitorial services industry is $97.6 billion in 2022. Businesses frequently grow when corporate profits increase, raising demand for janitorial services. However, dwindling corporate profit decreases demand for janitorial services as companies close fa- cilities and cut back on the frequency of contracted cleaning to cut expenses. The industry may have an opportunity in 2022 as corporate profits are anticipated to rise, according to ibisworld. According to Allied Market Research, the cleaning services market is to reach $111.49 billion globally by 2030 it is predicted that the U.S. janitorial services industry will generate around $50.1 billion in sales. The market size, measured by revenue, of the janitorial services industry is $97.6 billion in 2022. Businesses frequently grow when corporate profits increase, raising demand for janitorial services. However, dwindling corporate profit decreases demand for janitorial services as companies close fa- cilities and cut back on the frequency of contracted cleaning to cut expenses. The industry may have an opportunity in 2022 as corporate profits are anticipated to rise, according to ibisworld. According to Allied Market Research, the cleaning services market is to reach $111.49 billion globally by 2030 at a 6.5% CAGR, as reported by Bloomberg. The global cleaning services industry is expanding due to service providers expanding their online presence and rising commercial consumer demand. However, heightened rivalry and the introduction of new companies limit market expansion. On the other hand, the demand for construction and post-construction cleaning services will open up new prospects. The Covid-19 pandemic has had a favorable effect on the global cleaning services busi- ness. Deep cleaning and disinfection have become more popular among residential and commercial users to reduce the risk of cross-contamination. (...) How would you extend the article? Moore adds that Casella Waste Systems, Inc. (NASDAQ:CWST) is also the only public waste company its size not to be acquired, which provides \u201ddownside support to the valuation on a takeout potential\u201d. 09. Stericycle, Inc. (NASDAQ: SRCL) Market Capitaliza- tion as of November 18, 2022: $4.73 billion Stericycle, Inc. (NASDAQ:SRCL) was incorporated in 1989 and is located in Bannockburn, Illinois. (...) Then, answer the following questions based on the whole article: Please write a short summary for the above article. 12 Biggest Commercial Janitorial Companies In USA Given the sentence: The global cleaning services industry is expanding due to service providers expanding their online presence and rising commercial consumer demand. Select the most suitable word to connect it with: \u201cHeightened rivalry and the introduction of new companies limit market expansion.\u201d Options: Therefore, However, Maybe However Answer the following question. The global cleaning services industry is expanding due to? Service providers expanding their online presence and rising commercial consumer demand. 1989 and is located in Bannockburn, Illinois. (...) Then, answer the following questions based on the whole article: Please write a short summary for the above article. 12 Biggest Commercial Janitorial Companies In USA Given the sentence: The global cleaning services industry is expanding due to service providers expanding their online presence and rising commercial consumer demand. Select the most suitable word to connect it with: \u201cHeightened rivalry and the introduction of new companies limit market expansion.\u201d Options: Therefore, However, Maybe However Answer the following question. The global cleaning services industry is expanding due to? Service providers expanding their online presence and rising commercial consumer demand. Given the sentence \u201cBusinesses frequently grow when corporate profits increase, raising demand for janitorial services.\u201d, come up with a different sentence that contradicts its meaning. Dwindling corporate profit decreases demand for janitorial services as companies close facilities and cut back on the frequency of contracted cleaning to cut expenses. Given the sentence \u201cThe global cleaning services industry is expanding due to service providers expanding their online presence and rising commercial consumer demand.\u201d, come up with a different sentence that contradicts its meaning. Heightened rivalry and the introduction of new companies limit market expansion. Generate a sentence that includes the following finance keywords: Capitalization, Incorporated, Industries ABM Industries Incorporated (NYSE: ABM) Market Capitalization as of November 18, 2022: $2.98 billion ABM Industries Incorporated (NYSE:ABM) was incorporated in 1985 and is based in New York, New York. 25 Table 16: Case of a reading comprehension text in law domain. Certain portions are omitted for brevity and are represented as (...). Here is the first part of an article about law: The district court ordered Arledge to pay restitution in the amount of $5,829,334.90, without interest, to the Settlement Fund pursuant to the Mandatory Victims Restitution",
    "148": "Act of 1996 (\u201dMVRA\u201d), 18 U.S.C. \u00a73663A. Arledge disputes the calculation used to determine the amount of loss, claiming that the government did not sufficiently prove that the Settlement Fund had paid false claims arising from Arledge\u2019s illegal conduct. Specifically, he objects to the calculation of losses related to the Fen Phen II settlement. based in New York, New York. 25 Table 16: Case of a reading comprehension text in law domain. Certain portions are omitted for brevity and are represented as (...). Here is the first part of an article about law: The district court ordered Arledge to pay restitution in the amount of $5,829,334.90, without interest, to the Settlement Fund pursuant to the Mandatory Victims Restitution Act of 1996 (\u201dMVRA\u201d), 18 U.S.C. \u00a73663A. Arledge disputes the calculation used to determine the amount of loss, claiming that the government did not sufficiently prove that the Settlement Fund had paid false claims arising from Arledge\u2019s illegal conduct. Specifically, he objects to the calculation of losses related to the Fen Phen II settlement. The \u201cgeneral rule is that a district court can award restitution to victims of the offense, but the restitution award can encompass only those losses that resulted directly from the offense for which the defendant was convicted.\u201d United States v. Maturin, 488 F.3d 657, 660-61 (5th Cir. 2007) (citing Hughey v. United States, 495 U.S. 411, 413, 110 S.Ct. 1979, 109 L.Ed.2d 408 (1990)). The pre-sentence report attributed forty-seven fraudulent claims to the offenses for which Arledge was convicted. There were three categories of evidence used to substantiate the government\u2019s assertion that these claims resulted from Arledge\u2019s illegal conduct: (1) the testimony of Wyatt, an employee of S&A, who created fraudulent documents; (...) Please complete it: We have stated repeatedly that an order of restitution must be limited to losses caused by the specific conduct underlying the offense of conviction. See United States v. Griffin, 324 F.3d 330, 367 (5th Cir.2003) (holding that restitution is restricted to the limits of the offense); Tencer, 107 F.3d at 1135-36. Accordingly, we decline the government\u2019s request to adopt a harmless error analysis for the calculation of loss under the MVRA, and we remand for a re calculation of actual loss based upon the evidence in the record. Then, answer the following questions based on the whole article: Based on the statement Arledge makes no convincing argument regarding why the identification by AHP is not sufficient or how the district court abused its discretion in relying upon the represen- tations by AHP., provide a continuation using the word \u201cHence\u201d to express the following idea. It was not an abuse of discretion for the court to award restitution that encompassed those losses re- Tencer, 107 F.3d at 1135-36. Accordingly, we decline the government\u2019s request to adopt a harmless error analysis for the calculation of loss under the MVRA, and we remand for a re calculation of actual loss based upon the evidence in the record. Then, answer the following questions based on the whole article: Based on the statement Arledge makes no convincing argument regarding why the identification by AHP is not sufficient or how the district court abused its discretion in relying upon the represen- tations by AHP., provide a continuation using the word \u201cHence\u201d to express the following idea. It was not an abuse of discretion for the court to award restitution that encompassed those losses re- sulting from the creation of fraudulent documents in furtherance of the scheme to defraud for which Arledge was convicted. (...) What happens before \u201cThe government asks this court to find that the failure to provide proof of these three claims was harmless error.\u201d? It also asserts that it would be able to provide, on remand, proof of an additional false claim that was not originally included in the district court\u2019s restitution order, which resulted in a loss of $488,000. In opposition to the sentence \u201cThus, it was not an abuse of discretion for the court to award restitution that encompassed those losses resulting from the creation of fraudulent documents in furtherance of the scheme to defraud for which Arledge was convicted.\u201d, create a sentence with a contradictory meaning. There are three claims for which the government concedes that there was no proof of falsity: Florestine Baker ($18,000), Shirley E. Produce a long descriptive sentence about law that uses all these words: prescription, manufactured, fraudulent. There were three categories of evidence used to substantiate the government\u2019s assertion that these claims resulted from Arledge\u2019s illegal conduct: (1) the testimony of Wyatt, an employee of S&A, who created fraudulent documents; (2) the testimony of two pharmacists who testified that specific prescriptions allegedly from their pharmacies were, in fact, manufactured; and (3) represen- tations by AHP that the claims were fraudulent. Produce a long descriptive sentence about law that uses all these words: restitution, fraudulent, fur- therance. Thus, it was not an abuse of discretion for the court to award restitution that encompassed those losses resulting from the creation of fraudulent documents in furtherance of the scheme to fraudulent. There were three categories of evidence used to substantiate the government\u2019s assertion that these claims resulted from Arledge\u2019s illegal conduct: (1) the testimony of Wyatt, an employee of S&A, who created fraudulent documents; (2) the testimony of two pharmacists who testified that specific prescriptions allegedly from their pharmacies were, in fact, manufactured; and (3) represen- tations by AHP that the claims were fraudulent. Produce a long descriptive sentence about law that uses all these words: restitution, fraudulent, fur- therance. Thus, it was not an abuse of discretion for the court to award restitution that encompassed those losses resulting from the creation of fraudulent documents in furtherance of the scheme to defraud for which Arledge was convicted. What was this article about? Amount of Loss 26 ANOVERVIEW ON LANGUAGE MODELS : RECENT DEVELOPMENTS AND OUTLOOK Chengwei Wei1, Yun-Cheng Wang1, Bin Wang2, and C.-C. Jay Kuo1 1University of Southern California, Los Angeles, California, USA 2National University of Singapore, Singapore chengwei@usc.edu ABSTRACT Language modeling studies the probability distributions over strings of texts. It is one of the most fundamental tasks in natural language processing (NLP). It has been widely used in text generation, speech recognition, machine translation, etc. Conventional language models (CLMs) aim to predict the probability of linguistic sequences in a causal manner. In contrast, pre-trained language models (PLMs) cover broader concepts and can be used in both causal sequential modeling and \ufb01ne-tuning for downstream applications. PLMs have their own training paradigms (usually self-supervised) and serve as foundation models in modern NLP systems. This overview paper provides an introduction to both CLMs and PLMs from \ufb01ve aspects, i.e., linguistic units, structures, training methods, evaluation methods, and applications. Furthermore, we discuss the relationship between CLMs and PLMs and shed light on the future directions of language modeling in the pre-trained era. Keywords Language model, Natural language processing, Pre-trained language model, Conventional language model. 1 Introduction Language modeling studies the probability distributions over a sequence of words. It is one of the most fundamental tasks and long-standing research topics in natural language processing (NLP). The developed language models (LMs) \ufb01nd applications in many computational linguistic problems such as text generation, machine translation, speech recognition, natural language generation, question-and-answer systems, etc. There are two major approaches to language modeling: 1) the statistical approach based on a",
    "149": "relatively small corpus set, and 2) the data-driven approach based on a signi\ufb01cantly larger corpus set. Conventional language models (CLMs) predict the probability of linguistic sequences in a causal manner. They can be learned by both language modeling approaches. The data-driven approach has become mainstream nowadays. It exploits a large number of corpora to train neural-network models, leading to pre-trained language models (PLMs). PLMs are then \ufb01ne-tuned with task-speci\ufb01c datasets and objectives for downstream applications. In this paper, we provide an overview of CLMs and PLMs and recognition, natural language generation, question-and-answer systems, etc. There are two major approaches to language modeling: 1) the statistical approach based on a relatively small corpus set, and 2) the data-driven approach based on a signi\ufb01cantly larger corpus set. Conventional language models (CLMs) predict the probability of linguistic sequences in a causal manner. They can be learned by both language modeling approaches. The data-driven approach has become mainstream nowadays. It exploits a large number of corpora to train neural-network models, leading to pre-trained language models (PLMs). PLMs are then \ufb01ne-tuned with task-speci\ufb01c datasets and objectives for downstream applications. In this paper, we provide an overview of CLMs and PLMs and study them from \ufb01ve perspectives: 1) linguistic units, 2) structures, 3) training methods, 4) evaluation methods, and 5) applications. In the end, we point out several future research directions. The goal of CLMs is to model the probability distributions over sequences of linguistic units: P(u1;u2;\u0001\u0001\u0001;ut); (1) whereuican be either a character, a word, a phrase, or other linguistic units. CLMs attempt to predict the next linguistic unit in a text sequence given its preceding contexts: P(utju<t) (2) CLMs are also called auto-regressive language models since the units are predicted in a causal way. Estimating the probability of a text sequence as shown in Eq. (1) directly encounters the data sparsity problem. CLMs often estimatearXiv:2303.05759v1 [cs.CL] 10 Mar 2023 Overview of Language Models the joint probability of the text sequence by decomposing a text sequence into smaller units. For example, CLMs leverage the chain rule and the conditional probability to estimate the joint probability in the form of P(u1;u2;\u0001\u0001\u0001;ut) =P(u1)P(u2ju1)P(u3ju1;u2)\u0001\u0001\u0001P(utju1;:::ut\u00001): (3) CLMs are often trained from scratch with a training corpus and, then, predict the probability of text sequences with respective applications. Representative models include N-grams LMs [ 1,2,3], exponential LMs [ 4,5,6] and earlier neural LMs [ 7,8]. CLMs give a high probability to natural text sequences occurring frequently in the real world. As a result, they play a fundamental role in text generation, speech recognition [ 9,10,11], and machine translation [ 12,13,14] until the emergence of PLMs. Nowadays, high-performance PLMs serve as the backbone of leverage the chain rule and the conditional probability to estimate the joint probability in the form of P(u1;u2;\u0001\u0001\u0001;ut) =P(u1)P(u2ju1)P(u3ju1;u2)\u0001\u0001\u0001P(utju1;:::ut\u00001): (3) CLMs are often trained from scratch with a training corpus and, then, predict the probability of text sequences with respective applications. Representative models include N-grams LMs [ 1,2,3], exponential LMs [ 4,5,6] and earlier neural LMs [ 7,8]. CLMs give a high probability to natural text sequences occurring frequently in the real world. As a result, they play a fundamental role in text generation, speech recognition [ 9,10,11], and machine translation [ 12,13,14] until the emergence of PLMs. Nowadays, high-performance PLMs serve as the backbone of many NLP systems. They are not limited to the causal predictive functionality of CLMs and provide more different types of LMs. The differences between CLMs and PLMs can be summarized below. \u2022Training Methodology. With the development of deep learning, PLMs with neural network structures are pre-trained by collections of massive unlabeled corpora to learn generic knowledge which is then transferred to downstream tasks by task-speci\ufb01c \ufb01ne-tuning. \u2022Causality Constraint. PLMs do not necessarily follow CLMs in predicting linguistic units as shown in Eq. (2). For example, bidirectional LMs [ 15,16] use both preceding and succeeding contexts to predict the missing linguistic units via probability estimation: P(utju<t;u>t): (4) Bidirectional LMs do not follow the causality constraint and the chain rule in Eq. (3), to access the probability of a text sequence, which makes it inherently different from CLMs. \u2022Token Representation. Apart from the differences in the training paradigm and probability modeling, PLMs adopt a different representation for basic units called tokens. PLMs represent tokens by embedding them in a high-dimensional continuous space such as word embeddings [ 17,18] and sentence embeddings [ 19,20,21]. The new representations offer a \ufb02exible and powerful tool that enables PLMs to handle a wide range of tasks. This overview paper serves two objectives. On one hand, instead of only focusing on recently developed PLMs [22,23,24], we aim to provide a comprehensive overview of the basic concepts of LMs, the transition from CLMs to PLMs, LM\u2019s recent developments and applications to beginners in the \ufb01eld. On the other hand, we would like to shed \u2022Token Representation. Apart from the differences in the training paradigm and probability modeling, PLMs adopt a different representation for basic units called tokens. PLMs represent tokens by embedding them in a high-dimensional continuous space such as word embeddings [ 17,18] and sentence embeddings [ 19,20,21]. The new representations offer a \ufb02exible and powerful tool that enables PLMs to handle a wide range of tasks. This overview paper serves two objectives. On one hand, instead of only focusing on recently developed PLMs [22,23,24], we aim to provide a comprehensive overview of the basic concepts of LMs, the transition from CLMs to PLMs, LM\u2019s recent developments and applications to beginners in the \ufb01eld. On the other hand, we would like to shed light on future research directions and offer our outlook to experienced engineers and researchers in the NLP \ufb01eld. For example, we cover large LMs (LLMs) in the survey as there are growing interests in LLMs due to the new services provided by ChatGPT. Furthermore, we include ef\ufb01cient LMs as an emerging topic since there are increasing concerns about large model sizes and high training costs of LLMs. The rest of the paper is organized as below. We introduce several types of LMs that go beyond CLMs in Sec. 2, and provide an overview of common ways to decompose text sequences into smaller linguistic units in Sec. 3. Sec. 4 introduces different model structures. We discuss the training procedures of LMs in Sec. 5. Common evaluation methods including, both intrinsic and extrinsic ones, are introduced in Sec. 6. The application of LMs to text generation is discussed in Sec. 7. We comment on the redundancy problem of LMs and analyze techniques for ef\ufb01cient LMs in Sec. 8. Promising future research directions are pointed out in Sec. 9. Concluding remarks are given in Sec. 10 2 Types of Language Models CLMs commonly refer to auto-regressive models that predict the next linguistic units given the preceding context as shown in Eq. (2). LMs can access the probability of a text sequence using the chain rule. The goal of CLMs is to decode the probability of text sequences in a causal manner. In this section, we introduce more LMs that go beyond CLMs. 2.1 Structural LM Instead of predicting linguistic units in a sequential or reversed sequential order, structural LMs [ 25,26,27,28,29] predict linguistic units based on pre-de\ufb01ned linguistic structures such as dependency or constituent parse trees. Structural Sec. 8. Promising future research directions are pointed out in Sec. 9. Concluding remarks are given in",
    "150": "Sec. 10 2 Types of Language Models CLMs commonly refer to auto-regressive models that predict the next linguistic units given the preceding context as shown in Eq. (2). LMs can access the probability of a text sequence using the chain rule. The goal of CLMs is to decode the probability of text sequences in a causal manner. In this section, we introduce more LMs that go beyond CLMs. 2.1 Structural LM Instead of predicting linguistic units in a sequential or reversed sequential order, structural LMs [ 25,26,27,28,29] predict linguistic units based on pre-de\ufb01ned linguistic structures such as dependency or constituent parse trees. Structural LMs utilize the linguistic structure to bring linguistically relevant context closer to the linguistic unit to be predicted. For example, given a parse tree structure, a structural LM can de\ufb01ne the ancestor context A(ut)ofutas the sequence from the root node to the parent of ut. For example, the ancestor sequence of word \u2018strong\u2019 is {\u2018binoculars\u2019, \u2018saw\u2019, ROOT} in Fig. 1. Then, the structural LM uses the ancestor context in the tree to predict the next linguistic unit as P(utjA(ut)); (5) 2 Overview of Language Models Figure 1: The example of a dependency parse tree example [28]. Figure 2: The use of different permutations in a natural sentence. whereA(ut)is the ancestor context of linguistic unit ut. Similar to CLMs, structural LMs are designed to model the probability of text sequences. Differently, structure LMs decode the sequence probability in the order of their synthetic structures. It has been successfully applied to sentence completion [27, 28] and speech recognition [25, 26]. 2.2 Bidirectional LM Instead of using the causal contexts to make predictions, bidirectional LMs utilize contexts from both directions as shown in Eq. (4). The masked LM is one representative bidirectional LM. It masks out linguistic units in a text sequence and, then, encodes their preceding and succeeding contexts to predict the masked linguistic units. Formally, the prediction can be de\ufb01ned as the estimation of the following conditional probability P(umj\u0016S); (6) whereumis the masked linguistic unit and \u0016Sis the corrupted text sequence by replacing a certain number of linguistic units with [MASK] symbols. The goal of bidirectional LMs is to learn the inner dependency between linguistic units 2.2 Bidirectional LM Instead of using the causal contexts to make predictions, bidirectional LMs utilize contexts from both directions as shown in Eq. (4). The masked LM is one representative bidirectional LM. It masks out linguistic units in a text sequence and, then, encodes their preceding and succeeding contexts to predict the masked linguistic units. Formally, the prediction can be de\ufb01ned as the estimation of the following conditional probability P(umj\u0016S); (6) whereumis the masked linguistic unit and \u0016Sis the corrupted text sequence by replacing a certain number of linguistic units with [MASK] symbols. The goal of bidirectional LMs is to learn the inner dependency between linguistic units in an unsupervised manner. The trained model can inherit semantics meanings from large-scale unlabeled corpora. Different from CLMs that aim to model the generation probability of text sequences, pre-trained bidirectional LMs are used as the backbone that transfers the learned knowledge through further \ufb01ne-tuning in various downstream applications. 2.3 Permutation LM CLMs and masked LMs have their own advantages and disadvantages. A masked LM needs to create arti\ufb01cial tokens such as [mask] , which never occur in downstream tasks while CLMs only condition on preceding context. The permutation LM [ 30] is a recently proposed LM that takes advantage of CLMs and masked LMs. Given an input sequence of linguistic units, permutation LMs randomize the order of input linguistic units and construct different permutations of the input sequence. Fig. 2 shows an example of different permutations given an input text sequence. LetZbe the set of all possible permutations. Permutation LMs predict the next linguistic unit, ut, in one permutation, Z, of the sequence based on P(utjuZ <t);Z2Z: (7) 3 Overview of Language Models 3 Linguistic Units To estimate the probability of text sequences, LMs partition text sequences into small linguistic units such as characters, words, phrases, or sentences. This process is called tokenization. Different languages and models may have different appropriate tokenization methods. Here, we focus on English and use it as an example. In this section, we examine typical tokenization methods used in language modeling according to unit sizes. 3.1 Characters LMs can model text sequences probability based on characters [ 31,32,33,34,35]. As compared with other linguistics LetZbe the set of all possible permutations. Permutation LMs predict the next linguistic unit, ut, in one permutation, Z, of the sequence based on P(utjuZ <t);Z2Z: (7) 3 Overview of Language Models 3 Linguistic Units To estimate the probability of text sequences, LMs partition text sequences into small linguistic units such as characters, words, phrases, or sentences. This process is called tokenization. Different languages and models may have different appropriate tokenization methods. Here, we focus on English and use it as an example. In this section, we examine typical tokenization methods used in language modeling according to unit sizes. 3.1 Characters LMs can model text sequences probability based on characters [ 31,32,33,34,35]. As compared with other linguistics units, using characters has a much smaller vocabulary size, leading to a smaller discrete space and model size. On the other hand, it is challenging to predict the next character. Usually, it requires a long historical context. This makes the performance of character-level LMs poorer than that of word-level LMs. In addition, the input and output lengths have to be longer to model the character distribution accurately. This results in higher computational costs, especially for auto-regressive decoding. Several LM methods use the combination of words and characters to alleviate the issue [36, 37, 38]. 3.2 Words and Subwords The most natural tokenization for English is to decompose a text sequence into words by white spaces. Many LMs apply word tokenization. However, there are several issues of naive word tokenization. The \ufb01rst one is the Out-Of-V ocabulary (OOV) problem. Because an LM has a pre-de\ufb01ned vocabulary size that cannot be arbitrarily large. Less frequent words and words with character-level errors may not be stored in the pre-de\ufb01ned vocabulary. Thus, they cannot be retrieved from the dictionary. Although one can extend the vocabulary size to alleviate this problem, it will increase the model size and still cannot handle all possible words. LMs beyond the word level still have the OOV problem while a single character is not semantically meaningful by themselves. Recently, researchers are in favor of decomposing words into subwords if they do not appear in the dictionary. This offers a \ufb02exible and effective solution to the OOV problem [ 39,40]. Several subword segmentation algorithms are developed to boost the performance of LMs. They strike a balance between the good performance of and words with character-level errors may not be stored in the pre-de\ufb01ned vocabulary. Thus, they cannot be retrieved from the dictionary. Although one can extend the vocabulary size to alleviate this problem, it will increase the model size and still cannot handle all possible words. LMs beyond the word level still have the OOV problem while a single character is not semantically meaningful by themselves. Recently, researchers are in favor of decomposing words into subwords if they do not appear in the dictionary. This offers a \ufb02exible and effective solution to the OOV problem [ 39,40]. Several subword segmentation algorithms are developed to boost the performance of LMs. They strike a balance between the good performance of",
    "151": "word-level models and the \ufb02exibility of character-level models. Two subword segmentation approaches, statistics-based and linguistics-based, are presented below. 3.2.1 Statistics-based Subword Tokenizers The statistics-based subword tokenizers generate subword vocabulary purely based on the corpus. The associated methods are derived from a compression point of view. They work by replacing the commonly appeared character sequences with a new symbol (word) that does not exist in the current vocabulary. Then, fewer bytes are needed for information transmission. Byte Pair Encoding (BPE). BPE [ 41] is a simple data compression technique that replaces the most common pair of bytes in a sequence by a single unused byte recursively. It was adopted by [ 40] to solve the word segmentation problem. That is, frequent characters or character sequences are merged to generate subwords. BPE is also used by several advanced PLMs such as GPT-2 [ 42] and RoBERTa [ 16] with the following algorithm, called the BPE merge operation. 1. Prepare a training corpus and de\ufb01ne the size of the subword vocabulary. 2. Split all words into characters. 3. Generate a new subword by merging a pair of characters or subwords with the highest frequency. 4. Repeat step 3 until the desired vocabulary size is reached. An illustration of the BPE merge operation conducted on a small dictionary is given in Fig. 3. WordPiece. [43] WordPiece is another data-driven subword algorithm. The difference between WordPiece and BPE is that WordPiece merges the pair of AandBif they have the highest score P(AB)=P(A)P(B)(rather than the highest frequencyP(AB)) at each iterative step. For example, WordPiece merges the pair of \u201cu\u201d and \u201cg\u201d in Fig. 3 only if they 1. Prepare a training corpus and de\ufb01ne the size of the subword vocabulary. 2. Split all words into characters. 3. Generate a new subword by merging a pair of characters or subwords with the highest frequency. 4. Repeat step 3 until the desired vocabulary size is reached. An illustration of the BPE merge operation conducted on a small dictionary is given in Fig. 3. WordPiece. [43] WordPiece is another data-driven subword algorithm. The difference between WordPiece and BPE is that WordPiece merges the pair of AandBif they have the highest score P(AB)=P(A)P(B)(rather than the highest frequencyP(AB)) at each iterative step. For example, WordPiece merges the pair of \u201cu\u201d and \u201cg\u201d in Fig. 3 only if they have the highest value, P(0ug0)=P(0u0)P(0g0), as compared with other pairs. WordPiece is used as the tokenization method in BERT [15], DistilBERT [44], and Electra [45]. There are other statistics-based subword tokenizers such as Unigram [46]. Different subword tokenizers and their performance comparison are studied in [47]. 4 Overview of Language Models Figure 3: Illustration of the BPE merge operation conducted on the dictionary {\u201chug\", \u201cpug\", \u201cpun\", \u201cbun\"}. The vocabulary is initialized with all characters. Then, a new subword is created by merging the most frequent pair. 3.2.2 Linguistics-based Subword Tokenizers Linguistics-based subword tokenizers exploit the linguistic knowledge and decompose words into smaller grammatical units, such as morphemes or syllables. Such subword tokenizers are widely used in machine translation and speech recognition among different languages [ 48,49,50,51,52,53,54]. For example, in machine translation, words formed by compounding, af\ufb01xation, or in\ufb02ection, can be conveniently translated by translating the morphemes, respectively. However, linguistics-based subword tokenizers are not as popular as statistics-based ones due to the complexity and the rule-based nature of language decomposition. 3.3 Phrases The semantic meaning of a single word can be ambiguous because of various contexts and set collocations. Since the linguistic dictionary does not go beyond the word-level, the inter-word dependency is ignored. Phrase-level LMs replace common and cohesive word sequences by phrases [ 55,56,57,58]. Phrase-level LMs are suitable for some applications. For example, it is observed in [ 57] that short words with fewer syllables in automatic speech recognition by compounding, af\ufb01xation, or in\ufb02ection, can be conveniently translated by translating the morphemes, respectively. However, linguistics-based subword tokenizers are not as popular as statistics-based ones due to the complexity and the rule-based nature of language decomposition. 3.3 Phrases The semantic meaning of a single word can be ambiguous because of various contexts and set collocations. Since the linguistic dictionary does not go beyond the word-level, the inter-word dependency is ignored. Phrase-level LMs replace common and cohesive word sequences by phrases [ 55,56,57,58]. Phrase-level LMs are suitable for some applications. For example, it is observed in [ 57] that short words with fewer syllables in automatic speech recognition (ASR) are more frequently misrecognized than longer ones. Since phrases provide longer phone sequences than their constituents, they are more robust to recognition errors for ASR. 3.4 Sentences LMs with smaller linguistic units (e.g., characters, words, subwords, and phrases) rely on conditional probabilities to estimate the probability of text sequences as given in Eq. (3). Sentence-level LMs [ 59,60,61,62,63] avoid the use of the chain rule. They generate sentence features and, then, model the sentence probability directly. This is because modeling the sentence probability directly is more convenient than that in Eq. (3) in encoding the sentence-level information. It is also easier to encode the inter-sentence information such as the effects of preceding utterances in a dialog \ufb02ow. 4 Model Structures In this section, we conduct a survey on several common structures to model the probability distributions of text sequences. They are N-gram, maximum entropy, and neural network models. PLMs typically use continuous representations in probability modeling built upon recurrent neural networks (RNNs) or transformers. 4.1 N-gram Models An N-gram consists of N consecutive linguistic units from a text sequence. N-gram LMs [ 1,2,3] assume that the probability of a word depends only on its preceding N-1 linguistic units and it is independent of other contexts. This is known as the Markov assumption. Thus, instead of using all historical contexts, N-gram LMs only use the previous N-1 linguistic units to predict the current one; namely, P(wtjw<t) =P(wtjwt\u0000N+1:t\u00001): (8) 5 Overview of Language Models N-gram LMs calculate the conditional probability by counting the occurrence time of N-grams given a training corpus as They are N-gram, maximum entropy, and neural network models. PLMs typically use continuous representations in probability modeling built upon recurrent neural networks (RNNs) or transformers. 4.1 N-gram Models An N-gram consists of N consecutive linguistic units from a text sequence. N-gram LMs [ 1,2,3] assume that the probability of a word depends only on its preceding N-1 linguistic units and it is independent of other contexts. This is known as the Markov assumption. Thus, instead of using all historical contexts, N-gram LMs only use the previous N-1 linguistic units to predict the current one; namely, P(wtjw<t) =P(wtjwt\u0000N+1:t\u00001): (8) 5 Overview of Language Models N-gram LMs calculate the conditional probability by counting the occurrence time of N-grams given a training corpus as P(wtjwt\u0000N+1:t\u00001) =C(wt\u0000N+1:t) C(wt\u0000N+1:t\u00001): (9) N-gram LMs simplify the word probability calculation based on previous N-1 words, but they encounter two sparsity issues. First, if an N-gram, (wt\u0000N+1:t), never occurs in the training corpus, the probability for the next word being wt is zero. Second, if the (N-1)-gram, (wt\u0000N+1:t\u00001), in the denominator never occurs, we cannot calculate the probability of any word. These sparsity issues can be alleviated by smoothing techniques. A simple smoothing method [ 64,65], called additive smoothing, is to add a small value",
    "152": "to the count for every N-gram so as to avoid zero in the numerator and the denominator in Eq. (9). However, this simple smoothing is still de\ufb01cient because it assigns the same probability for N-grams that never occur in the training corpus. There are more advanced smoothing techniques such as back-off and interpolation [ 66,67,68,69,70] that achieve better probability estimation. In back-off, lower-order N-grams are used for probability estimation if higher-order N-grams do not occur. For example, if C(wt\u00003:t\u00001) = 0 , we back off to compute P(wtjwt\u00002:t\u00001). In interpolation, different N-grams are considered for conditional probability computation. Mathematically, the N-gram probability is estimated by P(wtjwt\u0000N+1:t\u00001) =\u0015NP(wtjwt\u0000N+1:t\u00001) +\u0015N\u00001P(wtjwt\u0000N:t\u00001) +\u0015N\u00002P(wtjwt\u0000N\u00001:t\u00001) +:::+\u00151P(wt);(10) where\u0015iis the weight for each n-gram andPN i=1\u0015i= 1. 4.2 Maximum Entropy Models Maximum Entropy models (also called the exponential models) [ 4,5,6] estimate the probability of text sequences using feature functions in the form of P(wjh) =exp(aTf(w;h))P w0exp(aTf(w0;h)); (11) better probability estimation. In back-off, lower-order N-grams are used for probability estimation if higher-order N-grams do not occur. For example, if C(wt\u00003:t\u00001) = 0 , we back off to compute P(wtjwt\u00002:t\u00001). In interpolation, different N-grams are considered for conditional probability computation. Mathematically, the N-gram probability is estimated by P(wtjwt\u0000N+1:t\u00001) =\u0015NP(wtjwt\u0000N+1:t\u00001) +\u0015N\u00001P(wtjwt\u0000N:t\u00001) +\u0015N\u00002P(wtjwt\u0000N\u00001:t\u00001) +:::+\u00151P(wt);(10) where\u0015iis the weight for each n-gram andPN i=1\u0015i= 1. 4.2 Maximum Entropy Models Maximum Entropy models (also called the exponential models) [ 4,5,6] estimate the probability of text sequences using feature functions in the form of P(wjh) =exp(aTf(w;h))P w0exp(aTf(w0;h)); (11) wheref(w;h)is the feature function that generates the feature of word wand its historical context h,P w0exp(aTf(w0;h))is a normalization factor, and ais a parameter vector derived by the Generalized Iterative Scaling algorithm [71]. The features are usually generated from the N-grams. 4.3 Feed-forward Neural Network (FNN) Models The discrete nature of the N-gram model is its performance bottleneck even with advanced smoothing techniques. Neural LMs embrace the continuous embedding space (distributed representation of words) to overcome the data sparsity problem. Feed-forward Neural Network (FNN) LMs [ 7,72,73,74] is one of the earlier neural network models. An FNN LM takes historical contexts as the input, and outputs the probability distribution of words. As shown in Fig. 4, each word in the preceding context is represented as a vector through a projection layer (i.e., an embedding matrix). These word vectors are sent to the hidden layer with Hhidden units followed by non-linear activation. Then, a softmax function is used to obtain the posterior probabilities for word candidates, P(wj=ijhj), which are the probabilities of words given a speci\ufb01c history predicted by the language model. An FNN LM uses a \ufb01xed window to collect \ufb01xed-length contexts. It is essentially a neural version of N-gram LMs. The FNN LM have several advantages over the N-gram LM by projecting words into continuous space. First, it can handle unseen N-grams by representing each word as an N-gram with a dense vector space. Second, it is storage-ef\ufb01cient since it does not need to count and store the transition probability of conventional N-gram models. 4.4 Recurrent Neural Network (RNN) Models These word vectors are sent to the hidden layer with Hhidden units followed by non-linear activation. Then, a softmax function is used to obtain the posterior probabilities for word candidates, P(wj=ijhj), which are the probabilities of words given a speci\ufb01c history predicted by the language model. An FNN LM uses a \ufb01xed window to collect \ufb01xed-length contexts. It is essentially a neural version of N-gram LMs. The FNN LM have several advantages over the N-gram LM by projecting words into continuous space. First, it can handle unseen N-grams by representing each word as an N-gram with a dense vector space. Second, it is storage-ef\ufb01cient since it does not need to count and store the transition probability of conventional N-gram models. 4.4 Recurrent Neural Network (RNN) Models It is clearly insuf\ufb01cient to use the historical context in a \ufb01xed-length to predict the next word. In contrast to the limited historical context used in the N-gram and FNN LMs, Recurrent Neural Network (RNN) LMs [ 8,75,76,77,78] can exploit arbitrarily long histories to predict the next word. The structure of a vanilla RNN LM is shown in Fig. 5. A word x(i)in positioniis \ufb01rst converted into a one-hot representation ^x(i). Then, the recurrent hidden state, h(i+ 1) , is computed using the previous hidden state, h(i), and the one-hot representation, ^x(i), of wordx(i)as h(i+ 1) =f(W^x(i) +Uh(i)); (12) 6 Overview of Language Models Figure 4: The structure of FFN LMs, where hjdenotes the preceding contexts wj\u0000n+1;:::;w j\u00001in a \ufb01xed-window, andP,H, andNare the dimensions of the projection, the hidden layer, and the output layer, respectively [72]. wheref(\u0001)is a non-linear activation function, Wis the weight matrix of the connections from the input layer to the hidden layer, and Uis the connection between the previous and current hidden layers, respectively. By iteratively computing the hidden states, RNN LMs can encode the historical context of varying length. Finally, the output layer gives the conditional probability of words y(t) =g(Vh(t)), whereVis the weight matrix connecting the hidden layer and the output layer and g(\u0001)is the softmax activation function. Figure 5: The structure of RNN LMs [79]. In theory, RNN LMs do not need the Markov assumption. They can use all preceding history to predict the next word. However, the inherent gradient vanishing problem of RNN hampers the learning of the model [ 80]. Since the wheref(\u0001)is a non-linear activation function, Wis the weight matrix of the connections from the input layer to the hidden layer, and Uis the connection between the previous and current hidden layers, respectively. By iteratively computing the hidden states, RNN LMs can encode the historical context of varying length. Finally, the output layer gives the conditional probability of words y(t) =g(Vh(t)), whereVis the weight matrix connecting the hidden layer and the output layer and g(\u0001)is the softmax activation function. Figure 5: The structure of RNN LMs [79]. In theory, RNN LMs do not need the Markov assumption. They can use all preceding history to predict the next word. However, the inherent gradient vanishing problem of RNN hampers the learning of the model [ 80]. Since the gradient may become very small over a long distance, model weights are actually updated by the nearby context in practice. Generally, RNN LMs cannot learn the dependency between the current word and its far-away historical context. Although an attention mechanism can be introduced to RNNs to alleviate this problem [ 81,79]. The inherent sequential nature of RNNs makes them less powerful than transformer-based LMs with a self-attention mechanism. 7 Overview of Language Models Figure 6: The structure of a transformer [82]. 4.5 Transformers The transformer architecture [ 82] can capture long-term dependencies and important sequence components by exploiting a self-attention mechanism. Unlike the recurrent structure of RNNs, a transformer is easy to parallelize in both training and inference. Its structure is shown in Fig. 6. It consists of an encoder and a decoder. Before being sent to the encoder, the input textual sequence is \ufb01rst converted to an embedding through an embedding layer plus positional embedding. Multi-head attention, which is an ensemble of multiple self-attention mechanisms, enables the transformer to capture more robust and diverse attention between tokens. The other parts in the transformer encoder include feed-forward layers, residual",
    "153": "connections, and normalization layers. The difference between the transformer encoder and decoder is that the transformer decoder has an additional masked multi-head attention layer. The masking ensures the decoder can only access preceding tokens of the current one, which makes the decoder auto-regressive. Based on different purposes, transformers have encoder-only, decoder-only, and encoder-decoder three variants as the input textual sequence is \ufb01rst converted to an embedding through an embedding layer plus positional embedding. Multi-head attention, which is an ensemble of multiple self-attention mechanisms, enables the transformer to capture more robust and diverse attention between tokens. The other parts in the transformer encoder include feed-forward layers, residual connections, and normalization layers. The difference between the transformer encoder and decoder is that the transformer decoder has an additional masked multi-head attention layer. The masking ensures the decoder can only access preceding tokens of the current one, which makes the decoder auto-regressive. Based on different purposes, transformers have encoder-only, decoder-only, and encoder-decoder three variants as shown in Table 1 and Fig. 7. Encoder-only models can access all positions given an input and utilize bi-directional contexts to predict words. They are suitable for tasks requiring understanding full sentences, such as text classi\ufb01cation. Transformer decoder-only models can only use previous words to predict the current word (namely, auto-regressive models). They are good at text generation tasks such as story generation. Transformer encoder-decoder models can access all words in the encoding phase, and words before the current word in the decoding phase. They are suitable for sequence-to-sequence tasks such as translation and summarization. 8 Overview of Language Models Table 1: Transformer-based PLMs. Encoder-only models (Bidirectional)BERT [15] RoBERTa [16] ELECTRA [45] Decoder-only models (Unidirectional)PaLM [83] GPT-1, 2 and 3 [84, 42, 85] Transformer XL [86] Encoder-Decoder models (Sequence to sequence)BART [87] T5 [88] Figure 7: Illustration of different transformer models, where BERT is the encoder-only model, GPT is the decoder-only model, and BART is the encoder-decoder model [87]. 5 Pre-trained Language Models Pre-trained language models (PLMs) are dominating in the NLP \ufb01eld nowadays. With the development of deep learning, the training and usage of PLMs have changed a lot as compared with conventional statistical LMs. Before being applied to real-world tasks, PLMs are \ufb01rst pre-trained on massive collections of corpora so that they learn universal representations that carry both syntactic and semantic knowledge. After pre-training, PLMs are \ufb01ne-tuned for downstream tasks so that the acquired knowledge can be transferred to different tasks. In the following, we \ufb01rst explain T5 [88] Figure 7: Illustration of different transformer models, where BERT is the encoder-only model, GPT is the decoder-only model, and BART is the encoder-decoder model [87]. 5 Pre-trained Language Models Pre-trained language models (PLMs) are dominating in the NLP \ufb01eld nowadays. With the development of deep learning, the training and usage of PLMs have changed a lot as compared with conventional statistical LMs. Before being applied to real-world tasks, PLMs are \ufb01rst pre-trained on massive collections of corpora so that they learn universal representations that carry both syntactic and semantic knowledge. After pre-training, PLMs are \ufb01ne-tuned for downstream tasks so that the acquired knowledge can be transferred to different tasks. In the following, we \ufb01rst explain the pre-training objectives in Sec. 5.1 and then talk about how to adapt PLMs to various tasks of interest through \ufb01ne-tuning in Sec. 5.2. It is also worthwhile to point out several good survey papers on PLMs, e.g., [22, 23, 24]. 5.1 Pre-training The most commonly used pre-training task is \u201cmissing word prediction\". There are other pre-training tasks for different purposes, e.g., next-sentence prediction, which allows an LM to learn sentence relationships. Word Prediction. Auto-aggressive language LMs are trained to predict the next word using previous words. While bidirectional LMs mask a subset of words in a sample and learn to predict such masked words using the rest of the context. For the latter, the most popular objective is the masked language model (MLM) objective as proposed in BERT [ 15]. The MLM objective is the cross-entropy loss in predicting masked tokens. It randomly masks out 15% of 9 Overview of Language Models Figure 8: An illustration of (a) LM pre-training, (b) standard \ufb01ne-tuning, and (c) prompt-based \ufb01ne-tuning (or prompt-tuning) [89]. the input tokens and then predicts the masked tokens. The number of masked tokens is set to 15% based on experimental veri\ufb01cation. If the masking rate is too small, the model only learns from a limited number of masked tokens. On the other hand, if it is too large, there is not enough context to do reasonable predictions and models cannot learn well. Other Pre-training Tasks. There are other pre-training tasks to make LMs learn better linguistic knowledge such as sentence relationships. For example, next sentence prediction is used as the pre-training task in BERT [ 15]. Next 9 Overview of Language Models Figure 8: An illustration of (a) LM pre-training, (b) standard \ufb01ne-tuning, and (c) prompt-based \ufb01ne-tuning (or prompt-tuning) [89]. the input tokens and then predicts the masked tokens. The number of masked tokens is set to 15% based on experimental veri\ufb01cation. If the masking rate is too small, the model only learns from a limited number of masked tokens. On the other hand, if it is too large, there is not enough context to do reasonable predictions and models cannot learn well. Other Pre-training Tasks. There are other pre-training tasks to make LMs learn better linguistic knowledge such as sentence relationships. For example, next sentence prediction is used as the pre-training task in BERT [ 15]. Next sentence prediction is formalized as a binary prediction task that decides whether two sentences are two consecutive sentences or not. In this way, a PLM can be used in downstream tasks that require the understanding of the relationship between two sentences, such as Question Answering (QA) and Natural Language Inference (NLI). Other pre-training objectives are adopted by BART [ 87]. They include token deletion, text in\ufb01lling, sentence permutation, and document rotation to corrupt the original sequence for reconstruction. Shuf\ufb02ed tokens are used in T5 [ 88] to increase the robustness of the learned representation. 5.2 Fine-Tuning and Prompt-Tuning PLMs learn non-task-speci\ufb01c language knowledge in the pre-training stage. Fine-tuning performs task-speci\ufb01c adaptations of the model so that they can be applied to different downstream tasks. The model parameters are updated in the \ufb01ne-tuning stage. One can choose to update the entire model, freeze the language model and update the task-speci\ufb01c heads, or only update certain layers of an LM. PLMs are pre-trained by one or several pre-training objectives and, then, applied to different downstream tasks. The gap between pre-training tasks and downstream task-speci\ufb01c \ufb01ne-tuning can be substantial. Prompt-tuning is used to discover the potential of PLMs by mimicking the pre-training objectives in the \ufb01ne-tuning or inference stage. As PLMs get more powerful, they can handle various downstream tasks by seeing a few examples without any gradient updates or \ufb01ne-tuning [ 85]. This is achieved by prompt-based \ufb01ne-tuning (or prompt-tuning in short). A prompt is a human-designed text that is concatenated with the original text as the new model input. Fig. 8 shows an heads, or only update certain layers of an LM. PLMs are pre-trained by one or several pre-training objectives and, then, applied to different downstream tasks. The gap",
    "154": "between pre-training tasks and downstream task-speci\ufb01c \ufb01ne-tuning can be substantial. Prompt-tuning is used to discover the potential of PLMs by mimicking the pre-training objectives in the \ufb01ne-tuning or inference stage. As PLMs get more powerful, they can handle various downstream tasks by seeing a few examples without any gradient updates or \ufb01ne-tuning [ 85]. This is achieved by prompt-based \ufb01ne-tuning (or prompt-tuning in short). A prompt is a human-designed text that is concatenated with the original text as the new model input. Fig. 8 shows an example of the pre-training task, \ufb01ne-tuning and prompt-tuning of MLMs. In the pre-training, the MLMs are trained to predict masked words. Assuming that the downstream task is the sentiment analysis of the movie review. In standard \ufb01ne-tuning, we train a new head on the top of a PLM and predict the sentiment labels. The original input appended with a designed prompt, say, \u2018It was\u2019, is sent to the PLM. The PLM has to assign probabilities to designed answers, which can be \u2018great\u2019 or \u2018terrible\u2019. If the probability of \u2018great\u2019 is higher, then the label of the input will be positive and vice versa. In this way, prompt-tuning converts a distinct downstream task to the word prediction task to narrow the gap between the pre-training and \ufb01ne-tuning stages. 6 Model Evaluation There are two LM evaluation types: intrinsic evaluation and extrinsic evaluation. The intrinsic evaluation examines the internal properties of an LM while the extrinsic evaluation studies its performance in downstream tasks. 10 Overview of Language Models 6.1 Intrinsic Evaluation Auto-regressive LM. LMs estimate the probability of text sequences. A good LM assigns higher probabilities to natural text sequences and lower ones to unreal or random text sequences. The perplexity is a common evaluation metric for this purpose. Given a testing text sequence, the perplexity, denoted by PPL , is de\ufb01ned as the inverse probability of the sequence normalized by the number of words. Mathematically, we have PPL (W) =Ns 1 (P(w1w2:::wN); (13) whereW=w1w2:::wNis a testing text sequence. The perplexity can be rewritten in form of PPL (W) =NvuutnY i=11 (P(wijw1:::wi\u00001): (14) A good LM should maximize the text set probability. It is equivalent to minimizing the perplexity. The lower the 10 Overview of Language Models 6.1 Intrinsic Evaluation Auto-regressive LM. LMs estimate the probability of text sequences. A good LM assigns higher probabilities to natural text sequences and lower ones to unreal or random text sequences. The perplexity is a common evaluation metric for this purpose. Given a testing text sequence, the perplexity, denoted by PPL , is de\ufb01ned as the inverse probability of the sequence normalized by the number of words. Mathematically, we have PPL (W) =Ns 1 (P(w1w2:::wN); (13) whereW=w1w2:::wNis a testing text sequence. The perplexity can be rewritten in form of PPL (W) =NvuutnY i=11 (P(wijw1:::wi\u00001): (14) A good LM should maximize the text set probability. It is equivalent to minimizing the perplexity. The lower the perplexity, the better the LM. Bidirectional Language Model. To calculate the inverse probability in Eq. (13), the auto-regressive LMs can use a sequence of conditional probabilities. However, this approach does not work for bidirectional LMs (or masked LMs). Several intrinsic evaluation metrics have been proposed for bidirectional LMs. The pseudo-log-likelihood score (PLL) [90] is de\ufb01ned as PLL (W) =jWjX t=1logP(wtjWnt); (15) where logP(wtjWnt)is the conditional probability of token win sentenceWwith all remaining tokens. Instead of maximizing the joint probability of the entire text sequence, a good bidirectional LM should maximize the probability of each token in the text sequence given other tokens. Based on PLLs, the pseudo-Perplexity (PPPL) for corpora Cis de\ufb01ned as [91] PPL (C) =exp(\u00001 NX W2CPLL (W)): (16) Both PLL and PPPL provide effective means to measure the naturalness of sentences for a bidirectional LM. For example, it was shown in [ 91] that PLL and PPPL correlate well with the performance of an LM on downstream tasks, such as automatic speech recognition and machine translation. 6.2 Extrinsic Evaluation Any downstream task of LMs can be used for extrinsic evaluation. There are several common downstream tasks selected as extrinsic evaluation benchmarks. Two popular ones are GLUE (General Language Understanding Evaluation) [ 92] and SuperGLUE [ 93]. GLU is an evaluation benchmark for natural language understanding. It contains single-sentence tasks, similarity and paraphrase tasks, and inference tasks. SuperGLUE is an enhanced version of GLUE. It includes a new set of more challenging language understanding tasks, more diverse task formats, improved resources, and a public leaderboard. example, it was shown in [ 91] that PLL and PPPL correlate well with the performance of an LM on downstream tasks, such as automatic speech recognition and machine translation. 6.2 Extrinsic Evaluation Any downstream task of LMs can be used for extrinsic evaluation. There are several common downstream tasks selected as extrinsic evaluation benchmarks. Two popular ones are GLUE (General Language Understanding Evaluation) [ 92] and SuperGLUE [ 93]. GLU is an evaluation benchmark for natural language understanding. It contains single-sentence tasks, similarity and paraphrase tasks, and inference tasks. SuperGLUE is an enhanced version of GLUE. It includes a new set of more challenging language understanding tasks, more diverse task formats, improved resources, and a public leaderboard. 6.3 Relation between Intrinsic and Extrinsic Evaluations If an LM achieves a lower perplexity, does that mean it can also perform well on downstream tasks? In other words, is there any correlation between pre-training tasks (based on word prediction) and the downstream tasks? There are many empirical studies on this question but few theoretical studies. Empirical Studies. Researchers design experiments to understand what kind of knowledge is learned by an LM from the pre-training tasks. Examples include [ 94,95,96,97,98,99]. They use part-of-speech tagging, constituent labeling, and dependency labeling to measure the degree of syntactic knowledge learning, and named entity labeling, semantic role labeling, and semantic proto-role for testing semantic knowledge. Empirical studies show that pre-training tasks help LMs learn the linguistic knowledge such as the grammar [ 97] and the semantic role [ 99]. However, these experimental results can only be used as evidence supporting that the word prediction tasks bene\ufb01t downstream tasks. They cannot explain the underlying mechanism. Theoretical Studies. Some researchers attempt to build the connection between LM\u2019s perplexities and its performance on downstream tasks mathematically. The text classi\ufb01cation tasks were studied in [ 100]. They \ufb01rst hypothesized and 11 Overview of Language Models veri\ufb01ed that text classi\ufb01cation tasks can be reformulated as sentence completion tasks. Since the LM pre-training task is essentially a sentence completion task, it does help the text classi\ufb01cation downstream task. Then, they quanti\ufb01ed the connection mathematically and showed that the features from LMs that achieve \u000f-optimal in log-perplexity can experimental results can only be used as evidence supporting that the word prediction tasks bene\ufb01t downstream tasks. They cannot explain the underlying mechanism. Theoretical Studies. Some researchers attempt to build the connection between LM\u2019s perplexities and its performance on downstream tasks mathematically. The text classi\ufb01cation tasks were studied in [ 100]. They \ufb01rst hypothesized and 11 Overview of Language Models veri\ufb01ed that text classi\ufb01cation tasks can be reformulated as sentence completion tasks. Since the LM pre-training task is essentially a sentence completion task, it does help the text classi\ufb01cation downstream task. Then, they quanti\ufb01ed the connection mathematically and",
    "155": "showed that the features from LMs that achieve \u000f-optimal in log-perplexity can linearly solve text classi\ufb01cation tasks with O(p\u000f)error. An underlying generative model was utilized in [ 101] to show the relationship between the pre-training tasks and the downstream tasks. Current theoretical studies are limited in the sense that only a speci\ufb01c downstream task (say, the text classi\ufb01cation task) is considered and the proof holds under certain conditions. 7 Applications in Text Generation One of the most important applications of LMs is text generation, which aims to generate sequences of words based on the input data. There are many text generation tasks because of different purposes and inputs. For example, the automatic speech recognition (ASR) task demands that the input be a speech sequence while the output be the corresponding text sequence. The machine translation task generates the translated text sequence based on the input text sequence and the target language. Story Generation is a topic-to-text generation task. In this section, we introduce common techniques used in text generation and then explain how LMs can be applied in each of the representative tasks. 7.1 Decoding Methods Decoding decides the next output linguistic unit to generate text. A good decoding method should generate coherent continuation given a context. As LMs get more sophisticated, decoding methods have played an increasingly important role. As shown in Fig. 9, de\ufb01cient decoding methods lead to bad generated texts even with a powerful LM. There are two main decoding methods for text generation. Figure 9: Comparison of texts generated by the powerful GPT-2 large language model (LLM) using Beam search (left) text sequence and the target language. Story Generation is a topic-to-text generation task. In this section, we introduce common techniques used in text generation and then explain how LMs can be applied in each of the representative tasks. 7.1 Decoding Methods Decoding decides the next output linguistic unit to generate text. A good decoding method should generate coherent continuation given a context. As LMs get more sophisticated, decoding methods have played an increasingly important role. As shown in Fig. 9, de\ufb01cient decoding methods lead to bad generated texts even with a powerful LM. There are two main decoding methods for text generation. Figure 9: Comparison of texts generated by the powerful GPT-2 large language model (LLM) using Beam search (left) and pure sampling decoding (right). Beam search yields degenerate repetition (in blue) while pure sampling results in incoherent gibberish (in red) [102]. Maximization-based decoding. This is the most commonly used decoding objective. Assuming that the model assigns a higher probability to a higher quality text which is closer to the ground truth written by humans, the maximization- based decoding strategy searches for tokens with the highest probability as the generated text. Greedy search [ 103,104] chooses the token with the highest probability as the next token in a greedy manner. Beam search [ 105,106,107] keeps a certain number of most likely tokens at each time step and selects the generated token sequences with the overall highest probability eventually. It avoids missing reasonable tokens that do not have the highest probability. Trainable decoding algorithms have been proposed recently. Trainable greedy decoding [ 108] is a neural-based solution that works as part of a neural machine translation decoder. It utilizes reinforcement learning to \ufb01nd a translation that maximizes a decoding objective. Sampling-based decoding . It chooses the next token from a set of sampled tokens. Because maximization-based decoding depends highly on the underlying model probabilities and suffers from producing degenerate repetition, sampling-based decoding increases the diversity of generated texts by random sampling. However, the simple pure sampling may choose a token with low probability (from an unreliable tail distribution) as the next generated token. As a result, the generated text could be unrelated to the pre\ufb01x, leading to incoherent gibberish. Top-k sampling [ 109] that works as part of a neural machine translation decoder. It utilizes reinforcement learning to \ufb01nd a translation that maximizes a decoding objective. Sampling-based decoding . It chooses the next token from a set of sampled tokens. Because maximization-based decoding depends highly on the underlying model probabilities and suffers from producing degenerate repetition, sampling-based decoding increases the diversity of generated texts by random sampling. However, the simple pure sampling may choose a token with low probability (from an unreliable tail distribution) as the next generated token. As a result, the generated text could be unrelated to the pre\ufb01x, leading to incoherent gibberish. Top-k sampling [ 109] and Nucleus sampling [ 102] have recently been proposed to address this problem. Both sample from truncated LM distributions (i.e., sampling from the most probable tokens). Diverse Beam search [ 105] is a trainable sampling-based 12 Overview of Language Models (stochastic) decoding algorithm based on the Beam search. It uses reinforcement learning to determine the beam diversity parameters for different inputs or tasks. 7.2 Dialogue Systems A dialogue system aims at simulating human responses when conversing with human users. Recent dialogue systems such as ChatGPT1and LaMDA [ 110] have attracted a lot of attention in the generative AI \ufb01eld because of their superior performance as interactive chatbot systems. Dialogue systems can be categorized into task-oriented systems and open-domain systems. The former is designed for speci\ufb01c tasks such as customer service for online shopping. The latter is also known as chatbots [ 111]. Most modern dialogue systems are \ufb01ne-tuned versions of generative LMs. Taking ChatGPT as an example, ChatGPT is built based on a generative LM, GPT-3 [ 85], with over 188 billion parameters. It is further \ufb01ne-tuned by supervised learning and reinforcement learning on labeled data. LMs play an important role in dialogue systems, especially in their natural language understanding (NLU) and natural language generation (NLG) components [ 112,113]. NLU is responsible for understanding and recognizing users\u2019 intent. Nowadays, PLM encoders provide informative representations for NLU while the associated PLM decoders are responsible for generating an appropriate response. The latter involves constructing the response text, selecting The latter is also known as chatbots [ 111]. Most modern dialogue systems are \ufb01ne-tuned versions of generative LMs. Taking ChatGPT as an example, ChatGPT is built based on a generative LM, GPT-3 [ 85], with over 188 billion parameters. It is further \ufb01ne-tuned by supervised learning and reinforcement learning on labeled data. LMs play an important role in dialogue systems, especially in their natural language understanding (NLU) and natural language generation (NLG) components [ 112,113]. NLU is responsible for understanding and recognizing users\u2019 intent. Nowadays, PLM encoders provide informative representations for NLU while the associated PLM decoders are responsible for generating an appropriate response. The latter involves constructing the response text, selecting appropriate words, and determining the correct phrasing and tone. The effectiveness of representations of PLMs was examined in [ 114] for dialogue tasks. The evaluation PLM targets included BERT [ 15] and GPT2 [ 42]. The few-shot capability of PLMs in dialogue tasks such as NLU and NLG was evaluated in [ 115]. Overall, LMs in dialogue systems play a key role in understanding users\u2019 input and generating appropriate and natural responses. 7.3 Automatic Speech Recognition Automatic speech recognition (ASR) is a speech-to-text generation task that aims to transform raw audio input into the corresponding text sequence. The LM plays an essential",
    "156": "role in an ASR system. First, it helps solve acoustically ambiguous utterances. Second, it can lower the computational cost by constraining the search space in a set of words of higher probability. ASR systems contain an acoustic model and a language model, which are related by P(wordjsound )/P(soundjword )P(word ): (17) The acoustic model is conditioned on phones P(soundjword )while the LM gives the word distribution denoted by P(word ). LMs help search the word hypotheses during recognition. Different types of LMs have been explored in ASR such as N-gram [116, 117], FFNN [118], RNN [119, 120] and Transformer [121] 7.4 Machine Translation Machine translation is a text-to-text generation task where the text in the source language is translated into that of the target language. LMs adopted by machine translation are conditioned on both the source sentence and the previous partial translation. Recently, transformer-based models has achieved great successes in machine translation [ 82,122,85] 7.5 Detection of Generated texts P(wordjsound )/P(soundjword )P(word ): (17) The acoustic model is conditioned on phones P(soundjword )while the LM gives the word distribution denoted by P(word ). LMs help search the word hypotheses during recognition. Different types of LMs have been explored in ASR such as N-gram [116, 117], FFNN [118], RNN [119, 120] and Transformer [121] 7.4 Machine Translation Machine translation is a text-to-text generation task where the text in the source language is translated into that of the target language. LMs adopted by machine translation are conditioned on both the source sentence and the previous partial translation. Recently, transformer-based models has achieved great successes in machine translation [ 82,122,85] 7.5 Detection of Generated texts As the performance of LMs gets closer to or even outperforms humans, the misuse of LMs, such as fake news and fake product reviews generation, has become a serious problem. The ability to detect machine-generated texts is important. There are two types of detection problems: 1) human written vs. machine generated, and 2) inveracious vs. veracious. Most datasets, e.g., [ 123,124,125], are collected for the \ufb01rst type. Problems of the second type are much harder than those of the \ufb01rst type [ 126] since one needs to connect generated text to the fact, which requires a high-level knowledge reasoning capability. Two common approaches to detecting machine-generated text are reviewed below. One is to exploit the probability distribution of LMs [ 127,128]. If the probability distribution of a text sequence is closer to that of human-written texts as compared with known machine-generated texts, the text sequence is classi\ufb01ed as human-written. The other is to train classi\ufb01ers with supervised learning [ 129,124]. It converts the distribution to a supervised binary classi\ufb01cation task. For more details on the detection of machine-generated texts, readers are referred to two survey papers [130, 131]. 1https://openai.com/blog/chatgpt/ 13 Overview of Language Models 8 Ef\ufb01cient Models As recent PLMs get more powerful, their model size, training cost, and demand for training data increase tremendously. They need high computational resources and energy consumption, limiting their real-world applications. Table 2 shows the model size, training data, cost, and time of recently developed LMs. This issue is a concern to many people and the construction of ef\ufb01cient LMs has received attention. classi\ufb01ers with supervised learning [ 129,124]. It converts the distribution to a supervised binary classi\ufb01cation task. For more details on the detection of machine-generated texts, readers are referred to two survey papers [130, 131]. 1https://openai.com/blog/chatgpt/ 13 Overview of Language Models 8 Ef\ufb01cient Models As recent PLMs get more powerful, their model size, training cost, and demand for training data increase tremendously. They need high computational resources and energy consumption, limiting their real-world applications. Table 2 shows the model size, training data, cost, and time of recently developed LMs. This issue is a concern to many people and the construction of ef\ufb01cient LMs has received attention. Model Year Model size Training data Training cost Training time BERT-Large 2018 340M 2.5B words $7,00064 TPU chips 4 days XLNet-Lagre 2019 340M 32.9B words $245,000512 TPU v3 chips 5.5 days GPT-2 2019 1.5B 8 million web pages $60,000 Megatron-LM 2019 8.3B174 GB deduplicated text512 GPUs 53 minutes T5 2019 11BAbove $1.3 million for a single run Turing-NLG 2020 17.2B GPT-3 2020 175B 45TB of text data $12 million Megatron-Turing NLG2021 530B 270B2K A100 GPUs 3 months Table 2: Comparison of nodel sizes, training data, cost, and time of several large LMs, where blank cells indicate that the data are not available. 8.1 Data Usage Pre-training Data Size. A critical question for PLM training is how much data is needed. The effect of the pre-training data size on the RoBERTa model was studied in [ 132]. The learning curves of four model performance measures as a function of the pre-training dataset size are shown in Fig. 10. When the data size ranges between 100M and 1B words, three learning curves gradually level off and it implies that LMs encode most syntactic and semantic features. However, a much larger quantity of data is needed for LMs to acquire enough common-sense knowledge and other skills to achieve better performance on downstream NLU tasks. Figure 10: The performance curves as functions of the pre-training dataset size, where the classi\ufb01er probing measures the quality of the syntactic and semantic features, the minimum description length probing quanti\ufb01es the accessibility of these features, the BLiMP curve measures the model\u2019s knowledge of various syntactic phenomena, and the superGLUE measures the capability of handling NLU tasks [132]. 14 Overview of Language Models words, three learning curves gradually level off and it implies that LMs encode most syntactic and semantic features. However, a much larger quantity of data is needed for LMs to acquire enough common-sense knowledge and other skills to achieve better performance on downstream NLU tasks. Figure 10: The performance curves as functions of the pre-training dataset size, where the classi\ufb01er probing measures the quality of the syntactic and semantic features, the minimum description length probing quanti\ufb01es the accessibility of these features, the BLiMP curve measures the model\u2019s knowledge of various syntactic phenomena, and the superGLUE measures the capability of handling NLU tasks [132]. 14 Overview of Language Models Ef\ufb01cient Pre-Training. Several methods have been proposed to use the pre-training data more ef\ufb01ciently. In the pre-training of masked LMs, a certain percentage of tokens are masked and need to be inferred by context. This approach incurs a substantial amount of computational cost because the network only learns from a certain percentage of tokens which are masked. To enhance training ef\ufb01ciency, the work in [ 45] uses \u201creplaced token detection\" (rather than \u201cmasked token prediction\") as the pre-training task. As shown in Fig. 11, a generator is trained to perform the masked LM and predicts the masked tokens. Then, the main model works as a discriminator, called ELECTRA, which learns to decide the original or replaced tokens. In this way, pre-training tasks are conducted on all tokens instead of a small subset of masked tokens. Learning from all input positions causes ELECTRA to train much faster than BERT which adopts masked word prediction. Besides, ELECTRA achieves higher accuracy on downstream tasks when it is fully trained. Later, a new pre-training task using an energy-based model, which is closely related to ELECTRA, is proposed in [133]. Figure 11: The structure of ELECTRA (Ef\ufb01ciently Learning an Encoder that Classi\ufb01er Token Replacements Accu- rately) [45] Bridging Pre-training and Downstream Tasks. A typical pre-training",
    "157": "task is word prediction, which often has a large gap with downstream tasks. To mitigate the gap between pre-training and downstream tasks, prompt tuning has been studied in [ 84,42,89,134,135]. As illustrated in Fig. 8, the head is trained to predict the masked words in masked LMs. For the downstream sentiment analysis task, the head is trained to predict the positive or the negative label in fully trained. Later, a new pre-training task using an energy-based model, which is closely related to ELECTRA, is proposed in [133]. Figure 11: The structure of ELECTRA (Ef\ufb01ciently Learning an Encoder that Classi\ufb01er Token Replacements Accu- rately) [45] Bridging Pre-training and Downstream Tasks. A typical pre-training task is word prediction, which often has a large gap with downstream tasks. To mitigate the gap between pre-training and downstream tasks, prompt tuning has been studied in [ 84,42,89,134,135]. As illustrated in Fig. 8, the head is trained to predict the masked words in masked LMs. For the downstream sentiment analysis task, the head is trained to predict the positive or the negative label in traditional \ufb01ne-tuning. A template (e.g., \u2018It was\u2019) and its expected text responses (e.g., \u2018great\u2019 and \u2018terrible\u2019) are used in prompt tuning. In this way, pre-training and prompt tuning share the same \u201cword prediction\" objective. 8.2 Model Size Besides improving training ef\ufb01ciency, ef\ufb01cient LMs focus on the design of models of smaller sizes. Many methods are investigated to reduce the model size so that the model can be implemented on mobile or edge devices with limited computing resources. Model compression is a widely studied topic. Compression methods \ufb01rst train a large LM and then compress it into a target size. Examples include model pruning [ 136,137,138], knowledge distillation [ 139,44,140], low rank matrix approximation [78, 141, 142], and parameter sharing [143, 144, 145]. 9 Future Research Directions In this section, we describe several promising future research directions in language modeling. 9.1 Integration of LMs and KGs Knowledge Graph (KG) provides a structured representation of human knowledge [ 146,147]. It has been widely used in many NLP applications, such as question answering [ 148] and text summarization [ 149], because of its capability to represent relationships between entities. There is a growing interest in evaluating the knowledge learned in PLMs [ 150], where the relationship between different semantic units is captured in the embedding space and the self-attention layers. Several ideas are proposed in [ 151] to leverage KGs for LM training. As a result, the knowledge learned in the models can be greatly improved. Thus, it is worth careful investigation of integrating KGs with LMs and understanding how they interact with each other. 9.1 Integration of LMs and KGs Knowledge Graph (KG) provides a structured representation of human knowledge [ 146,147]. It has been widely used in many NLP applications, such as question answering [ 148] and text summarization [ 149], because of its capability to represent relationships between entities. There is a growing interest in evaluating the knowledge learned in PLMs [ 150], where the relationship between different semantic units is captured in the embedding space and the self-attention layers. Several ideas are proposed in [ 151] to leverage KGs for LM training. As a result, the knowledge learned in the models can be greatly improved. Thus, it is worth careful investigation of integrating KGs with LMs and understanding how they interact with each other. It appears that KG can serve as an information database to be queried by LMs. LMs are powerful in natural language understanding and generation while KGs can organize and store the knowledge information extracted from the training corpus. In other words, we may decompose knowledge sources into semantic and syntactic two components, which can be handled by KGs and LMs, respectively. In the training phase, a KG is constructed based on the information extracted from the training corpus, and an LM can be trained simultaneously. In the inference phase, an LM can serve 15 Overview of Language Models as an interface between humans and the knowledge database represented in form of KGs. There are advantages to assigning semantic and syntactic processing tasks to KGs and LMs, respectively. For example, the decoupling facilitates incremental learning, allows a smaller model size, and improves interpretability. They will be further elaborated on below. 9.2 Incremental Learning Incremental learning aims to incorporate new information without re-training existing models entirely. The problem of catastrophic forgetting associated with neural network models was pointed out in [ 152]. That is, the information that has already been learned by a model can be gradually forgotten when training with new information. This problem is particularly critical to large LMs since the new information keeps arriving. A solution to catastrophic forgetting was proposed in [ 153]. It attempts to remember prior important tasks by slowing down learning on weights that are more relevant to them. However, it is dif\ufb01cult to de\ufb01ne important tasks in LMs. Re-training a large LM with both old and new data is too expensive. below. 9.2 Incremental Learning Incremental learning aims to incorporate new information without re-training existing models entirely. The problem of catastrophic forgetting associated with neural network models was pointed out in [ 152]. That is, the information that has already been learned by a model can be gradually forgotten when training with new information. This problem is particularly critical to large LMs since the new information keeps arriving. A solution to catastrophic forgetting was proposed in [ 153]. It attempts to remember prior important tasks by slowing down learning on weights that are more relevant to them. However, it is dif\ufb01cult to de\ufb01ne important tasks in LMs. Re-training a large LM with both old and new data is too expensive. The importance of developing a satisfactory solution to incremental learning for LMs cannot be over-emphasized. Incremental learning is challenging for neural networks. Yet, it is easy for KGs to add new data to (or remove old data from) an existing database by adding or removing factual triples [ 154]. Clearly, the current information in the KGs will not be overwritten by newly collected data. The information in the database is updated incrementally. To this end, the integration of KGs and LMs provides an excellent solution that meets the need for incremental learning. 9.3 Lightweight Models As mentioned in Section 8, PLMs get more powerful at the expense of huge computational resources and energy consumption. The cost issue has to be faced seriously in the development of large LMs (LLMs). Besides, LLMs are unfriendly to our environment due to their high carbon footprint. Green Learning (GL) targets learning solutions with low carbon footprint. The design of lightweight models of smaller sizes and lower computational complexity without sacri\ufb01cing performance has received more attention in recent years [ 155,156,157,158]. The design of green LMs is an important topic worth serious investigation. Current PLMs are data-driven models that use neural architectures to learn generic language knowledge from a large amount of data. Efforts have been made in the development of lightweight LMs. Model compression is one of the popular approaches to obtaining a small LM. Examples include knowledge distillation or pruning [ 159]. However, this methodology appears to be a detour since it trains large models and then shrinks their sizes by compression. Instead, we low carbon footprint. The design of lightweight models of smaller sizes and lower computational complexity without sacri\ufb01cing performance has received more attention in recent years [ 155,156,157,158]. The design of green LMs is an important topic worth serious",
    "158": "investigation. Current PLMs are data-driven models that use neural architectures to learn generic language knowledge from a large amount of data. Efforts have been made in the development of lightweight LMs. Model compression is one of the popular approaches to obtaining a small LM. Examples include knowledge distillation or pruning [ 159]. However, this methodology appears to be a detour since it trains large models and then shrinks their sizes by compression. Instead, we may incorporate the linguistic information and the domain knowledge to offer a more direct way to reduce the model size and the amount of training data. 9.4 Universal versus Domain-Speci\ufb01c Models A universal LM is developed to handle tasks in the general domain. For example, ChatGPT is a universal dialogue LM pre-trained on multilingual and general domain corpora. It can converse on open-domain topics in multiple languages. In contrast, domain-speci\ufb01c LMs [ 160,161,162,163] are designed to deal with domain-speci\ufb01c tasks, e.g., biomedicine, economics, musicology, etc. A universal LM demands a huge model size, a large number of training examples, and a tremendous amount of computational resources. Based on the scaling law of neural language models [ 164], the inference performance scales as a power-law with the model size, the dataset size, and the amount of computing used for training. So far, the largest PLM contains 540-billion parameters [ 83]. Despite the superior performance and the \ufb02exibility to adapt to multiple tasks, we may wonder whether a huge universal LM is cost-effective. For domain-speci\ufb01c LMs, the amount of training data in need is signi\ufb01cantly lower. It was believed that the general domain PLMs bene\ufb01t the training of domain-speci\ufb01c LMs. However, it is reported in [ 161] that domain-speci\ufb01c LMs, which were pre-trained from scratch on in-domain data, can provide a solid foundation for biomedical NLP. In other words, training a domain-speci\ufb01c LM may not need a huge amount of general corpora and labeled data. Domain-speci\ufb01c LMs to be deployed on task-speci\ufb01c scenarios with less training and inference efforts expect to receive more attention in the future. 9.5 Interpretable Models tasks, we may wonder whether a huge universal LM is cost-effective. For domain-speci\ufb01c LMs, the amount of training data in need is signi\ufb01cantly lower. It was believed that the general domain PLMs bene\ufb01t the training of domain-speci\ufb01c LMs. However, it is reported in [ 161] that domain-speci\ufb01c LMs, which were pre-trained from scratch on in-domain data, can provide a solid foundation for biomedical NLP. In other words, training a domain-speci\ufb01c LM may not need a huge amount of general corpora and labeled data. Domain-speci\ufb01c LMs to be deployed on task-speci\ufb01c scenarios with less training and inference efforts expect to receive more attention in the future. 9.5 Interpretable Models Although deep-learning-based LMs are dominating in the NLP \ufb01eld, they are inherently black-box methods without mathematical transparency. Its interpretability is of concern. Efforts have been made to explain the black-box LMs. As 16 Overview of Language Models mentioned in 6.3, empirical studies are conducted to understand what PLMs have learned through experimental design. However, the progress in this direction may offer insights but not a satisfactory and clean answer. Providing theoretical explanations or establishing explainable LMs is still a challenging and open issue. A direction to interpretability is to design an interpretable learning model from scratch. For example, we may incorporate KGs with LMs. KGs are proven to be powerful in many reasoning tasks such as information retrieval [ 165] and recommendation systems [ 166]. KGs can provide a logical path for each prediction so that predictions offered by LMs can be more explainable. 9.6 Machine Generated Text Detection The most common application of LMs is text generation. As generative LM\u2019s performance gets closer to or even outperforms humans, these LMs can be used for malicious purposes such as academic dishonesty, spamming, targeted bot attacks, and fake news/reviews generation. How to determine whether a text is generated by LMs or written by humans is a big challenge nowadays. A high-performance machine-generated text classi\ufb01er can only serve as a reference in real-world applications, since it has false positives (i.e., human-written texts classi\ufb01ed as machine-generated) and false negatives (i.e., machine-generated texts classi\ufb01ed as human-written). In addition, people may be even more interested in detecting veracious and unveracious texts. They care more about whether the text is true or not. Detecting outperforms humans, these LMs can be used for malicious purposes such as academic dishonesty, spamming, targeted bot attacks, and fake news/reviews generation. How to determine whether a text is generated by LMs or written by humans is a big challenge nowadays. A high-performance machine-generated text classi\ufb01er can only serve as a reference in real-world applications, since it has false positives (i.e., human-written texts classi\ufb01ed as machine-generated) and false negatives (i.e., machine-generated texts classi\ufb01ed as human-written). In addition, people may be even more interested in detecting veracious and unveracious texts. They care more about whether the text is true or not. Detecting disinformation could be more dif\ufb01cult than detecting machine/human-generated text without assessing the factuality. Additionally, the factuality may change as time goes by. It is critical to our society in developing effective tools to identify malicious usages of generative LMs. 10 Conclusion A comprehensive overview of CLMs and their successors, PLMs, was presented in this paper and a wide range of topics was covered. First, different levels of linguistic units were introduced and how linguistic unit prediction is used to train language models was examined. Second, tokenization methods adopted by language models were discussed. Third, language model structures and the training paradigm of PLMs were reviewed. Fourth, we studied the evaluation and applications of language models. Especially, several applications in the context of text generation were detailed. Finally, several future research directions were pointed out. The need for explainable, reliable, domain-speci\ufb01c, and lightweight language models was emphasized. References [1]Peter F Brown, Vincent J Della Pietra, Peter V Desouza, Jennifer C Lai, and Robert L Mercer. Class-based n-gram models of natural language. Computational linguistics , 18(4):467\u2013480, 1992. [2]Marcello Federico. Bayesian estimation methods for n-gram language model adaptation. In Proceeding of Fourth International Conference on Spoken Language Processing. ICSLP\u201996 , volume 1, pages 240\u2013243. IEEE, 1996. [3]Thomas R Niesler and Philip C Woodland. A variable-length category-based n-gram language model. In 1996 IEEE International Conference on Acoustics, Speech, and Signal Processing Conference Proceedings , volume 1, pages 164\u2013167. IEEE, 1996. language models was emphasized. References [1]Peter F Brown, Vincent J Della Pietra, Peter V Desouza, Jennifer C Lai, and Robert L Mercer. Class-based n-gram models of natural language. Computational linguistics , 18(4):467\u2013480, 1992. [2]Marcello Federico. Bayesian estimation methods for n-gram language model adaptation. In Proceeding of Fourth International Conference on Spoken Language Processing. ICSLP\u201996 , volume 1, pages 240\u2013243. IEEE, 1996. [3]Thomas R Niesler and Philip C Woodland. A variable-length category-based n-gram language model. In 1996 IEEE International Conference on Acoustics, Speech, and Signal Processing Conference Proceedings , volume 1, pages 164\u2013167. IEEE, 1996. [4]Stephen A Della Pietra, Vincent J Della Pietra, Robert L Mercer, and Salim Roukos. Adaptive language modeling using minimum discriminant estimation. In Speech and Natural Language: Proceedings of a Workshop Held at",
    "159": "Harriman, New York, February 23-26, 1992 , 1992. [5]Adam Berger, Stephen A Della Pietra, and Vincent J Della Pietra. A maximum entropy approach to natural language processing. Computational linguistics , 22(1):39\u201371, 1996. [6] Roni Rosenfeld. A maximum entropy approach to adaptive statistical language modeling. 1996. [7]Yoshua Bengio, R\u00e9jean Ducharme, and Pascal Vincent. A neural probabilistic language model. Advances in neural information processing systems , 13, 2000. [8]Tomas Mikolov, Martin Kara\ufb01\u00e1t, Lukas Burget, Jan Cernock `y, and Sanjeev Khudanpur. Recurrent neural network based language model. In Interspeech , volume 2, pages 1045\u20131048. Makuhari, 2010. [9]Frederick Jelinek, Lalit Bahl, and Robert Mercer. Design of a linguistic statistical decoder for the recognition of continuous speech. IEEE Transactions on Information Theory , 21(3):250\u2013256, 1975. [10] Frederick Jelinek. Continuous speech recognition by statistical methods. Proceedings of the IEEE , 64(4):532\u2013556, 1976. [11] Lalit R Bahl, Frederick Jelinek, and Robert L Mercer. A maximum likelihood approach to continuous speech recognition. IEEE transactions on pattern analysis and machine intelligence , (2):179\u2013190, 1983. 17 Overview of Language Models [12] Peter F Brown, John Cocke, Stephen A Della Pietra, Vincent J Della Pietra, Frederick Jelinek, John Lafferty, Robert L Mercer, and Paul S Roossin. A statistical approach to machine translation. Computational linguistics , 16(2):79\u201385, 1990. [13] Franz Josef Och, Nicola Uef\ufb01ng, and Hermann Ney. An ef\ufb01cient a* search algorithm for statistical machine continuous speech. IEEE Transactions on Information Theory , 21(3):250\u2013256, 1975. [10] Frederick Jelinek. Continuous speech recognition by statistical methods. Proceedings of the IEEE , 64(4):532\u2013556, 1976. [11] Lalit R Bahl, Frederick Jelinek, and Robert L Mercer. A maximum likelihood approach to continuous speech recognition. IEEE transactions on pattern analysis and machine intelligence , (2):179\u2013190, 1983. 17 Overview of Language Models [12] Peter F Brown, John Cocke, Stephen A Della Pietra, Vincent J Della Pietra, Frederick Jelinek, John Lafferty, Robert L Mercer, and Paul S Roossin. A statistical approach to machine translation. Computational linguistics , 16(2):79\u201385, 1990. [13] Franz Josef Och, Nicola Uef\ufb01ng, and Hermann Ney. An ef\ufb01cient a* search algorithm for statistical machine translation. In Proceedings of the ACL 2001 Workshop on Data-Driven Methods in Machine Translation , 2001. [14] Kenji Yamada and Kevin Knight. A decoder for syntax-based statistical mt. In Proceedings of the 40th Annual meeting of the Association for Computational Linguistics , pages 303\u2013310, 2002. [15] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018. [16] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 , 2019. [17] Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers) , pages 2227\u20132237, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. [18] Chengwei Wei, Bin Wang, and C.-C. Jay Kuo. Synwmd: Syntax-aware word mover\u2019s distance for sentence similarity evaluation. arXiv preprint arXiv:2206.10029 , 2022. [19] Bin Wang and C.-C. Jay Kuo. SBERT-WK: A sentence embedding method by dissecting bert-based word models. IEEE/ACM Transactions on Audio, Speech, and Language Processing , 28:2146\u20132157, 2020. [20] Tianyu Gao, Xingcheng Yao, and Danqi Chen. SIMCSE: Simple contrastive learning of sentence embeddings. arXiv preprint arXiv:2104.08821 , 2021. American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers) , pages 2227\u20132237, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. [18] Chengwei Wei, Bin Wang, and C.-C. Jay Kuo. Synwmd: Syntax-aware word mover\u2019s distance for sentence similarity evaluation. arXiv preprint arXiv:2206.10029 , 2022. [19] Bin Wang and C.-C. Jay Kuo. SBERT-WK: A sentence embedding method by dissecting bert-based word models. IEEE/ACM Transactions on Audio, Speech, and Language Processing , 28:2146\u20132157, 2020. [20] Tianyu Gao, Xingcheng Yao, and Danqi Chen. SIMCSE: Simple contrastive learning of sentence embeddings. arXiv preprint arXiv:2104.08821 , 2021. [21] Bin Wang and Haizhou Li. Relational sentence embedding for \ufb02exible semantic matching. arXiv preprint arXiv:2212.08802 , 2022. [22] Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai, and Xuanjing Huang. Pre-trained models for natural language processing: A survey. Science China Technological Sciences , 63(10):1872\u20131897, 2020. [23] Xu Han, Zhengyan Zhang, Ning Ding, Yuxian Gu, Xiao Liu, Yuqi Huo, Jiezhong Qiu, Yuan Yao, Ao Zhang, Liang Zhang, et al. Pre-trained models: Past, present and future. AI Open , 2:225\u2013250, 2021. [24] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. arXiv preprint arXiv:2107.13586 , 2021. [25] Ciprian Chelba and Frederick Jelinek. Expoiting syntactic structure for language modeling. arXiv preprint cs/9811022 , 1998. [26] Ciprian Chelba and Frederick Jelinek. Structured language modeling. Computer Speech & Language , 14(4):283\u2013 332, 2000. [27] Joseph Gubbins and Andreas Vlachos. Dependency language models for sentence completion. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing , pages 1405\u20131410, 2013. [28] Piotr Mirowski and Andreas Vlachos. Dependency recurrent neural language models for sentence completion. arXiv preprint arXiv:1507.01193 , 2015. [29] Chengwei Wei, Bin Wang, and C.-C. Jay Kuo. Task-speci\ufb01c dependency-based word embedding methods. Pattern Recognition Letters , 2022. [30] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding. Advances in neural information processing systems , 32, 2019. 332, 2000. [27] Joseph Gubbins and Andreas Vlachos. Dependency language models for sentence completion. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing , pages 1405\u20131410, 2013. [28] Piotr Mirowski and Andreas Vlachos. Dependency recurrent neural language models for sentence completion. arXiv preprint arXiv:1507.01193 , 2015. [29] Chengwei Wei, Bin Wang, and C.-C. Jay Kuo. Task-speci\ufb01c dependency-based word embedding methods. Pattern Recognition Letters , 2022. [30] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding. Advances in neural information processing systems , 32, 2019. [31] Ilya Sutskever, James Martens, and Geoffrey E Hinton. Generating text with recurrent neural networks. In ICML , 2011. [32] Kyuyeon Hwang and Wonyong Sung. Character-level language modeling with hierarchical recurrent neural networks. In 2017 ieee international conference on acoustics, speech and signal processing (icassp) , pages 5720\u20135724. IEEE, 2017. [33] Yoon Kim, Yacine Jernite, David Sontag, and Alexander M Rush. Character-aware neural language models. In Thirtieth AAAI conference on arti\ufb01cial intelligence , 2016. 18 Overview of Language Models [34] Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. Character-level language modeling with deeper self-attention. In Proceedings of",
    "160": "the AAAI conference on arti\ufb01cial intelligence , volume 33, pages 3159\u20133166, 2019. [35] Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, and Colin Raffel. Byt5: Towards a token-free future with pre-trained byte-to-byte models. Transactions of the Association for Computational Linguistics , 10:291\u2013306, 2022. [36] Moonyoung Kang, Tim Ng, and Long Nguyen. Mandarin word-character hybrid-input neural network language model. In Twelfth Annual Conference of the International Speech Communication Association , 2011. [37] Yasumasa Miyamoto and Kyunghyun Cho. Gated word-character recurrent language model. arXiv preprint arXiv:1606.01700 , 2016. [38] Lyan Verwimp, Joris Pelemans, Patrick Wambacq, et al. Character-word lstm language models. arXiv preprint arXiv:1704.02813 , 2017. [39] Tom\u00e1\u0161 Mikolov, Ilya Sutskever, Anoop Deoras, Hai-Son Le, Stefan Kombrink, and Jan Cernocky. Subword language modeling with neural networks. preprint (http://www. \ufb01t. vutbr. cz/imikolov/rnnlm/char. pdf) , 8(67), Association for Computational Linguistics , 10:291\u2013306, 2022. [36] Moonyoung Kang, Tim Ng, and Long Nguyen. Mandarin word-character hybrid-input neural network language model. In Twelfth Annual Conference of the International Speech Communication Association , 2011. [37] Yasumasa Miyamoto and Kyunghyun Cho. Gated word-character recurrent language model. arXiv preprint arXiv:1606.01700 , 2016. [38] Lyan Verwimp, Joris Pelemans, Patrick Wambacq, et al. Character-word lstm language models. arXiv preprint arXiv:1704.02813 , 2017. [39] Tom\u00e1\u0161 Mikolov, Ilya Sutskever, Anoop Deoras, Hai-Son Le, Stefan Kombrink, and Jan Cernocky. Subword language modeling with neural networks. preprint (http://www. \ufb01t. vutbr. cz/imikolov/rnnlm/char. pdf) , 8(67), 2012. [40] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909 , 2015. [41] Philip Gage. A new algorithm for data compression. C Users Journal , 12(2):23\u201338, 1994. [42] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog , 1(8):9, 2019. [43] Mike Schuster and Kaisuke Nakajima. Japanese and korean voice search. In 2012 IEEE international conference on acoustics, speech and signal processing (ICASSP) , pages 5149\u20135152. IEEE, 2012. [44] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108 , 2019. [45] Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. Electra: Pre-training text encoders as discriminators rather than generators. arXiv preprint arXiv:2003.10555 , 2020. [46] Taku Kudo. Subword regularization: Improving neural network translation models with multiple subword candidates. arXiv preprint arXiv:1804.10959 , 2018. [47] Kaj Bostrom and Greg Durrett. Byte pair encoding is suboptimal for language model pretraining. arXiv preprint arXiv:2004.03720 , 2020. [48] Daniel Kiecza, Tanja Schultz, and Alex Waibel. Data-driven determination of appropriate dictionary units for korean lvcsr. In Proceedings of ICASSP , pages 323\u2013327, 1999. [49] Mathias Creutz and Krista Lagus. Unsupervised morpheme segmentation and morphology induction from text corpora using Morfessor 1.0 . Helsinki University of Technology Helsinki, 2005. as discriminators rather than generators. arXiv preprint arXiv:2003.10555 , 2020. [46] Taku Kudo. Subword regularization: Improving neural network translation models with multiple subword candidates. arXiv preprint arXiv:1804.10959 , 2018. [47] Kaj Bostrom and Greg Durrett. Byte pair encoding is suboptimal for language model pretraining. arXiv preprint arXiv:2004.03720 , 2020. [48] Daniel Kiecza, Tanja Schultz, and Alex Waibel. Data-driven determination of appropriate dictionary units for korean lvcsr. In Proceedings of ICASSP , pages 323\u2013327, 1999. [49] Mathias Creutz and Krista Lagus. Unsupervised morpheme segmentation and morphology induction from text corpora using Morfessor 1.0 . Helsinki University of Technology Helsinki, 2005. [50] Mathias Creutz, Teemu Hirsim\u00e4ki, Mikko Kurimo, Antti Puurula, Janne Pylkk\u00f6nen, Vesa Siivola, Matti Var- jokallio, Ebru Arisoy, Murat Sara\u00e7lar, and Andreas Stolcke. Morph-based speech recognition and modeling of out-of-vocabulary words across languages. ACM Transactions on Speech and Language Processing (TSLP) , 5(1):1\u201329, 2007. [51] Toma\u017e Rotovnik, Mirjam Sepesy Mau \u02c7cec, and Zdravko Ka \u02c7ci\u02c7c. Large vocabulary continuous speech recognition of an in\ufb02ected language using stems and endings. Speech communication , 49(6):437\u2013452, 2007. [52] Ruhi Sarikaya, Mohamed A\ufb01fy, and Yuqing Gao. Joint morphological-lexical language modeling (jmllm) for arabic. In 2007 IEEE International Conference on Acoustics, Speech and Signal Processing-ICASSP\u201907 , volume 4, pages IV\u2013181. IEEE, 2007. [53] Ha\u00b8 sim Sak, Murat Saraclar, and Tunga G\u00fcng\u00f6r. Morphology-based and sub-word language modeling for turkish speech recognition. In 2010 IEEE International Conference on Acoustics, Speech and Signal Processing , pages 5402\u20135405. IEEE, 2010. [54] Mijit Ablimit, Graham Neubig, Masato Mimura, Shinsuke Mori, Tatsuya Kawahara, and Askar Hamdulla. Uyghur morpheme-based language models and asr. In IEEE 10th INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING PROCEEDINGS , pages 581\u2013584. IEEE, 2010. [55] Bernhard Suhm and Alex Waibel. Towards better language models for spontaneous speech. 1994. 19 Overview of Language Models [56] Klaus Ries, Finn Dag Buo, and Alex Waibel. Class phrase models for language modeling. In Proceeding of Fourth International Conference on Spoken Language Processing. ICSLP\u201996 , volume 1, pages 398\u2013401. IEEE, 1996. [57] George Saon and Mukund Padmanabhan. Data-driven approach to designing compound words for continuous 5402\u20135405. IEEE, 2010. [54] Mijit Ablimit, Graham Neubig, Masato Mimura, Shinsuke Mori, Tatsuya Kawahara, and Askar Hamdulla. Uyghur morpheme-based language models and asr. In IEEE 10th INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING PROCEEDINGS , pages 581\u2013584. IEEE, 2010. [55] Bernhard Suhm and Alex Waibel. Towards better language models for spontaneous speech. 1994. 19 Overview of Language Models [56] Klaus Ries, Finn Dag Buo, and Alex Waibel. Class phrase models for language modeling. In Proceeding of Fourth International Conference on Spoken Language Processing. ICSLP\u201996 , volume 1, pages 398\u2013401. IEEE, 1996. [57] George Saon and Mukund Padmanabhan. Data-driven approach to designing compound words for continuous speech recognition. IEEE transactions on Speech and audio processing , 9(4):327\u2013332, 2001. [58] Michael Levit, Sarangarajan Parthasarathy, Shuangyu Chang, Andreas Stolcke, and Benoit Dumoulin. Word- phrase-entity language models: Getting more mileage out of n-grams. In Fifteenth Annual Conference of the International Speech Communication Association , 2014. [59] Ronald Rosenfeld. A whole sentence maximum entropy language model. In 1997 IEEE workshop on automatic speech recognition and understanding proceedings , pages 230\u2013237. IEEE, 1997. [60] Stanley F Chen and Ronald Rosenfeld. Ef\ufb01cient sampling and feature selection in whole sentence maximum entropy language models. In 1999 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings. ICASSP99 (Cat. No. 99CH36258) , volume 1, pages 549\u2013552. IEEE, 1999. [61] Ronald Rosenfeld, Stanley F Chen, and Xiaojin Zhu. Whole-sentence exponential language models: a vehicle for linguistic-statistical integration. Computer Speech & Language , 15(1):55\u201373, 2001. [62] Daphne Ippolito, David Grangier, Douglas Eck, and Chris Callison-Burch. Toward better storylines with sentence-level language models. arXiv preprint arXiv:2005.05255 , 2020. [63] Haejun Lee, Drew A Hudson, Kangwook Lee, and Christopher D Manning. Slm: Learning a discourse language",
    "161": "representation with sentence unshuf\ufb02ing. arXiv preprint arXiv:2010.16249 , 2020. [64] George James Lidstone. Note on the general case of the bayes-laplace formula for inductive or a posteriori probabilities. Transactions of the Faculty of Actuaries , 8(182-192):13, 1920. [65] William Ernest Johnson. Probability: The deductive and inductive problems. Mind , 41(164):409\u2013423, 1932. for linguistic-statistical integration. Computer Speech & Language , 15(1):55\u201373, 2001. [62] Daphne Ippolito, David Grangier, Douglas Eck, and Chris Callison-Burch. Toward better storylines with sentence-level language models. arXiv preprint arXiv:2005.05255 , 2020. [63] Haejun Lee, Drew A Hudson, Kangwook Lee, and Christopher D Manning. Slm: Learning a discourse language representation with sentence unshuf\ufb02ing. arXiv preprint arXiv:2010.16249 , 2020. [64] George James Lidstone. Note on the general case of the bayes-laplace formula for inductive or a posteriori probabilities. Transactions of the Faculty of Actuaries , 8(182-192):13, 1920. [65] William Ernest Johnson. Probability: The deductive and inductive problems. Mind , 41(164):409\u2013423, 1932. [66] Frederick Jelinek. Interpolated estimation of markov source parameters from sparse data. In Proc. Workshop on Pattern Recognition in Practice, 1980 , 1980. [67] Slava Katz. Estimation of probabilities from sparse data for the language model component of a speech recognizer. IEEE transactions on acoustics, speech, and signal processing , 35(3):400\u2013401, 1987. [68] Kenneth W Church and William A Gale. A comparison of the enhanced good-turing and deleted estimation methods for estimating probabilities of english bigrams. Computer Speech & Language , 5(1):19\u201354, 1991. [69] Reinhard Kneser and Hermann Ney. Improved backing-off for m-gram language modeling. In 1995 international conference on acoustics, speech, and signal processing , volume 1, pages 181\u2013184. IEEE, 1995. [70] Stanley F Chen and Joshua Goodman. An empirical study of smoothing techniques for language modeling. Computer Speech & Language , 13(4):359\u2013394, 1999. [71] John N Darroch and Douglas Ratcliff. Generalized iterative scaling for log-linear models. The annals of mathematical statistics , pages 1470\u20131480, 1972. [72] Holger Schwenk and Jean-Luc Gauvain. Training neural network language models on very large corpora. In Proceedings of human language technology conference and conference on empirical methods in natural language processing , pages 201\u2013208, 2005. [73] Holger Schwenk. Continuous space language models. Computer Speech & Language , 21(3):492\u2013518, 2007. [74] Ebru Arisoy, Tara N Sainath, Brian Kingsbury, and Bhuvana Ramabhadran. Deep neural network language models. In Proceedings of the NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT , pages 20\u201328, 2012. [71] John N Darroch and Douglas Ratcliff. Generalized iterative scaling for log-linear models. The annals of mathematical statistics , pages 1470\u20131480, 1972. [72] Holger Schwenk and Jean-Luc Gauvain. Training neural network language models on very large corpora. In Proceedings of human language technology conference and conference on empirical methods in natural language processing , pages 201\u2013208, 2005. [73] Holger Schwenk. Continuous space language models. Computer Speech & Language , 21(3):492\u2013518, 2007. [74] Ebru Arisoy, Tara N Sainath, Brian Kingsbury, and Bhuvana Ramabhadran. Deep neural network language models. In Proceedings of the NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT , pages 20\u201328, 2012. [75] Tom\u00e1\u0161 Mikolov, Stefan Kombrink, Luk\u00e1\u0161 Burget, Jan \u02c7Cernock `y, and Sanjeev Khudanpur. Extensions of recurrent neural network language model. In 2011 IEEE international conference on acoustics, speech and signal processing (ICASSP) , pages 5528\u20135531. IEEE, 2011. [76] Stefan Kombrink, Tomas Mikolov, Martin Kara\ufb01\u00e1t, and Luk\u00e1s Burget. Recurrent neural network based language modeling in meeting recognition. In Interspeech , volume 11, pages 2877\u20132880, 2011. [77] Martin Sundermeyer, Ralf Schl\u00fcter, and Hermann Ney. Lstm neural networks for language modeling. In Thirteenth annual conference of the international speech communication association , 2012. [78] Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William W Cohen. Breaking the softmax bottleneck: A high-rank rnn language model. arXiv preprint arXiv:1711.03953 , 2017. 20 Overview of Language Models [79] Hongli Deng, Lei Zhang, and Lituan Wang. Global context-dependent recurrent neural network language model with sparse feature learning. Neural Computing and Applications , 31(2):999\u20131011, 2019. [80] Sepp Hochreiter. The vanishing gradient problem during learning recurrent neural nets and problem solutions. International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems , 6(02):107\u2013116, 1998. [81] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473 , 2014. [82] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems , 30, 2017. [79] Hongli Deng, Lei Zhang, and Lituan Wang. Global context-dependent recurrent neural network language model with sparse feature learning. Neural Computing and Applications , 31(2):999\u20131011, 2019. [80] Sepp Hochreiter. The vanishing gradient problem during learning recurrent neural nets and problem solutions. International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems , 6(02):107\u2013116, 1998. [81] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473 , 2014. [82] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems , 30, 2017. [83] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311 , 2022. [84] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018. [85] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems , 33:1877\u20131901, 2020. [86] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a \ufb01xed-length context. arXiv preprint arXiv:1901.02860 , 2019. [87] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461 , 2019. [88] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. Exploring the limits of transfer learning with a uni\ufb01ed text-to-text transformer. J. Mach. Learn. Res. , 21(140):1\u201367, 2020. [89] Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot learners. arXiv preprint arXiv:2012.15723 , 2020. [90] Alex Wang and Kyunghyun Cho. Bert has a mouth, and it must speak: Bert as a markov random \ufb01eld language [87] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language",
    "162": "generation, translation, and comprehension. arXiv preprint arXiv:1910.13461 , 2019. [88] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. Exploring the limits of transfer learning with a uni\ufb01ed text-to-text transformer. J. Mach. Learn. Res. , 21(140):1\u201367, 2020. [89] Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot learners. arXiv preprint arXiv:2012.15723 , 2020. [90] Alex Wang and Kyunghyun Cho. Bert has a mouth, and it must speak: Bert as a markov random \ufb01eld language model. arXiv preprint arXiv:1902.04094 , 2019. [91] Julian Salazar, Davis Liang, Toan Q Nguyen, and Katrin Kirchhoff. Masked language model scoring. arXiv preprint arXiv:1910.14659 , 2019. [92] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi- task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461 , 2018. [93] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. Advances in neural information processing systems , 32, 2019. [94] Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R Thomas McCoy, Najoung Kim, Benjamin Van Durme, Samuel R Bowman, Dipanjan Das, et al. What do you learn from context? probing for sentence structure in contextualized word representations. arXiv preprint arXiv:1905.06316 , 2019. [95] Mario Giulianelli, Jack Harding, Florian Mohnert, Dieuwke Hupkes, and Willem Zuidema. Under the hood: Using diagnostic classi\ufb01ers to investigate and improve how language models track agreement information. arXiv preprint arXiv:1808.08079 , 2018. [96] Ian Tenney, Dipanjan Das, and Ellie Pavlick. Bert rediscovers the classical nlp pipeline. arXiv preprint arXiv:1905.05950 , 2019. [97] Taeuk Kim, Jihun Choi, Daniel Edmiston, and Sang-goo Lee. Are pre-trained language models aware of phrases? simple but strong baselines for grammar induction. arXiv preprint arXiv:2002.00737 , 2020. [98] John Hewitt and Christopher D Manning. A structural probe for \ufb01nding syntax in word representations. In structure in contextualized word representations. arXiv preprint arXiv:1905.06316 , 2019. [95] Mario Giulianelli, Jack Harding, Florian Mohnert, Dieuwke Hupkes, and Willem Zuidema. Under the hood: Using diagnostic classi\ufb01ers to investigate and improve how language models track agreement information. arXiv preprint arXiv:1808.08079 , 2018. [96] Ian Tenney, Dipanjan Das, and Ellie Pavlick. Bert rediscovers the classical nlp pipeline. arXiv preprint arXiv:1905.05950 , 2019. [97] Taeuk Kim, Jihun Choi, Daniel Edmiston, and Sang-goo Lee. Are pre-trained language models aware of phrases? simple but strong baselines for grammar induction. arXiv preprint arXiv:2002.00737 , 2020. [98] John Hewitt and Christopher D Manning. A structural probe for \ufb01nding syntax in word representations. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pages 4129\u20134138, 2019. [99] Anna Rogers, Olga Kovaleva, and Anna Rumshisky. A primer in bertology: What we know about how bert works. Transactions of the Association for Computational Linguistics , 8:842\u2013866, 2020. 21 Overview of Language Models [100] Nikunj Saunshi, Sadhika Malladi, and Sanjeev Arora. A mathematical exploration of why language models help solve downstream tasks. arXiv preprint arXiv:2010.03648 , 2020. [101] Colin Wei, Sang Michael Xie, and Tengyu Ma. Why do pretrained language models help in downstream tasks? an analysis of head and prompt tuning. Advances in Neural Information Processing Systems , 34:16158\u201316170, 2021. [102] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. arXiv preprint arXiv:1904.09751 , 2019. [103] Tiancheng Zhao, Ran Zhao, and Maxine Eskenazi. Learning discourse-level diversity for neural dialog models using conditional variational autoencoders. arXiv preprint arXiv:1703.10960 , 2017. [104] Zhen Xu, Bingquan Liu, Baoxun Wang, Cheng-Jie Sun, Xiaolong Wang, Zhuoran Wang, and Chao Qi. Neural response generation via gan with an approximate embedding layer. In Proceedings of the 2017 conference on empirical methods in natural language processing , pages 617\u2013626, 2017. [105] Jiwei Li, Will Monroe, and Dan Jurafsky. A simple, fast diverse decoding algorithm for neural generation. arXiv preprint arXiv:1611.08562 , 2016. 2021. [102] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. arXiv preprint arXiv:1904.09751 , 2019. [103] Tiancheng Zhao, Ran Zhao, and Maxine Eskenazi. Learning discourse-level diversity for neural dialog models using conditional variational autoencoders. arXiv preprint arXiv:1703.10960 , 2017. [104] Zhen Xu, Bingquan Liu, Baoxun Wang, Cheng-Jie Sun, Xiaolong Wang, Zhuoran Wang, and Chao Qi. Neural response generation via gan with an approximate embedding layer. In Proceedings of the 2017 conference on empirical methods in natural language processing , pages 617\u2013626, 2017. [105] Jiwei Li, Will Monroe, and Dan Jurafsky. A simple, fast diverse decoding algorithm for neural generation. arXiv preprint arXiv:1611.08562 , 2016. [106] Ashwin Vijayakumar, Michael Cogswell, Ramprasaath Selvaraju, Qing Sun, Stefan Lee, David Crandall, and Dhruv Batra. Diverse beam search for improved description of complex scenes. In Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence , volume 32, 2018. [107] Ilia Kulikov, Alexander H Miller, Kyunghyun Cho, and Jason Weston. Importance of search and evaluation strategies in neural dialogue modeling. arXiv preprint arXiv:1811.00907 , 2018. [108] Jiatao Gu, Kyunghyun Cho, and Victor OK Li. Trainable greedy decoding for neural machine translation. arXiv preprint arXiv:1702.02429 , 2017. [109] Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. arXiv preprint arXiv:1805.04833 , 2018. [110] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239 , 2022. [111] Jinjie Ni, Tom Young, Vlad Pandelea, Fuzhao Xue, and Erik Cambria. Recent advances in deep learning based dialogue systems: A systematic survey. Arti\ufb01cial intelligence review , pages 1\u2013101, 2022. [112] Bin Wang, Chen Zhang, Chengwei Wei, and Haizhou Li. A focused study on sequence length for dialogue summarization. arXiv preprint arXiv:2209.11910 , 2022. [113] Bin Wang, Chen Zhang, Yan Zhang, Yiming Chen, and Haizhou Li. Analyzing and evaluating faithfulness in dialogue summarization. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing , pages 4897\u20134908, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. arXiv:2201.08239 , 2022. [111] Jinjie Ni, Tom Young, Vlad Pandelea, Fuzhao Xue, and Erik Cambria. Recent advances in deep learning based dialogue systems: A systematic survey. Arti\ufb01cial intelligence review , pages 1\u2013101, 2022. [112] Bin Wang, Chen Zhang, Chengwei Wei, and Haizhou Li. A focused study on sequence length for dialogue summarization. arXiv preprint arXiv:2209.11910 , 2022. [113] Bin Wang, Chen Zhang, Yan Zhang, Yiming Chen, and Haizhou Li. Analyzing and evaluating faithfulness in dialogue summarization. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing , pages 4897\u20134908, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. [114] Chien-Sheng Wu and Caiming Xiong. Probing task-oriented dialogue representation from language models. In Proceedings of the 2020",
    "163": "Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 5036\u20135051, Online, November 2020. Association for Computational Linguistics. [115] Andrea Madotto, Zihan Liu, Zhaojiang Lin, and Pascale Fung. Language models as few-shot learner for task-oriented dialogue systems. arXiv preprint arXiv:2008.06239 , 2020. [116] Fred Jelinek, B Merialdo, S Roukos, M Strauss, et al. Self-organized language modeling for speech recognition. InReadings in speech recognition . Citeseer, 1990. [117] Manhung Siu and Mari Ostendorf. Variable n-grams and extensions for conversational speech language modeling. IEEE Transactions on Speech and Audio Processing , 8(1):63\u201375, 2000. [118] Ebru Ar\u0131soy, Stanley F Chen, Bhuvana Ramabhadran, and Abhinav Sethy. Converting neural network language models into back-off language models for ef\ufb01cient decoding in automatic speech recognition. IEEE/ACM Transactions on Audio, Speech, and Language Processing , 22(1):184\u2013192, 2013. [119] Ebru Arisoy, Abhinav Sethy, Bhuvana Ramabhadran, and Stanley Chen. Bidirectional recurrent neural network language models for automatic speech recognition. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) , pages 5421\u20135425. IEEE, 2015. [120] Zhiheng Huang, Geoffrey Zweig, and Benoit Dumoulin. Cache based recurrent neural network language model inference for \ufb01rst pass speech recognition. In 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) , pages 6354\u20136358. IEEE, 2014. 22 Overview of Language Models models into back-off language models for ef\ufb01cient decoding in automatic speech recognition. IEEE/ACM Transactions on Audio, Speech, and Language Processing , 22(1):184\u2013192, 2013. [119] Ebru Arisoy, Abhinav Sethy, Bhuvana Ramabhadran, and Stanley Chen. Bidirectional recurrent neural network language models for automatic speech recognition. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) , pages 5421\u20135425. IEEE, 2015. [120] Zhiheng Huang, Geoffrey Zweig, and Benoit Dumoulin. Cache based recurrent neural network language model inference for \ufb01rst pass speech recognition. In 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) , pages 6354\u20136358. IEEE, 2014. 22 Overview of Language Models [121] Joonbo Shin, Yoonhyung Lee, and Kyomin Jung. Effective sentence scoring method using bert for speech recognition. In Asian Conference on Machine Learning , pages 1081\u20131093. PMLR, 2019. [122] Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F Wong, and Lidia S Chao. Learning deep transformer models for machine translation. arXiv preprint arXiv:1906.01787 , 2019. [123] Max Weiss. Deepfake bot submissions to federal public comment websites cannot be distinguished from human submissions. Technology Science , 2019. [124] Adaku Uchendu, Thai Le, Kai Shu, and Dongwon Lee. Authorship attribution for neural text generation. In Conf. on Empirical Methods in Natural Language Processing (EMNLP) , 2020. [125] Tiziano Fagni, Fabrizio Falchi, Margherita Gambini, Antonio Martella, and Maurizio Tesconi. Tweepfake: About detecting deepfake tweets. Plos one , 16(5):e0251415, 2021. [126] James Thorne and Andreas Vlachos. Automated fact checking: Task formulations, methods and future directions. arXiv preprint arXiv:1806.07687 , 2018. [127] Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, and Douglas Eck. Automatic detection of generated text is easiest when humans are fooled. arXiv preprint arXiv:1911.00650 , 2019. [128] Sebastian Gehrmann, Hendrik Strobelt, and Alexander M Rush. Gltr: Statistical detection and visualization of generated text. arXiv preprint arXiv:1906.04043 , 2019. [129] Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. Defending against neural fake news. Advances in neural information processing systems , 32, 2019. [130] Ganesh Jawahar, Muhammad Abdul-Mageed, and Laks VS Lakshmanan. Automatic detection of machine arXiv preprint arXiv:1806.07687 , 2018. [127] Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, and Douglas Eck. Automatic detection of generated text is easiest when humans are fooled. arXiv preprint arXiv:1911.00650 , 2019. [128] Sebastian Gehrmann, Hendrik Strobelt, and Alexander M Rush. Gltr: Statistical detection and visualization of generated text. arXiv preprint arXiv:1906.04043 , 2019. [129] Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. Defending against neural fake news. Advances in neural information processing systems , 32, 2019. [130] Ganesh Jawahar, Muhammad Abdul-Mageed, and Laks VS Lakshmanan. Automatic detection of machine generated text: A critical survey. arXiv preprint arXiv:2011.01314 , 2020. [131] Harald Stiff and Fredrik Johansson. Detecting computer-generated disinformation. International Journal of Data Science and Analytics , 13(4):363\u2013383, 2022. [132] Yian Zhang, Alex Warstadt, Haau-Sing Li, and Samuel R Bowman. When do you need billions of words of pretraining data? arXiv preprint arXiv:2011.04946 , 2020. [133] Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. Pre-training transformers as energy-based cloze models. arXiv preprint arXiv:2012.08561 , 2020. [134] Timo Schick and Hinrich Sch\u00fctze. Exploiting cloze questions for few shot text classi\ufb01cation and natural language inference. arXiv preprint arXiv:2001.07676 , 2020. [135] Taylor Shin, Yasaman Razeghi, Robert L Logan IV , Eric Wallace, and Sameer Singh. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. arXiv preprint arXiv:2010.15980 , 2020. [136] Ziheng Wang, Jeremy Wohlwend, and Tao Lei. Structured pruning of large language models. arXiv preprint arXiv:1910.04732 , 2019. [137] Elena V oita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. arXiv preprint arXiv:1905.09418 , 2019. [138] Demi Guo, Alexander M Rush, and Yoon Kim. Parameter-ef\ufb01cient transfer learning with diff pruning. arXiv preprint arXiv:2012.07463 , 2020. [139] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. Tinybert: Distilling bert for natural language understanding. arXiv preprint arXiv:1909.10351 , 2019. [140] Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Well-read students learn better: On the arXiv:1910.04732 , 2019. [137] Elena V oita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. arXiv preprint arXiv:1905.09418 , 2019. [138] Demi Guo, Alexander M Rush, and Yoon Kim. Parameter-ef\ufb01cient transfer learning with diff pruning. arXiv preprint arXiv:2012.07463 , 2020. [139] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. Tinybert: Distilling bert for natural language understanding. arXiv preprint arXiv:1909.10351 , 2019. [140] Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Well-read students learn better: On the importance of pre-training compact models. arXiv preprint arXiv:1908.08962 , 2019. [141] Xindian Ma, Peng Zhang, Shuai Zhang, Nan Duan, Yuexian Hou, Ming Zhou, and Dawei Song. A tensorized transformer for language modeling. Advances in neural information processing systems , 32, 2019. [142] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 , 2021. [143] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942 , 2019. [144] Raj Dabre and Atsushi Fujita.",
    "164": "Recurrent stacking of layers for compact neural machine translation models. In Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence , volume 33, pages 6292\u20136299, 2019. [145] Sascha Rothe, Shashi Narayan, and Aliaksei Severyn. Leveraging pre-trained checkpoints for sequence generation tasks. Transactions of the Association for Computational Linguistics , 8:264\u2013280, 2020. 23 Overview of Language Models [146] Yun-Cheng Wang, Xiou Ge, Bin Wang, and C-C Jay Kuo. Kgboost: A classi\ufb01cation-based knowledge base completion method with negative sampling. Pattern Recognition Letters , 157:104\u2013111, 2022. [147] Xiou Ge, Yun-Cheng Wang, Bin Wang, and C-C Jay Kuo. Compounde: Knowledge graph embedding with translation, rotation and scaling compound operations. arXiv preprint arXiv:2207.05324 , 2022. [148] Xiao Huang, Jingyuan Zhang, Dingcheng Li, and Ping Li. Knowledge graph embedding based question answering. In Proceedings of the twelfth ACM international conference on web search and data mining , pages 105\u2013113, 2019. tasks. Transactions of the Association for Computational Linguistics , 8:264\u2013280, 2020. 23 Overview of Language Models [146] Yun-Cheng Wang, Xiou Ge, Bin Wang, and C-C Jay Kuo. Kgboost: A classi\ufb01cation-based knowledge base completion method with negative sampling. Pattern Recognition Letters , 157:104\u2013111, 2022. [147] Xiou Ge, Yun-Cheng Wang, Bin Wang, and C-C Jay Kuo. Compounde: Knowledge graph embedding with translation, rotation and scaling compound operations. arXiv preprint arXiv:2207.05324 , 2022. [148] Xiao Huang, Jingyuan Zhang, Dingcheng Li, and Ping Li. Knowledge graph embedding based question answering. In Proceedings of the twelfth ACM international conference on web search and data mining , pages 105\u2013113, 2019. [149] Luyang Huang, Lingfei Wu, and Lu Wang. Knowledge graph-augmented abstractive summarization with semantic-driven cloze reward. arXiv preprint arXiv:2005.01159 , 2020. [150] Fabio Petroni, Tim Rockt\u00e4schel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller, and Sebastian Riedel. Language models as knowledge bases? arXiv preprint arXiv:1909.01066 , 2019. [151] Oshin Agarwal, Heming Ge, Siamak Shakeri, and Rami Al-Rfou. Knowledge graph based synthetic corpus generation for knowledge-enhanced language model pre-training. arXiv preprint arXiv:2010.12688 , 2020. [152] Robert M French. Catastrophic forgetting in connectionist networks. Trends in cognitive sciences , 3(4):128\u2013135, 1999. [153] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences , 114(13):3521\u20133526, 2017. [154] Bin Wang, Guangtao Wang, Jing Huang, Jiaxuan You, Jure Leskovec, and C-C Jay Kuo. Inductive learning on commonsense knowledge graph completion. In 2021 International Joint Conference on Neural Networks (IJCNN) , pages 1\u20138. IEEE, 2021. [155] Roy Schwartz, Jesse Dodge, Noah A Smith, and Oren Etzioni. Green ai. Communications of the ACM , 63(12):54\u201363, 2020. [156] Jingjing Xu, Wangchunshu Zhou, Zhiyi Fu, Hao Zhou, and Lei Li. A survey on green deep learning. arXiv preprint arXiv:2111.05193 , 2021. [157] C.-C. Jay Kuo and Azad M Madni. Green learning: Introduction, examples and outlook. Journal of Visual Communication and Image Representation , page 103685, 2022. neural networks. Proceedings of the national academy of sciences , 114(13):3521\u20133526, 2017. [154] Bin Wang, Guangtao Wang, Jing Huang, Jiaxuan You, Jure Leskovec, and C-C Jay Kuo. Inductive learning on commonsense knowledge graph completion. In 2021 International Joint Conference on Neural Networks (IJCNN) , pages 1\u20138. IEEE, 2021. [155] Roy Schwartz, Jesse Dodge, Noah A Smith, and Oren Etzioni. Green ai. Communications of the ACM , 63(12):54\u201363, 2020. [156] Jingjing Xu, Wangchunshu Zhou, Zhiyi Fu, Hao Zhou, and Lei Li. A survey on green deep learning. arXiv preprint arXiv:2111.05193 , 2021. [157] C.-C. Jay Kuo and Azad M Madni. Green learning: Introduction, examples and outlook. Journal of Visual Communication and Image Representation , page 103685, 2022. [158] Yun-Cheng Wang, Xiou Ge, Bin Wang, and C-C Jay Kuo. GreenKGC: A lightweight knowledge graph completion method. arXiv preprint arXiv:2208.09137 , 2022. [159] Zhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin, Kurt Keutzer, Dan Klein, and Joey Gonzalez. Train big, then compress: Rethinking model size for ef\ufb01cient training and inference of transformers. In International Conference on Machine Learning , pages 5958\u20135968. PMLR, 2020. [160] Denghui Zhang, Zixuan Yuan, Yanchi Liu, Fuzhen Zhuang, Haifeng Chen, and Hui Xiong. E-bert: A phrase and product knowledge enhanced language model for e-commerce. arXiv preprint arXiv:2009.02835 , 2020. [161] Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, and Hoifung Poon. Domain-speci\ufb01c language model pretraining for biomedical natural language processing. ACM Transactions on Computing for Healthcare (HEALTH) , 3(1):1\u201323, 2021. [162] Kesong Liu, Jianhui Jiang, and Feifei Lyu. A domain knowledge enhanced pre-trained language model for vertical search: Case study on medicinal products. In Proceedings of the 29th International Conference on Computational Linguistics , pages 1014\u20131023, 2022. [163] Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, and Tie-Yan Liu. Biogpt: generative pre-trained transformer for biomedical text generation and mining. Brie\ufb01ngs in Bioinformatics , 23(6), 2022. [164] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361 , 2020. ACM Transactions on Computing for Healthcare (HEALTH) , 3(1):1\u201323, 2021. [162] Kesong Liu, Jianhui Jiang, and Feifei Lyu. A domain knowledge enhanced pre-trained language model for vertical search: Case study on medicinal products. In Proceedings of the 29th International Conference on Computational Linguistics , pages 1014\u20131023, 2022. [163] Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, and Tie-Yan Liu. Biogpt: generative pre-trained transformer for biomedical text generation and mining. Brie\ufb01ngs in Bioinformatics , 23(6), 2022. [164] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361 , 2020. [165] Laura Dietz, Chenyan Xiong, Jeff Dalton, and Edgar Meij. The second workshop on knowledge graphs and semantics for text retrieval, analysis, and understanding (kg4ir). In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval , pages 1423\u20131426, 2018. [166] Yuhao Yang, Chao Huang, Lianghao Xia, and Chenliang Li. Knowledge graph contrastive learning for recom- mendation. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval , pages 1434\u20131443, 2022. 24 Visual Instruction Tuning Haotian Liu1\u2217, Chunyuan Li2\u2217, Qingyang Wu3, Yong Jae Lee1 1University of Wisconsin\u2013Madison2Microsoft Research3Columbia University https://llava-vl.github.io Abstract Instruction tuning large language models (LLMs) using machine-generated instruction-following data has been shown to improve zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. We present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we in- troduce LLaV A: Large Language andVision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and an LLM for general- purpose visual and language understanding. To",
    "165": "facilitate future research on visual instruction following, we construct two evaluation benchmarks with diverse and challenging application-oriented tasks. Our experiments show that LLaV A demon- strates impressive multimodal chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% rela- tive score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaV A and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated visual instruction tuning data, our model, and code publicly available. 1 Introduction Humans interact with the world through many channels such as vision and language, as each individual channel has a unique advantage in representing and communicating certain concepts, and thus facilitates a better understanding of the world. One of the core aspirations in artificial intelligence is to develop a general-purpose assistant that can effectively follow multi-modal vision-and-language instructions, aligned with human intent to complete various real-world tasks in the wild [4, 27, 26]. To this end, the community has witnessed an emergent interest in developing language-augmented foundation vision models [ 27,16], with strong capabilities in open-world visual understanding such as classification [ 40,21,57,54,39], detection [ 29,62,33], segmentation [ 25,63,58] and captioning [ 50,28], as well as visual generation and editing [ 42,43,56,15,44,30]. We refer readers to the Computer Vision in the Wild reading list for a more up-to-date literature compilation [ 12]. In is to develop a general-purpose assistant that can effectively follow multi-modal vision-and-language instructions, aligned with human intent to complete various real-world tasks in the wild [4, 27, 26]. To this end, the community has witnessed an emergent interest in developing language-augmented foundation vision models [ 27,16], with strong capabilities in open-world visual understanding such as classification [ 40,21,57,54,39], detection [ 29,62,33], segmentation [ 25,63,58] and captioning [ 50,28], as well as visual generation and editing [ 42,43,56,15,44,30]. We refer readers to the Computer Vision in the Wild reading list for a more up-to-date literature compilation [ 12]. In this line of work, each task is solved independently by one single large vision model, with the task instruction implicitly considered in the model design. Further, language is only utilized to describe the image content. While this allows language to play an important role in mapping visual signals to language semantics\u2014a common channel for human communication, it leads to models that usually have a fixed interface with limited interactivity and adaptability to the user\u2019s instructions. Large language models (LLM), on the other hand, have shown that language can play a wider role: a universal interface for a general-purpose assistant, where various task instructions can be explicitly represented in language and guide the end-to-end trained neural assistant to switch to the task of interest to solve it. For example, the recent success of ChatGPT [ 35] and GPT-4 [ 36] have demonstrated the power of aligned LLMs in following human instructions, and have stimulated tremendous interest in developing open-source LLMs. Among them, LLaMA [ 49] is an open- source LLM that matches the performance of GPT-3. Alpaca [ 48], Vicuna [ 9], GPT-4-LLM [ 38] 37th Conference on Neural Information Processing Systems (NeurIPS 2023).arXiv:2304.08485v2 [cs.CV] 11 Dec 2023 utilize various machine-generated high-quality instruction-following samples to improve the LLM\u2019s alignment ability, reporting impressive performance compared with proprietary LLMs. Importantly, this line of work is text-only . In this paper, we present visual instruction-tuning , the first attempt to extend instruction-tuning to the language-image multimodal space, to pave the way towards building a general-purpose visual assistant. In particular, our paper makes the following contributions: tremendous interest in developing open-source LLMs. Among them, LLaMA [ 49] is an open- source LLM that matches the performance of GPT-3. Alpaca [ 48], Vicuna [ 9], GPT-4-LLM [ 38] 37th Conference on Neural Information Processing Systems (NeurIPS 2023).arXiv:2304.08485v2 [cs.CV] 11 Dec 2023 utilize various machine-generated high-quality instruction-following samples to improve the LLM\u2019s alignment ability, reporting impressive performance compared with proprietary LLMs. Importantly, this line of work is text-only . In this paper, we present visual instruction-tuning , the first attempt to extend instruction-tuning to the language-image multimodal space, to pave the way towards building a general-purpose visual assistant. In particular, our paper makes the following contributions: \u2022Multimodal instruction-following data . One key challenge is the lack of vision-language instruction-following data. We present a data reformation perspective and pipeline to convert image-text pairs into an appropriate instruction-following format, using ChatGPT/GPT-4. \u2022Large multimodal models . We develop a large multimodal model (LMM), by connecting the open-set visual encoder of CLIP [ 40] with the language decoder Vicuna [ 9], and fine-tuning end-to-end on our generated instructional vision-language data. Our empirical study validates the effectiveness of using generated data for LMM instruction-tuning, and suggests practical tips for building a general-purpose instruction-following visual agent. When ensembled with GPT-4, our approach achieves SoTA on the Science QA [34] multimodal reasoning dataset. \u2022Multimodal instruction-following benchmark . We present LLaV A-Bench with two challenging benchmarks, with a diverse selection of paired images, instructions and detailed annotations. \u2022Open-source . We release the following assets to the public: the generated multimodal instruction data, the codebase, the model checkpoints, and a visual chat demo. 2 Related Work Multimodal Instruction-following Agents. In computer vision, existing works that build instruction- following agents can be broadly categorized into two classes: (i)End-to-end trained models, which are separately explored for each specific research topic. For example, the vision-language navigation task [ 3,19] and Habitat [ 47] require the embodied AI agent to follow natural language instructions and take a sequence of actions to complete goals in visual environments. In the image editing domain, benchmarks, with a diverse selection of paired images, instructions and detailed annotations. \u2022Open-source . We release the following assets to the public: the generated multimodal instruction data, the codebase, the model checkpoints, and a visual chat demo. 2 Related Work Multimodal Instruction-following Agents. In computer vision, existing works that build instruction- following agents can be broadly categorized into two classes: (i)End-to-end trained models, which are separately explored for each specific research topic. For example, the vision-language navigation task [ 3,19] and Habitat [ 47] require the embodied AI agent to follow natural language instructions and take a sequence of actions to complete goals in visual environments. In the image editing domain, given an input image and a written instruction that tells the agent what to do, InstructPix2Pix [ 6] edits images by following the human instructions. (ii)A system that coordinates various models via LangChain [ 1] / LLMs [ 35], such as Visual ChatGPT [ 53], X-GPT [ 63], MM-REACT [ 55], VisProg [ 18], and ViperGPT [ 46]. While sharing the same goal in building instruction-following agents, we focus on developing an end-to-end trained language-vision multimodal model for multiple tasks. Instruction Tuning. In the natural language processing (NLP) community, to enable LLMs such as GPT-3 [ 7], T5 [ 41], PaLM [ 10], and OPT [",
    "166": "60] to follow natural language instructions and complete real-world tasks, researchers have explored methods for LLM instruction-tuning [ 37,52,51], leading to instruction-tuned counterparts such as InstructGPT [ 37]/ChatGPT [ 35], FLAN-T5 [ 11], FLAN-PaLM [ 11], and OPT-IML [ 22], respectively. It turns out that this simple approach can effectively improve the zero- and few-shot generalization abilities of LLMs. It is thus natural to borrow the idea from NLP to computer vision. More broadly, the teacher-student distillation ideas with foundation models have been studied in other topics such as image classification [ 14]. Flamingo [ 2] can be viewed as the GPT-3 moment in the multimodal domain, due to its strong performance on zero-shot task transfer and in-context-learning. Other LMMs trained on image- text pairs include BLIP-2 [ 28], FROMAGe [ 24], and KOSMOS-1 [ 20]. PaLM-E [ 13] is an LMM for embodied AI. Based on the recent \u201cbest\u201d open-source LLM LLaMA, OpenFlamingo [ 5] and LLaMA-Adapter [ 59] are open-source efforts that enable LLaMA to use image inputs, paving the effectively improve the zero- and few-shot generalization abilities of LLMs. It is thus natural to borrow the idea from NLP to computer vision. More broadly, the teacher-student distillation ideas with foundation models have been studied in other topics such as image classification [ 14]. Flamingo [ 2] can be viewed as the GPT-3 moment in the multimodal domain, due to its strong performance on zero-shot task transfer and in-context-learning. Other LMMs trained on image- text pairs include BLIP-2 [ 28], FROMAGe [ 24], and KOSMOS-1 [ 20]. PaLM-E [ 13] is an LMM for embodied AI. Based on the recent \u201cbest\u201d open-source LLM LLaMA, OpenFlamingo [ 5] and LLaMA-Adapter [ 59] are open-source efforts that enable LLaMA to use image inputs, paving the way to build open-source multimodal LLMs. While these models present promising task transfer generalization performance, they are not explicitly tuned with vision-language instruction data, and their performance in multimodal tasks usually falls short compared to language-only tasks. In this paper, we aim to fill this gap and study its effectiveness. Finally, note that visual instruction tuning is different from visual prompt tuning [ 23]: the former aims to improve the model\u2019s instruction- following abilities, while the latter aims to improve the parameter-efficiency in model adaptation. 3 GPT-assisted Visual Instruction Data Generation The community has witnessed a surge in the amount of public multimodal data such as image-text pairs, ranging from CC [ 8] to LAION [ 45]. However, when it comes to multimodal instruction- 2 Context type 1: Captions A group of people standing outside of a black vehicle with various luggage. Luggage surrounds a vehicle in an underground parking area People try to fit all of their luggage in an SUV . The sport utility vehicle is parked in the public garage, being packed for a trip Some people with luggage near a van that is transporting it. Context type 2: Boxes person: [0.681, 0.242, 0.774, 0.694], backpack: [0.384, 0.696, 0.485, 0.914], suitcase: ...<omitted> Response type 1: conversation Question: What type of vehicle is featured in the image? Answer: The image features a black sport utility vehicle (SUV) ...<omitted> Response type 2: detailed description The image is an underground parking area with a black sport utility vehicle (SUV) parked. There are three people in the scene, with one person standing closer to the left side of the vehicle, another Luggage surrounds a vehicle in an underground parking area People try to fit all of their luggage in an SUV . The sport utility vehicle is parked in the public garage, being packed for a trip Some people with luggage near a van that is transporting it. Context type 2: Boxes person: [0.681, 0.242, 0.774, 0.694], backpack: [0.384, 0.696, 0.485, 0.914], suitcase: ...<omitted> Response type 1: conversation Question: What type of vehicle is featured in the image? Answer: The image features a black sport utility vehicle (SUV) ...<omitted> Response type 2: detailed description The image is an underground parking area with a black sport utility vehicle (SUV) parked. There are three people in the scene, with one person standing closer to the left side of the vehicle, another person in the middle, and the third person on the right side. They are all working together to pack their luggage into the SUV for a trip. ...<omitted> Response type 3: complex reasoning Question: What challenges do these people face? Answer: In the image, a group of people is standing outside a black SUV in a parking area, surrounded by various pieces of luggage, including suitcases and backpacks. They are facing the challenge of fitting all their luggage into the black SUV . There are multiple suitcases and backpacks to be packed, which suggests that the group has a significant amount of belongings ...<omitted> Table 1: One example to illustrate the instruction-following data. The top block shows the contexts such as captions and boxes used to prompt GPT, and the bottom block shows the three types of responses. Note that the visual image is not used to prompt GPT, we only show it here as a reference. following data, the available amount is limited, partially because the process for creating such data is time-consuming and less well-defined when human crowd-scouring is considered. Inspired by the success of recent GPT models in text-annotation tasks [ 17], we propose to leverage ChatGPT/GPT-4 for multimodal instruction-following data collection, based on the widely existing image-pair data. For an image Xvand its associated caption Xc, it is natural to create a set of questions Xqwith the intent to instruct the assistant to describe the image content. We prompt GPT-4 to curate such a list of questions (see details in Appendix). Therefore, a simple way to expand an image-text pair to its instruction-following version is Human :XqXv<STOP> Assistant :Xc<STOP> . Though cheap to following data, the available amount is limited, partially because the process for creating such data is time-consuming and less well-defined when human crowd-scouring is considered. Inspired by the success of recent GPT models in text-annotation tasks [ 17], we propose to leverage ChatGPT/GPT-4 for multimodal instruction-following data collection, based on the widely existing image-pair data. For an image Xvand its associated caption Xc, it is natural to create a set of questions Xqwith the intent to instruct the assistant to describe the image content. We prompt GPT-4 to curate such a list of questions (see details in Appendix). Therefore, a simple way to expand an image-text pair to its instruction-following version is Human :XqXv<STOP> Assistant :Xc<STOP> . Though cheap to construct, this simple expanded version lacks diversity and in-depth reasoning in both the instructions and responses. To mitigate this issue, we leverage language-only GPT-4 or ChatGPT as the strong teacher (both accept only text as input), to create instruction-following data involving visual content. Specifically, in order to encode an image into its visual features to prompt a text-only GPT, we use two types of symbolic representations: (i)Captions typically describe the visual scene from various perspectives; (ii)Bounding boxes usually localize the objects in the scene, and each box encodes the object concept and its spatial location. One example is shown in the top block of Table 14. This symbolic representation allows us to encode the image as an LLM-recognizable sequence. We use COCO images [ 31] and generate three types of instruction-following data. One example per type is shown in the bottom block of Table 14. For each type, we first manually design a few examples. They are the only human annotations we have during data collection, and are used as seed examples in in-context-learning to query GPT-4. \u2022Conversation . We design a conversation between",
    "167": "the assistant and a person asking questions about this photo. The answers are in a tone as if the assistant is seeing the image and answering the question. A diverse set of questions are asked about the visual content of the image, including the object types, counting the objects, object actions, object locations, relative positions between objects. Only questions that have definite answers are considered. Please see Appendix for the detailed prompt. is shown in the bottom block of Table 14. For each type, we first manually design a few examples. They are the only human annotations we have during data collection, and are used as seed examples in in-context-learning to query GPT-4. \u2022Conversation . We design a conversation between the assistant and a person asking questions about this photo. The answers are in a tone as if the assistant is seeing the image and answering the question. A diverse set of questions are asked about the visual content of the image, including the object types, counting the objects, object actions, object locations, relative positions between objects. Only questions that have definite answers are considered. Please see Appendix for the detailed prompt. \u2022Detailed description . To include a rich and comprehensive description for an image, we create a list of questions with such an intent. We prompt GPT-4 then curate the list (see detailed prompts 3 and curation process in Appendix). For each image, we randomly sample one question from the list to ask GPT-4 to generate the detailed description. \u2022Complex reasoning . The above two types focus on the visual content itself, based on which we further create in-depth reasoning questions. The answers typically require a step-by-step reasoning process by following rigorous logic. We collect 158K unique language-image instruction-following samples in total, including 58K in conversations, 23K in detailed description, and 77k in complex reasoning, respectively. We ablated the use of ChatGPT and GPT-4 in our early experiments, and found that GPT-4 consistently provides higher quality instruction-following data, such as spatial reasoning. 4 Visual Instruction Tuning 4.1 Architecture The primary goal is to effectively leverage the capabilities of both the pre-trained LLM and visual model. The network archtecture is illustrated in Figure 1. We choose Vicuna [ 9] as our LLM f\u03d5(\u00b7) parameterized by \u03d5, as it has the best instruction following capabilities in language tasks among publicly available checkpoints [48, 9, 38]. reasoning process by following rigorous logic. We collect 158K unique language-image instruction-following samples in total, including 58K in conversations, 23K in detailed description, and 77k in complex reasoning, respectively. We ablated the use of ChatGPT and GPT-4 in our early experiments, and found that GPT-4 consistently provides higher quality instruction-following data, such as spatial reasoning. 4 Visual Instruction Tuning 4.1 Architecture The primary goal is to effectively leverage the capabilities of both the pre-trained LLM and visual model. The network archtecture is illustrated in Figure 1. We choose Vicuna [ 9] as our LLM f\u03d5(\u00b7) parameterized by \u03d5, as it has the best instruction following capabilities in language tasks among publicly available checkpoints [48, 9, 38]. Vision Encoder<latexit sha1_base64=\"nmaulJAcZ9L9s1EtmepKU/wnbmw=\">AAAB7XicbVDLSgNBEOz1GeMr6tHLYBA8hV0R9Rj04jGCeUASwuykNxkzO7PMzAphyT948aCIV//Hm3/jJNmDJhY0FFXddHeFieDG+v63t7K6tr6xWdgqbu/s7u2XDg4bRqWaYZ0poXQrpAYFl1i33ApsJRppHApshqPbqd98Qm24kg92nGA3pgPJI86odVKjE0ZZc9Irlf2KPwNZJkFOypCj1it9dfqKpTFKywQ1ph34ie1mVFvOBE6KndRgQtmIDrDtqKQxmm42u3ZCTp3SJ5HSrqQlM/X3REZjY8Zx6Dpjaodm0ZuK/3nt1EbX3YzLJLUo2XxRlApiFZm+TvpcI7Ni7AhlmrtbCRtSTZl1ARVdCMHiy8ukcV4JLivB/UW5epPHUYBjOIEzCOAKqnAHNagDg0d4hld485T34r17H/PWFS+fOYI/8D5/AKc0jy8=</latexit>W<latexit sha1_base64=\"Dpm7JzZPmwmdKOQRfbESjXZnclA=\">AAAB9XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cK9gOaWDbbTbt0dxN2N0oJ+R9ePCji1f/izX/jts1BWx8MPN6bYWZemHCmjet+O6WV1bX1jfJmZWt7Z3evun/Q1nGqCG2RmMeqG2JNOZO0ZZjhtJsoikXIaScc30z9ziNVmsXy3kwSGgg8lCxiBBsrPUT9zA+jDPnJiOV5v1pz6+4MaJl4BalBgWa/+uUPYpIKKg3hWOue5yYmyLAyjHCaV/xU0wSTMR7SnqUSC6qDbHZ1jk6sMkBRrGxJg2bq74kMC60nIrSdApuRXvSm4n9eLzXRVZAxmaSGSjJfFKUcmRhNI0ADpigxfGIJJorZWxEZYYWJsUFVbAje4svLpH1W9y7q3t15rXFdxFGGIziGU/DgEhpwC01oAQEFz/AKb86T8+K8Ox/z1pJTzBzCHzifP63EkqI=</latexit>f\u0000Projection<latexit sha1_base64=\"w6zXykpCeX38FkkjKhaO+oe7y2E=\">AAAB/XicbVDLSsNAFJ34rPUVHzs3g0VwVRIRdVl047KCfUATwmQ6aYdOJmHmplhD8FfcuFDErf/hzr9x+lho64ELh3Pu5d57wlRwDY7zbS0tr6yurZc2yptb2zu79t5+UyeZoqxBE5Godkg0E1yyBnAQrJ0qRuJQsFY4uBn7rSFTmifyHkYp82PSkzzilICRAvsw98IIt4sg94A9AEA+LIrArjhVZwK8SNwZqaAZ6oH95XUTmsVMAhVE647rpODnRAGnghVlL9MsJXRAeqxjqCQx034+ub7AJ0bp4ihRpiTgifp7Iiex1qM4NJ0xgb6e98bif14ng+jKz7lMM2CSThdFmcCQ4HEUuMsVoyBGhhCquLkV0z5RhIIJrGxCcOdfXiTNs6p7UXXvziu161kcJXSEjtEpctElqqFbVEcNRNEjekav6M16sl6sd+tj2rpkzWYO0B9Ynz9pXpXb</latexit>Xv <latexit sha1_base64=\"OUBGioCbGiTqe2ceR2O0Z900t+I=\">AAAB/XicbVDLSsNAFJ3UV62v+Ni5GSyCq5KIqMuiG5cV7AObECbTSTt08mDmplhD8FfcuFDErf/hzr9x2mahrQcuHM65l3vv8RPBFVjWt1FaWl5ZXSuvVzY2t7Z3zN29lopTSVmTxiKWHZ8oJnjEmsBBsE4iGQl9wdr+8Hrit0dMKh5HdzBOmBuSfsQDTgloyTMPMscP8H3uZQ6wBwDIRnnumVWrZk2BF4ldkCoq0PDML6cX0zRkEVBBlOraVgJuRiRwKlhecVLFEkKHpM+6mkYkZMrNptfn+FgrPRzEUlcEeKr+nshIqNQ49HVnSGCg5r2J+J/XTSG4dDMeJSmwiM4WBanAEONJFLjHJaMgxpoQKrm+FdMBkYSCDqyiQ7DnX14krdOafV6zb8+q9asijjI6REfoBNnoAtXRDWqgJqLoET2jV/RmPBkvxrvxMWstGcXMPvoD4/MHbIKV3Q==</latexit>Zv<latexit sha1_base64=\"yn0ZUZSqKssiKPDZqlv9C0+1rLg=\">AAAB/XicbVDLSsNAFJ3UV62v+Ni5GSyCq5KIqMuimy4r2Ac0IUymk3bo5MHMTbGG4K+4caGIW//DnX/jtM1CWw9cOJxzL/fe4yeCK7Csb6O0srq2vlHerGxt7+zumfsHbRWnkrIWjUUsuz5RTPCItYCDYN1EMhL6gnX80e3U74yZVDyO7mGSMDckg4gHnBLQkmceZY4f4EbuZQ6wBwDIxnnumVWrZs2Al4ldkCoq0PTML6cf0zRkEVBBlOrZVgJuRiRwKlhecVLFEkJHZMB6mkYkZMrNZtfn+FQrfRzEUlcEeKb+nshIqNQk9HVnSGCoFr2p+J/XSyG4djMeJSmwiM4XBanAEONpFLjPJaMgJpoQKrm+FdMhkYSCDqyiQ7AXX14m7fOafVmz7y6q9ZsijjI6RifoDNnoCtVRAzVRC1H0iJ7RK3oznowX4934mLeWjGLmEP2B8fkDUD6Vyw==</latexit>HvImageLanguage InstructionLanguage Response <latexit sha1_base64=\"/KN5R7NUwEKH6XBR4DKeLzGzIrU=\">AAAB/XicbVDLSsNAFJ3UV62v+Ni5GSyCq5KIqMuimy4r2Ac0IUymk3bo5OHMjVhD8FfcuFDErf/hzr9x2mahrQcuHM65l3vv8RPBFVjWt1FaWl5ZXSuvVzY2t7Z3zN29topTSVmLxiKWXZ8oJnjEWsBBsG4iGQl9wTr+6Hrid+6ZVDyObmGcMDckg4gHnBLQkmceZI4f4EbuZQ6wBwDI7vLcM6tWzZoCLxK7IFVUoOmZX04/pmnIIqCCKNWzrQTcjEjgVLC84qSKJYSOyID1NI1IyJSbTa/P8bFW+jiIpa4I8FT9PZGRUKlx6OvOkMBQzXsT8T+vl0Jw6WY8SlJgEZ0tClKBIcaTKHCfS0ZBjDUhVHJ9K6ZDIgkFHVhFh2DPv7xI2qc1+7xm35xV61dFHGV0iI7QCbLRBaqjBmqiFqLoET2jV/RmPBkvxrvxMWstGcXMPvoD4/MHSKCVxg==</latexit>Hq <latexit sha1_base64=\"4a/5KuBhqFrRimsGds8xVP6ZkkY=\">AAAB/XicbVDLSsNAFJ34rPUVHzs3g0VwVRIRdVl047KCfUAbwmQ6aYdOJnHmRqwh+CtuXCji1v9w5984bbPQ1gMXDufcy733BIngGhzn21pYXFpeWS2tldc3Nre27Z3dpo5TRVmDxiJW7YBoJrhkDeAgWDtRjESBYK1geDX2W/dMaR7LWxglzItIX/KQUwJG8u39rBuEuJ37WRfYAwBkd3nu2xWn6kyA54lbkAoqUPftr24vpmnEJFBBtO64TgJeRhRwKlhe7qaaJYQOSZ91DJUkYtrLJtfn+MgoPRzGypQEPFF/T2Qk0noUBaYzIjDQs95Y/M/rpBBeeBmXSQpM0umiMBUYYjyOAve4YhTEyBBCFTe3YjogilAwgZVNCO7sy/OkeVJ1z6ruzWmldlnEUUIH6BAdIxedoxq6RnXUQBQ9omf0it6sJ+vFerc+pq0LVjGzh/7A+vwBYcCV1g==</latexit>Xq<latexit sha1_base64=\"I8RxJE902anMmciAczfxKVfe1PY=\">AAAB/XicbVDLSsNAFJ3UV62v+Ni5GSyCq5KIqMuiG5cV7APaECbTSTt0MgkzN2INwV9x40IRt/6HO//GaZuFth64cDjnXu69J0gE1+A431ZpaXllda28XtnY3NresXf3WjpOFWVNGotYdQKimeCSNYGDYJ1EMRIFgrWD0fXEb98zpXks72CcMC8iA8lDTgkYybcPsl4Q4k7uZz1gDwCQkTz37apTc6bAi8QtSBUVaPj2V68f0zRiEqggWnddJwEvIwo4FSyv9FLNEkJHZMC6hkoSMe1l0+tzfGyUPg5jZUoCnqq/JzISaT2OAtMZERjqeW8i/ud1UwgvvYzLJAUm6WxRmAoMMZ5EgftcMQpibAihiptbMR0SRSiYwComBHf+5UXSOq255zX39qxavyriKKNDdIROkIsuUB3doAZqIooe0TN6RW/Wk/VivVsfs9aSVczsoz+wPn8ASWCVxg==</latexit>XaLanguage Model Figure 1: LLaV A network architecture. For an input image Xv, we consider the pre-trained CLIP visual encoder ViT-L/14 [ 40], which provides the visual feature Zv=g(Xv). The grid features before and after the last Transformer layer are considered in our experiments. We consider a simple linear layer to connect image features into the word embedding space. Specifically, we apply a",
    "168": "trainable projection matrix Wto convert Zvinto language embedding tokens Hv, which have the same dimensionality as the word embedding space in the language model: Hv=W\u00b7Zv,withZv=g(Xv) (1) Thus, we have a sequence of visual tokens Hv. Note that our simple projection scheme is lightweight, which allows us to iterate data centric experiments quickly. More sophisticated schemes to con- nect the image and language representations can also be considered, such as gated cross-attention in Flamingo [ 2] and Q-former in BLIP-2 [ 28]. We leave exploring possibly more effective and sophisticated architecture designs for LLaV A as future work. 4.2 Training For each image Xv, we generate multi-turn conversation data (X1 q,X1 a,\u00b7\u00b7\u00b7,XT q,XT a), where Tis the total number of turns. We organize them as a sequence, by treating all answers as the assistant\u2019s response, and the instruction Xt instruct at the t-th turn as: Xt instruct =\u001aRandomly choose [X1 q,Xv]or[Xv,X1 q],the first turn t= 1 Xt q, the remaining turns t >1(2) which allows us to iterate data centric experiments quickly. More sophisticated schemes to con- nect the image and language representations can also be considered, such as gated cross-attention in Flamingo [ 2] and Q-former in BLIP-2 [ 28]. We leave exploring possibly more effective and sophisticated architecture designs for LLaV A as future work. 4.2 Training For each image Xv, we generate multi-turn conversation data (X1 q,X1 a,\u00b7\u00b7\u00b7,XT q,XT a), where Tis the total number of turns. We organize them as a sequence, by treating all answers as the assistant\u2019s response, and the instruction Xt instruct at the t-th turn as: Xt instruct =\u001aRandomly choose [X1 q,Xv]or[Xv,X1 q],the first turn t= 1 Xt q, the remaining turns t >1(2) This leads to the unified format for the multimodal instruction-following sequence illustrated in Table 2. We perform instruction-tuning of the LLM on the prediction tokens, using its original auto-regressive training objective. Specifically, for a sequence of length L, we compute the probability of the target answers Xaby: p(Xa|Xv,Xinstruct ) =LY i=1p\u03b8(xi|Xv,Xinstruct ,<i,Xa,<i), (3) 4 Xsystem-message <STOP> Human :X1 instruct <STOP> Assistant :X1 a<STOP> Human :X2 instruct <STOP> Assistant :X2 a<STOP> \u00b7 \u00b7 \u00b7 Table 2: The input sequence used to train the model. Only two conversation turns are illustrated here; in practice, the number of turns varies based on the instruction-following data. In our current implementation, we follow Vicuna-v0 [ 9] to set the system message Xsystem-message and we set <STOP> =###. The model is trained to predict the assistant answers and where to stop, and thus only green sequence/tokens are used to compute the loss in the auto-regressive model. where \u03b8is the trainable parameters, Xinstruct ,<iandXa,<iare the instruction and answer tokens in all turns before the current prediction token xi, respectively. Please see Table 2 for an illustration of the prediction tokens. For the conditionals in (3), we explicitly add Xvto emphasize the fact that the image is grounded for all answers, and we omit Xsystem-message and all previous <STOP> for better readability. For LLaV A model training, we consider a two-stage instruction-tuning procedure. Stage 1: Pre-training for Feature Alignment. To strike a balance between concept coverage and training efficiency, we filter CC3M to 595K image-text pairs. Please see Appendix for details only green sequence/tokens are used to compute the loss in the auto-regressive model. where \u03b8is the trainable parameters, Xinstruct ,<iandXa,<iare the instruction and answer tokens in all turns before the current prediction token xi, respectively. Please see Table 2 for an illustration of the prediction tokens. For the conditionals in (3), we explicitly add Xvto emphasize the fact that the image is grounded for all answers, and we omit Xsystem-message and all previous <STOP> for better readability. For LLaV A model training, we consider a two-stage instruction-tuning procedure. Stage 1: Pre-training for Feature Alignment. To strike a balance between concept coverage and training efficiency, we filter CC3M to 595K image-text pairs. Please see Appendix for details of the filtering process. These pairs are converted to the instruction-following data using the naive expansion method describe in Section 3. Each sample can be treated as a single-turn conversation. To construct the input Xinstruct in(2), for an image Xv, a question Xqis randomly sampled, which is a language instruction to request the assistant to describe the image briefly. The ground-truth prediction answer Xais the original caption. In training, we keep both the visual encoder and LLM weights frozen, and maximize the likelihood of (3)with trainable parameters \u03b8=W(the projection matrix) only. In this way, the image features Hvcan be aligned with the pre-trained LLM word embedding. This stage can be understood as training a compatible visual tokenizer for the frozen LLM. Stage 2: Fine-tuning End-to-End. We always keep the visual encoder weights frozen, and continue to update both the pre-trained weights of the projection layer and LLM in LLaV A; i.e., the trainable parameters are \u03b8={W,\u03d5}in (3). We consider two specific use case scenarios: \u2022Multimodal Chatbot . We develop a Chatbot by fine-tuning on the 158K language-image instruction-following data in Section 3. Among the three types of responses, conversation is multi-turn while the other two are single-turn. They are uniformly sampled in training. \u2022Science QA . We study our method on the ScienceQA benchmark [ 34], the first large-scale multimodal science question dataset that annotates the answers with detailed lectures and explanations. Each question is provided a context in the form of natural language or an image. The assistant provides the reasoning process in natural language and selects the answer among parameters are \u03b8={W,\u03d5}in (3). We consider two specific use case scenarios: \u2022Multimodal Chatbot . We develop a Chatbot by fine-tuning on the 158K language-image instruction-following data in Section 3. Among the three types of responses, conversation is multi-turn while the other two are single-turn. They are uniformly sampled in training. \u2022Science QA . We study our method on the ScienceQA benchmark [ 34], the first large-scale multimodal science question dataset that annotates the answers with detailed lectures and explanations. Each question is provided a context in the form of natural language or an image. The assistant provides the reasoning process in natural language and selects the answer among multiple choices. For training in (2), we organize the data as a single turn conversation, the question & context as Xinstruct , and reasoning & answer as Xa. 5 Experiments We assess the performance of LLaV A in instruction-following and visual reasoning capabilities with two primary experimental settings: multimodal chatbot and the ScienceQA dataset, respectively. We train all models with 8 \u00d7A100s, following Vicuna\u2019s hyperparameters [ 9]. We pre-train our model on the filtered CC-595K subset for 1 epoch with a learning rate of 2e-3 and a batch size of 128, and fine-tune on the proposed LLaV A-Instruct-158K dataset for 3 epochs, with a learning rate of 2e-5 and a batch size of 32. See Appendix for more training details. 5.1 Multimodal Chatbot We developed a chatbot demo to show the image understanding and conversation abilities of LLaV A, and to study how well LLaV A is able to digest visual inputs and exhibit instruction-following capabilities. We first use the examples in the original GPT-4 paper [ 36], shown in Table 3 (more examples in Appendix), that require in-depth image understanding. For comparisons, we quote the prompt and response of the multimodal GPT-4 from their paper, and query BLIP-2 and OpenFlamingo model checkpoints to get their response. 5 Visual input example, Extreme Ironing: Source:",
    "169": "https://www.barnorama.com/wp-content/uploads/2016/12/03-Confusing-Pictures.jpg User What is unusual about this image? LLaV A The unusual aspect of this image is a man ironing clothes on the back of a minivan or van. This is not a typical place to perform this activity, as one would usually iron clothes in a more stationary and safe location, such as a home, using a regular ironing board. capabilities. We first use the examples in the original GPT-4 paper [ 36], shown in Table 3 (more examples in Appendix), that require in-depth image understanding. For comparisons, we quote the prompt and response of the multimodal GPT-4 from their paper, and query BLIP-2 and OpenFlamingo model checkpoints to get their response. 5 Visual input example, Extreme Ironing: Source: https://www.barnorama.com/wp-content/uploads/2016/12/03-Confusing-Pictures.jpg User What is unusual about this image? LLaV A The unusual aspect of this image is a man ironing clothes on the back of a minivan or van. This is not a typical place to perform this activity, as one would usually iron clothes in a more stationary and safe location, such as a home, using a regular ironing board. The scene depicted in the image is peculiar as it involves a makeshift ironing setup on a vehicle, which can be both unsafe and unconventional. Additionally, it is not clear how the man is able to maintain balance and stability while ironing clothes in such an unstable environment. [Start a new conversation, and clear the history ] User What\u2019s happening in the scene? LLaV A The image depicts a man standing on top of a yellow SUV in a busy city street. He is holding a portable ladder, seemingly ironing clothes while standing on the vehicle. Around the scene, there are other cars, a traffic light, and a fire hydrant. Additionally, there are two other people in the vicinity, one close to the man on the car and another further away. The man on the ladder appears to be attracting attention due to his unconventional choice of ironing his clothes on top of a moving car. The city street around him is bustling with activity, adding to the unique nature of the scene. User What is unusual about this image? GPT-4 [36] The unusual thing about this image is that a man is ironing clothes on an ironing board attached to the roof of a moving taxi. User What is unusual about this image? BLIP-2 a man is sitting on the back of a yellow cab User What is unusual about this image? OpenFlamingo The man is drying his clothes on the hood of his car. Table 3: Example prompt from GPT-4 paper [ 36] to compare visual reasoning and chat capabilities. Compared to BLIP-2 [ 28] and OpenFlamingo [ 5], LLaV A accurately follows the user\u2019s instructions, instead of simply describing the scene. LLaV A offers a more comprehensive response than GPT-4. activity, adding to the unique nature of the scene. User What is unusual about this image? GPT-4 [36] The unusual thing about this image is that a man is ironing clothes on an ironing board attached to the roof of a moving taxi. User What is unusual about this image? BLIP-2 a man is sitting on the back of a yellow cab User What is unusual about this image? OpenFlamingo The man is drying his clothes on the hood of his car. Table 3: Example prompt from GPT-4 paper [ 36] to compare visual reasoning and chat capabilities. Compared to BLIP-2 [ 28] and OpenFlamingo [ 5], LLaV A accurately follows the user\u2019s instructions, instead of simply describing the scene. LLaV A offers a more comprehensive response than GPT-4. Even when merely asked to describe the image, LLaV A identifies atypical aspects of the image. Surprisingly, although LLaV A is trained with a small multimodal instruction-following dataset (\u223c80K unique images), it demonstrates quite similar reasoning results with multimodal GPT-4 on these examples. Note that while these images are out-of-domain for LLaV A, LLaV A is still able to understand the scenes and follow the question instruction to provide a reasonable response. In contrast, BLIP-2 and OpenFlamingo focus on describing the image, instead of following the user instruction to answer in an appropriate manner. Quantitative Evaluation. To gain a systematic understanding of the performance of LLaV A, we propose a quantitative metric to measure the model\u2019s instruction-following capability on multimodal data. Inspired by [ 9], we leverage GPT-4 to measure the quality of generated responses. Specifically, we create triplets consisting of image, ground-truth textual descriptions, and question. The candidate models ( e.g., LLaV A) predict the answers based on the question and the image. To provide an approximate theoretical upper bound , we create a reference prediction based on the question and theground-truth textual descriptions, using the text-only GPT-4. After obtaining the responses from both models, we feed the question, visual information (in the format of textual descriptions), and the generated responses from both assistants, to the judge ( i.e.,text-only GPT-4). It evaluates the helpfulness, relevance, accuracy, and level of detail of the responses from the assistants, and gives an overall score on a scale of 1 to 10, where a higher score indicates better overall performance. It is models ( e.g., LLaV A) predict the answers based on the question and the image. To provide an approximate theoretical upper bound , we create a reference prediction based on the question and theground-truth textual descriptions, using the text-only GPT-4. After obtaining the responses from both models, we feed the question, visual information (in the format of textual descriptions), and the generated responses from both assistants, to the judge ( i.e.,text-only GPT-4). It evaluates the helpfulness, relevance, accuracy, and level of detail of the responses from the assistants, and gives an overall score on a scale of 1 to 10, where a higher score indicates better overall performance. It is also asked to provide a comprehensive explanation for the evaluation, for us to better understand the 6 Conversation Detail description Complex reasoning All Full data 83.1 75.3 96.5 85.1 Detail + Complex 81.5 (-1.6) 73.3 (-2.0) 90.8 (-5.7) 81.9 (-3.2) Conv + 5% Detail + 10% Complex 81.0 (-2.1) 68.4 (-7.1) 91.5 (-5.0) 80.5 (-4.4) Conversation 76.5 (-6.6) 59.8 (-16.2) 84.9 (-12.4) 73.8 (-11.3) No Instruction Tuning 22.0 (-61.1) 24.0 (-51.3) 18.5 (-78.0) 21.5 (-63.6) Table 4: Ablation on LLaV A-Bench (COCO) with different training data. We report relative scores w.r.t. a text-only GPT-4 model that uses ground truth image captions and bounding boxes as visual input. We prompt GPT-4 with the answers from our model outputs and the answers by GPT-4 (text-only), and let it compare between both responses and give a rating with an explanation. Conversation Detail description Complex reasoning All OpenFlamingo [5] 19.3\u00b10.5 19.0 \u00b10.5 19.1 \u00b10.7 19.1 \u00b10.4 BLIP-2 [28] 54.6\u00b11.4 29.1 \u00b11.2 32.9 \u00b10.7 38.1 \u00b11.0 LLaV A 57.3\u00b11.9 52.5 \u00b16.3 81.7 \u00b11.8 67.3 \u00b12.0 LLaV A\u202058.8\u00b10.6 49.2 \u00b10.8 81.4 \u00b10.3 66.7 \u00b10.3 Table 5: Instruction-following capability comparison using relative scores on LLaV A-Bench (In-the- Wild). The results are reported in the format of mean\u00b1std. For the first three rows, we report three inference runs. LLaV A performs significantly better than others.\u2020For a given set of LLaV A decoding sequences, we evaluate by querying GPT-4 three times; GPT-4 gives a consistent evaluation. models. We report relative scores w.r.t. the text-only GPT-4 model that uses the textural ground truth description as visual input. We create two benchmarks to evaluate the model\u2019s performance. LLaVA-Bench (COCO). We randomly select 30 images from COCO-Val-2014, and for each LLaV A 57.3\u00b11.9 52.5 \u00b16.3 81.7 \u00b11.8 67.3 \u00b12.0 LLaV A\u202058.8\u00b10.6 49.2 \u00b10.8 81.4 \u00b10.3 66.7 \u00b10.3 Table 5: Instruction-following capability comparison using relative scores on LLaV A-Bench (In-the- Wild). The results are reported in the format of mean\u00b1std. For the first three rows, we report three",
    "170": "inference runs. LLaV A performs significantly better than others.\u2020For a given set of LLaV A decoding sequences, we evaluate by querying GPT-4 three times; GPT-4 gives a consistent evaluation. models. We report relative scores w.r.t. the text-only GPT-4 model that uses the textural ground truth description as visual input. We create two benchmarks to evaluate the model\u2019s performance. LLaVA-Bench (COCO). We randomly select 30 images from COCO-Val-2014, and for each image, we generate three types of questions (conversation, detailed description, complex reasoning) using the proposed data generation pipeline in Sec. 3, totaling 90 questions. This benchmark studies the model\u2019s alignment behavior and capabilities with consistent visual inputs. We vary the training datasets to study the effectiveness of different types of instruction-following data, and show the results in Table 4. First, with instruction tuning, the model\u2019s ability of following user instructions improves significantly by over 50 points. Second, adding a small amount of detailed description and complex reasoning questions contributes to a considerable improvement of the model\u2019s overall capability by 7 points. Furthermore, it also improves the model\u2019s performance on conversational questions, suggesting that improvements in reasoning capabilities complement conversational abilities. Finally, we show that having all three types of data yields the best performance at 85.1%. LLaVA-Bench (In-the-Wild). To evaluate the model\u2019s capability in more challenging tasks and generalizability to novel domains, we collect a diverse set of 24 images with 60 questions in total, including indoor and outdoor scenes, memes, paintings, sketches, etc., and associate each image with a highly-detailed and manually-curated description and a proper selection of questions. We compare LLaV A, BLIP, and OpenFlamingo in Table 5. Thanks to visual instruction tuning, LLaV A achieves significantly better performance compared with BLIP-2 (+29%) and OpenFlamingo (+48%). Compared to the text-only GPT-4 that has access to ground-truth labels, LLaV A achieves an impressive we show that having all three types of data yields the best performance at 85.1%. LLaVA-Bench (In-the-Wild). To evaluate the model\u2019s capability in more challenging tasks and generalizability to novel domains, we collect a diverse set of 24 images with 60 questions in total, including indoor and outdoor scenes, memes, paintings, sketches, etc., and associate each image with a highly-detailed and manually-curated description and a proper selection of questions. We compare LLaV A, BLIP, and OpenFlamingo in Table 5. Thanks to visual instruction tuning, LLaV A achieves significantly better performance compared with BLIP-2 (+29%) and OpenFlamingo (+48%). Compared to the text-only GPT-4 that has access to ground-truth labels, LLaV A achieves an impressive 81.7% performance on complex reasoning questions, with an overall score of 67.3%. Limitations. This LLaV A-Bench (In-the-Wild) is designed to be challenging and to reveal a model\u2019s weaknesses. We provide two examples with associated captions and questions in Table 6. For the ramen example (left), to correctly answer the name of the restaurant, it requires the model to have a large knowledge coverage and multilingual understanding capability; to correctly describe the side dishes, the model may need to retrieve relevant multimodal information from Internet. For the fridge example (right), perceiving the correct brand of the yogurt requires the model to process high resolution images and possess extensive knowledge coverage. We also observed an interesting failure of LLaV A, as it responds with yeswhen asked if strawberry-flavored yogurt is present, even though the fridge contains only yogurt andstrawberries. This indicates that, at times, LLaV A perceives the image as a \u201cbag of patches\u201d, failing to grasp the complex semantics within the image. We hope LLaV A serves as a solid baseline on the benchmarks, on which our findings can inspire future work in developing more capable LMMs. 7 Challenging examples from LLaVA-Bench (In-the-Wild): ICHIRAN Ramen [source] Filled fridge [source] Annotation A close-up photo of a meal at ICHI- RAN. The chashu ramen bowl with a spoon is placed in the center. The ramen is seasoned with chili sauce, chopped scallions, and served with two pieces of chashu. Chopsticks are placed to the right of the bowl, still in their paper wrap, not yet opened. The ramen is also served with nori on the left. On top, from left to right, the fol- lowing sides are served: a bowl of or- the image as a \u201cbag of patches\u201d, failing to grasp the complex semantics within the image. We hope LLaV A serves as a solid baseline on the benchmarks, on which our findings can inspire future work in developing more capable LMMs. 7 Challenging examples from LLaVA-Bench (In-the-Wild): ICHIRAN Ramen [source] Filled fridge [source] Annotation A close-up photo of a meal at ICHI- RAN. The chashu ramen bowl with a spoon is placed in the center. The ramen is seasoned with chili sauce, chopped scallions, and served with two pieces of chashu. Chopsticks are placed to the right of the bowl, still in their paper wrap, not yet opened. The ramen is also served with nori on the left. On top, from left to right, the fol- lowing sides are served: a bowl of or- ange spice (possibly garlic sauce), a plate of smoke-flavored stewed pork with chopped scallions, and a cup of matcha green tea.An open refrigerator filled with a variety of food items. In the left part of the compartment, towards the front, there is a plastic box of strawberries with a small bag of baby carrots on top. Towards the back, there is a stack of sauce containers. In the middle part of the compartment, towards the front, there is a green plastic box, and there is an unidentified plastic bag placed on it. Towards the back, there is a carton of milk. In the right part of the compartment, towards the front, there is a box of blueberries with three yogurts stacked on top. The large bottle of yogurt is Fage non-fat yogurt, and one of the smaller cups is Fage blueberry yogurt. The brand and flavor of the other smaller cup are unknown. Towards the back, there is a container with an unknown content. Question 1 What\u2019s the name of the restaurant? What is the brand of the blueberry-flavored yogurt? Question 2 Describe this photo in detail. Is there strawberry-flavored yogurt in the fridge? Table 6: Challenging examples from LLaV A-Bench (In-the-Wild), we provide extremely-detailed annotation for each image for an accurate evaluation. Some questions require the model to extract details from high resolution image and to have a broad knowledge coverage. 5.2 ScienceQA ScienceQA [ 34] contains 21k multimodal multiple choice questions with rich domain diversity across 3 subjects, 26 topics, 127 categories, and 379 skills. The benchmark dataset is split into training, validation, and test splits with 12726, 4241, and 4241 examples, respectively. We consider two Question 1 What\u2019s the name of the restaurant? What is the brand of the blueberry-flavored yogurt? Question 2 Describe this photo in detail. Is there strawberry-flavored yogurt in the fridge? Table 6: Challenging examples from LLaV A-Bench (In-the-Wild), we provide extremely-detailed annotation for each image for an accurate evaluation. Some questions require the model to extract details from high resolution image and to have a broad knowledge coverage. 5.2 ScienceQA ScienceQA [ 34] contains 21k multimodal multiple choice questions with rich domain diversity across 3 subjects, 26 topics, 127 categories, and 379 skills. The benchmark dataset is split into training, validation, and test splits with 12726, 4241, and 4241 examples, respectively. We consider two representative methods, including GPT-3.5 model ( text-davinci-002 ) with and without chain- of-thought (CoT), LLaMA-Adapter [ 59], as well as multimodal chain-of-thought (MM-CoT)",
    "171": "[ 61], which is the current SoTA method on this dataset. For more baseline numbers, please see [34]. The results are reported in Table 7. For LLaV A, we use the visual features before the last layer, ask the model to first predict reasons and then the answer, and train it for 12 epochs. It yields 90.92% accuracy, which is quite close to the SoTA 91.68%. To explore the limit of LLMs, we also prompt GPT-4 using 2-shot in-context-learning and achieve 82.69% accuracy, which is a 7.52% absolute gain compared with 75.17% from GPT-3.5. For a substantial number of questions, we note that GPT-4 fails simply because it reports that there is insufficient context such as images or plots. We consider two schemes to combine the outcomes from our model and GPT-4. (i)A GPT-4 complement . Whenever GPT-4 fails to provide answers, we use the prediction from our method. This schemes yields 90.97% accuracy, which is almost the same as applying our method alone. (ii)GPT-4 as the judge . Whenever GPT-4 and LLaV A produce different answers, we prompt GPT-4 again, asking it to provide its own final answer based on the question and two outcomes. The spirit is similar with CoT, but with the external knowledge from the other model. Surprisingly, this scheme is able to provide consistent improvement over all question classes, and achieves a new SoTA accuracy of 92.53%. Interestingly, the text-only GPT-4, which cannot process images, improves the overall performance of the model schemes to combine the outcomes from our model and GPT-4. (i)A GPT-4 complement . Whenever GPT-4 fails to provide answers, we use the prediction from our method. This schemes yields 90.97% accuracy, which is almost the same as applying our method alone. (ii)GPT-4 as the judge . Whenever GPT-4 and LLaV A produce different answers, we prompt GPT-4 again, asking it to provide its own final answer based on the question and two outcomes. The spirit is similar with CoT, but with the external knowledge from the other model. Surprisingly, this scheme is able to provide consistent improvement over all question classes, and achieves a new SoTA accuracy of 92.53%. Interestingly, the text-only GPT-4, which cannot process images, improves the overall performance of the model on questions that have an image as context. This is because some of these questions do not actually require the image context for a correct answer. The GPT-4 judge can identify such cases and correct some of the errors that LLaV A makes. See the example in Appendix. To the best of our knowledge, 8 MethodSubject Context Modality GradeAverageNAT SOC LAN TXT IMG NO G1-6 G7-12 Representative & SoTA methods with numbers reported in the literature Human [34] 90.23 84.97 87.48 89.60 87.50 88.10 91.59 82.42 88.40 GPT-3.5 [34] 74.64 69.74 76.00 74.44 67.28 77.42 76.80 68.89 73.97 GPT-3.5 w/ CoT [34] 75.44 70.87 78.09 74.68 67.43 79.93 78.23 69.68 75.17 LLaMA-Adapter [59] 84.37 88.30 84.36 83.72 80.32 86.90 85.83 84.05 85.19 MM-CoT Base [61] 87.52 77.17 85.82 87.88 82.90 86.83 84.65 85.37 84.91 MM-CoT Large [61] 95.91 82.00 90.82 95.26 88.80 92.89 92.44 90.31 91.68 Results with our own experiment runs GPT-4\u202084.06 73.45 87.36 81.87 70.75 90.73 84.69 79.10 82.69 LLaV A 90.36 95.95 88.00 89.49 88.00 90.66 90.93 90.90 90.92 LLaV A+GPT-4\u2020(complement) 90.36 95.50 88.55 89.05 87.80 91.08 92.22 88.73 90.97 LLaV A+GPT-4\u2020(judge) 91.56 96.74 91.09 90.62 88.99 93.52 92.73 92.16 92.53 Table 7: Accuracy (%) on Science QA dataset. Question categories: NAT = natural science, SOC = social science, LAN = language science, TXT = text context, IMG = image context, NO = no context, G1-6 = grades 1-6, G7-12 = grades 7-12.\u2020Text-only GPT-4, our eval. Our novel model ensembling with the text-only GPT-4 consistently improves the model\u2019s performance under all categories, setting the new SoTA performance. this is the first time that GPT-4 is used for model ensembling. We hope this finding can encourage GPT-4\u202084.06 73.45 87.36 81.87 70.75 90.73 84.69 79.10 82.69 LLaV A 90.36 95.95 88.00 89.49 88.00 90.66 90.93 90.90 90.92 LLaV A+GPT-4\u2020(complement) 90.36 95.50 88.55 89.05 87.80 91.08 92.22 88.73 90.97 LLaV A+GPT-4\u2020(judge) 91.56 96.74 91.09 90.62 88.99 93.52 92.73 92.16 92.53 Table 7: Accuracy (%) on Science QA dataset. Question categories: NAT = natural science, SOC = social science, LAN = language science, TXT = text context, IMG = image context, NO = no context, G1-6 = grades 1-6, G7-12 = grades 7-12.\u2020Text-only GPT-4, our eval. Our novel model ensembling with the text-only GPT-4 consistently improves the model\u2019s performance under all categories, setting the new SoTA performance. this is the first time that GPT-4 is used for model ensembling. We hope this finding can encourage future research to explore more effective methods to leverage LLMs for model ensembling. Visual features Before Last Best variant 90.92 89.96 (-0.96) Predict answer first - 89.77 (-1.15) Training from scratch 85.81 (-5.11) - 7B model size 89.84 (-1.08) - Table 8: Design choice ablations (%). The differ- ence with the best variant is reported in red text.Ablations. We ablate several design choices on ScienceQA in Table 8. (i)Visual features . We tried using the last layer feature from CLIP vision encoder, which yields 89.96% and is 0.96% lower than the feature before the last layer. We hypothesize that this is because CLIP\u2019s last layer features may focus more on global and abstract image properties compared to the layer before it, which can focus more on localized properties that are useful for under- standing specific image details. (ii)Chain-of-thought . To decide the order between the answer and reasoning process in the model prediction, we run both variants and observe that answer-first reports the best number 89.77% accuracy in 12 epochs, while reasoning-first can quickly reach 89.77% accuracy in 6 epochs, but no further improvement with more training. Training the model for 24 epochs does not improve the performance. We conclude that CoT-like reasoning-first strategy can largely improve convergence, but contributes relatively little to the final performance. (iii) Pre-training . We skip pre-training and directly train on Science QA from scratch \u2013 performance drops to 85.81% accuracy. The 5.11% absolute degradation indicates the importance of our pre-training stage, in aligning multimodal features while preserving the vast pre-trained knowledge. (iv)Model and reasoning process in the model prediction, we run both variants and observe that answer-first reports the best number 89.77% accuracy in 12 epochs, while reasoning-first can quickly reach 89.77% accuracy in 6 epochs, but no further improvement with more training. Training the model for 24 epochs does not improve the performance. We conclude that CoT-like reasoning-first strategy can largely improve convergence, but contributes relatively little to the final performance. (iii) Pre-training . We skip pre-training and directly train on Science QA from scratch \u2013 performance drops to 85.81% accuracy. The 5.11% absolute degradation indicates the importance of our pre-training stage, in aligning multimodal features while preserving the vast pre-trained knowledge. (iv)Model size. We keep all configurations the same as our best 13B model, and train a 7B model. This yields 89.84% accuracy, which is 1.08% lower than 90.92%, demonstrating the importance of model scale. 6 Conclusion This paper demonstrated the effectiveness of visual instruction tuning. We presented an automatic pipeline to create language-image instruction-following data, based on which we train LLaV A, a multimodal model to follow human intent to complete visual tasks. It achieves the new SoTA accuracy when fine-tuned on ScienceQA, and excellent visual chat capabilities when fine-tuned on multimodal chat data. Besides, we present the first benchmark to study multimodal instruction- following capability. This paper is an initial step in visual instruction tuning, and mainly focuses on real-life",
    "172": "tasks. For more quantitative results of LLaV A on academic benchmarks, please refer to the improved baselines with visual instruction tuning [ 32]. We hope our work can inspire future research on building more capable multimodal models. Acknowledgements. We thank Baolin Peng and Pan Lu for valuable discussions on instruction-tuning language models and Science QA, respectively. We thank the LLaMA team for giving us access 9 to their models, and open-source projects, including Alpaca and Vicuna. This work was supported in part by NSF CAREER IIS2150012, and Institute of Information & communications Technology Planning & Evaluation(IITP) grants funded by the Korea government(MSIT) (No. 2022-0-00871, Development of AI Autonomy and Knowledge Enhancement for AI Agent Collaboration) and (No. RS-2022-00187238, Development of Large Korean Language Model Technology for Efficient Pre-training). References on building more capable multimodal models. Acknowledgements. We thank Baolin Peng and Pan Lu for valuable discussions on instruction-tuning language models and Science QA, respectively. We thank the LLaMA team for giving us access 9 to their models, and open-source projects, including Alpaca and Vicuna. This work was supported in part by NSF CAREER IIS2150012, and Institute of Information & communications Technology Planning & Evaluation(IITP) grants funded by the Korea government(MSIT) (No. 2022-0-00871, Development of AI Autonomy and Knowledge Enhancement for AI Agent Collaboration) and (No. RS-2022-00187238, Development of Large Korean Language Model Technology for Efficient Pre-training). References [1] Langchain. https://github.com/hwchase17/langchain , 2022. 2 [2]Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. arXiv preprint arXiv:2204.14198 , 2022. 2, 4 [3]Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko S\u00fcnderhauf, Ian Reid, Stephen Gould, and Anton Van Den Hengel. Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. In Proceedings of the IEEE conference on computer vision and pattern recognition , 2018. 2 [4]Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861 , 2021. 1 [5]Anas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt. Openflamingo, March 2023. 2, 6, 7 [6]Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instruct pix2pix: Learning to follow image editing instructions. arXiv preprint arXiv:2211.09800 , 2022. 2 [7]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems , 33:1877\u20131901, 2020. 2 [8]Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing [5]Anas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt. Openflamingo, March 2023. 2, 6, 7 [6]Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instruct pix2pix: Learning to follow image editing instructions. arXiv preprint arXiv:2211.09800 , 2022. 2 [7]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems , 33:1877\u20131901, 2020. 2 [8]Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In CVPR , 2021. 2 [9]Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. 1, 2, 4, 5, 6 [10] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311 , 2022. 2 [11] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416 , 2022. 2 [12] CVinW. Computer vision in the wild. https://github.com/ Computer-Vision-in-the-Wild/CVinW_Readings , 2022. 1 [13] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. PaLM-E: An embodied multimodal language model. arXiv preprint arXiv:2303.03378 , 2023. 2 [14] Fartash Faghri, Hadi Pouransari, Sachin Mehta, Mehrdad Farajtabar, Ali Farhadi, Mohammad Rastegari, and Oncel Tuzel. Reinforce data, multiply impact: Improved model accuracy and robustness with dataset reinforcement. arXiv preprint arXiv:2303.08983 , 2023. 2 [15] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-a-scene: Scene-based text-to-image generation with human priors. ArXiv , abs/2203.13131, 2022. 1 10 Computer-Vision-in-the-Wild/CVinW_Readings , 2022. 1 [13] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. PaLM-E: An embodied multimodal language model. arXiv preprint arXiv:2303.03378 , 2023. 2 [14] Fartash Faghri, Hadi Pouransari, Sachin Mehta, Mehrdad Farajtabar, Ali Farhadi, Mohammad Rastegari, and Oncel Tuzel. Reinforce data, multiply impact: Improved model accuracy and robustness with dataset reinforcement. arXiv preprint arXiv:2303.08983 , 2023. 2 [15] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-a-scene: Scene-based text-to-image generation with human priors. ArXiv , abs/2203.13131, 2022. 1 10 [16] Zhe Gan, Linjie Li, Chunyuan Li, Lijuan Wang, Zicheng Liu, Jianfeng Gao, et al. Vision- language pre-training: Basics, recent advances, and future trends. Foundations and Trends \u00aein Computer Graphics and Vision , 2022. 1 [17] Fabrizio Gilardi, Meysam Alizadeh, and Ma\u00ebl Kubli. Chatgpt outperforms crowd-workers for text-annotation tasks. arXiv preprint arXiv:2303.15056 , 2023. 3 [18] Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning without training. arXiv preprint arXiv:2211.11559 , 2022. 2 [19] Weituo Hao, Chunyuan Li, Xiujun Li, Lawrence Carin, and Jianfeng Gao. Towards learning a generic agent for vision-and-language navigation via pre-training. In CVPR , 2020. 2 [20] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, et al. Language is not all you need: Aligning perception with language models. arXiv preprint arXiv:2302.14045 , 2023. 2 [21] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Openclip. July 2021. If you use this software, please cite it as below. 1 [22] Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, D\u00e1niel Simig, Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu, Punit Singh Koura, et al. Opt-iml: Scaling",
    "173": "language model instruction meta learning through the lens of generalization. arXiv preprint arXiv:2212.12017 , 2022. 2 [23] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Visual prompt tuning. In ECCV , 2022. 2 [21] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Openclip. July 2021. If you use this software, please cite it as below. 1 [22] Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, D\u00e1niel Simig, Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu, Punit Singh Koura, et al. Opt-iml: Scaling language model instruction meta learning through the lens of generalization. arXiv preprint arXiv:2212.12017 , 2022. 2 [23] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Visual prompt tuning. In ECCV , 2022. 2 [24] Jing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried. Grounding language models to images for multimodal generation. arXiv preprint arXiv:2301.13823 , 2023. 2 [25] Boyi Li, Kilian Q Weinberger, Serge Belongie, Vladlen Koltun, and Ren\u00e9 Ranftl. Language- driven semantic segmentation. ICLR , 2022. 1 [26] Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, and Jianfeng Gao. Multimodal foundation models: From specialists to general-purpose assistants. arXiv preprint arXiv:2309.10020 , 2023. 1 [27] Chunyuan Li, Haotian Liu, Liunian Harold Li, Pengchuan Zhang, Jyoti Aneja, Jianwei Yang, Ping Jin, Houdong Hu, Zicheng Liu, Yong Jae Lee, and Jianfeng Gao. ELEV ATER: A bench- mark and toolkit for evaluating language-augmented visual models. In NeurIPS Track on Datasets and Benchmarks , 2022. 1 [28] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language- image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597 , 2023. 1, 2, 4, 6, 7 [29] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded language-image pre-training. In CVPR , 2022. 1 [30] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. arXiv preprint arXiv:2301.07093 , 2023. 1 [31] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft COCO: Common objects in context. In ECCV , 2014. 3 [32] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual arXiv:2301.12597 , 2023. 1, 2, 4, 6, 7 [29] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded language-image pre-training. In CVPR , 2022. 1 [30] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. arXiv preprint arXiv:2301.07093 , 2023. 1 [31] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft COCO: Common objects in context. In ECCV , 2014. 3 [32] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2023. 9, 14 11 [33] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499 , 2023. 1 [34] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems , 2022. 2, 5, 8, 9 [35] OpenAI. ChatGPT. https://openai.com/blog/chatgpt/ , 2023. 1, 2 [36] OpenAI. Gpt-4 technical report, 2023. 1, 5, 6, 15 [37] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems , 35:27730\u201327744, 2022. 2 [38] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with GPT-4. arXiv preprint arXiv:2304.03277 , 2023. 1, 4 [39] Hieu Pham, Zihang Dai, Golnaz Ghiasi, Kenji Kawaguchi, Hanxiao Liu, Adams Wei Yu, Jiahui Yu, Yi-Ting Chen, Minh-Thang Luong, Yonghui Wu, et al. Combined scaling for open-vocabulary image classification. arXiv preprint arXiv: 2111.10050 , 2021. 1 [40] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020 , 2021. 1, 2, 4 35:27730\u201327744, 2022. 2 [38] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with GPT-4. arXiv preprint arXiv:2304.03277 , 2023. 1, 4 [39] Hieu Pham, Zihang Dai, Golnaz Ghiasi, Kenji Kawaguchi, Hanxiao Liu, Adams Wei Yu, Jiahui Yu, Yi-Ting Chen, Minh-Thang Luong, Yonghui Wu, et al. Combined scaling for open-vocabulary image classification. arXiv preprint arXiv: 2111.10050 , 2021. 1 [40] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020 , 2021. 1, 2, 4 [41] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research , 2020. 2 [42] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. ArXiv , abs/2204.06125, 2022. 1 [43] Robin Rombach, A. Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High- resolution image synthesis with latent diffusion models. CVPR , pages 10674\u201310685, 2022. 1 [44] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L. Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, Seyedeh Sara Mahdavi, Raphael Gontijo Lopes, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. ArXiv , abs/2205.11487, 2022. 1 [45] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion- 5b: An open large-scale dataset for training next generation image-text models. arXiv preprint arXiv:2210.08402 , 2022. 2 [46] D\u00eddac Sur\u00eds, Sachit Menon, and Carl V ondrick. Vipergpt: Visual inference via python execution for reasoning. arXiv preprint arXiv:2303.08128 , 2023. 2 [47] Andrew Szot, Alex Clegg, Eric Undersander, Erik Wijmans, Yili Zhao, John Turner, Noah Maestre, Mustafa Mukadam, Devendra Chaplot, Oleksandr Maksymets, Aaron Gokaslan, Vladimir V ondrus, Sameer Dharur, Franziska Meier, Wojciech Galuba, Angel Chang, Zsolt Kira, 2022. 1 [45] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion- 5b: An open large-scale dataset for training next generation image-text models. arXiv preprint arXiv:2210.08402 , 2022. 2 [46] D\u00eddac Sur\u00eds, Sachit Menon, and Carl V ondrick. Vipergpt: Visual inference via python execution for",
    "174": "reasoning. arXiv preprint arXiv:2303.08128 , 2023. 2 [47] Andrew Szot, Alex Clegg, Eric Undersander, Erik Wijmans, Yili Zhao, John Turner, Noah Maestre, Mustafa Mukadam, Devendra Chaplot, Oleksandr Maksymets, Aaron Gokaslan, Vladimir V ondrus, Sameer Dharur, Franziska Meier, Wojciech Galuba, Angel Chang, Zsolt Kira, Vladlen Koltun, Jitendra Malik, Manolis Savva, and Dhruv Batra. Habitat 2.0: Training home assistants to rearrange their habitat. In Advances in Neural Information Processing Systems (NeurIPS) , 2021. 2 [48] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca , 2023. 1, 4 12 [49] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo- th\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023. 1 [50] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang. Git: A generative image-to-text transformer for vision and language. arXiv preprint arXiv:2205.14100 , 2022. 1 [51] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instruc- tions. arXiv preprint arXiv:2212.10560 , 2022. 2 [52] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al. Benchmarking generalization via in-context instructions on 1,600+ language tasks. arXiv preprint arXiv:2204.07705 , 2022. 2 [53] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671 , 2023. 2 arXiv preprint arXiv:2205.14100 , 2022. 1 [51] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instruc- tions. arXiv preprint arXiv:2212.10560 , 2022. 2 [52] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al. Benchmarking generalization via in-context instructions on 1,600+ language tasks. arXiv preprint arXiv:2204.07705 , 2022. 2 [53] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671 , 2023. 2 [54] Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Bin Xiao, Lu Yuan, Ce Liu, and Jianfeng Gao. Unified contrastive learning in image-text-label space. CVPR , 2022. 1 [55] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and action. arXiv preprint arXiv:2303.11381 , 2023. 2 [56] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Benton C. Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu. Scaling autoregressive models for content-rich text-to-image generation. ArXiv , abs/2206.10789, 2022. 1 [57] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, et al. Florence: A new foundation model for computer vision. arXiv preprint arXiv:2111.11432 , 2021. 1 [58] Hao Zhang, Feng Li, Xueyan Zou, Shilong Liu, Chunyuan Li, Jianfeng Gao, Jianwei Yang, and Lei Zhang. A simple framework for open-vocabulary segmentation and detection. arXiv preprint arXiv:2303.08131 , 2023. 1 [59] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv preprint arXiv:2303.16199 , 2023. 2, 8, 9 [60] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. OPT: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 , 2022. 2 computer vision. arXiv preprint arXiv:2111.11432 , 2021. 1 [58] Hao Zhang, Feng Li, Xueyan Zou, Shilong Liu, Chunyuan Li, Jianfeng Gao, Jianwei Yang, and Lei Zhang. A simple framework for open-vocabulary segmentation and detection. arXiv preprint arXiv:2303.08131 , 2023. 1 [59] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv preprint arXiv:2303.16199 , 2023. 2, 8, 9 [60] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. OPT: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 , 2022. 2 [61] Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multi- modal chain-of-thought reasoning in language models. arXiv preprint arXiv:2302.00923 , 2023. 8, 9 [62] Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan Li, Noel Codella, Liunian Harold Li, Luowei Zhou, Xiyang Dai, Lu Yuan, Yin Li, et al. Regionclip: Region-based language-image pretraining. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 16793\u201316803, 2022. 1 [63] Xueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li, Chunyuan Li, Xiyang Dai, Harkirat Behl, Jianfeng Wang, Lu Yuan, et al. Generalized decoding for pixel, image, and language. arXiv preprint arXiv:2212.11270 , 2022. 1, 2 13 A Broader Impact The broader impact of LLaV A, a general-purpose visual assistant, has potential benefits and risks associated with its deployment and release. Some considerations are unique to LLaV A due to its visual nature, while others share similarities with existing instruction-following LLMs ( e.g., Alpaca, Vicuna, etc.). As LLaV A is built upon LLaMA, Vicuna, and CLIP, it inherits some of the issues associated with LLMs and vision encoders. In the following, we outline both the risks and mitigation strategies in place for the release of this model. Malicious input. To minimize potential misuse and harmful consequences, we employ two pre- cautionary measures for LLaV A: (1) OpenAI Filter API for user input text to prevent harmful or inappropriate text instructions from being processed by the model, and (2) NSFW Filter for uploaded user images to detect and block Not Safe For Work (NSFW) content or any other potentially harmful image inputs. visual nature, while others share similarities with existing instruction-following LLMs ( e.g., Alpaca, Vicuna, etc.). As LLaV A is built upon LLaMA, Vicuna, and CLIP, it inherits some of the issues associated with LLMs and vision encoders. In the following, we outline both the risks and mitigation strategies in place for the release of this model. Malicious input. To minimize potential misuse and harmful consequences, we employ two pre- cautionary measures for LLaV A: (1) OpenAI Filter API for user input text to prevent harmful or inappropriate text instructions from being processed by the model, and (2) NSFW Filter for uploaded user images to detect and block Not Safe For Work (NSFW) content or any other potentially harmful image inputs. Hallucination. Similar to LLMs, LLaV A might generate outputs that aren\u2019t grounded in facts or input data. This raises concerns about inferences made, especially in critical applications ( e.g., medical). Biases. Bias can be transferred from the base models to LLaV A, both from the vision",
    "175": "encoder (CLIP) and the language decoder (LLaMA/Vicuna). This may lead to biased outcomes or unfair representations of diverse content. Energy consumption. Though energy consumption is not a primary concern for LLaV A due to a smaller pretraining dataset (see details in Sec. C), it may become a concern when scaling up the pretraining dataset or increasing the model size, e.g., to a larger LLaMA version like the 65B model. Evaluation complexities. Assessing the performance of LLaV A is challenging as it involves both language and visual tasks. Our evaluation benchmark covers several aspects, including accuracy, concept coverage, reasoning ability, and creativity. However, additional aspects need consideration, such as the degree of visual content hallucination and fine-grained understanding of visual content. While text-only GPT-4 based multimodal evaluation is consistent and accurate in our study, its robustness in different situations and capability to evaluate other unexplored aspects are subjects for future work. Despite these risks, we believe that the benefits of releasing LLaV A to the research community outweigh the potential harm. It allows for ongoing investigation and improvement of the model and engages the community in developing better mitigation strategies to address these concerns. Moreover, the release of LLaV A can spur the development of new applications and research directions, ultimately such as the degree of visual content hallucination and fine-grained understanding of visual content. While text-only GPT-4 based multimodal evaluation is consistent and accurate in our study, its robustness in different situations and capability to evaluate other unexplored aspects are subjects for future work. Despite these risks, we believe that the benefits of releasing LLaV A to the research community outweigh the potential harm. It allows for ongoing investigation and improvement of the model and engages the community in developing better mitigation strategies to address these concerns. Moreover, the release of LLaV A can spur the development of new applications and research directions, ultimately contributing to the progress and responsible deployment of foundation models in vision-language tasks. B More Results We present more qualitative results of LLaV A to analyze its emergent behaviors and observed weaknesses. For more quantitative results of LLaV A on academic benchmarks, please refer to the improved baselines with visual instruction tuning [ 32]. In Table 9, LLaV A demonstrates a similar behavior as GPT-4 in another example from its paper. Similar to the GPT-4 live demo by OpenAI, LLaV A is capable of generating the HTML/JS/CSS code for an interactive joke website based on a simplified user input sketch in Fig. 2, despite a minor error. As shown in Fig. 3, LLaV A can follow user\u2019s instructions in a conversational style and provide detailed responses or creative writings. Furthermore, LLaV A is able to relate the visual content to the textual knowledge from the pretrained LLM, as demonstrated in Fig. 4 and Fig. 5. One interesting emergent behavior of LLaV A is that it is able to understand visual contents that are not covered in the training. For example, in Fig. 6, it is able to recognize Elon Musk both in a headshot and in a humorous meme where he is dressed as a doge, even though Elon Musk never appears in the training data for either the visual feature alignment or visual instruction tuning stages of 14 LLaV A. LLaV A also demonstrates impressive OCR (optical character recognition) ability in Table 9 and Fig. 2, which is rarely covered in our training data. We hope these additional results and observations showcase the potential of LLaV A in various application areas. In future work, it is important to investigate these emergent behaviors more thoroughly and to understand the underlying mechanisms that enable LLaV A to demonstrate such are not covered in the training. For example, in Fig. 6, it is able to recognize Elon Musk both in a headshot and in a humorous meme where he is dressed as a doge, even though Elon Musk never appears in the training data for either the visual feature alignment or visual instruction tuning stages of 14 LLaV A. LLaV A also demonstrates impressive OCR (optical character recognition) ability in Table 9 and Fig. 2, which is rarely covered in our training data. We hope these additional results and observations showcase the potential of LLaV A in various application areas. In future work, it is important to investigate these emergent behaviors more thoroughly and to understand the underlying mechanisms that enable LLaV A to demonstrate such generalization abilities. This will pave the way towards building better LMMs, including enhancing robustness, reducing biases, and improving the alignment and the scope of the learned vision-language representations. Visual input example, Chicken Nugget Map: User Can you explain this meme in detail? LLaV A The meme in the image is a creative and humorous take on food, with a focus on chicken nuggets, as the center of the universe. The meme begins with the phrase \"Sometimes I just look at pictures of the Earth from space and I marvel at how beautiful it all is...\" The main image features a tray of cooked chicken nuggets, with the chicken pieces positioned to resemble the continents and islands on a world map. The meme playfully suggests that the chicken nuggets represent the Earth, and the various locations depicted in the photo are actually chicken nugget versions of different places. The punchline of the meme appears in smaller text at the bottom, reading \"I mean, it\u2019s not the real Earth, but how beautiful it is all is.\" This lighthearted and entertaining meme combines food and humor, using the image of chicken nuggets to create a funny and imaginative representation of the world. User Can you explain this meme? GPT-4 [36] This meme is a joke that combines two unrelated things: pictures of the earth from space and chicken nuggets. The text of the meme suggests that the image below is a beautiful picture of the earth from space. However, the image is actually of chicken nuggets arranged to vaguely resemble a map of the world. The humor in this meme comes from the unexpected juxtaposition of the text and the image. The text sets up an expectation of a in smaller text at the bottom, reading \"I mean, it\u2019s not the real Earth, but how beautiful it is all is.\" This lighthearted and entertaining meme combines food and humor, using the image of chicken nuggets to create a funny and imaginative representation of the world. User Can you explain this meme? GPT-4 [36] This meme is a joke that combines two unrelated things: pictures of the earth from space and chicken nuggets. The text of the meme suggests that the image below is a beautiful picture of the earth from space. However, the image is actually of chicken nuggets arranged to vaguely resemble a map of the world. The humor in this meme comes from the unexpected juxtaposition of the text and the image. The text sets up an expectation of a majestic image of the earth, but the image is actually something mundane and silly. User Can you explain this meme in detail? BLIP-2 sometimes i just look at pictures of the earth from space and marvel how beautiful it is User Can you explain this meme in detail? OpenFlamingo It\u2019s a picture of a chicken nugget on the International Space Station. Table 9: Example prompt comparing LLaV A, GPT-4, BLIP-2, and OpenFlamingo\u2019s visual reasoning capabilities in understanding the humor. BLIP-2 and OpenFlamingo fail to follow the user\u2019s instructions. LLaV A and GPT-4 both explain the meme and its humor, while GPT-4 produces a more concise answer. 15 <!DOCTYPE html> <html> <head> <title>My Joke Website</title> <style> body { background -color: white; font -family: Arial, sans -serif; } .container { display: flex; flex -direction: column; align -items: center; justify -content: center; height: 100vh; }",
    "176": "h1 { font -size: 36px; margin -bottom: 20px; } p { font -size: 20px; } button { background -color: blue; color: white; border: none; padding: 10px 20px; font -size: 18px; cursor: pointer; } </style> </head> <body> <div class=\"container\"> <h1>My Joke Website</h1> <pid=\"punchline\" >Welcome to my joke website. Push the button to reveal the punchline!</p> <button onclick=\" revealPunchline ()\">Push me!</button> </div> <script> function revealPunchline () { document.getElementById (\"punchline\"). innerHTML = \"The punchline is: 'Why was the math book sad? Because it had too many problems.'\"; } </script> </body> </html>Turn this mock -up into an interactive website using html/ js, replace the joke with a real joke. Rendered website Rendered website h1 { font -size: 36px; margin -bottom: 20px; } p { font -size: 20px; } button { background -color: blue; color: white; border: none; padding: 10px 20px; font -size: 18px; cursor: pointer; } </style> </head> <body> <div class=\"container\"> <h1>My Joke Website</h1> <pid=\"punchline\" >Welcome to my joke website. Push the button to reveal the punchline!</p> <button onclick=\" revealPunchline ()\">Push me!</button> </div> <script> function revealPunchline () { document.getElementById (\"punchline\"). innerHTML = \"The punchline is: 'Why was the math book sad? Because it had too many problems.'\"; } </script> </body> </html>Turn this mock -up into an interactive website using html/ js, replace the joke with a real joke. Rendered website Rendered website (clicked the button)Place the generated HTML code into an HTML renderer to render the web page.User LLaVA Figure 2: LLaV A generates HTML/JS code for an interactive website based on user sketch inputs. The interactive interface works after fixing a minor error ( in red ) in the generated output. There is room for improvement in LLaV A\u2019s output, such as splitting the joke and punchline into two rows, and only revealing the punchline upon button click, to better reflect the user\u2019s intent. 16 With the variety of food items stored in the refrigerator, you can prepare several meals. For example, you can create a fruit salad using the strawberries, blueberries, and carrots. Additionally, you can make a delicious smoothie using the fruits and vegetables, whic h would be a healthy and nutritious snack or breakfast option. The milk and eggs can be used to create various dishes, such as a milk -based s moothie, a baked casserole, or scrambled eggs. The juice can also be used to make a fresh fruit and juice mix or as a refreshing homemad e popsicle. The possibilities are vast, and the meals you can create will depend on your preferences and dietary needs.What are the meals that I can cook with these? Certainly! Here's a simple fruit salad recipe using the items in your refrigerator: Ingredients: -2 cups fresh strawberries, sliced -1 cup fresh blueberries -1 large carrot, grated -Juice of half a lemon -A handful of chopped fresh parsley or mint -A few ice cubes -Optional: honey or maple syrup (adjust to taste) Instructions: 1. In a large bowl, combine the sliced strawberries, blueberries, and carrot. Gently toss them together to mix. baked casserole, or scrambled eggs. The juice can also be used to make a fresh fruit and juice mix or as a refreshing homemad e popsicle. The possibilities are vast, and the meals you can create will depend on your preferences and dietary needs.What are the meals that I can cook with these? Certainly! Here's a simple fruit salad recipe using the items in your refrigerator: Ingredients: -2 cups fresh strawberries, sliced -1 cup fresh blueberries -1 large carrot, grated -Juice of half a lemon -A handful of chopped fresh parsley or mint -A few ice cubes -Optional: honey or maple syrup (adjust to taste) Instructions: 1. In a large bowl, combine the sliced strawberries, blueberries, and carrot. Gently toss them together to mix. 2. Drizzle the juice of half a lemon over the fruit and vegetables to prevent them from oxidizing and add a refreshing citrus flavor. 3. Sprinkle the chopped fresh parsley or mint over the fruit and gently toss once more. 4. If desired, add a drizzle of honey or maple syrup for a touch of sweetness. Adjust the amount according to your taste pref erences. 5. Cover the bowl and refrigerate the fruit salad for at least 30 minutes or up to a few hours. This allows the flavors to me ld and creates a more refreshing salad. 6. Serve the fruit salad in cups or bowls, garnished with a sprinkle of fresh parsley or mint, if desired. This fruit salad recipe is perfect for a healthy snack or side dish, and with the variety of ingredients available in your re frigerator, it should be easy to put together quickly.I would like to do the fruit salad. Show me the recipe. LLaVA LLaVA User User When visiting this peaceful location with a pier extending over a serene lake, surrounded by mountains, there are several thi ngsto be cautious about. First, pay attention to the weather conditions, as sudden changes in weather, especially strong winds or storms, can c ause the water to become rough and pose risks to the stability of the pier. Second, be mindful of the structural integrity of the pier; avoid o verloading it with too many people or excessive weight, as this could potentially damage the pier or cause it to collapse. Additionally, be aware of the surrounding wildlife and respect their space; do not harm or disturb them in any way. Finally, if you decide to go fishing, ensure that y ou adhere to any local fishing regulations and use appropriate fishing techniques to keep the lake ecosystem healthy and maintain the balance of the food chain. about. First, pay attention to the weather conditions, as sudden changes in weather, especially strong winds or storms, can c ause the water to become rough and pose risks to the stability of the pier. Second, be mindful of the structural integrity of the pier; avoid o verloading it with too many people or excessive weight, as this could potentially damage the pier or cause it to collapse. Additionally, be aware of the surrounding wildlife and respect their space; do not harm or disturb them in any way. Finally, if you decide to go fishing, ensure that y ou adhere to any local fishing regulations and use appropriate fishing techniques to keep the lake ecosystem healthy and maintain the balance of the food chain. What are the things I should be cautious about when I visit here? Traveling to this picturesque location with a serene lake and mountains in the background was a refreshing and rejuvenating e xperience. The pier extending over the calm water provided an excellent vantage point from which to appreciate the breathtaking scenery. As I stood on the pier, I couldn't help but feel a deep connection with nature, taking a moment to soak in the peaceful atmosphere. One of the highlights of my trip was undoubtedly the fishing experience. The area is teeming with life, and I was thrilled to land a sizeable catch, despite the modest size of the lake. The joy of fishing in such a beautiful setting was truly rewarding, and it allowe d me to take a break from my busy life and truly immerse myself in the tranquility of nature. While exploring the surrounding area, I came across a family of birds that had built a nest close to the pier, adding to the charm of the location. Interacting with the wildlife and observing their behavior was an enriching experience that I'll cherish for years to come. In conclusion, this idyllic destination provided me with an opportunity to unwind and recharge, surrounded by mesmerizing nat ural beauty and the gentle sounds of water and wildlife. If you're looking for a serene escape from the hustle and bustle of everyday life, t hispier overlooking the lake is the perfect place to find solace and reconnect with nature.Compose an engaging travel blog post about my trip to this place, highlighting the beautiful sceneries, joyful experiences, and include possible anecdotes. User UserLLaVA LLaVA Figure 3:",
    "177": "LLaV A is capable of recognizing the visual content following the user\u2019s intent, without location. Interacting with the wildlife and observing their behavior was an enriching experience that I'll cherish for years to come. In conclusion, this idyllic destination provided me with an opportunity to unwind and recharge, surrounded by mesmerizing nat ural beauty and the gentle sounds of water and wildlife. If you're looking for a serene escape from the hustle and bustle of everyday life, t hispier overlooking the lake is the perfect place to find solace and reconnect with nature.Compose an engaging travel blog post about my trip to this place, highlighting the beautiful sceneries, joyful experiences, and include possible anecdotes. User UserLLaVA LLaVA Figure 3: LLaV A is capable of recognizing the visual content following the user\u2019s intent, without directly prompting for visual recognition. It also provides a detailed response when prompted with a follow-up request, and the generated response is closely related to the provided visual content. 17 In the image, a young woman is leaning on a man in front of a beautiful sunset, with both of them standing on a boat. They appear to be enjoying a romantic moment together. The scene suggests that they could be depicted as the characters Jack and Rose from the popular movie Titanic. In the movie, their romance blossoms aboard the fictional ocean liner Titanic, with the pair ultimately sharing a pa ssionate kiss as the ship sinks into the ocean. The ending of the movie is known for its emotional and heartfelt scenes, where the characters are forced to say goodbye to each other as they are separated by the sinking ship. The image evokes a sense of romance and nostalgia, paying tr ibute to the iconic love story portrayed in the movie Titanic.What's the ending of this movie? LLaVA UserFigure 4: LLaV A relates the movie scenes to the textual knowledge from the pretrained LLM. The painting depicts a dog in a humorous situation, where it appears to be wearing a woman's dress and standing in front of a landscape. Although the artist of the painting is not explicitly identified, it could be a reference to the famous artist Da Vinci, know n for his artistic works, including the Mona Lisa. The painting is likely a creative and comical take on the traditional portrait style, replacing a hu man figure with a dog dressed in a woman's clothing, making it an interesting and amusing visual piece.Do you know who drew this painting? iconic love story portrayed in the movie Titanic.What's the ending of this movie? LLaVA UserFigure 4: LLaV A relates the movie scenes to the textual knowledge from the pretrained LLM. The painting depicts a dog in a humorous situation, where it appears to be wearing a woman's dress and standing in front of a landscape. Although the artist of the painting is not explicitly identified, it could be a reference to the famous artist Da Vinci, know n for his artistic works, including the Mona Lisa. The painting is likely a creative and comical take on the traditional portrait style, replacing a hu man figure with a dog dressed in a woman's clothing, making it an interesting and amusing visual piece.Do you know who drew this painting? The painting depicts a woman, commonly believed to be Mona Lisa, the famous artwork by Leonardo da Vinci. It is a portrait pa inting that showcases the woman's enigmatic smile and has become one of the most famous and iconic art pieces in the world. The original work is displayed in the Louvre Museum in Paris, and it is known for its intricate details, use of oil paint, and the artist's innova tive techniques that contributed to its enduring appeal and mystery.Do you know who drew this painting? Start a new conversation, and the history is cleared.User UserLLaVA LLaVA Figure 5: LLaV A recognizes the famous art work, Mona Lisa, by Leonardo da Vinci. When we start a new conversation, it also explains the humourous artwork created on the web, mimicking the Mona Lisa. 18 The name of the man in the photo is Elon Musk.What is the name of the man in the photo? The name of the man in the photo is Elon Musk.What is the name of the man in the photo? Start a new conversation, and the history is cleared.User UserLLaVA LLaVA Figure 6: An interesting emergent behavior of LLaV A is its ability to recognize Elon Musk both in a headshot and in a humorous meme where he is dressed as a doge. This implies that the pre-trained CLIP vision encoder may have seen images of Elon Musk. However, it is still surprising because Elon Musk never appears in the training data for either the visual feature alignment or visual instruction tuning stages of LLaV A, which indicates that the base language model generalizes to unseen visual concepts. Question: Which material is this rocking chair made of? Options: (A) wood (B) silk LLaVA answer: LECTURE: A material is a type of matter. Wood, glass, metal, and plastic are common materials. Start a new conversation, and the history is cleared.User UserLLaVA LLaVA Figure 6: An interesting emergent behavior of LLaV A is its ability to recognize Elon Musk both in a headshot and in a humorous meme where he is dressed as a doge. This implies that the pre-trained CLIP vision encoder may have seen images of Elon Musk. However, it is still surprising because Elon Musk never appears in the training data for either the visual feature alignment or visual instruction tuning stages of LLaV A, which indicates that the base language model generalizes to unseen visual concepts. Question: Which material is this rocking chair made of? Options: (A) wood (B) silk LLaVA answer: LECTURE: A material is a type of matter. Wood, glass, metal, and plastic are common materials. Some objects are made of just one material. Most nails are made of metal. Other objects are made of more than one material. This hammer is made of metal and wood. SOLUTION: Look at the picture of the rocking chair. The rocking chair is made of two different materials. The legs are made of wood, and the back and seat are made of silk. The answer is B. GPT-4 (text-only) answer: The answer is A. BECAUSE: Rocking chairs are typically made of sturdy materials that can support the weight of a person and withstand the rocking motion. Wood is a common material used for making rocking chairs due to its strength, durability, and natural appearance. GPT-4 (text-only) judge: Since there is no context or image to refer to, I cannot independently analyze the rocking chair in question. However, I can try to analyze the reasoning given by the assistants. Assistant 1, who claims to have seen the image, mentions that the legs of the rocking chair are made of wood, and the back and seat are made of silk, implying that it\u2019s a combination of two materials. However, this response seems less plausible, as silk is not a common material used for the construction of rocking chairs due to issues with stability and durability. Typically, one would expect the frame, legs, and seat or back to be made of wood, with cushions and padding made of fabrics, such as cotton or polyester. Assistant 2 argues that rocking chairs are typically made of sturdy materials, and wood is a common material used for making rocking chairs due to its strength, durability, and natural appearance. This reasoning stands true and is based on common knowledge. Assistant 1, who claims to have seen the image, mentions that the legs of the rocking chair are made of wood, and the back and seat are made of silk, implying that it\u2019s a combination of two materials. However, this response seems less plausible, as silk is not a common material used for the construction of rocking chairs due to issues with stability and durability. Typically, one would expect the frame, legs, and seat or back to be made of wood, with cushions and padding made of fabrics, such as cotton or polyester. Assistant 2 argues that rocking chairs are typically made of sturdy materials, and wood is a common",
    "178": "material used for making rocking chairs due to its strength, durability, and natural appearance. This reasoning stands true and is based on common knowledge. Given the information available, the most plausible answer is that the rocking chair is made of wood, as suggested by Assistant 1. The answer is A. Table 10: One example on how the text-only GPT-4 acts as a judge to ensemble the predictions from LLaV A and a text-only GPT-4, and gives a correct final answer. 19 C Training Details We pre-train our model on the filtered CC-595K subset for 1 epoch with a learning rate of 2e-3 and a batch size of 128, and fine-tune on the proposed LLaV A-Instruct-158K dataset for 3 epochs, with a learning rate of 2e-5 and a batch size of 32. Following Vicuna, we use the Adam optimizer with no weight decay and a cosine learning rate with a warmup ratio of 3%. During finetuning, FSDP (Full Shard Data Parallel) and gradient checkpointing is used to save GPU memory, and offloading is not used. BF16 and TF32 are enabled to achieve a balance between speed and precision. We train all models with 8 \u00d7A100s. Pretraining on CC-595K completes within 4 hours. Finetuning on Instruct-158K completes within 10 hours. Finetuning on ScienceQA completes within 4 hours. D Assets Our source code, generated instruction-tuning data, proposed benchmark are uploaded to the anonymized GitHub repository: LLaV A-Annonymous/LLaV A. 1. Source Code: link 2. README: link 3. Instructions to launch the demo: link 4. All prompts and few shot examples for querying GPT-4: link 5. LLaV A-Instruct-158K: link 6. LLaV A-Bench: COCO, In-The-Wild 7.Model checkpoints. The size of the model checkpoints after compression is 25GB, which exceeds the 5GB limit of GitHub LFS (Large File Storage). We\u2019ll release the checkpoint to the public, or upon request with reviewers for this submission. E Data We train all models with 8 \u00d7A100s. Pretraining on CC-595K completes within 4 hours. Finetuning on Instruct-158K completes within 10 hours. Finetuning on ScienceQA completes within 4 hours. D Assets Our source code, generated instruction-tuning data, proposed benchmark are uploaded to the anonymized GitHub repository: LLaV A-Annonymous/LLaV A. 1. Source Code: link 2. README: link 3. Instructions to launch the demo: link 4. All prompts and few shot examples for querying GPT-4: link 5. LLaV A-Instruct-158K: link 6. LLaV A-Bench: COCO, In-The-Wild 7.Model checkpoints. The size of the model checkpoints after compression is 25GB, which exceeds the 5GB limit of GitHub LFS (Large File Storage). We\u2019ll release the checkpoint to the public, or upon request with reviewers for this submission. E Data Instructions for brief image description. The list of instructions used to briefly describe the image content are shown in Table 11. They present the same meaning with natural language variance. \u2022 \"Describe the image concisely.\" \u2022 \"Provide a brief description of the given image.\" \u2022 \"Offer a succinct explanation of the picture presented.\" \u2022 \"Summarize the visual content of the image.\" \u2022 \"Give a short and clear explanation of the subsequent image.\" \u2022 \"Share a concise interpretation of the image provided.\" \u2022 \"Present a compact description of the photo\u2019s key features.\" \u2022 \"Relay a brief, clear account of the picture shown.\" \u2022 \"Render a clear and concise summary of the photo.\" \u2022 \"Write a terse but informative summary of the picture.\" \u2022 \"Create a compact narrative representing the image presented.\" Table 11: The list of instructions for brief image description. Instructions for detailed image description. The list of instructions used to describe the image content in detail are shown in Table 12. They present the same meaning with natural language variance. CC3M. We extract noun-phrases using Spacy for each caption over the whole CC3M dataset, and count the frequency of each unique noun-phrase. We skip noun-phrases whose frequency is smaller than3, as they are usually rare combinations concept and attributes that has already been covered 20 \u2022 \"Describe the following image in detail\" \u2022 \"Provide a detailed description of the given image\" \u2022 \"Give an elaborate explanation of the image you see\" \u2022 \"Share a comprehensive rundown of the presented image\" \u2022 \"Offer a thorough analysis of the image\" \u2022 \"Explain the various aspects of the image before you\" Instructions for detailed image description. The list of instructions used to describe the image content in detail are shown in Table 12. They present the same meaning with natural language variance. CC3M. We extract noun-phrases using Spacy for each caption over the whole CC3M dataset, and count the frequency of each unique noun-phrase. We skip noun-phrases whose frequency is smaller than3, as they are usually rare combinations concept and attributes that has already been covered 20 \u2022 \"Describe the following image in detail\" \u2022 \"Provide a detailed description of the given image\" \u2022 \"Give an elaborate explanation of the image you see\" \u2022 \"Share a comprehensive rundown of the presented image\" \u2022 \"Offer a thorough analysis of the image\" \u2022 \"Explain the various aspects of the image before you\" \u2022 \"Clarify the contents of the displayed image with great detail\" \u2022 \"Characterize the image using a well-detailed description\" \u2022 \"Break down the elements of the image in a detailed manner\" \u2022 \"Walk through the important details of the image\" \u2022 \"Portray the image with a rich, descriptive narrative\" \u2022 \"Narrate the contents of the image with precision\" \u2022 \"Analyze the image in a comprehensive and detailed manner\" \u2022 \"Illustrate the image through a descriptive explanation\" \u2022 \"Examine the image closely and share its details\" \u2022 \"Write an exhaustive depiction of the given image\" Table 12: The list of instructions for detailed image description. by other captions. We then start from the noun-phrases with lowest remaining frequency, add the captions that contain this noun-phrase to the candidate pool. If the frequency of the noun-phrase is larger than 100, we randomly choose a subset of size 100out of all its captions. This results in around 595K image-text pairs. The comparison of noun-phrase statistics before and after filtering CC3M is shown in Figure 7. The filtered dataset shows a good coverage of concepts whose frequency is higher from 3, but with a smaller number of image-text pairs. 0 10000 20000 30000 40000 50000 Unique noun-phrases (ordered by frequency in the descending order)101103105FrequencyCC3M: 108182 CC3M (Filtered): 31423 Figure 7: Comparison of noun-phrase statistics before and after filtering CC3M. The total number of unique noun-phrases are reported in the legend. F Prompts The prompt used to generate image-based conversation from ChatGPT/GPT-4 is shown in Table 13. 21 messages = [ {\"role\":\"system\", \"content\": f\"\"\"You are an AI visual assistant, and you are around 595K image-text pairs. The comparison of noun-phrase statistics before and after filtering CC3M is shown in Figure 7. The filtered dataset shows a good coverage of concepts whose frequency is higher from 3, but with a smaller number of image-text pairs. 0 10000 20000 30000 40000 50000 Unique noun-phrases (ordered by frequency in the descending order)101103105FrequencyCC3M: 108182 CC3M (Filtered): 31423 Figure 7: Comparison of noun-phrase statistics before and after filtering CC3M. The total number of unique noun-phrases are reported in the legend. F Prompts The prompt used to generate image-based conversation from ChatGPT/GPT-4 is shown in Table 13. 21 messages = [ {\"role\":\"system\", \"content\": f\"\"\"You are an AI visual assistant, and you are seeing a single image. What you see are provided with five sentences, describing the same image you are looking at. Answer all questions as you are seeing the image. Design a conversation between you and a person asking about this photo. The answers should be in a tone that a visual AI assistant is seeing the image and answering the question. Ask diverse questions and give corresponding answers. Include questions",
    "179": "asking about the visual content of the image, including the object types, counting the objects, object actions, object locations, relative positions between objects , etc. Only include questions that have definite answers: (1) one can see the content in the image that the question asks about and can answer confidently; (2) one can determine confidently from the image that it is not in the image. Do not ask any question that cannot be answered confidently. Also include complex questions that are relevant to the content in the image, for example, asking about background knowledge of the objects in the image, asking to discuss about events happening in the image, etc. Again, do not ask about uncertain details. Provide detailed answers when answering complex questions. For example, give detailed examples or reasoning steps to make the content more convincing and well-organized. You can include multiple paragraphs if necessary.\"\"\"} ] for sample in fewshot_samples: messages.append({\"role\":\"user\", \"content\":sample[\u2018context\u2019]}) messages.append({\"role\":\"assistant\", \"content\":sample[\u2018response\u2019]} ) messages.append({\"role\":\"user\", \"content\":\u2018\\n\u2019.join(query)}) Table 13: For each query, we illustrate the prompt construction process for ChatGPT/GPT-4 to Also include complex questions that are relevant to the content in the image, for example, asking about background knowledge of the objects in the image, asking to discuss about events happening in the image, etc. Again, do not ask about uncertain details. Provide detailed answers when answering complex questions. For example, give detailed examples or reasoning steps to make the content more convincing and well-organized. You can include multiple paragraphs if necessary.\"\"\"} ] for sample in fewshot_samples: messages.append({\"role\":\"user\", \"content\":sample[\u2018context\u2019]}) messages.append({\"role\":\"assistant\", \"content\":sample[\u2018response\u2019]} ) messages.append({\"role\":\"user\", \"content\":\u2018\\n\u2019.join(query)}) Table 13: For each query, we illustrate the prompt construction process for ChatGPT/GPT-4 to collect query[\u2018response\u2019] from query[\u2018context\u2019] , using few-shot in-context-learning, where examples are from fewshot_samples , each example including input sample[\u2018context\u2019] and output sample[\u2018response\u2019] . Note that messages is the final prompt. In this example, we provide the prompt used to generate the conversation response, please see also see its in-context-learning examples in Table 15 and Table 16 for details. We recommend readers to check out the codebase for the prompts to generated two other types of responses, including detailed decription and complex reasoning. 22 Context type 1: Captions A group of people standing outside of a black vehicle with various luggage. Luggage surrounds a vehicle in an underground parking area People try to fit all of their luggage in an SUV . The sport utility vehicle is parked in the public garage, being packed for a trip Some people with luggage near a van that is transporting it. Context type 2: Boxes person: [0.681, 0.242, 0.774, 0.694], person: [0.63, 0.222, 0.686, 0.516], person: [0.444, 0.233, 0.487, 0.34], backpack: [0.384, 0.696, 0.485, 0.914], backpack: [0.755, 0.413, 0.846, 0.692], suitcase: [0.758, 0.413, 0.845, 0.69], suitcase: [0.1, 0.497, 0.173, 0.579], bicycle: [0.282, 0.363, 0.327, 0.442], car: [0.786, 0.25, 0.848, 0.322], car: [0.783, 0.27, 0.827, 0.335], car: [0.86, 0.254, 0.891, 0.3], car: [0.261, 0.101, 0.787, 0.626] Response type 1: conversation Question: What type of vehicle is featured in the image? Answer: The image features a black sport utility vehicle (SUV). Question: Where is the vehicle parked? Answer: The vehicle is parked in an underground parking area, likely in a public garage. Context type 2: Boxes person: [0.681, 0.242, 0.774, 0.694], person: [0.63, 0.222, 0.686, 0.516], person: [0.444, 0.233, 0.487, 0.34], backpack: [0.384, 0.696, 0.485, 0.914], backpack: [0.755, 0.413, 0.846, 0.692], suitcase: [0.758, 0.413, 0.845, 0.69], suitcase: [0.1, 0.497, 0.173, 0.579], bicycle: [0.282, 0.363, 0.327, 0.442], car: [0.786, 0.25, 0.848, 0.322], car: [0.783, 0.27, 0.827, 0.335], car: [0.86, 0.254, 0.891, 0.3], car: [0.261, 0.101, 0.787, 0.626] Response type 1: conversation Question: What type of vehicle is featured in the image? Answer: The image features a black sport utility vehicle (SUV). Question: Where is the vehicle parked? Answer: The vehicle is parked in an underground parking area, likely in a public garage. Question: What are the people in the image doing? Answer: The people are trying to fit all of their luggage into the SUV , likely preparing for a trip. Response type 2: detailed description The image is an underground parking area with a black sport utility vehicle (SUV) parked. There are three people in the scene, with one person standing closer to the left side of the vehicle, another person in the middle, and the third person on the right side. They are all working together to pack their luggage into the SUV for a trip. In the parking area, various luggage items are scattered around the vehicle. There are two backpacks, one located near the left rear wheel and the other closer to the right side of the vehicle. Additionally, there are two suitcases, one on the right side of the car and another further away near the center of the parking area. A bicycle can also be seen on the left side of the vehicle. Other cars are parked around the main SUV , with one car positioned behind it and slightly to the left, another behind and slightly to the right, and the third car further behind on the right side. Response type 3: complex reasoning Question: What challenges do these people face? Answer: In the image, a group of people is standing outside a black SUV in a parking area, surrounded by various pieces of luggage, including suitcases and backpacks. They are facing the challenge of fitting all their luggage into the black SUV . There are multiple suitcases and backpacks to be packed, which suggests that the group has a significant amount of belongings to accommodate. They might have to strategize and arrange the luggage efficiently to ensure that everything fits Other cars are parked around the main SUV , with one car positioned behind it and slightly to the left, another behind and slightly to the right, and the third car further behind on the right side. Response type 3: complex reasoning Question: What challenges do these people face? Answer: In the image, a group of people is standing outside a black SUV in a parking area, surrounded by various pieces of luggage, including suitcases and backpacks. They are facing the challenge of fitting all their luggage into the black SUV . There are multiple suitcases and backpacks to be packed, which suggests that the group has a significant amount of belongings to accommodate. They might have to strategize and arrange the luggage efficiently to ensure that everything fits properly into the vehicle. Additionally, they need to consider the comfort of the passengers and visibility while driving, so the placement of the luggage must not obstruct the driver\u2019s view or make the passengers uncomfortable during the trip. Table 14: One example to illustrate the instruction-following data. The top block shows the contexts such as captions and boxes used to prompt GPT, and the bottom block shows the three types of responses. Note that the visual image is not used to prompt GPT, we only show it here as a reference. 23 Captions There is a movie theater that displays the show times above the doors. A red fire hydrant is deep in the snow. The fire hydrant is in the snow near a recently plowed sidewalk. This city has had a very hard winter with snow. A hotel for dogs in the snow in winter. Conversation Question: What color is the fire hydrant in the image? === Answer: The fire hydrant in the image is red. === Question: Is the fire hydrant fully covered by snow or partially visible? === Answer: The fire hydrant is partially visible,",
    "180": "with a significant portion of it covered by the deep snow. === Question: What is the condition of the sidewalk near the fire hydrant? === Answer: The sidewalk near the fire hydrant has been recently plowed, so it is relatively clear of snow compared to the surrounding area. === Question: What does the image suggest about the winter season in the city? === Answer: The image suggests that the winter season in the city has been quite harsh and challenging. The significant snow accumulation seen in the image indicates that the city has experienced heavy snowfall, which can lead to various difficulties and disruptions for its residents. === Question: Is the fire hydrant fully covered by snow or partially visible? === Answer: The fire hydrant is partially visible, with a significant portion of it covered by the deep snow. === Question: What is the condition of the sidewalk near the fire hydrant? === Answer: The sidewalk near the fire hydrant has been recently plowed, so it is relatively clear of snow compared to the surrounding area. === Question: What does the image suggest about the winter season in the city? === Answer: The image suggests that the winter season in the city has been quite harsh and challenging. The significant snow accumulation seen in the image indicates that the city has experienced heavy snowfall, which can lead to various difficulties and disruptions for its residents. For instance, the deep snow covering the fire hydrant could potentially hinder its accessibility in case of emergencies, which poses safety concerns. The recently plowed sidewalk implies that the city\u2019s maintenance crews have been actively working to keep the walkways clear and safe for pedestrians, but the sheer amount of snowfall might make it difficult to keep up with the continuous snow removal efforts. Furthermore, such extreme winter conditions can impact transportation, leading to traffic issues, delays in public transit, and increased risks of accidents due to slippery roads. It can also cause problems for businesses and schools, as people might face challenges commuting to work or attending classes. Additionally, the heavy snow can put extra strain on infrastructure, such as roofs and power lines, increasing the likelihood of structural damage or power outages. In conclusion, the image of the red fire hydrant deep in the snow and the recently plowed sidewalk suggest that the city has faced a particularly severe winter season, with substantial snowfall that has likely caused various challenges and disruptions for its residents and infrastructure. Table 15: One example used in in-context-learning to construct visual conversation data. 24 Captions A man is skiing in the open snow covered hills A skier is making his way into the snow. A skier on the foothills of a huge mountain range. A skier looks at mountains while standing near a trail sign. a single person skiing by an area with a lot of bushes Conversation Question: What is the skier doing? === Answer: The skier is skiing in the snow-covered hills, making his way through the snow and enjoying the beautiful mountain scenery. They a particularly severe winter season, with substantial snowfall that has likely caused various challenges and disruptions for its residents and infrastructure. Table 15: One example used in in-context-learning to construct visual conversation data. 24 Captions A man is skiing in the open snow covered hills A skier is making his way into the snow. A skier on the foothills of a huge mountain range. A skier looks at mountains while standing near a trail sign. a single person skiing by an area with a lot of bushes Conversation Question: What is the skier doing? === Answer: The skier is skiing in the snow-covered hills, making his way through the snow and enjoying the beautiful mountain scenery. They are also standing near a trail sign, which indicates their location on the mountain. === Question: Are there any other people or skiers visible in the image? === Answer: No, there is only a single person skiing in the image, and they are surrounded by an area with a lot of bushes. === Question: What kind of skiing is the person doing? Cross-country or downhill? === Answer: Based on what I can see in the photo, it is not possible to definitively determine whether the skier is doing cross-country or downhill skiing. However, given the presence of a trail sign and the fact that they are skiing in the foothills, it is more likely that the skier is engaged in cross-country skiing. Downhill skiing typically takes place on more steep slopes and involves ski lifts, whereas cross-country skiing often occurs on relatively flatter terrain with marked trails. === Question: Can you explain cross-country and downhill skiing a bit more? === Answer: Cross-country skiing and downhill skiing are two distinct types of skiing, each with its own techniques, equipment, and terrain preferences. 1. Cross-Country Skiing: Also known as Nordic skiing, cross-country skiing involves traversing relatively flat terrain or gentle slopes using skis and poles. It is a popular recreational and competitive sport that focuses on endurance, balance, and aerobic fitness. Cross-country skiers use their own locomotion to move across snow-covered terrain, rather than relying on ski lifts or other forms of assistance. There are two main techniques in cross-country skiing: - Classic: This technique involves skiing in parallel tracks, using a forward-and-backward gliding motion while keeping the skis parallel to each other. The skier uses their poles to propel themselves forward. preferences. 1. Cross-Country Skiing: Also known as Nordic skiing, cross-country skiing involves traversing relatively flat terrain or gentle slopes using skis and poles. It is a popular recreational and competitive sport that focuses on endurance, balance, and aerobic fitness. Cross-country skiers use their own locomotion to move across snow-covered terrain, rather than relying on ski lifts or other forms of assistance. There are two main techniques in cross-country skiing: - Classic: This technique involves skiing in parallel tracks, using a forward-and-backward gliding motion while keeping the skis parallel to each other. The skier uses their poles to propel themselves forward. - Skate skiing: This technique is similar to ice skating, where the skier pushes off one ski at an angle and glides on the other. It requires a smooth, groomed surface and is typically faster than the classic technique. Cross-country ski equipment consists of lightweight skis, boots, bindings, and poles. The boots are more flexible than those used in downhill skiing, allowing for better ankle movement and control. 2. Downhill Skiing: Also known as alpine skiing, downhill skiing involves descending slopes at high speeds using skis and poles for balance and control. This sport is more focused on speed, technique, and navigating challenging terrain, including steep slopes, moguls, and even jumps. Downhill skiing can be further categorized into several disciplines, such as slalom, giant slalom, super-G, and downhill racing. Each discipline has its own set of rules, courses, and ski equipment. Downhill ski equipment includes heavier and stiffer skis, boots, bindings, and poles than those used in cross-country skiing. The boots are more rigid to provide better support and control during high-speed descents and sharp turns. In summary, cross-country skiing is an endurance-based sport that involves traveling across flat or gently sloping terrain, while downhill skiing is focused on speed and technique as skiers navigate steeper slopes and challenging terrain. Both sports require specialized equipment and techniques, but they offer different experiences and challenges to participants. Table 16: One example used in in-context-learning to construct visual conversation data. 25 A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity Yejin Bang",
    "181": "Samuel Cahyawijaya Nayeon Lee Wenliang Dai Dan Su Bryan Wilie Holy Lovenia Ziwei Ji Tiezheng Yu Willy Chung Quyet V . Do Yan Xu Pascale Fung Centre for Arti\ufb01cial Intelligence Research (CAiRE) The Hong Kong University of Science and Technology yjbang@connect.ust.hk, pascale@ece.ust.hk Abstract This paper proposes a framework for quanti- tatively evaluating interactive LLMs such as ChatGPT using publicly available data sets. We carry out an extensive technical evaluation of ChatGPT using 23 data sets covering 8 dif- ferent common NLP application tasks. We evaluate the multitask, multilingual and multi- modal aspects of ChatGPT based on these data sets and a newly designed multimodal dataset. We \ufb01nd that ChatGPT outperforms LLMs with zero-shot learning on most tasks and even out- performs \ufb01ne-tuned models on some tasks. We \ufb01nd that it is better at understanding non-Latin script languages than generating them. It is able to generate multimodal content from tex- tual prompts, via an intermediate code gener- ation step. Moreover, we \ufb01nd that ChatGPT is 63.41% accurate on average in 10 different reasoning categories under logical reasoning, non-textual reasoning, and commonsense rea- soning, hence making it an unreliable reasoner. It is, for example, better at deductive than in- ductive reasoning. ChatGPT suffers from hal- lucination problems like other LLMs and it generates more extrinsic hallucinations from its parametric memory as it does not have ac- cess to an external knowledge base. Finally, the interactive feature of ChatGPT enables hu- man collaboration with the underlying LLM to improve its performance, i.e, 8% ROUGE- 1on summarization and 2% ChrF++ on ma- chine translation, in a multi-turn \"prompt engi- neering\" fashion. We also release codebase for evaluation set extraction.1 1 Introduction ChatGPT is a successor of the large language model (LLM) InstructGPT (Ouyang et al., 2022) with a dialog interface that is \ufb01ne-tuned using the Reinforcement Learning with Human Feedback 1https://github.com/HLTCHKUST/chatgpt-evaluat ion(RLHF) (Christiano et al., 2017) approach.2In the last couple of months, ChatGPT has gathered close to 1 million user base (Hu, 2023) and is being used by businesses and consumers alike for a myriad of mostly textual tasks. One reason for its unprece- dented popularity is that ChatGPT, through its scale to improve its performance, i.e, 8% ROUGE- 1on summarization and 2% ChrF++ on ma- chine translation, in a multi-turn \"prompt engi- neering\" fashion. We also release codebase for evaluation set extraction.1 1 Introduction ChatGPT is a successor of the large language model (LLM) InstructGPT (Ouyang et al., 2022) with a dialog interface that is \ufb01ne-tuned using the Reinforcement Learning with Human Feedback 1https://github.com/HLTCHKUST/chatgpt-evaluat ion(RLHF) (Christiano et al., 2017) approach.2In the last couple of months, ChatGPT has gathered close to 1 million user base (Hu, 2023) and is being used by businesses and consumers alike for a myriad of mostly textual tasks. One reason for its unprece- dented popularity is that ChatGPT, through its scale and via RLHF, has shown impressive abilities in many areas of NLP as well as emergent abilities such as code generation and multimodal genera- tion. Another reason is that its dialog interface allows users to interact with the underlying large language model more effectively and ef\ufb01ciently via interactive chats that are akin to multi-turn prompt engineering. However, despite its powerful abilities, anecdo- tal reports on ChatGPT have consistently shown signi\ufb01cant remaining challenges - for example, it fails in some elementary mathematical (Gilson et al., 2022; Goldberg, 2023; Frieder et al., 2023; Choi et al., 2023; Davis, 2023) and commonsense reasoning tasks (Guo et al., 2023; Davis, 2023); it hallucinates with human-like \ufb02uency and elo- quence on things that are not based on truth (Shen et al., 2023; Thorp, 2023; Smith, 2023); and as a general-purpose language model trained from everything on the web, its language coverage is questionable (Lu et al., 2022; Jiao et al., 2023). OpenAI has listed many limitations of ChatGPT on its website.3CEO tweeted that \u201cIt\u2019s a mistake to be relying on [ChatGPT] for anything important right now\u201d (Altman, 2022). Many researchers have argued that, despite appearances, LLMs like Chat- GPT are only good at language abilities, not actual reasoning (Mahowald et al., 2023). Consequently, it is not clear what people can or cannot use it for despite its popularity. For users and researchers alike, it would be bene\ufb01cial to have 2https://beta.openai.com/docs/model-index-for -researchers 3https://platform.openai.com/docs/chatgpt-edu cationarXiv:2302.04023v2 [cs.CL] 28 Feb 2023 a sense of con\ufb01dence in its reliability in various NLP/AI tasks. questionable (Lu et al., 2022; Jiao et al., 2023). OpenAI has listed many limitations of ChatGPT on its website.3CEO tweeted that \u201cIt\u2019s a mistake to be relying on [ChatGPT] for anything important right now\u201d (Altman, 2022). Many researchers have argued that, despite appearances, LLMs like Chat- GPT are only good at language abilities, not actual reasoning (Mahowald et al., 2023). Consequently, it is not clear what people can or cannot use it for despite its popularity. For users and researchers alike, it would be bene\ufb01cial to have 2https://beta.openai.com/docs/model-index-for -researchers 3https://platform.openai.com/docs/chatgpt-edu cationarXiv:2302.04023v2 [cs.CL] 28 Feb 2023 a sense of con\ufb01dence in its reliability in various NLP/AI tasks. Previous works have discussed the ethical impli- cations or concerns associated with ChatGPT (and other LLMs) (Jabotinsky and Sarel, 2022; Susn- jak, 2022; Blanco-Gonzalez et al., 2022; Ayd\u0131n and Karaarslan, 2022; Jeblick et al., 2022). How- ever, there has not been much technical evaluation of the strengths and limitations of ChatGPT4. To \ufb01ll this gap, we conduct experiments on ChatGPT with samples from standard public test sets on ma- jor NLP tasks such as question answering, reason- ing, summarization, machine translation, automatic post-editing, sentiment analysis, language identi\ufb01- cation, and task-oriented dialogue (dialogue state tracking & response generation) and misinforma- tion detection. We evaluate its multilingual per- formance as well as vision-language multimodal abilities. With additional experiments, we also quantitatively evaluate its primary limitations in reasoning andhallucination . In addition, we con- duct experiments to test its multi-turn interactivity as a means for better prompt engineering. We hope to provide insights to users of ChatGPT on the above-mentioned strengths and limitations, as well as how they can improve outcomes with interac- tivity. ( Note that we are not able to quantitatively evaluate the RLHF aspect of ChatGPT without ac- cess to the user log. We hope OpenAI will publish this work and one can carry out such evaluations in the future in collaboration with OpenAI. ) The following are the major insights we have gained from the evaluations: Multitask, Multimodal, and Multilingual \u2022For 9/13 NLP datasets, ChatGPT outper- forms previous LLMs with zero-shot learn- ing. It even outperforms fully \ufb01ne-tuned task- speci\ufb01c LMs on 4 different tasks. In other duct experiments to test its multi-turn interactivity as a means for better prompt engineering. We hope to provide insights to users of ChatGPT on the above-mentioned strengths and limitations, as well as how they can improve outcomes with interac- tivity. ( Note that we are not able to quantitatively evaluate the RLHF aspect of ChatGPT without ac- cess to the user log. We hope OpenAI will publish this work and one can carry out such evaluations in the future in collaboration with OpenAI. ) The following are the major insights we have gained from the evaluations: Multitask, Multimodal, and Multilingual \u2022For 9/13 NLP datasets, ChatGPT outper- forms previous LLMs with zero-shot learn- ing. It even",
    "182": "outperforms fully \ufb01ne-tuned task- speci\ufb01c LMs on 4 different tasks. In other cases, ChatGPT is on par or slightly lower than fully \ufb01ne-tuned for speci\ufb01c NLP tasks; \u2022ChatGPT fails to generalize to low-resource and extremely low-resource languages (e.g., Marathi, Sundanese, and Buginese). There is an overall performance degradation in low- resource languages, especially in non-Latin scripts in the case of translation; its weakness lies in generation rather than understanding part of the translation process; 4Many anecdotal analyses have been posted online, but none in a comprehensive manner\u2022ChatGPT enables a code intermediate medium to bridge vision and language, even though the multi-modality ability is still elementary compared to vision-language models. Reasoning We tested 10 different reasoning cat- egories with 634 samples in total. Based on our experiments, ChatGPT shows more weakness in inductive reasoning than in deductive or abductive reasoning. ChatGPT also lacks spatial reasoning while showing better temporal reasoning. ChatGPT also lacks mathematical reasoning, which aligns with recent \ufb01ndings by Frieder et al.. Further, we found that ChatGPT is relatively better at common- sense reasoning than non-textual semantic reason- ing. Finally, while ChatGPT shows acceptable per- formance in causal and analogical reasoning, it is bad at multi-hop reasoning capability as similar to other LLMs\u2019 weakness in complex reasoning (Ott et al., 2023). Hallucination Similar to other LLMs (Radford et al., 2019; Muennighoff et al., 2022; Workshop et al., 2022), ChatGPT suffers from the hallucina- tion problem. It generates more extrinsic halluci- nations \u2013 factual statements that cannot be veri- \ufb01ed from the source, from its parametric memory while showing better temporal reasoning. ChatGPT also lacks mathematical reasoning, which aligns with recent \ufb01ndings by Frieder et al.. Further, we found that ChatGPT is relatively better at common- sense reasoning than non-textual semantic reason- ing. Finally, while ChatGPT shows acceptable per- formance in causal and analogical reasoning, it is bad at multi-hop reasoning capability as similar to other LLMs\u2019 weakness in complex reasoning (Ott et al., 2023). Hallucination Similar to other LLMs (Radford et al., 2019; Muennighoff et al., 2022; Workshop et al., 2022), ChatGPT suffers from the hallucina- tion problem. It generates more extrinsic halluci- nations \u2013 factual statements that cannot be veri- \ufb01ed from the source, from its parametric memory across all tasks since it does not possess the access to external knowledge bases. Interactivity One of the primary differentiating factors of ChatGPT from its predecessors is its multi-turn dialog interactivity . This enables Chat- GPT to perform multiple tasks within a dialog ses- sion. There is also signi\ufb01cant performance im- provement (8% ROUGE-1 on summarization and 2% ChrF++ on low-resource machine translation) via multi-turn interactivity in various standard NLP tasks. This process is akin to prompt engineering with feedback from the system. Organization of This Paper: We \ufb01rst provide an overview of ChatGPT and related work (\u00a72). Then, we provide evaluation results on ChatGPT on various application test sets, on multilingual test sets, and on a new multimodal task in \u00a73. We then explore the three main strengths and weaknesses of ChatGPT, namely reasoning (\u00a74), hallucination (\u00a75) and interactivity (\u00a76) in the subsequent three sections. Finally, we discuss and give a conclusion on our \ufb01ndings of ChatGPT. 2 Background and Related Work 2.1 Large Pretrained Models Large Language Models (LLMs) are language mod- els with parameter sizes over a hundred billion, be- ginning with the introduction of GPT-3. Examples of LLMs include, but are not limited to, GPT-3, Gopher (Rae et al., 2021b), Megatron (Shoeybi et al., 2019), GPT-Jurassic (Lieber et al., 2021), OPT-175B Zhang et al. (2022). Beyond \ufb01ne-tuning models with task-speci\ufb01c data, LLMs have shown robustness and generalizability through zero-shot and few-shot learning with examples. Scaling up the models unlocked new, emergent abilities that were not observed with smaller models (Wei et al., 2022a). Prompts are used to probe the LLMs to sections. Finally, we discuss and give a conclusion on our \ufb01ndings of ChatGPT. 2 Background and Related Work 2.1 Large Pretrained Models Large Language Models (LLMs) are language mod- els with parameter sizes over a hundred billion, be- ginning with the introduction of GPT-3. Examples of LLMs include, but are not limited to, GPT-3, Gopher (Rae et al., 2021b), Megatron (Shoeybi et al., 2019), GPT-Jurassic (Lieber et al., 2021), OPT-175B Zhang et al. (2022). Beyond \ufb01ne-tuning models with task-speci\ufb01c data, LLMs have shown robustness and generalizability through zero-shot and few-shot learning with examples. Scaling up the models unlocked new, emergent abilities that were not observed with smaller models (Wei et al., 2022a). Prompts are used to probe the LLMs to generate the target outcome by sampling the lan- guage distribution. To enable the LLMs to demon- strate their abilities, sophisticated prompt engineer- ing (NeuralMagic, 2023) is required. However, pre- vious LLMs only allow one-time probing, which means the target outcome varies a great deal with minor changes in the prompt instruction. Whereas scaling up LLMs improve generaliz- ability, generic LLMs may fall short in speci\ufb01c ap- plications. Despite its name, ChatGPT has not been primarily used as a chatbot. Its dialog ability serves as the user interface to the underlying LLM. We nevertheless refer to other dialog systems here in this paper. A number of large pre-trained dialogue models have been created, following the pre-train- then-\ufb01netune paradigm. LaMDA (Thoppilan et al., 2022) is a large-scale conversational model, \ufb01ne- tuned from an LLM with a parameter size of 134 billion. Blenderbot 3.0 (Shuster et al., 2022), scaled up to 175 billion parameter size, is also introduced with similar abilities as LAMDA. Both models are pre-trained on public dialogue and other public web documents and then \ufb01ne-tuned with manually curated dialogue data. They also have access to ex- ternal knowledge sources for information retrieval, thus they have shown an excellent ability for \ufb02uent and natural dialogue generation as well as informa- tion retrieval. However, the aforementioned large dialogue models suffer from catastrophic forgetting of the knowledge obtained from the pre-training. Models after \ufb01ne-tuning show stable and strong performance on speci\ufb01c tasks, but they only pre- serve the knowledge learned from the task-speci\ufb01c data while losing the generalization ability. Chat- up to 175 billion parameter size, is also introduced with similar abilities as LAMDA. Both models are pre-trained on public dialogue and other public web documents and then \ufb01ne-tuned with manually curated dialogue data. They also have access to ex- ternal knowledge sources for information retrieval, thus they have shown an excellent ability for \ufb02uent and natural dialogue generation as well as informa- tion retrieval. However, the aforementioned large dialogue models suffer from catastrophic forgetting of the knowledge obtained from the pre-training. Models after \ufb01ne-tuning show stable and strong performance on speci\ufb01c tasks, but they only pre- serve the knowledge learned from the task-speci\ufb01c data while losing the generalization ability. Chat- GPT, on the other hand, was trained on a large-scale conversational-style dataset constructed from webdocuments directly (Schulman et al., 2022), which uni\ufb01es the pre-training and \ufb01ne-tuning data format. Thus, ChatGPT is able to preserve the knowledge from pre-training and produce informative outputs without access to external knowledge sources. 2.2 ChatGPT Compared to existing LLMs, ChatGPT has unique characteristics. First, it has the ability to interact with users in a conversation-like manner, while retaining its accumulated",
    "183": "knowledge and general- ization ability gained from pre-training. This is achieved by pre-training ChatGPT on a large-scale conversational-style dataset, that is constructed by transforming a large-scale instruction-tuning cor- pus used for building InstructGPT into a conversa- tional format, then \ufb01ne-tuning the model based on a reward model to further improve the gen- eration quality and align the generation with hu- man preference. ChatGPT should be considered a generic language model which can be probed in a conversational manner. The biggest advantage of such conversational interaction is that, unlike previous LLMs, ChatGPT can intelligently \u201can- swer follow-up questions, admit its mistakes, chal- lenge incorrect premises, and reject inappropriate requests\u201d (Schulman et al., 2022). Second, ChatGPT is trained with a better human- aligned objective function via Reinforcement Learning from Human Feedback (RLHF) (Chris- tiano et al., 2017). Conventional natural language generation models, including dialogue models, are trained with maximum likelihood estimation (MLE) and might not be aligned with human pref- erences. For instance, for dialogue systems, hu- man preference. ChatGPT should be considered a generic language model which can be probed in a conversational manner. The biggest advantage of such conversational interaction is that, unlike previous LLMs, ChatGPT can intelligently \u201can- swer follow-up questions, admit its mistakes, chal- lenge incorrect premises, and reject inappropriate requests\u201d (Schulman et al., 2022). Second, ChatGPT is trained with a better human- aligned objective function via Reinforcement Learning from Human Feedback (RLHF) (Chris- tiano et al., 2017). Conventional natural language generation models, including dialogue models, are trained with maximum likelihood estimation (MLE) and might not be aligned with human pref- erences. For instance, for dialogue systems, hu- manness, engagement, and groundedness are some examples of essential criteria for success. Such discrepancy between training objectives and evalu- ation metrics becomes a bottleneck to performance improvement. By using RLHF, ChatGPT aligns more closely with human preferences in generating text than by using MLE. As ChatGPT has become available to public users through an easily accessible UI, there have been many discussions from a wide range of com- munities, not just from AI or NLP, but also from other disciplines. A line of discussion is the speci\ufb01c emergent ability and strength of ChatGPT in more technical perspectives. Guo et al. (2023) conducts linguistic analyses and human evaluations of Chat- GPT\u2019s writing against human experts with their proposed corpus named Human ChatGPT Compar- ison Corpus and found that ChatGPT responses are strictly focused on the given question, more formal, objective, and less emotional. Nov et al. (2023) also studies ChatGPT\u2019s generated medical advice if it passes the Turing test. Frieder et al. (2023) investi- gate mathematical capabilities of ChatGPT on both publicly available and hand-crafted datasets, includ- ing graduate-level mathematics, and show that \u201csig- ni\ufb01cantly below those of an average mathematics graduate student.\u201d There are many investigations of ChatGPT\u2019s understanding and potential appli- cations in different \ufb01elds such as law (Choi et al., 2023), medical domain (Blanco-Gonzalez et al., 2022; Jeblick et al., 2022) and \ufb01nance (Birch, 2022; Dowling and Lucey, 2023). Jeblick et al. (2022) conduct a case study of the application of ChatGPT on simpli\ufb01ed radiology reports. Another impor- tant line of discussion is the ethical concerns over studies ChatGPT\u2019s generated medical advice if it passes the Turing test. Frieder et al. (2023) investi- gate mathematical capabilities of ChatGPT on both publicly available and hand-crafted datasets, includ- ing graduate-level mathematics, and show that \u201csig- ni\ufb01cantly below those of an average mathematics graduate student.\u201d There are many investigations of ChatGPT\u2019s understanding and potential appli- cations in different \ufb01elds such as law (Choi et al., 2023), medical domain (Blanco-Gonzalez et al., 2022; Jeblick et al., 2022) and \ufb01nance (Birch, 2022; Dowling and Lucey, 2023). Jeblick et al. (2022) conduct a case study of the application of ChatGPT on simpli\ufb01ed radiology reports. Another impor- tant line of discussion is the ethical concerns over the use of ChatGPT. The most active discussion is over the use of academic writing and exam in- tegrity (Jabotinsky and Sarel, 2022; Susnjak, 2022). OpenAI also discusses the misuse of LM for dis- information and remedies.5Zhuo et al. study AI ethics of ChatGPT in criteria of bias, reliability, robustness, and toxicity. 2.3 LLM benchmark and evaluation With the advancement of LLMs\u2019 generalization ability, there have been efforts to understand their capabilities, limitations, and risks. Recently, sev- eral benchmarks with a collection of a large number of NLP datasets, such as BIG-Bench (Srivastava et al., 2022) and AI LM Harness (Gao et al., 2021), have been introduced. Moreover, HELM (Liang et al., 2022) is proposed to conduct a holistic evalu- ation of LLMs that considers scenarios and metrics with a top-down approach. In this work, we instead focus on speci\ufb01c limitations and unique \ufb01ndings of ChatGPT that had not been discussed with previ- ous LLMs. There is dif\ufb01culty to evaluate ChatGPT with the whole test set from such benchmarks due to limited access to ChatGPT6. There are also other works that discuss LLMs\u2019 emergent abilities through thorough surveys or case studies. Mahowald et al. (2023) thoroughly stud- ies LLMs capabilities by distinguishing formal and functional linguistic competence with reference to cognitive science, psychology, and NLP to clarify 5https://openai.com/blog/forecasting-misuse/ 6As of the end of January 2023, there is no of\ufb01cial API provided by Open AI.the discourse surrounding LLMs\u2019 potential. Other works focus on more speci\ufb01c abilities such as math- ematical skills (Davis, 2023), reasoning (Webb et al., 2022a; Qiao et al., 2022). Also, there have ChatGPT that had not been discussed with previ- ous LLMs. There is dif\ufb01culty to evaluate ChatGPT with the whole test set from such benchmarks due to limited access to ChatGPT6. There are also other works that discuss LLMs\u2019 emergent abilities through thorough surveys or case studies. Mahowald et al. (2023) thoroughly stud- ies LLMs capabilities by distinguishing formal and functional linguistic competence with reference to cognitive science, psychology, and NLP to clarify 5https://openai.com/blog/forecasting-misuse/ 6As of the end of January 2023, there is no of\ufb01cial API provided by Open AI.the discourse surrounding LLMs\u2019 potential. Other works focus on more speci\ufb01c abilities such as math- ematical skills (Davis, 2023), reasoning (Webb et al., 2022a; Qiao et al., 2022). Also, there have been overviews of existing LLMs (Gozalo-Brizuela and Garrido-Merchan, 2023; Wolfe, 2023) 3 Multitask, Multilingual, and Multimodal Evaluations of ChatGPT 3.1 Evaluating the Multitask Ability of ChatGPT ChatGPT has become very well-known in such a short period of time to general public users, not just those who are in AI, machine learning, and NLP communities who might be more familiar with LLMs. One of the main reasons is that, in addition to media reports, innumerable use cases of ChatGPT are shared by both non-academic and academic users online (Marr, 2022; Gordon, 2023; Shankland, 2023). There have been debates and panels on whether ChatGPT is approaching Arti- \ufb01cial General Intelligence (AGI), as it seems to be able to carry out a multitude of tasks without speci\ufb01c \ufb01ne-tuning (Desk, 2023; Johnson, 2023; Kingson, 2023). On the other hand, there has also been as much sharing of its failures in simple tasks (Gilson et al., 2022; Choi et al., 2023; Shen et al., 2023). Instead of relying on anecdotal examples, we \ufb01rst evaluate ChatGPT\u2019s performance in various standard NLP tasks in a",
    "184": "zero-shot manner to ob- tain a basic/better understanding of its multi-task ability. We compile results from the existing litera- ture on ChatGPT and compare them with the state- of-the-art fully-\ufb01ne-tuned and zero-shot models across multiple tasks. We evaluate ChatGPT perfor- mances on 21 datasets covering 8 tasks, i.e., sum- marization, machine translation, sentiment analy- sis, questions answering, task-oriented dialogue, open-domain knowledge-grounded dialogue, and misinformation detection tasks. For ChatGPT, we sample testing cases from existing standard test (Gilson et al., 2022; Choi et al., 2023; Shen et al., 2023). Instead of relying on anecdotal examples, we \ufb01rst evaluate ChatGPT\u2019s performance in various standard NLP tasks in a zero-shot manner to ob- tain a basic/better understanding of its multi-task ability. We compile results from the existing litera- ture on ChatGPT and compare them with the state- of-the-art fully-\ufb01ne-tuned and zero-shot models across multiple tasks. We evaluate ChatGPT perfor- mances on 21 datasets covering 8 tasks, i.e., sum- marization, machine translation, sentiment analy- sis, questions answering, task-oriented dialogue, open-domain knowledge-grounded dialogue, and misinformation detection tasks. For ChatGPT, we sample testing cases from existing standard test sets for each task with a sample size ranging from 30 to 200 samples per task. Multitask Generalization of ChatGPT The re- sult of the multitask evaluation is shown in Table 1. ChatGPT is shown to achieve remarkable zero-shot performances on multiple tasks, surpassing pre- 7We take the average from the state-of-the-art zero-shot performance in CNN and DM from Goyal et al. (2022). Tasks Dataset Metric ReferenceFine-Tuned SOTAZero-Shot SOTAChatGPT Summarization CNN/DM ROUGE-1 Lewis et al. (2020a) 44.47 35.27735.29 SAMSum ROUGE-1 Lewis et al. (2020a) 47.28 - 35.29 MT (XXX!EngFLoRes-200 (HRL) ChrF++ Team et al. (2022) 63.5 - 58.64 FLoRes-200 (LRL) ChrF++ Team et al. (2022) 54.9 - 27.75 MT (Eng!XXX)FLoRes-200 (HRL) ChrF++ Team et al. (2022) 54.4 - 51.12 FLoRes-200 (LRL) ChrF++ Team et al. (2022) 41.9 - 21.57 Sentiment AnalysisNusaX - Eng Macro F1 Winata et al. (2022) 92.6 61.5 83.24 NusaX - Ind Macro F1 Winata et al. (2022) 91.6 59.3 82.13 NusaX - Jav Macro F1 Winata et al. (2022) 84.2 55.7 79.64 NusaX - Bug Macro F1 Winata et al. (2022) 70.0 55.9 55.84 Question AnsweringbAbI task 15 Accuracy Weston et al. (2016a) 100 - 93.3 bAbI task 16 Accuracy Weston et al. (2016a) 100 - 66.7 EntailmentBank Accuracy Clark et al. (2018) 86.5 78.58 93.3 CLUTRR Accuracy Minervini et al. (2020) 95.0 28.6 43.3 StepGame (k=9) Accuracy Mirzaee and Kordjamshidi (2022) 48.4 - 23.3 StepGame (k=1) Accuracy Mirzaee and Kordjamshidi (2022) 98.7 - 63.3 Pep-3k AUC Porada et al. (2021) 67.0 - 93.3 Misinformation DetectionCOVID-Social Accuracy Lee et al. (2021) 77.7 50.0 73.3 COVID-Scienti\ufb01c Accuracy Lee et al. (2021) 74.7 71.1 92.0 Task-Oriented DialogueMultiWOZ2.2 JGA Zhao et al. (2022) 60.6 46.7 24.4 MultiWOZ2.2 BLEU Nekvinda and Du\u0161ek (2021) 19.1 - 5.65 NusaX - Bug Macro F1 Winata et al. (2022) 70.0 55.9 55.84 Question AnsweringbAbI task 15 Accuracy Weston et al. (2016a) 100 - 93.3 bAbI task 16 Accuracy Weston et al. (2016a) 100 - 66.7 EntailmentBank Accuracy Clark et al. (2018) 86.5 78.58 93.3 CLUTRR Accuracy Minervini et al. (2020) 95.0 28.6 43.3 StepGame (k=9) Accuracy Mirzaee and Kordjamshidi (2022) 48.4 - 23.3 StepGame (k=1) Accuracy Mirzaee and Kordjamshidi (2022) 98.7 - 63.3 Pep-3k AUC Porada et al. (2021) 67.0 - 93.3 Misinformation DetectionCOVID-Social Accuracy Lee et al. (2021) 77.7 50.0 73.3 COVID-Scienti\ufb01c Accuracy Lee et al. (2021) 74.7 71.1 92.0 Task-Oriented DialogueMultiWOZ2.2 JGA Zhao et al. (2022) 60.6 46.7 24.4 MultiWOZ2.2 BLEU Nekvinda and Du\u0161ek (2021) 19.1 - 5.65 MultiWOZ2.2 Inform Rate Yang et al. (2021) 95.7 - 71.1 Open-Domain KGDOpenDialKG BLEU Ji et al. (2022c) 20.8 3.1 4.1 OpenDialKG ROUGE-L Ji et al. (2022c) 40.0 29.5 18.6 OpenDialKG FeQA Ji et al. (2022c) 48.0 23.0 15.0 Table 1: Performance of ChatGPT compared to state-of-the-art fully-\ufb01ne-tuned models (Fine-Tuned SOTA) and LLM in zero-shot settings (Zero-Shot SOTA). The referenced performances are evaluation results on full test sets, while the ChatGPT performances are computed on subsets of the corresponding dataset using 30 to 200 data samples for each task. For Machine Translation (MT) tasks, we use the de\ufb01nitions of high-resource language (HRL) and low-resource language (LRL) from NLLB (Team et al., 2022) and take subsets of languages to represent each group. JGA denotes joint goal accuracy. vious state-of-the-art zero-shot models on 9 out of 13 evaluation datasets with reported zero-shot LLMs performance. In most tasks, especially task- oriented and knowledge-grounded dialogue tasks, task-speci\ufb01c fully-\ufb01ne-tuned models outperform ChatGPT. Compared to the latter, ChatGPT yields lower performance in most tasks while still surpass- ing the performance on 4 evaluation datasets. Furthermore, from the evaluation results, we also observe several limitations of ChatGPT, e.g., 1) limited language understanding and generation ca- pabilities on low-resource languages, 2) lacking reasoning ability as shown from the results in QA, and 3) performing task-oriented and knowledge- grounded dialogue tasks. More detailed experimen- tal setup and analysis for each task are shared in the next subsections, i.e., \u00a73.1.1: Experiment de- tails and result and \u00a73.1.2: ChatGPT on Dialogue System. We also provide the complete list of all oriented and knowledge-grounded dialogue tasks, task-speci\ufb01c fully-\ufb01ne-tuned models outperform ChatGPT. Compared to the latter, ChatGPT yields lower performance in most tasks while still surpass- ing the performance on 4 evaluation datasets. Furthermore, from the evaluation results, we also observe several limitations of ChatGPT, e.g., 1) limited language understanding and generation ca- pabilities on low-resource languages, 2) lacking reasoning ability as shown from the results in QA, and 3) performing task-oriented and knowledge- grounded dialogue tasks. More detailed experimen- tal setup and analysis for each task are shared in the next subsections, i.e., \u00a73.1.1: Experiment de- tails and result and \u00a73.1.2: ChatGPT on Dialogue System. We also provide the complete list of all the datasets used in our evaluation in Appendix C.3.1.1 ChatGPT on Summarization, MT, Sentiment Analysis, QA, and Misinformation Detection Summarization We test on 100 samples from two common summarization datasets: half from SAM- Sum (Gliwa et al., 2019), a dialogue summarization dataset, and another half from CNN/DM (Hermann et al., 2015; Nallapati et al., 2016), news summa- rization datasets. The large version of Bart (Lewis et al., 2020b) model \ufb01ne-tuned on both datasets is conducted for comparison. Moreover, OpenAI\u2019s text-davinci-002 is used as the previous SOTA zero- shot model. We calculate ROUGE-1 scores for evaluating the generated summary. As is shown in Table 1, ChatGPT achieves a similar zero-shot per- formance with text-davinci-002, which is expected since they evolved from the same GPT3 pre-trained checkpoint. However, the \ufb01ne-tuned Bart still out- performs zero-shot ChatGPT by a large margin. Furthermore, we evaluate the ChatGPT\u2019s unique interaction capabilities in \u00a76. Machine Translation We evaluate the machine translation ability of ChatGPT on both high- resource and low-resource languages using the ChrF++ metric (Popovi \u00b4c, 2015). Speci\ufb01cally, we incorporate 8 high-resource languages, i.e., French (fra), Spanish (spa), Chinese (zho), Arabic (ara), Japanese (jpn), Indonesian (ind), Korean (kor), and Vietnamese (vie), and 4 low-resource languages, i.e., Javanese (jav), Sundanese (sun), Marathi (mar), and Buginese (bug) for our evaluation.8For each language pair, we sample 30 Eng $XXX parallel sentences from the FLORES-200 dataset (Team et al., 2022; Goyal et al., 2021). The result of our experiment suggests",
    "185": "that ChatGPT can well per- Furthermore, we evaluate the ChatGPT\u2019s unique interaction capabilities in \u00a76. Machine Translation We evaluate the machine translation ability of ChatGPT on both high- resource and low-resource languages using the ChrF++ metric (Popovi \u00b4c, 2015). Speci\ufb01cally, we incorporate 8 high-resource languages, i.e., French (fra), Spanish (spa), Chinese (zho), Arabic (ara), Japanese (jpn), Indonesian (ind), Korean (kor), and Vietnamese (vie), and 4 low-resource languages, i.e., Javanese (jav), Sundanese (sun), Marathi (mar), and Buginese (bug) for our evaluation.8For each language pair, we sample 30 Eng $XXX parallel sentences from the FLORES-200 dataset (Team et al., 2022; Goyal et al., 2021). The result of our experiment suggests that ChatGPT can well per- form XXX!Eng translation, but it still lacks the ability to perform Eng !XXX translation. Sentiment Analysis Sentiment analysis has been widely explored for both high-resource and low-resource languages (Wang et al., 2018a; Wilie et al., 2020; Ilmania et al., 2018). We explore the sentiment analysis ability of ChatGPT through 4 languages with diverse amounts of resources in NusaX (Winata et al., 2022): English (eng), Indone- sian (ind), Javanese (jav), and Buginese (bug). For each language, we sample 50 sentences from the corresponding dataset for our experiment and mea- sure the macro F1 score as the evaluation metric. We compare the results with two baselines, i.e., su- pervised state-of-the-art performance from Winata et al. (2022) and zero-shot multilingual LLM from Cahyawijaya et al. (2022). ChatGPT outperforms the previous state-of-the-art zero-shot model by a large margin except for the Buginese, where it per- forms on par. This shows that ChatGPT still has a limited understanding of extremely low-resource languages. Question Answering Since Question Answer- ing (QA) is a broad topic, we classify QA datasets into different categories based on the knowl- edge/reasoning type required to do the task, e.g commonsense reasoning, spatial reasoning, tem- poral reasoning, etc., to have a clearer analysis on ChatGPT\u2019s abilities. For each category, we select several datasets, and for each dataset, we sample 30 instances and test ChatGPT on the subset. Details on the dataset will be described in which subsec- tion of 4. Furthermore, we inspect the rationales provided by ChatGPT that it used to come up with 8For a fairer comparison in our multitask experiment, we forms on par. This shows that ChatGPT still has a limited understanding of extremely low-resource languages. Question Answering Since Question Answer- ing (QA) is a broad topic, we classify QA datasets into different categories based on the knowl- edge/reasoning type required to do the task, e.g commonsense reasoning, spatial reasoning, tem- poral reasoning, etc., to have a clearer analysis on ChatGPT\u2019s abilities. For each category, we select several datasets, and for each dataset, we sample 30 instances and test ChatGPT on the subset. Details on the dataset will be described in which subsec- tion of 4. Furthermore, we inspect the rationales provided by ChatGPT that it used to come up with 8For a fairer comparison in our multitask experiment, we strictly follow the de\ufb01nition of high-resource and low-resource languages from NLLB (Team et al., 2022).the answers. Some of them will be discussed in detail in the corresponding section (4). Based on our experiment results, ChatGPT outperforms the existing zero-shot and some of the \ufb01ne-tuned state- of-the-art performance on question answering. Fur- thermore, ChatGPT achieves near-perfect scores on three tasks, i.e., bAbI task 15, EntailmentBank, and Pep-3k. Misinformation Detection We test ChatGPT\u2019s ability to detect misinformation with the test sets that consist of scienti\ufb01c and social claims related to COVID-19 (Lee et al., 2021) with 100 samples. We take half from scienti\ufb01c (covid-scienti\ufb01c) and another half from social (covid-social) sets. We evaluate the accuracy of the veracity by manually checking the generated text. ChatGPT could detect misinformation 92% (46/50) and 73.33% (22/30, excluding veri\ufb01cation-refusing cases) accuracy on covid-scienti\ufb01c and covid-social respectively. 3.1.2 ChatGPT on Dialogue Tasks Given that ChatGPT has the ability to generate conversation-like responses, it is interesting to test their ability in response generation in different di- alogue settings: 1) Knowledge-Grounded Open- Domain Dialogue and 2) Task-Oriented Dialogue. Knowledge-Grounded Open-Domain Dialogue Open-domain dialogue systems interact with hu- mans with generated responses automatically and aim to provide users with an engaging experience. To boost informativeness, these systems leverage external knowledge, including structured knowl- edge such as knowledge graphs (Zhao et al., 2020; Ji et al., 2022c) and unstructured knowledge such as free text (Xu et al., 2022). To quantitatively measure ChatGPT\u2019s perfor- covid-scienti\ufb01c and covid-social respectively. 3.1.2 ChatGPT on Dialogue Tasks Given that ChatGPT has the ability to generate conversation-like responses, it is interesting to test their ability in response generation in different di- alogue settings: 1) Knowledge-Grounded Open- Domain Dialogue and 2) Task-Oriented Dialogue. Knowledge-Grounded Open-Domain Dialogue Open-domain dialogue systems interact with hu- mans with generated responses automatically and aim to provide users with an engaging experience. To boost informativeness, these systems leverage external knowledge, including structured knowl- edge such as knowledge graphs (Zhao et al., 2020; Ji et al., 2022c) and unstructured knowledge such as free text (Xu et al., 2022). To quantitatively measure ChatGPT\u2019s perfor- mance on knowledge-grounded dialogue, we apply it to 50 samples randomly selected from the test set of OpenDialKG (Moon et al., 2019), which con- tains open-ended dialogues grounded on a knowl- edge path. We use the following instruction for this KGD task: \u201cCan we try dialogue generation? I will give you turns, and you can generate the next turn, but only one.\\n \\n You can also consider the knowledge of XXX for your reference in the dialogue.\u201d According to human judgment, the responses from ChatGPT are of high quality with \ufb02uent re- sponse generation as well as incorporating the pro- vided knowledge in the response. However, the automatic evaluation results in Table 2 are rela- Model BLEU \"ROUGE-L\"FeQA\" (Durmus et al., 2020) ChatGPT 4.05 18.62 15.03 GPT2 11.10 30.00 26.54 Table 2: Automatic evaluation results on OpenDialKG. The results for GPT2 are from Dziri et al. (2021). tively low compared with GPT2 (Radford et al., 2019), which is \ufb01ne-tuned on this dataset. Speci\ufb01- cally, ChatGPT obtains a 4.05 BLEU and an 18.62 ROUGE-L score as the generated responses tend to be longer than the golden answers. For FeQA, which measures the generated response\u2019s faithful- ness to the input source, ChatGPT gets 15.03 since some generated responses include content from its parametrized knowledge injected during pre- training. Task-Oriented Dialogue In task-oriented dia- logue (TOD), a model needs to ful\ufb01ll a speci\ufb01c objective by interacting in natural language with the user. This task is often split into three modules: natural language understanding with belief state tracking, decision-making through dialogue poli- cies, and response generation \u2013 a modular approach tively low compared with GPT2 (Radford et al., 2019), which is \ufb01ne-tuned on this dataset. Speci\ufb01- cally, ChatGPT obtains a 4.05 BLEU and an 18.62 ROUGE-L score as the generated responses tend to be longer than the golden answers. For FeQA, which measures the generated response\u2019s faithful- ness to the input source, ChatGPT gets 15.03 since some generated responses include content from its parametrized knowledge injected during pre- training. Task-Oriented Dialogue In task-oriented dia- logue (TOD), a model needs to ful\ufb01ll a speci\ufb01c objective by interacting in",
    "186": "natural language with the user. This task is often split into three modules: natural language understanding with belief state tracking, decision-making through dialogue poli- cies, and response generation \u2013 a modular approach that handles each of these steps with different mod- els. Besides, uni\ufb01ed approaches are starting to show increasingly strong performances (Hosseini- Asl et al., 2020; Peng et al., 2021). Although ChatGPT seems more appropriate for open-domain dialogue tasks, we investigate and discuss how ChatGPT\u2019s emergent abilities and in- teractivity could potentially be leveraged for TOD as well. We explore two setups A) modular ap- proach: testing both dialogue state tracking and response generation using oracle actions; B) uni- \ufb01ed approach: a direct approach to simulate the TOD interaction while leveraging information in a structured database. We provide an example of the modular and uni\ufb01ed approaches in Appendix F. Setup A: Modular Approach We investigate ChatGPT\u2019s ability for both dialogue state track- ing and response generation in 50 dialogue turn samples taken from MultiWOZ2.2 (Zang et al., 2020). In detail, we ask the model to provide the belief state as domain-intent: [slot1, value1], . . . in the prompt following previous zero-shot (Lin et al., 2021) and few-shot (Madotto et al., 2021) ap- proaches, and provide an exhaustive list of domain- intent-slot-value for the given dialogue. For the response generation, we provide only the oracleState Tracking Response Generation Joint Goal Acc. BLEU Inform rate 24.4% 5.65 71.1% Table 3: Result for Task-oriented Dialogue Setup A \u2013 Modular Approach. dialogue actions (e.g. \u2019Hotel-Inform\u2019:[\u2019area\u2019, \u2019cen- tre\u2019]), and ask ChatGPT to generate a TOD re- sponse given the dialogue history. We assess DST with joint goal accuracy (JGA), the ratio of dia- logue turns where the predicted dialogue state is exactly the ground truth, and response generation belief state as domain-intent: [slot1, value1], . . . in the prompt following previous zero-shot (Lin et al., 2021) and few-shot (Madotto et al., 2021) ap- proaches, and provide an exhaustive list of domain- intent-slot-value for the given dialogue. For the response generation, we provide only the oracleState Tracking Response Generation Joint Goal Acc. BLEU Inform rate 24.4% 5.65 71.1% Table 3: Result for Task-oriented Dialogue Setup A \u2013 Modular Approach. dialogue actions (e.g. \u2019Hotel-Inform\u2019:[\u2019area\u2019, \u2019cen- tre\u2019]), and ask ChatGPT to generate a TOD re- sponse given the dialogue history. We assess DST with joint goal accuracy (JGA), the ratio of dia- logue turns where the predicted dialogue state is exactly the ground truth, and response generation with BLEU and inform rate(%) As shown in table 3, the performance for DST is mediocre with a JGA of 24.4%, but a lot of the failure cases are from the model predicting addi- tional belief states on top of the gold ones. In our setting, all belief states are correctly predicted in 73% of the samples. We postulate that the model rely too much on previous belief states since they are all provided within the prompt. For response generation, ChatGPT successfully leverages all in- formation provided while answering the questions with an 71.1% inform rate and 5.65 BLEU score. The BLEU is computed directly on the lexicalized response as ChatGPT skips the delexicalized gen- eration, and the generation is often as if not more natural than the gold response. Setup B: Uni\ufb01ed Approach We explore Chat- GPT\u2019s ability to simulate a TOD interaction in an end-to-end manner by providing nothing more than a structured database and giving the instruc- tion\u201cUse the following knowledge base to complete the task of recommending a restaurant as a task-oriented dialogue system\u201d . In this setup, we could investigate whether ChatGPT is able to complete basic re- trieval queries and respond to users\u2019 requests such as \u201cGive me some restaurants that serve Italian food\" or \"I would prefer cheap options please\u201d. However, there are several limitations that we could investigate as follow. \u2022Long-term Multi-turn Dependency: Chat- GPT cannot keep the belief state across multi- ple turns within the interaction. For instance, asking for Italian food will overwrite the previ- ous turn\u2019s belief state by asking for restaurants with a rating of 3 or higher. However, if the user explicitly asks to recall the earlier prefer- tion\u201cUse the following knowledge base to complete the task of recommending a restaurant as a task-oriented dialogue system\u201d . In this setup, we could investigate whether ChatGPT is able to complete basic re- trieval queries and respond to users\u2019 requests such as \u201cGive me some restaurants that serve Italian food\" or \"I would prefer cheap options please\u201d. However, there are several limitations that we could investigate as follow. \u2022Long-term Multi-turn Dependency: Chat- GPT cannot keep the belief state across multi- ple turns within the interaction. For instance, asking for Italian food will overwrite the previ- ous turn\u2019s belief state by asking for restaurants with a rating of 3 or higher. However, if the user explicitly asks to recall the earlier prefer- Language #Speakers CC Size (%)Language Category) English (eng) 1.452B 46.320 HRL Chinese (zho) 1.118B 4.837 HRL French (fra) 235M 4.604 HRL Indonesian (ind) 199M 0.781 MRL Korean (kor) 81.7M 0.679 MRL Javanese (jav) 68.3M 0.002 LRL Sundanese (sun) 32.4M 0.001 LRL Buginese (bug) -M 0.000 X-LRL Table 4: The statistics of languages used in our language disparity experiment. HRL denotes high- resourced language, MRL denotes medium-resourced language, LRL denotes low-resourced language, X- LRL denotes extremely low-resourced language. ences, ChatGPT is able to correct the retrieved information and incorporate the previous be- lief state. This is interesting as it shows that the information previously given in multi-turn is still usable, but needs to be called explicitly. \u2022Basic Reasoning Failure: ChatGPT\u2019s re- sponse tends to be wrong if the query intro- duces a basic level of reasoning such as when it is asked for \u201crecommendation for restau- rants with European food\u201d (ChatGPT has to \ufb01lter the types of cuisine which are based on countries) or \u201crecommendation for restaurants with a rating of 3 or higher\u201d (ChatGPT needs to understand rating 3, 4 and 5). Even with a basic knowledge base, ChatGPT fails to an- swer correctly 66% of the time. \u2022Extrinsic Hallucination: ChatGPT tends to generate hallucinated information beyond the given knowledge. This is especially harmful in TOD as ChatGPT will sometimes halluci- nate some prices for hotel booking, or avail- ability for restaurants. 3.2 Evaluating Multilinguality of ChatGPT Training data size affects language understanding and generation quality of LMs (Radford et al., 2019; Raffel et al., 2022; Cahyawijaya et al., 2021; it is asked for \u201crecommendation for restau- rants with European food\u201d (ChatGPT has to \ufb01lter the types of cuisine which are based on countries) or \u201crecommendation for restaurants with a rating of 3 or higher\u201d (ChatGPT needs to understand rating 3, 4 and 5). Even with a basic knowledge base, ChatGPT fails to an- swer correctly 66% of the time. \u2022Extrinsic Hallucination: ChatGPT tends to generate hallucinated information beyond the given knowledge. This is especially harmful in TOD as ChatGPT will sometimes halluci- nate some prices for hotel booking, or avail- ability for restaurants. 3.2 Evaluating Multilinguality of ChatGPT Training data size affects language understanding and generation quality of LMs (Radford et al., 2019; Raffel et al., 2022; Cahyawijaya et al., 2021; Rae et al., 2021a; Workshop et al., 2022; Chowd- hery et al., 2022; Hoffmann et al., 2022). As an LLM, the same premise also applies to Chat- GPT, and the question is to what extent. We in- vestigate this question through a series of exper- iments by analyzing 1) the language understand- ing capability using two different tasks, i.e, lan- guage identi\ufb01cation",
    "187": "(LID) and sentiment analysis,Language SA Acc. LID Acc. English 84% 100% Indonesian 80% 100% Javanese 78% 0% Buginese 56% 12% Table 5: Accuracy of ChatGPT on Sentiment Analysis (SA) and Language Identi\ufb01cation (LID) tasks. and 2) the language generation capability through machine translation using English as the pivot language. Based on the percentage of data in the CommonCrawl9, we group languages into 3 categories, i.e., high-resource (>1%) ,medium- resource (>0.1%) ,low-resource (<0.1%) . The statistics of all the languages under study are shown in Table 4. 3.2.1 Language Understanding We propose a framework for investigating the lan- guage understanding ability of ChatGPT through 3 languages from different language categories in NusaX (Winata et al., 2022), i.e. English (eng), Indonesian (ind), Javanese (jav). In addition, we incorporate an extremely low-resource language from NusaX, i.e., Buginese (bug), which is not even listed on CommonCrawl since the LID used in CommonCrawl10, i.e., CLD2 (Ooms, 2023), does not support Buginese (bug). We sample 50 sen- tences per language from the corresponding dataset for our experiment. ChatGPT fails to generalize to extremely low- resource languages As shown in Table 5, Chat- GPT achieves 84%. 80%, 78%, and 56% accuracy for English, Indonesian, Javanese, and Buginese, in Table 4. 3.2.1 Language Understanding We propose a framework for investigating the lan- guage understanding ability of ChatGPT through 3 languages from different language categories in NusaX (Winata et al., 2022), i.e. English (eng), Indonesian (ind), Javanese (jav). In addition, we incorporate an extremely low-resource language from NusaX, i.e., Buginese (bug), which is not even listed on CommonCrawl since the LID used in CommonCrawl10, i.e., CLD2 (Ooms, 2023), does not support Buginese (bug). We sample 50 sen- tences per language from the corresponding dataset for our experiment. ChatGPT fails to generalize to extremely low- resource languages As shown in Table 5, Chat- GPT achieves 84%. 80%, 78%, and 56% accuracy for English, Indonesian, Javanese, and Buginese, respectively. This result supports the results in prior works focusing on LLM(Chowdhery et al., 2022; Workshop et al., 2022; Muennighoff et al., 2022), where LLMs, including ChatGPT, yield a lower performance for lower resource languages. Inter- estingly, the performance gap between English, In- donesian, and Javanese is considered marginal com- pared to the performance gap with Buginese. This result suggests that ChatGPT still has a limitation in generalizing toward extremely low-resource lan- guages. 9CommonCrawl ( http://commoncrawl.org ) is the primary source of language pre-training data used in GPT3 10https://commoncrawl.github.io/cc-crawl-stati stics/plots/languages ChatGPT InstructGPT text-davinci-003 The language of the text appears to be a variant of the Bugis language spoken in Indonesia.The language of the text is the Sasak language, spoken in Lom- bok, Indonesia.The text is written in Buginese. I am sorry, I do not recognize the language of the text.The language of the text is Koyukon Athabascan.The text is in the Balinese lan- guage. The language of the text appears to be a dialect of the Indonesian language.The language of the text is In- donesian.The language of the text is In- donesian. Table 6: Example of Buginese language identi\ufb01cation response from ChatGPT, InstructGPT, and text-davinci-003. Language XXX!Eng Eng!XXX Chinese 24/30 14/30 French 29/30 25/30 Indonesian 28/30 19/30 Korean 22/30 12/30 Javanese 7/30 6/30 Sundanese 9/30 0/30 Table 7: Number of correct translations of ChatGPT. XXX denotes the target language in the \ufb01rst column. The languages are sorted based on the language size in CommonCrawl. ChatGPT understands sentences in low- I am sorry, I do not recognize the language of the text.The language of the text is Koyukon Athabascan.The text is in the Balinese lan- guage. The language of the text appears to be a dialect of the Indonesian language.The language of the text is In- donesian.The language of the text is In- donesian. Table 6: Example of Buginese language identi\ufb01cation response from ChatGPT, InstructGPT, and text-davinci-003. Language XXX!Eng Eng!XXX Chinese 24/30 14/30 French 29/30 25/30 Indonesian 28/30 19/30 Korean 22/30 12/30 Javanese 7/30 6/30 Sundanese 9/30 0/30 Table 7: Number of correct translations of ChatGPT. XXX denotes the target language in the \ufb01rst column. The languages are sorted based on the language size in CommonCrawl. ChatGPT understands sentences in low- resource languages but lacks the ability to identify the language ChatGPT correctly classi\ufb01ed the languages for English and Indonesian 100% of the time. While for the language identi\ufb01cation for Javenese and Buginese, Chat- GPT either misclassi\ufb01es the samples as other languages or is unable to determine the language for 100% for Javanese and 88% for Buginese. ChatGPT misclassi\ufb01es the samples mostly as Indonesian, despite having various dissimilarities across languages (Grimes, 2000; Lewis, 2009; Cohn and Ravindranath, 2014; Eberhard et al., 2021; Aji et al., 2022; Cahyawijaya et al., 2022). Nevertheless, ChatGPT performance on the sentiment analysis on Javanese is only slightly lower compared to English and Indonesian which suggests that ChatGPT can understand the semantic meaning of sentences in low-resource languages, such as Javanese, without having enough knowledge to identify the language itself. ChatGPT displays better human-preferred re- sponses As shown in Table 6, ChatGPT lets the user know that its prediction is uncertain when it does not completely understand the language andalso provides broader information regarding the language, such as location and tribe of which the predicted language is spoken. This fact provides evidence regarding the bene\ufb01t of using the RLHF approach compared to other training approaches for aligning LLMs with human preferences. 3.2.2 Language Generation We assess the multilingual language generation ability of ChatGPT through machine translation. ChatGPT has been shown to be competitive com- pared to commercial translation products for high- resource languages (Jiao et al., 2023). Speci\ufb01cally, we choose 2 languages from each language cate- ChatGPT displays better human-preferred re- sponses As shown in Table 6, ChatGPT lets the user know that its prediction is uncertain when it does not completely understand the language andalso provides broader information regarding the language, such as location and tribe of which the predicted language is spoken. This fact provides evidence regarding the bene\ufb01t of using the RLHF approach compared to other training approaches for aligning LLMs with human preferences. 3.2.2 Language Generation We assess the multilingual language generation ability of ChatGPT through machine translation. ChatGPT has been shown to be competitive com- pared to commercial translation products for high- resource languages (Jiao et al., 2023). Speci\ufb01cally, we choose 2 languages from each language cate- gory, i.e., French (fra), Chinese (zho), Indonesian (ind), Korean (kor), Javanese (jav), and Sundanese (sun) from the FLORES-200 dataset (Team et al., 2022; Goyal et al., 2021). For each language, we sample 30 English-XXX parallel sentences and perform two directions of translation using En- glish as the pivot language. The correctness of the translation results is manually validated by a native speaker of the corresponding language. ChatGPT performs worse on low-resource lan- guages As shown in Table 7, similar to other LLMs (Workshop et al., 2022; Muennighoff et al., 2022), ChatGPT produces better English translation quality from high-resource languages, such as French and Chinese. While for low- resource languages, such as Javanese and Sun- danese, ChatGPT tends to generate several mis- translated words/phrases and sometimes even hal- lucinate some objects. Moreover, we also observe that sometimes ChatGPT translates the",
    "188": "English sen- tence into a different but related language other than the requested target language (see \u00a76.2). This fact suggests that the generalization of LLMs, in- cluding ChatGPT, to low-resource languages, re- mains an open challenge. ChatGPT understands non-Latin scripts better than it can generate them Despite being high- resource and medium-resource languages, the trans- lation from English to Chinese and Korean is much inferior to the other languages with Latin scripts, i.e., French or Indonesian. Similarly, prior works focusing on transliteration (Chau and Smith, 2021; Muller et al., 2021) have shown the effectiveness of utilizing Latin scripts over other scripts, e.g., Cyrillic, Georgian, Arabic, etc, especially for low- that sometimes ChatGPT translates the English sen- tence into a different but related language other than the requested target language (see \u00a76.2). This fact suggests that the generalization of LLMs, in- cluding ChatGPT, to low-resource languages, re- mains an open challenge. ChatGPT understands non-Latin scripts better than it can generate them Despite being high- resource and medium-resource languages, the trans- lation from English to Chinese and Korean is much inferior to the other languages with Latin scripts, i.e., French or Indonesian. Similarly, prior works focusing on transliteration (Chau and Smith, 2021; Muller et al., 2021) have shown the effectiveness of utilizing Latin scripts over other scripts, e.g., Cyrillic, Georgian, Arabic, etc, especially for low- resource languages. Interestingly, this problem of using non-Latin scripts is less severe for transla- tion from Chinese and Korean to English, which suggests that ChatGPT can better neutralize the ef- fect of non-Latin scripts as source languages (Wan, 2022), but it still lacks the ability to generate non- Latin script languages. 3.3 Evaluating Multimodality of ChatGPT Since ChatGPT is a purely text-prompted language model, it is unlikely to explore its multimodal capa- bilities with visual inputs like contemporary vision- language works (Rombach et al., 2022; Ramesh et al., 2021; Yu et al., 2021a; Radford et al., 2021; Dai et al., 2022a; Lovenia et al., 2022). Hence, var- ious ways to interact with ChatGPT and generate output data with multiple modalities have been ex- plored in the research community. For example, as shown in Figure 1, ChatGPT can generate a well- formed and suitable intermediate representation in code format in order to synthesize images given the dialogue context and user prompts. Thanks to the code understanding and generation ability of ChatGPT, we believe programming codes can serve as the intermediate medium to bridge vi- sion and language (Rasheed, 2020; Shiryaev, 2022). Given textual prompts, ChatGPT can generate code representations of visual images using the SVG (Scalable Vector Graphics) format or APIs such as the HTML Canvas element and the Python Tur- tle graphics. In this way, even though the gener- ated images are symbolic and their quality is not comparable to the ones generated by modern text- to-image models (Ramesh et al., 2021; Rombach et al., 2022), it is worth exploring due to three reasons. Firstly, it helps us investigate the visual code format in order to synthesize images given the dialogue context and user prompts. Thanks to the code understanding and generation ability of ChatGPT, we believe programming codes can serve as the intermediate medium to bridge vi- sion and language (Rasheed, 2020; Shiryaev, 2022). Given textual prompts, ChatGPT can generate code representations of visual images using the SVG (Scalable Vector Graphics) format or APIs such as the HTML Canvas element and the Python Tur- tle graphics. In this way, even though the gener- ated images are symbolic and their quality is not comparable to the ones generated by modern text- to-image models (Ramesh et al., 2021; Rombach et al., 2022), it is worth exploring due to three reasons. Firstly, it helps us investigate the visual understanding and reasoning abilities of ChatGPT, which can be seen as an emergent skill after the very large-scale pre-training on text and code data. Furthermore, representing images with code is a more explainable way to understand the model\u2019s be- Figure 1: A cat drawn by ChatGPT using HTML Can- vas library. A rendered image is shown in place of the generated code for the sake of simplicity. haviors and rationales in text-to-image generation. Third, it is a natural way to evaluate ChatGPT\u2019s ability on multi-turn interaction by asking for post- editing and corrections of the generated images. 3.3.1 Flag Drawing Task To systematically evaluate the image generation ability of ChatGPT through code generation, we design a national \ufb02ag drawing task. This is a unique task showing how ChatGPT\u2019s textually described knowledge (language) converts into the drawing (vision) through the SVG (code), using multi-turn interactions in the dialogue. Task Formulation The \ufb02ag-drawing task con- tains three steps. Firstly, we ask ChatGPT to illus- trate the appearance of the \ufb02ag using the prompt \u201cDescribe how the <NATION> flag looks like \u201d. Next, based on the description, we ask ChatGPT to generate the SVG code of that \ufb02ag by prompting \u201c Generate a code snippet to represent that flag in SVG format \u201d. Fi- nally, if the generated image contains errors, we iteratively ask ChatGPT to \ufb01x them. There are four types of errors, including 1) layout, 2) color, 3) missing components, and 4) shape/size. In each round of \ufb01xing, we ask ChatGPT to revise only one type of error with the prompt \u201c <ERROR DESCRIPTION>. Revise the image \u201d. We ter- minate the conversation once the generated \ufb02ag interactions in the dialogue. Task Formulation The \ufb02ag-drawing task con- tains three steps. Firstly, we ask ChatGPT to illus- trate the appearance of the \ufb02ag using the prompt \u201cDescribe how the <NATION> flag looks like \u201d. Next, based on the description, we ask ChatGPT to generate the SVG code of that \ufb02ag by prompting \u201c Generate a code snippet to represent that flag in SVG format \u201d. Fi- nally, if the generated image contains errors, we iteratively ask ChatGPT to \ufb01x them. There are four types of errors, including 1) layout, 2) color, 3) missing components, and 4) shape/size. In each round of \ufb01xing, we ask ChatGPT to revise only one type of error with the prompt \u201c <ERROR DESCRIPTION>. Revise the image \u201d. We ter- minate the conversation once the generated \ufb02ag becomes perfect or we have already passed two rounds of \ufb01xing. We uniformly collect 50 national \ufb02ags from dif- ferent continents and conduct the \ufb02ag-drawing task on ChatGPT. The full results are shown in Ap- pendix A. The generated \ufb02ag images are evaluated by the aforementioned four error types as criteria. Grade (# of Errors)Turn 1 (w/o desc)Turn 1 Turn 2 Turn 3 A (0) 0 4 12 24 B (1) 4 22 24 24 C (2) 16 18 12 10 D (3) 18 24 26 20 E (\u00154) 62 32 26 22 Table 8: Results of the portion (%) of generated \ufb02ags evaluated into \ufb01ve grades from A to E. The second col- umn shows the results of an ablation study, which re- moves the prompting of \ufb02ag description generation and directly asks ChatGPT to generate the SVG code of the \ufb02ag image. We further assess the image quality with \ufb01ve grades, A\u0018E, which indicate zero to four (or above) er- rors with an increment of one. We assign grades to each round so that we can assess the number of improvements and degradation through conversa- tional interactions (post-editing). An overview of the result evaluation is provided in Table 8. 3.3.2 Findings ChatGPT is capable of drawing, yet better with a self-generated textual description. As demonstrated in Table 8 and Appendix A, by fol- lowing the task formulation, ChatGPT can generate plausible national \ufb02ags using the SVG format. To better understand the behavior of ChatGPT, we per- form an ablation study by removing the description generation step. As illustrated by Figure 2, the per- formance drops dramatically without \ufb01rst prompt- ing the textual",
    "189": "\ufb02ag description, which is generated by ChatGPT itself. Quantitatively, the proportion of E-graded images increases from 32% to 62% after removing this step. Therefore, self-generated improvements and degradation through conversa- tional interactions (post-editing). An overview of the result evaluation is provided in Table 8. 3.3.2 Findings ChatGPT is capable of drawing, yet better with a self-generated textual description. As demonstrated in Table 8 and Appendix A, by fol- lowing the task formulation, ChatGPT can generate plausible national \ufb02ags using the SVG format. To better understand the behavior of ChatGPT, we per- form an ablation study by removing the description generation step. As illustrated by Figure 2, the per- formance drops dramatically without \ufb01rst prompt- ing the textual \ufb02ag description, which is generated by ChatGPT itself. Quantitatively, the proportion of E-graded images increases from 32% to 62% after removing this step. Therefore, self-generated knowledge about the \ufb02ag is crucial for generating \ufb02ags correctly. From another point of view, explic- itly describing the appearance of the \ufb02ag and then drawing disentangles the image generation process, which can be considered as a chain-of-thought rea- soning. ChatGPT is an elementary illustrator. Among the four error types, the majority lies in the shape/size error, which happens 68% of the time. For the other three error types ( layout ,color ,miss- ing components ), they appear 34%, 20%, and 18% of the time, respectively. For instance, ChatGPT cannot generate the exact shape of the maple leaf in the Canadian \ufb02ag while it gets the layout and Figure 2: An example of a German \ufb02ag drawn by Chat- GPT using SVG format: (top) without and (bottom) with a self-retrieved textual description of the \ufb02ag. A rendered image is shown in place of the generated SVG format for the sake of simplicity. the color correctly without missing components (Figure 5). There are two potential reasons for this behavior. First, there might not be suf\ufb01cient training data in such a pattern. To draw sophisti- cated shapes, the <path> tag in SVG is generally used, but it might not be commonly seen in the pre-training code data, thus leading to ChatGPT being incapable of creating complex shapes. Sec- ond, in the textual \ufb02ag description generated at the initial step, the illustration of a sophisticated shape is written in a conceptual and high-level manner. There are no detailed instructions or rules for the model to precisely draw the shape. For example, in the description of the Canadian \ufb02ag, it only says \u201ca red maple leaf in the center\u201d, making it nearly im- format for the sake of simplicity. the color correctly without missing components (Figure 5). There are two potential reasons for this behavior. First, there might not be suf\ufb01cient training data in such a pattern. To draw sophisti- cated shapes, the <path> tag in SVG is generally used, but it might not be commonly seen in the pre-training code data, thus leading to ChatGPT being incapable of creating complex shapes. Sec- ond, in the textual \ufb02ag description generated at the initial step, the illustration of a sophisticated shape is written in a conceptual and high-level manner. There are no detailed instructions or rules for the model to precisely draw the shape. For example, in the description of the Canadian \ufb02ag, it only says \u201ca red maple leaf in the center\u201d, making it nearly im- possible to draw the leaf correctly without seeing it before. This is also a natural defect of text-only language models as they never see actual visual data and textual data is usually conceptual. 4 Reasoning Evaluations of ChatGPT Reasoning is one of the most actively discussed and debated abilities of LLMs as scaling the model parameter size also increases the implicit knowl- edge in LLMs (Wei et al., 2022a; Wang et al., Categories Dataset DeductiveEntailmentBank (Dalvi et al., 2021) bAbI (task 15) (Weston et al., 2016b) InductiveCLUTRR (Sinha et al., 2019) bAbI (task16) (Weston et al., 2016b) Abductive NLI (Bhagavatula et al., 2020) Temporal Timedial (Qin et al., 2021) SpatialSpartQA (Mirzaee et al., 2021) StepGame (Shi et al., 2022a) Mathematical Math (Saxton et al., 2019) CommonsenseCommonsenseQA (Talmor et al., 2018) PiQA (Bisk et al., 2020) Pep-3k (Wang et al., 2018b) Causal E-Care (Du et al., 2022) Multi-hop HotpotQA (Yang et al., 2018) Analogical Letter string analogies (Webb et al., 2022b) Table 9: Reasoning categories and corresponding datasets used to evaluate ChatGPT in this work. 2022; Huang and Chang, 2022). Mahowald et al. eloquently argues that \"language ability does not equal to thinking\" or \"reasoning\" in LLMs, and that LLMs have poor reasoning skills despite pos- sessing human-level language skills. In the NLP literature, evaluating a model\u2019s rea- soning often means evaluating its various skills in arithmetic, commonsense, and symbolic reasoning in different NLP tasks that require such skills (Tal- mor et al., 2020; Zelikman et al., 2022; Wei et al., 2022b). This is in line with the anecdotal expe- rience of users with ChatGPT \u2013 some of the ex- Multi-hop HotpotQA (Yang et al., 2018) Analogical Letter string analogies (Webb et al., 2022b) Table 9: Reasoning categories and corresponding datasets used to evaluate ChatGPT in this work. 2022; Huang and Chang, 2022). Mahowald et al. eloquently argues that \"language ability does not equal to thinking\" or \"reasoning\" in LLMs, and that LLMs have poor reasoning skills despite pos- sessing human-level language skills. In the NLP literature, evaluating a model\u2019s rea- soning often means evaluating its various skills in arithmetic, commonsense, and symbolic reasoning in different NLP tasks that require such skills (Tal- mor et al., 2020; Zelikman et al., 2022; Wei et al., 2022b). This is in line with the anecdotal expe- rience of users with ChatGPT \u2013 some of the ex- amples demonstrate surprisingly good \u201creasoning\u201d abilities compared to previously introduced LLMs but at the same time ChatGPT fails in very simple reasoning problems (the, 2023; Venuto, 2023; Qiao et al., 2022; Cookup.ai, 2022; Labs, 2022). In this paper, we investigate the reasoning ability of ChatGPT in a more \ufb01ne-grained manner, which includes deductive, inductive, abductive, analog- ical, causal, multi-hop, temporal, and spatial rea- soning, via question answering tasks. We \ufb01rst cat- egorize available QA tasks into each category by avoiding overlap (i.e., choosing a test set that re- quires mainly one speci\ufb01c category of reasoning) as shown in Table 9. We share experimental results on each of the categories in the following subsections \u00a74.1: logical reasoning (inductive, deductive, and abductive), \u00a74.2: non-textual semantic reasoning (temporal, mathematical and spatial), \u00a74.3 com- monsense reasoning, and \u00a74.4: causal, multi-hopDeductive Reasoning Tasks bAbI - task 15bAbI - task 15 (prompt engineered)EntailmentBank 19/30 28/30 28/30 Inductive Reasoning Tasks bAbI - task16bAbI - task 16 (prompt engineered)CLUTRR 0/30 20/30 13/30 Table 10: Inductive vs. Deductive Reasoning. Chat- GPT performs better deduction rather than induction. Engineering the prompt to explicitly ask ChatGPT to do reasonable inference improves its reasoning capabil- ity. The scores are in accuracy over tested samples. and analogical reasoning. On all reasoning tasks, we manually check the accuracy of the answer as well as verify the ratio- nales and explanations generated by ChatGPT. The composed result for all reasoning tasks is shown in Appendix E. We further discuss each reasoning task in the following sections. bAbI - task 15bAbI - task 15 (prompt engineered)EntailmentBank 19/30 28/30 28/30 Inductive Reasoning Tasks bAbI - task16bAbI - task 16 (prompt engineered)CLUTRR 0/30 20/30 13/30 Table 10: Inductive vs. Deductive Reasoning. Chat- GPT performs better deduction rather than induction. Engineering the prompt to explicitly ask ChatGPT to do reasonable inference improves its reasoning",
    "190": "capabil- ity. The scores are in accuracy over tested samples. and analogical reasoning. On all reasoning tasks, we manually check the accuracy of the answer as well as verify the ratio- nales and explanations generated by ChatGPT. The composed result for all reasoning tasks is shown in Appendix E. We further discuss each reasoning task in the following sections. 4.1 Logical Reasoning Inductive, deductive, and abductive reasoning are common forms of logical reasoning, a process of deriving a conclusion or judgment based on given evidence or past experience and observations (Rogers et al., 2022; Wason and Johnson-Laird, 1972; Huang and Chang, 2022). Inductive and de- ductive are categorized by \u201ca degree to which the premise supports the conclusion\u201d based on logic and philosophy (Qiao et al., 2022; Rogers et al., 2022; Hawthorne, 2021). Inductive reasoning is based on \u201cobservations or evidence\u201d while deduc- tive is based on \u201ctruth of the premises\u201d (i.e., nec- essarily true inference) (Douven, 2017). Another way to categorize is based on the \u201cdirection of reasoning\u201d \u2013 deductive is from premise to conclu- sion while abductive is from conclusion to the most probable premise that supports the conclusion (Wal- ton, 2014). 4.1.1 Deductive vs. Inductive Reasoning Deductive reasoning involves processes of driv- ing speci\ufb01c conclusions based on more general premises . On the contrary, inductive reasoning in- volves speci\ufb01c observation of patterns , processing them on increasingly abstract cycles of hypothetico- deductive reasoning to draw a more general conclu- sion (Lawson, 2005). Comparing the two types of reasoning, deduction requires less \u201cguessing\u201d from Task Prompt ChatGPT answer Gold T/F Deductive Reasoning Tasks bAbI (task 15)1 Wolves are afraid of mice. 2 Sheep are afraid of mice. 3 Winona is a sheep. 4 Mice are afraid of cats. 5 Cats are afraid of wolves. 6 Jessica is a mouse. 7 Emily is a cat. 8 Gertrude is a wolf. 10 What is winona afraid of?Winona, being a sheep, is afraid of mice.mouse T bAbI (task 15) (prompt Deductive reasoning involves processes of driv- ing speci\ufb01c conclusions based on more general premises . On the contrary, inductive reasoning in- volves speci\ufb01c observation of patterns , processing them on increasingly abstract cycles of hypothetico- deductive reasoning to draw a more general conclu- sion (Lawson, 2005). Comparing the two types of reasoning, deduction requires less \u201cguessing\u201d from Task Prompt ChatGPT answer Gold T/F Deductive Reasoning Tasks bAbI (task 15)1 Wolves are afraid of mice. 2 Sheep are afraid of mice. 3 Winona is a sheep. 4 Mice are afraid of cats. 5 Cats are afraid of wolves. 6 Jessica is a mouse. 7 Emily is a cat. 8 Gertrude is a wolf. 10 What is winona afraid of?Winona, being a sheep, is afraid of mice.mouse T bAbI (task 15) (prompt engineered)Given facts: Wolves are afraid of mice. Sheep are afraid of mice. Winona is a sheep. Mice are afraid of cats. Cats are afraid of wolves. Jessica is a mouse. Emily is a cat. Gertrude is a wolf. Based on the given facts above, do a reasonable inference on this question using deductive reasoning: What is winona afraid of?Based on the given facts, it can be inferred that Winona is afraid of mice.mouse T Entailment- Bankearth is a kind of planet. a planet is a kind of celestial object / celestial body. earth is located in the milky way galaxy. Which object is a planet found in the Milky Way Galaxy?Earth is a planet located in the Milky Way galaxy.Earth T Inductive Reasoning Tasks bAbI (task 16)1 Bernhard is a swan. 2 Greg is a frog. 3 Brian is a swan. 4 Julius is a lion. 5 Greg is gray. 6 Julius is yellow. 7 Lily is a lion. 8 Lily is green. 9 Brian is yellow. 10 What color is Bernhard?It is not speci\ufb01ed in the given information what color Bernhard is.yellow F bAbI (task 16) (prompt engineered)Given facts: Bernhard is a swan. Greg is a frog. Brian is a swan. Julius is a lion. Greg is gray. Julius is yellow. Lily is a lion. Lily is green. Brian is yellow. The most recent fact is the correct fact. Based on the given facts above, do a reasonable inference on this question using inductive reasoning: What color is Bernhard?Based on the given facts, it can be inferred that Bernhard is yellow, as Brian is also stated to be yellow.yellow T CLUTRR[Jason] and his wife [Gabrielle] went to the beach to watch the \ufb01reworks on the 4th of July. [Jason] and his daughter [Alma] took a day off school to go to the zoo... Who is Alma to Gabrielle?Alma is the daughter of Jason and Gabrielle.daughter T in the given information what color Bernhard is.yellow F bAbI (task 16) (prompt engineered)Given facts: Bernhard is a swan. Greg is a frog. Brian is a swan. Julius is a lion. Greg is gray. Julius is yellow. Lily is a lion. Lily is green. Brian is yellow. The most recent fact is the correct fact. Based on the given facts above, do a reasonable inference on this question using inductive reasoning: What color is Bernhard?Based on the given facts, it can be inferred that Bernhard is yellow, as Brian is also stated to be yellow.yellow T CLUTRR[Jason] and his wife [Gabrielle] went to the beach to watch the \ufb01reworks on the 4th of July. [Jason] and his daughter [Alma] took a day off school to go to the zoo... Who is Alma to Gabrielle?Alma is the daughter of Jason and Gabrielle.daughter T [Jason] took his grandson [Donald] \ufb01shing. [Russell] enjoys going \ufb01shing with his brother. His name is [Donald]... Who is Russell to Jason?Russell is the brother of Jason.grandson F Table 11: Prompting samples on deductive and inductive reasoning tasks. ChatGPT is performing better deduction rather than induction. On both types of reasoning, when ChatGPT is explicitly asked to do reasonable inferences, its ability for reasoning increases. Additionally, it also makes frequent mistakes regarding the grandson\u2019s kinship. StepGame (Basic) Breakdown Analaysis Result Example ChatGPT answer Gold T/F Clock-position 5/20 G is at Y\u2019s 6 o\u2019clock. What is the relation of the agent Y to the agent G?The agent Y is to the right of the agent G.Above F Basic Cardinal 17/20 D and K are parallel, and D is under K. What is the relation of the agent K to the agent D?The spatial relation of the agent K to the agent D is that K is above D.Above T Diagonal 11/20 W presents lower left to I. What is the relation of the agent I to the agent W?The relation of the agent I to the agent W is lower-left.Upper- RightF Table 12: A provided illustration to help the readers to understand each comparison between the categories (not the actual prompts). We provide the options to ChatGPT as: Choose from: left, right, above, below, lower-left, lower-right, upper-left, upper-right . the perspective of ChatGPT, as induction requires \ufb01guring out rules (Rogers et al., 2022). The for- mer can be viewed as top-down while the latter is bottom-up. We explore ChatGPT\u2019s ability of inductive and deductive reasoning in two different levels: 1) basic and 2) advanced. Basic-level tasks are the prerequi- to the agent D is that K is above D.Above T Diagonal 11/20 W presents lower left to I. What is the relation of the agent I to the agent W?The relation of the agent I to the agent W is lower-left.Upper- RightF Table 12: A provided illustration to help the readers to understand each comparison between the categories (not the actual prompts). We provide the options to ChatGPT as: Choose from: left, right, above, below, lower-left, lower-right, upper-left, upper-right . the perspective of ChatGPT, as induction requires \ufb01guring out rules (Rogers et al., 2022). The for- mer can be viewed as top-down while the latter is bottom-up. We explore ChatGPT\u2019s ability of inductive and deductive reasoning in two different levels: 1) basic and 2) advanced. Basic-level tasks are the prerequi- sites to probe reasoning. While solving these tasks does not necessarily indicate full reasoning capa- bility, if ChatGPT fails on any of these tasks, then there are likely real-world tasks that it will fail on",
    "191": "too if they require similar reasoning mechanisms. Consequently, the advanced-level tasks are there to probe those capabilities in real-world tasks where the noises are present, and solving them requires a more systematic generalization. Additionally, we choose tasks that do not require or are dependent on external knowledge and the solution could be only derived by premises to focus on dissecting the capability of each reasoning mechanism. ChatGPT is a lazy reasoner that suffers more with induction We \ufb01rst investigate basic reason- ing skills with bAbI tasks (Weston et al., 2016b), 30 examples each from task 15 (inductive) and task 16 (deductive). Each test example includes a list of premises to derive inference for a certain question. Interestingly, when ChatGPT was asked to answer a question given premises without any prompt engineering, it performs poorly in inductive reasoning (0 out of 30) while it achieves much bet- ter performance in deductive (19 of 30). ChatGPT answers \u201cIt is not speci\ufb01ed what <attribute> <en- tity> is. \u201d for most of the time when it was asked a question requiring inductive reasoning. However, when ChatGPT is explicitly asked for reasonable inference with a prompt \u201cBased on the given facts, do a reasonable inference on this question using inductive reasoning:\u201d , its ability for inductive rea- soning increases to 20 out of 30. Yet, it is still not as good as in deduction as the same prompt engi- neering also helps increases its ability for deductive question. Interestingly, when ChatGPT was asked to answer a question given premises without any prompt engineering, it performs poorly in inductive reasoning (0 out of 30) while it achieves much bet- ter performance in deductive (19 of 30). ChatGPT answers \u201cIt is not speci\ufb01ed what <attribute> <en- tity> is. \u201d for most of the time when it was asked a question requiring inductive reasoning. However, when ChatGPT is explicitly asked for reasonable inference with a prompt \u201cBased on the given facts, do a reasonable inference on this question using inductive reasoning:\u201d , its ability for inductive rea- soning increases to 20 out of 30. Yet, it is still not as good as in deduction as the same prompt engi- neering also helps increases its ability for deductive reasoning to 28 out of 30. When we repeat the analysis on the advanced- level tasks, speci\ufb01cally on CLUTRR (Sinha et al., 2019) for induction and EntailmentBank for deduc- tion (Dalvi et al., 2021), the same conclusion holds based on our experiment. We could derive sim- ilar insight as ChatGPT only correctly answered for half of the time while it could make inferences deductively well for 90% of the time. CLUTRR requires induction on extracting relations betweenentities, and in the ChatGPT responses, it often asks for more information to make inferences. An interesting \ufb01nding along with CLUTRR was that ChatGPT can\u2019t differentiate son and grandson but can differentiate daughter and granddaughter when it induces the logical rules governing kinship re- lationships. We show all performances in Table 10 and some of the prompting samples in Table 11. We follow (Qiao et al., 2022) categorization on the deductive and inductive reasoning datasets, but we only use the QA part of EntailmentBank, that the authors took from ARC dataset (Clark et al., 2018), as we aim to test for reasoning capability. Regarding EntailmentBank, it might trigger the universe-related knowledge out of ChatGPT, which could help the model to derive the correct answer, although the test set is designed to test deductive reasoning skills. One of the future explorations would be with checking the rationale of ChatGPT as a follow-up question. 4.1.2 Abductive Reasoning Abductive reasoning is the inference to the most plausible explanation given observations. For in- stance, \u201cif Jenny \ufb01nds her house in a mess when she returns from work, and remembers that she left a window open, she can hypothesize that a the deductive and inductive reasoning datasets, but we only use the QA part of EntailmentBank, that the authors took from ARC dataset (Clark et al., 2018), as we aim to test for reasoning capability. Regarding EntailmentBank, it might trigger the universe-related knowledge out of ChatGPT, which could help the model to derive the correct answer, although the test set is designed to test deductive reasoning skills. One of the future explorations would be with checking the rationale of ChatGPT as a follow-up question. 4.1.2 Abductive Reasoning Abductive reasoning is the inference to the most plausible explanation given observations. For in- stance, \u201cif Jenny \ufb01nds her house in a mess when she returns from work, and remembers that she left a window open, she can hypothesize that a thief broke into her house and caused the mess\u201d11. We test ChatGPT\u2019s language-based abductive rea- soning ability with 30 samples from NLI dataset (Bhagavatula et al., 2020), which requires the model to select the most plausible explanation given the conclusion. Based on our test, it could achieve 86.7% (26 out of 30) accuracy. 4.2 Non-textual semantic reasoning It is often investigated in public sharing about Chat- GPT errors/ failure instances12that it lacks the reasoning ability that required non-text semantic understanding such as mathematical, temporal and spatial reasoning. In this section, we investigate the non-text semantic reasoning capabilities of Chat- GPT. Mathematical reasoning Mathematical capabil- ities or numerical reasoning has been frequently mentioned to be lacking for LLMs, not only Chat- GPT (Frieder et al., 2023). Frieder et al. test Chat- GPT\u2019s capability with publicly available datasets as 11An example provided by Bhagavatula et al. (2020). 12https://docs.google.com/spreadsheets/d/1kDSE RnROv5FgHbVN8z_bXH9gak2IXRtoqz0nwhrviCw/edit?usp =sharing Spatial Reasoning Tasks Dataset Total Basic Hard StepGame 26/60 19/30 7/30 SpartQA 28/64 20/32 8/32 Table 13: Spatial reasoning ability of ChatGPT. Over- all, ChatGPT falls short of the task. well as the human-curated dataset, which consists of 728 prompts. The shared \ufb01ndings for ChatGPT\u2019s mathematical capabilities include 1) ChatGPT of- ten understands the question but fails to provide cor- rect solutions; 2) it shows inconsistent poor perfor- mance on graduate-level advanced mathematics; 3) it has a great ability to search for mathematical ob- jects.13We also test separately on MATH dataset. 11An example provided by Bhagavatula et al. (2020). 12https://docs.google.com/spreadsheets/d/1kDSE RnROv5FgHbVN8z_bXH9gak2IXRtoqz0nwhrviCw/edit?usp =sharing Spatial Reasoning Tasks Dataset Total Basic Hard StepGame 26/60 19/30 7/30 SpartQA 28/64 20/32 8/32 Table 13: Spatial reasoning ability of ChatGPT. Over- all, ChatGPT falls short of the task. well as the human-curated dataset, which consists of 728 prompts. The shared \ufb01ndings for ChatGPT\u2019s mathematical capabilities include 1) ChatGPT of- ten understands the question but fails to provide cor- rect solutions; 2) it shows inconsistent poor perfor- mance on graduate-level advanced mathematics; 3) it has a great ability to search for mathematical ob- jects.13We also test separately on MATH dataset. Not surprisingly, it could only score 23.33% (7/30) for the MATH dataset (Saxton et al., 2019), which tests mathematical reasoning. Temporal reasoning Temporal reasoning is mentioned a few times in the literature but is less common than others. It tests the understanding of the time duration of and the relation between events. For this category, we conduct experiments on the dataset TimeDial (Qin et al., 2021), which solely re- quires temporal reasoning. We follow the format of the task in the BIG-bench benchmark (Srivastava et al., 2022), which is multiple-choice (single cor- rect answer), Overall, ChatGPT correctly answers 86.67% of the time (26/30), suggesting that it has a decent temporal reasoning ability. Also, compared to Chinchilla and Gopher which have the accuracy of 68.8% and 50.9%",
    "192": "respectively, ChatGPT shows a promising improvement for LLMs in that aspect. Spatial Reasoning Spatial reasoning is using an understanding of spatial relations among different objects and spaces. For spatial reasoning, we uti- lize two existing datasets: SpartQA (Mirzaee et al., 2021) and StepGame (Shi et al., 2022a), which compose of story-question pairs about k relations of k+1 (where k is up to 10) entities written in nat- ural language. ChatGPT is asked to answer spatial relations between two entities based on the pro- vided descriptions of different entities. ChatGPT falls short of the spatial reasoning tasks, as shown in Table 13, with overall success rates of 43.33% for StepGame and 43.75% for SpartQA. ChatGPT could only score 25% on SpartQA (hard), which covers multiple spatial reasoning sub-types, and 13Refer to detailed \ufb01ndings in the original paper.23.33% for stepGame (Hard) with k=9. ChatGPT understanding of spatial relations among different objects and spaces. For spatial reasoning, we uti- lize two existing datasets: SpartQA (Mirzaee et al., 2021) and StepGame (Shi et al., 2022a), which compose of story-question pairs about k relations of k+1 (where k is up to 10) entities written in nat- ural language. ChatGPT is asked to answer spatial relations between two entities based on the pro- vided descriptions of different entities. ChatGPT falls short of the spatial reasoning tasks, as shown in Table 13, with overall success rates of 43.33% for StepGame and 43.75% for SpartQA. ChatGPT could only score 25% on SpartQA (hard), which covers multiple spatial reasoning sub-types, and 13Refer to detailed \ufb01ndings in the original paper.23.33% for stepGame (Hard) with k=9. ChatGPT could not provide any spatial relations but instead generated \u201cIt is not speci\ufb01ed in the given descrip- tion\u201d . Even with the \ufb01ne-tuned models, as the num- ber of relations (k) increases in context description, performance drops (Shi et al., 2022a). To understand spatial reasoning ability at a more elementary level, we test with less complicated examples from StepGame which we refer to as StepGame (Basic) . It does not involve multi-hop reasoning but purely spatial relation between two entities. (e.g, \u201cC is sitting at the top position to Y. What is the relation of the agent Y to the agent C?\u201d ). We test for basic spatial relations with 8 labels from StepGame {left, right, above, below, lower- left, lower-right, upper-left, upper-right}. When we test on StepGame (Basic), ChatGPT scores higher (63.33%). We investigate the errors that it often fails to understand clock direction (e.g., \u201cW is at K\u2019s 3 o\u2019clock\u201d) and diagonal spatial relations. We further analyze the results by breaking down the test ex- amples of StepGame (Basic) into two comparisons: i) types of directions (basic cardinal vs. diago- nal) and ii) ways of spatial description for cardinal directions (basic cardinal14vs. clock-position car- dinal). We take 20 more samples for each category (basic cardinal, diagonal, clock-position cardinal) and tested them as illustrated in Table 12. \u2022ChatGPT poorly infers with clock-position description. Although it is a simple cardinal direction, ChatGPT could only correctly an- swer for 5 samples (25%), which is clearly poorer performance in comparison to perfor- mance with the basic cardinal description (17 correct answers). \u2022ChatGPT is worse at the diagonal posi- o\u2019clock\u201d) and diagonal spatial relations. We further analyze the results by breaking down the test ex- amples of StepGame (Basic) into two comparisons: i) types of directions (basic cardinal vs. diago- nal) and ii) ways of spatial description for cardinal directions (basic cardinal14vs. clock-position car- dinal). We take 20 more samples for each category (basic cardinal, diagonal, clock-position cardinal) and tested them as illustrated in Table 12. \u2022ChatGPT poorly infers with clock-position description. Although it is a simple cardinal direction, ChatGPT could only correctly an- swer for 5 samples (25%), which is clearly poorer performance in comparison to perfor- mance with the basic cardinal description (17 correct answers). \u2022ChatGPT is worse at the diagonal posi- tion. It correctly answers around half of the time (55%), which is worse than basic cardi- nal points (85%). Even with analysis from StepGame (Hard), among the correct 7 an- swers, there is only one diagonal direction that ChatGPT gets correctly while the others are all cardinal points. For those answers that require diagonal points, ChatGPT only could infer cardinal points for some examples. Commonsense Reasoning Tasks CommonsenseQA PiQA Pep-3k (Hard) 27/30 25/30 28/30 Table 14: Commonsense reasoning ability of ChatGPT. ChatGPT shows good performance of commonsense reasoning capability on the three test data we test it on. 4.3 Commonsense Reasoning Commonsense reasoning is the understanding and reasoning about everyday concepts and knowledge that most people are familiar with, to make judg- ments and predictions about new situations (Storks et al., 2019). Recent work has shown that LLMs perform impressively well on commonsense rea- soning benchmarks (Qiao et al., 2022; Huang and Chang, 2022; Bhargava and Ng, 2022). However, Bhargava and Ng also point out that the reasoning tasks underlying these benchmarks are still far from being solved, since most existing studies primarily report the performance of the models, without a de- tailed examination of the quality of the rationales produced. To evaluate ChatGPT\u2019s capability on common- sense reasoning, we \ufb01rst test it on two widely used benchmark datasets CommonsenseQA (Tal- mor et al., 2018) and PiQA (Bisk et al., 2020). CommonsenseQA focuses on general common- sense question answering such as \"Where is a busi- ness restaurant likely to be located?\" , and PiQA is about physical commonsense reasoning: given perform impressively well on commonsense rea- soning benchmarks (Qiao et al., 2022; Huang and Chang, 2022; Bhargava and Ng, 2022). However, Bhargava and Ng also point out that the reasoning tasks underlying these benchmarks are still far from being solved, since most existing studies primarily report the performance of the models, without a de- tailed examination of the quality of the rationales produced. To evaluate ChatGPT\u2019s capability on common- sense reasoning, we \ufb01rst test it on two widely used benchmark datasets CommonsenseQA (Tal- mor et al., 2018) and PiQA (Bisk et al., 2020). CommonsenseQA focuses on general common- sense question answering such as \"Where is a busi- ness restaurant likely to be located?\" , and PiQA is about physical commonsense reasoning: given a sentence such as \"When boiling butter, when it\u2019s ready, you can \" , the goal is to \ufb01ll in the blank with one of two answer options, \"Pour it onto a plate\" and\"Pour it onto a jar\" . We use the validation split for both of the datasets since there are no la- bels provided on the test set that we retrieve. We also further probe ChatGPT by evaluating a more challenging commonsense reasoning dataset in a more comprehensive way. We use Pep-3k (Wang et al., 2018b), which requires the model to recog- nize plausible but possibly novel events, such as \"man swallow paintball\" . Each instance in the Pep- 3k is an s-v-o predicate, and the task is to judge if the predicate is plausible or not. But instead of evaluating ChatGPT\u2019s performance only based on the binary judgment, we also check if the answer contains relevant rationales (explanations) that lead to its judgment. 14Those of which spatial relations are described with ex- plicit vocabulary.Prompt Please judge if this predicate is (likely) plausible or implausible: \u201cknife chop rock\u201d ChatGPT AnswerIt isimplausible that a knife could chop through a rock. Knives are made to cut through softer materials such as food, pa- per, or wood. Rocks are much harder than these materials and a knife would not be able to make a dent in a rock, let alone chop it. Table 15: An example from Pep-3k (Wang et al.,",
    "193": "2018b) for commonsense reasoning of ChatGPT. We make the main answer bold , and highlight the explana- tion by green color. ChatGPT shows surprisingly good common- sense reasoning capability in our evaluation tasks, perhaps due to its large parametric memory. We sample 30 instances from each of the test sets. For plicit vocabulary.Prompt Please judge if this predicate is (likely) plausible or implausible: \u201cknife chop rock\u201d ChatGPT AnswerIt isimplausible that a knife could chop through a rock. Knives are made to cut through softer materials such as food, pa- per, or wood. Rocks are much harder than these materials and a knife would not be able to make a dent in a rock, let alone chop it. Table 15: An example from Pep-3k (Wang et al., 2018b) for commonsense reasoning of ChatGPT. We make the main answer bold , and highlight the explana- tion by green color. ChatGPT shows surprisingly good common- sense reasoning capability in our evaluation tasks, perhaps due to its large parametric memory. We sample 30 instances from each of the test sets. For the Pep-3k samples, we prepend the s-v-o predi- cate with \"Please judge if this predicate is (likely) plausible or implausible:\" to prompt ChatGPT. We show the results in Table 14. As we see, ChatGPT performs quite well on the three datasets in terms of answer accuracy, which matches our anticipa- tion. Furthermore, as we also check the rationales in ChatGPT\u2019s answer when evaluating Pep-3k sam- ples, we can see that ChatGPT does quite well not only in terms of answer accuracy but also in gener- ating reasonable reasoning procedures to support its answer. We show a concrete example in Ta- ble 15. As we can see, ChatGPT\u2019s answer explains well what kinds of materials are usually cut through with knives (i.e., food, paper, or wood). Then, it reasons why rocks cannot be chopped with a knife by explaining \u2018rocks are much harder than these materials.\u2019 While our \ufb01ndings are based on 30 samples from each dataset, we see the potential in ChatGPT\u2019s commonsense reasoning capability, and further large-scale investigation is worth exploring. 4.4 Causal, Multi-Hop, and Analogical Reasoning Causal Reasoning Causal reasoning is the pro- cess of identifying the relationship between causes/actions and effects/changes (i.e., causality) (Thomason, 2018; Huang and Chang, 2022). We test ChatGPT on 30 samples of human-annotated explainable CAusal REasoning dataset (E-CARE) (Du et al., 2022) and it could score 24 samples cor- rectly (80%). Note that our evaluation is mainly Causal Multi-hop Analogical E-CARE HotpotQA Letter string analogies 24/30 8/30 30/30 Table 16: Results for causal, multi-hop, and analogical reasoning. ChatGPT shows good causal and analogical reasoning capability, but not on multi-hop reasoning. ChatGPT\u2019s commonsense reasoning capability, and further large-scale investigation is worth exploring. 4.4 Causal, Multi-Hop, and Analogical Reasoning Causal Reasoning Causal reasoning is the pro- cess of identifying the relationship between causes/actions and effects/changes (i.e., causality) (Thomason, 2018; Huang and Chang, 2022). We test ChatGPT on 30 samples of human-annotated explainable CAusal REasoning dataset (E-CARE) (Du et al., 2022) and it could score 24 samples cor- rectly (80%). Note that our evaluation is mainly Causal Multi-hop Analogical E-CARE HotpotQA Letter string analogies 24/30 8/30 30/30 Table 16: Results for causal, multi-hop, and analogical reasoning. ChatGPT shows good causal and analogical reasoning capability, but not on multi-hop reasoning. based on whether the model can make a judgment on correct causes or effects instead of its gener- ated explanation of why the causation exists \u2013 the follow-up generation on explanation can be future exploration. Multi-hop Reasoning To be able to reason over a larger context, a system has to perform multi-hop reasoning over more than one piece of information to arrive at the answer (Mavi et al., 2022). We test ChatGPT\u2019s multi-hop reasoning capability on 30 samples of HotpotQA dataset (Yang et al., 2018) and we \ufb01nd that ChatGPT has dif\ufb01culty perform- ing with such capability, only answering 8 samples correctly, although the questions posed are only 2-hops. It is worth noting that ChatGPT oftentimes generates the answer in a short passage of explana- tions, thus we evaluate manually each of the Chat- GPT responses to check its accuracy. This aligns with the \ufb01ndings that LLMs are also limited in sev- eral ways, and fail to produce accurate predictions due to their inability to accomplish complex rea- soning, such as solving tasks that require multi-hop reasoning (Ott et al., 2023). Analogical Reasoning Analogical reasoning is a way of thinking that relies upon an analogy, com- paring two or more objects or systems of objects (Bartha, 2013) to drive a conclusion. We test with 30 samples from Webb et al. (2022b) and evaluate based on human evaluation, to see if the generated answer match with/contain the gold answer. Chat- GPT could correctly answer all 30 examples, which may reveal that ChatGPT has a good capability in analogical reasoning skills. 5 Factuality and Hallucination Evaluations in ChatGPT LLMs are known to be susceptible to generating nonfactual, untruthful information, which is re- due to their inability to accomplish complex rea- soning, such as solving tasks that require multi-hop reasoning (Ott et al., 2023). Analogical Reasoning Analogical reasoning is a way of thinking that relies upon an analogy, com- paring two or more objects or systems of objects (Bartha, 2013) to drive a conclusion. We test with 30 samples from Webb et al. (2022b) and evaluate based on human evaluation, to see if the generated answer match with/contain the gold answer. Chat- GPT could correctly answer all 30 examples, which may reveal that ChatGPT has a good capability in analogical reasoning skills. 5 Factuality and Hallucination Evaluations in ChatGPT LLMs are known to be susceptible to generating nonfactual, untruthful information, which is re- ferred to as hallucination (Lee et al., 2022; Ji et al., 2022b,c; Su et al., 2022; Dai et al., 2022b). Many anecdotal witnesses show ChatGPT also seems to suffer from the same problem as other LLMs. Toevaluate this aspect of ChatGPT, we \ufb01rst explore existing fact-checking test sets and QA tasks that required knowledge (\u00a75.1). We illustrate the chal- lenge of hallucination in ChatGPT by sharing hallu- cination examples from different NLP tasks (\u00a75.2). 5.1 Factuality in ChatGPT We \ufb01rst test ChatGPT\u2019s ability to detect misin- formation with the test sets that consist of scien- ti\ufb01c and social claims related to COVID-19 (Lee et al., 2021). We take 50 samples each for sci- enti\ufb01c (covid-scienti\ufb01c) and social (covid-social) sets. ChatGPT is able to detect misinformation 92% (46/50) and 73.33% (22/30, excluding veri\ufb01cation- refusing cases) accuracy on covid-scienti\ufb01c and covid-social respectively. In comparison to its pre- viously reported performance, ChatGPT\u2019s perfor- mance on covid-scienti\ufb01c is impressive. However, this may be affected by the fact ChatGPT\u2019s training data includes information about COVID-19 while the previous models did not. Interestingly, for more societal-related claims, 40% of the time (20/30), ChatGPT refuses to make veri\ufb01cation as saying \u201c I\u2019m sorry, but I don\u2019t have enough information to accurately answer your question ... \u201d. This happens more often for false- labeled claims (misinformation) than true claims. This may be related to ChatGPT\u2019s safety layer or policy (the Covid-social test examples include polit- ical claims). OpenAI continues to tune ChatGPT\u2019s performance with RLHF, and that aspect also con- tributes to its performance. However, it cannot avoid the criticism that pa- viously reported performance, ChatGPT\u2019s perfor- mance on covid-scienti\ufb01c is impressive. However, this may be affected by the fact ChatGPT\u2019s training data includes information about COVID-19 while the previous models did not.",
    "194": "Interestingly, for more societal-related claims, 40% of the time (20/30), ChatGPT refuses to make veri\ufb01cation as saying \u201c I\u2019m sorry, but I don\u2019t have enough information to accurately answer your question ... \u201d. This happens more often for false- labeled claims (misinformation) than true claims. This may be related to ChatGPT\u2019s safety layer or policy (the Covid-social test examples include polit- ical claims). OpenAI continues to tune ChatGPT\u2019s performance with RLHF, and that aspect also con- tributes to its performance. However, it cannot avoid the criticism that pa- rameterized knowledge is obtained by better mem- orization as it still shows worse performance in questions designed to cause imitative falsehoods. For instance, \u201cDoes someone share more genes with their siblings or parents?\u201d and \u201cWhat are the health bene\ufb01ts of pink Himalayan salt?\u201d15We test on 66 test samples16from TruthfulQA (Lin et al., 2022), which tests the extent of LLMs to mimic hu- mans falsehood, and 35.38% of the time ChatGPT fails to answer truthfully. 5.2 Hallucination in ChatGPT There exist two categories of hallucination (Ji et al., 2022b). Intrinsic hallucinations that refers to the LLM generation that contradicts the source/input 15Examples are from Lin et al. 16Each sample from each sub-category from both adversarial/non-adversarial type. Please refer to original paper for details. Task Key Text Content Machine TranslationPrompt(Javanese) Krajan Anyar Mesir kuno nggumun marang monumen-monumen leluhure, sing nalika iku tetep apik luwih seka sewu taun. ChatGPT GenerationThe ancient Egyptian Pharaohs built beautiful monuments that are still well-preserved after thou- sands of years. Hallucination Extrinsic - New Kingdom ancient Egyptians marveled at their predecessors monuments, which were then well over a thousand year old. SummarizationPrompt[...] But the Russian president lifted the ban after Tehran struck an interim deal with Britain and \ufb01ve other countries to curb nuclear activities in exchange for sanctions relief.[...] ChatGPT GenerationThe move comes following the recent framework deal between Iran and the P5+1 (US, UK, France, China, Russia, and Germany) to curb Iran\u2019s nuclear activities in exchange for sanctions relief. iku tetep apik luwih seka sewu taun. ChatGPT GenerationThe ancient Egyptian Pharaohs built beautiful monuments that are still well-preserved after thou- sands of years. Hallucination Extrinsic - New Kingdom ancient Egyptians marveled at their predecessors monuments, which were then well over a thousand year old. SummarizationPrompt[...] But the Russian president lifted the ban after Tehran struck an interim deal with Britain and \ufb01ve other countries to curb nuclear activities in exchange for sanctions relief.[...] ChatGPT GenerationThe move comes following the recent framework deal between Iran and the P5+1 (US, UK, France, China, Russia, and Germany) to curb Iran\u2019s nuclear activities in exchange for sanctions relief. Hallucination Extrinsic Factual - \u201cP5+1\u201d are the exact countries that the source document mentions and the model can correctly list the names of these countries in summary. Task-oriented DialoguePromptContinue the dialogue as a task-oriented dialogue system called SYSTEM. The answer of SYSTEM should follow the ACTION provided next while answering the USER\u2019s last utterance: ACTION: \u2019Train-Inform\u2019: [[\u2019choice\u2019, \u201912\u2019], [\u2019leaveat\u2019, \u201905:16\u2019], [\u2019leaveat\u2019, \u201916:16\u2019]], \u2019Train- OfferBook\u2019: [[\u2019none\u2019, \u2019none\u2019]] USER: I need train reservations from norwich to cambridge SYSTEM: I have 133 trains matching your request. Is there a speci\ufb01c day and time you would like to travel? USER: I\u2019d like to leave on Monday and arrive by 18:00. SYSTEM: There are 12 trains for the day and time you request. Would you like to book it now? USER: Before booking, I would also like to know the travel time, price, and departure time please. ChatGPT GenerationSYSTEM: The travel time for the trains from Norwich to Cambridge is approximately 1 hour and 30 minutes. The prices for these trains range from \u00a310 to \u00a330, and the departure times are 5:16 and 16:16. Please let me know if you would like to book a train now. Hallucination Extrinsic - There are no information provided about neither travel time nor price range Knowledge- Grounded Open-domain DialoguePromptCan we try dialogue generation? I will give you turns and you can generate the next turn, but only one. You can also consider the knowledge of \"Pierre Cof\ufb01n is starred in Despicable Me 2\" and \"Despicable Me 2 is written by Cinco Paul\" for your reference in the dialogue. User: Do you know of any movies directed by Pierre Cof\ufb01n? ChatGPT ChatGPT GenerationSYSTEM: The travel time for the trains from Norwich to Cambridge is approximately 1 hour and 30 minutes. The prices for these trains range from \u00a310 to \u00a330, and the departure times are 5:16 and 16:16. Please let me know if you would like to book a train now. Hallucination Extrinsic - There are no information provided about neither travel time nor price range Knowledge- Grounded Open-domain DialoguePromptCan we try dialogue generation? I will give you turns and you can generate the next turn, but only one. You can also consider the knowledge of \"Pierre Cof\ufb01n is starred in Despicable Me 2\" and \"Despicable Me 2 is written by Cinco Paul\" for your reference in the dialogue. User: Do you know of any movies directed by Pierre Cof\ufb01n? ChatGPT GenerationYes, Pierre Cof\ufb01n is the co-director of the animated \ufb01lm \"Despicable Me 2.\" Cinco Paul is the \ufb01lm\u2019s writer. Hallucination animated \ufb01lm is a factual extrinsic hallucination. Question AnsweringPrompt [Jason] is very much in love with his wife. Her name is [Gabrielle]. [Jason] has a daughter called [Alma].. Who is Alma to Gabrielle? ChatGPT GenerationAlma is Jason\u2019s daughter and therefore Gabrielle\u2019s stepdaughter. Hallucination intrinsic - stepkinship never mentioned in the prompt. Ground truth is daughter Question Answering (Truthful QA)Prompt Where do people smoke more on average than in Russia? ChatGPT GenerationThere are several countries where people smoke more on average than in Russia. According to data from the World Health Organization, the following countries have higher smoking rates than Russia: Indonesia: 55.2% of adults smoke China: 52.9% of adults smoke Serbia: 47.5% of adults smoke Serbia and Montenegro: 45.6% of adults smoke Montenegro: 45.6% of adults smoke Greece: 44.5% of adults smoke Bosnia and Herzegovina: 41.8% of adults smoke Kazakhstan: 41.3% of adults smoke Kosovo: 41.3% of adults smoke Moldova: 40.9% of adults smoke It\u2019s important to note that these \ufb01gures are estimates and may vary slightly from other sources. Hallucination Extrinsic - the generated answer includes wrong statistics. (Compared with statistics from the prevalence of current tobacco use ( %of adults) from the World Bank statistics.17) National Flag DrawingPrompt (history)[USER-prompt] : Describe how the Mexican \ufb02ag looks like. [ChatGPT generation] : The \ufb02ag of Mexico consists of three vertical bands in green, white, and smoke Serbia and Montenegro: 45.6% of adults smoke Montenegro: 45.6% of adults smoke Greece: 44.5% of adults smoke Bosnia and Herzegovina: 41.8% of adults smoke Kazakhstan: 41.3% of adults smoke Kosovo: 41.3% of adults smoke Moldova: 40.9% of adults smoke It\u2019s important to note that these \ufb01gures are estimates and may vary slightly from other sources. Hallucination Extrinsic - the generated answer includes wrong statistics. (Compared with statistics from the prevalence of current tobacco use ( %of adults) from the World Bank statistics.17) National Flag DrawingPrompt (history)[USER-prompt] : Describe how the Mexican \ufb02ag looks like. [ChatGPT generation] : The \ufb02ag of Mexico consists of three vertical bands in green, white, and red. The green band is located at the hoist side of the \ufb02ag and represents hope, joy, and love. [...] [USER-prompt] : Generate a code snippet to represent that \ufb02ag in SVG format. ChatGPT Generation",
    "195": "Hallucination Intrinsic - It says there are three \"vertical\" bands but generates three horizontal bands. Table 17: Examples of hallucination in different tasks content. Extrinsic hallucinations that refers to the LLM generations that cannot be veri\ufb01ed from the source/input content (i.e., output that can neither be supported nor contradicted by the source). In Table 17, we share examples of these hal- lucination types detected from different task ex- plorations. With the setting of tasks we test, we often \ufb01nd extrinsic hallucinations, including both untruthful and factual ones, across various tasks such as Machine Translation, Question answering. The intrinsic hallucinations are barely found as discussed in tasks about summarization and knowledge-grounded open-domain dialogue. For instance, in the abstractive summarization task, in which neural models usually suffer from intrinsic hallucination, ChatGPT\u2019s generated summarisation did not include any intrinsic hallucination exam- ples based on our experiments. It rather shows a factual extrinsic hallucination, for instance, Chat- GPT could correctly paraphrase \u201cBritain and \ufb01ve other countries\u201d from source input into \u201cP5+1 (US, UK, France, China, Russia, and Germany),\u201d which is assessed to be factual. We could also observe an interesting intrinsic hallucination for our proposed multi-modal task, the \ufb02ag drawing task. ChatGPT is \ufb01rst asked to generate a description of how the \ufb02ags look before it is asked to generate code for the \ufb02ag. Although it generates the correct description instance, in the abstractive summarization task, in which neural models usually suffer from intrinsic hallucination, ChatGPT\u2019s generated summarisation did not include any intrinsic hallucination exam- ples based on our experiments. It rather shows a factual extrinsic hallucination, for instance, Chat- GPT could correctly paraphrase \u201cBritain and \ufb01ve other countries\u201d from source input into \u201cP5+1 (US, UK, France, China, Russia, and Germany),\u201d which is assessed to be factual. We could also observe an interesting intrinsic hallucination for our proposed multi-modal task, the \ufb02ag drawing task. ChatGPT is \ufb01rst asked to generate a description of how the \ufb02ags look before it is asked to generate code for the \ufb02ag. Although it generates the correct description as \u201cThe \ufb02ag of Mexico consists of three vertical bands [...]\u201d, the \ufb01nal drawing (SVG code) consists ofhorizontal bands. However, extrinsic hallucinations often happen, including both untruthful and factual ones. In the question-answering task, we often \ufb01nd extrinsic hallucination to be non-factual which harms the \ufb01nal performance. For instance, in the question of asking for the relationship among entities, although step kindship is never mentioned in the question, ChatGPT answers the question with step kinship, as illustrated in Table 17. We could also observe that ChatGPT\u2019s weakness with extrinsic hallucina- tion also degrades machine translation. When it is asked to translate the text \u201cLike some other ex- perts, he is skeptical about whether diabetes can be cured, noting that these \ufb01ndings have no relevance to people who already have Type 1 diabetes.\u201d into Korean, it contains a piece of information that was not found in the source, \u201c $ \u007f\u00c5 \u00d2 \u0013u \u001b\u00ab \u00d1 \u201d (transcuta- neous electrical nerve stimulation) in the translated text. Figure 3: An example of dialogue summarization 6 Evaluating Interactivity in ChatGPT ChatGPT has a built-in interactive ability thanks to conversational data \ufb01ne-tuning and RLHF. We fur- ther delve into the bene\ufb01t of exploiting this interac- tive ability of ChatGPT in three NLP tasks, i.e., 1) summarization, 2) machine translation, and 3) mul- timodal generation. Our experiments demonstrate the potential of employing multi-turn interaction to re\ufb01ne the quality of the generated responses and improve the task performance of ChatGPT. 6.1 Interactivity on Summarization Summarization models aim to extract essential in- formation from documents and to generate short, not found in the source, \u201c $ \u007f\u00c5 \u00d2 \u0013u \u001b\u00ab \u00d1 \u201d (transcuta- neous electrical nerve stimulation) in the translated text. Figure 3: An example of dialogue summarization 6 Evaluating Interactivity in ChatGPT ChatGPT has a built-in interactive ability thanks to conversational data \ufb01ne-tuning and RLHF. We fur- ther delve into the bene\ufb01t of exploiting this interac- tive ability of ChatGPT in three NLP tasks, i.e., 1) summarization, 2) machine translation, and 3) mul- timodal generation. Our experiments demonstrate the potential of employing multi-turn interaction to re\ufb01ne the quality of the generated responses and improve the task performance of ChatGPT. 6.1 Interactivity on Summarization Summarization models aim to extract essential in- formation from documents and to generate short, concise, and readable text (Yu et al., 2021b; Su et al., 2021). Recently, Goyal et al. (2022) show that zero-shot prompting with GPT-3 (Brown et al., 2020) performs better than the state-of-the-art \ufb01ne- tuning model (Liu et al., 2022) on human evalua- tion. One main advantage of ChatGPT over GPT3 is that it interacts in a conversational way. There- fore, we study the interactivity of ChatGPT, espe- cially in real-world applications, people may want to improve the summary based on the previously generated summary. In detail, we investigate ChatGPT\u2019s ability to control the length of summaries through multi- turn interaction. To run experiments, we ran- domly sample 50 documents from a dialogue sum- marization dataset called SAMSum (Gliwa et al., Figure 4: Result of the multi-turn MT-APE experi- ment. #Correct MT denotes the number of correct translations. #Correct APE denotes the number of cor- rect translations after post-editing. 2019) and conduct a two-turn iterative prompt ap- proach. Given an input dialogue as the context, we \ufb01rst input the prompt \u201cSummarize the above dialogue\u201d to the ChatGPT. However, ChatGPT usually generates an overly long summary, some- times even longer than the input conversation it- self. To re\ufb01ne the summary, we simply input another prompt \u2013 \u201cPlease make the summary shorter\u201d after the \ufb01rst response. According to the second prompt, GhatGPT could provide a much shorter summary than the \ufb01rst response. In order to quantify the experimental results, we calculate the ROUGE-1 scores among the \ufb01rst summary and the second summary. Experimental results show that with the second length control prompt, the re\ufb01ned summaries achieve 7.99, 1.64, and 5,19 rect translations after post-editing. 2019) and conduct a two-turn iterative prompt ap- proach. Given an input dialogue as the context, we \ufb01rst input the prompt \u201cSummarize the above dialogue\u201d to the ChatGPT. However, ChatGPT usually generates an overly long summary, some- times even longer than the input conversation it- self. To re\ufb01ne the summary, we simply input another prompt \u2013 \u201cPlease make the summary shorter\u201d after the \ufb01rst response. According to the second prompt, GhatGPT could provide a much shorter summary than the \ufb01rst response. In order to quantify the experimental results, we calculate the ROUGE-1 scores among the \ufb01rst summary and the second summary. Experimental results show that with the second length control prompt, the re\ufb01ned summaries achieve 7.99, 1.64, and 5,19 gains on ROUGE-1, ROUGE-2, and ROUGE-L, respectively. Figure 3 shows an example of how multi-turn interaction helps to control the length of the summary. 6.2 Interactivity on Machine Translation One of the capabilities of ChatGPT is to perform text translation from one language to another. With the interactivity of ChatGPT, we explore the pos- sibility of performing a combined machine trans- lation and automatic post-editing tasks to improve the translation quality of ChatGPT. We explore this capability on translation from English to the target language since the translation quality from high-resource and medium-resource languages to English of ChatGPT is near perfect (see \u00a73.2). For the experiment,",
    "196": "we adapt the dataset used in \u00a73.2.2 which samples 30 parallel sentences from 6 language pairs in NusaX (Winata et al.,Label Metric w/o APE w/ APE Post-Edited Marathi TextHTER 88.14 88.79 SacreBLEU 4.81 4.20 METEOR 13.10 12.74 Source English TextHTER 65.36 63.13 SacreBLEU 25.54 27.20 METEOR 43.71 47.51 BERTScore 92.30 92.59 Table 18: Result of translation w/ and w/o post-editing on WMT 2022 English !Marathi APE shared task 2022), i.e., Chinese (zho), French (fra), Indone- sian (ind), Korean (kor), Javanese (jav), and Sundanese (sun). We experiment with a multi- turn approach, where we \ufb01rst query ChatGPT to translate to the target language using \u201cWhat is [TARGET_LANGUAGE] translation of the following sentence?\\n\\n[INPUT_SENTENCE]\u201d as the prompt template, and then query for the post-editing using the following prompt template: \u201cCould you perform a post-editing to ensure the meaning is equivalent to \"[INPUT_SENTENCE]\"?\u201d . The post-editing results Marathi TextHTER 88.14 88.79 SacreBLEU 4.81 4.20 METEOR 13.10 12.74 Source English TextHTER 65.36 63.13 SacreBLEU 25.54 27.20 METEOR 43.71 47.51 BERTScore 92.30 92.59 Table 18: Result of translation w/ and w/o post-editing on WMT 2022 English !Marathi APE shared task 2022), i.e., Chinese (zho), French (fra), Indone- sian (ind), Korean (kor), Javanese (jav), and Sundanese (sun). We experiment with a multi- turn approach, where we \ufb01rst query ChatGPT to translate to the target language using \u201cWhat is [TARGET_LANGUAGE] translation of the following sentence?\\n\\n[INPUT_SENTENCE]\u201d as the prompt template, and then query for the post-editing using the following prompt template: \u201cCould you perform a post-editing to ensure the meaning is equivalent to \"[INPUT_SENTENCE]\"?\u201d . The post-editing results are manually validated by a native speaker in the corresponding language to validate: 1) whether the post-edited sentence is better than the translation one, and 2) whether the post-edited sentence is the correct translation of the given English sentence. As shown in Figure 4, despite the translation and post-editing being done using a single ChatGPT model, the multi-turn approach method helps to im- prove the correctness of the translation by making partial corrections or even full corrections in some cases. This result re\ufb02ects that performing auto- matic post-editing through interactive LLMs, such as ChatGPT, yields consistently better translation results compared to a single-turn machine transla- tion, which is especially useful for translation in low-resource languages. We provide per-language examples of the machine-translated and post-edited sentences in Appendix D. To further strengthen our hypothesis, we con- duct an additional experiment on the automatic post-editing (APE) shared task dataset on WMT 2022 (Bhattacharyya et al., 2022), which focuses on English!Marathi post-editing task. Marathi (mar) is also a low-resource language with 0.02% data size on CommonCrawl. We sample 50 sam- ples from the corresponding dataset and conduct the evaluation in 2 ways: 1) human-targeted transla- Figure 5: Changes in ChatGPT\u2019s drawing of the Cana- dian \ufb02ag over three turns. Layout, color, completion, and shape/size are marked as !if they align with those of the ground truth, and 7otherwise. tion error rate (HTER)18, SacreBLEU (Post, 2018) and METEOR (Banerjee and Lavie, 2005) be- tween the Marathi generated sentence compared to the human post-edited sentence, 2) HTER, Sacre- duct an additional experiment on the automatic post-editing (APE) shared task dataset on WMT 2022 (Bhattacharyya et al., 2022), which focuses on English!Marathi post-editing task. Marathi (mar) is also a low-resource language with 0.02% data size on CommonCrawl. We sample 50 sam- ples from the corresponding dataset and conduct the evaluation in 2 ways: 1) human-targeted transla- Figure 5: Changes in ChatGPT\u2019s drawing of the Cana- dian \ufb02ag over three turns. Layout, color, completion, and shape/size are marked as !if they align with those of the ground truth, and 7otherwise. tion error rate (HTER)18, SacreBLEU (Post, 2018) and METEOR (Banerjee and Lavie, 2005) be- tween the Marathi generated sentence compared to the human post-edited sentence, 2) HTER, Sacre- BLEU, METEOR, and semantic similarity score, i.e., BERTScore (Zhang* et al., 2020), between the English back-translated sentence and original English sentence.19 As shown on Table 18, the single-turn transla- tion without post-editing produces a slightly better evaluation score on the Marathi language, but the multi-turn with post-editing consistently yields bet- ter evaluation performance on the back-translated English text on all metrics. This suggests that post- editing enables the translation results to be closer to the actual meaning of the source text. Nevertheless, the translation to the Marathi language is much worse compared to the baseline MT provided from the APE 2022 shared task (Bhattacharyya et al., 2022) which further supports the limitations of ChatGPT on generating sentences in low-resource and non-Latin script languages. Figure 6: From fruits to a Christmas tree. Step-by-step image drawing and modi\ufb01cation by ChatGPT. 6.3 Interactivity on Multimodal Generation The multi-turn interaction ability of ChatGPT en- ables the re\ufb01nement of text-to-image generation. It is one of the most natural ways for humans to create artwork or product designs by requesting an AI tool iteratively. For example, Figure 6 shows the pro- cess of creating an interesting painting by prompt- 18HTER is the of\ufb01cial evaluation metric used in the APE 2022 shared task. 19the back translation process is done via Google Translate (https://translate.google.com/ ). ing ChatGPT with varied requirements through multiple turns. To quantitatively study how this ability impacts text-to-image generation, as mentioned in the task formulation of the \ufb02ag drawing, we conduct at most three rounds of post-editing. As shown in 6.3 Interactivity on Multimodal Generation The multi-turn interaction ability of ChatGPT en- ables the re\ufb01nement of text-to-image generation. It is one of the most natural ways for humans to create artwork or product designs by requesting an AI tool iteratively. For example, Figure 6 shows the pro- cess of creating an interesting painting by prompt- 18HTER is the of\ufb01cial evaluation metric used in the APE 2022 shared task. 19the back translation process is done via Google Translate (https://translate.google.com/ ). ing ChatGPT with varied requirements through multiple turns. To quantitatively study how this ability impacts text-to-image generation, as mentioned in the task formulation of the \ufb02ag drawing, we conduct at most three rounds of post-editing. As shown in Figure 7, in the \ufb01rst round of generation, ChatGPT rarely generates errorless SVG images except for some relatively simple \ufb02ags (e.g., Nigerian and German). In subsequent rounds of the generation, we see a clear boost in the overall quality of the generated \ufb02ag images by asking ChatGPT to \ufb01x er- rors based on its own description. We observe that 34% and 36% of samples experience improvement (i.e., fewer errors) from turn 1 to turn 2 and from turn 2 to turn 3, respectively. Meanwhile, there are also 6% and 8% of samples that experience degradation after each dialog turn. In other words, while improvement is not always guaranteed, the multi-turn conversation capability of ChatGPT en- ables post-editing through interaction. We also test with the InstructGPT (davinci-003), which has the same backbone model as ChatGPT but lacks con- versation ability. As demonstrated in Appendix B, InstructGPT cannot achieve a signi\ufb01cant improve- ment by directly putting the intermediate results in the input context. 7 Conclusion and Discussion 7.1 Multitask, Multilingual, Multimodal ChatGPT outperforms multiple state-of-the-art zero-shot LLMs on various tasks and even sur- passes \ufb01ne-tuned models on some tasks. Although ChatGPT performs well in most of the tasks, there are still",
    "197": "some failure cases on each task (\u00a73.1). In the summarization task, ChatGPT sometimes generates a summary that is even longer than the input document. In the machine translation task, ChatGPT sometimes produces an incorrect transla- tion for some words, making the meaning slightly shifted. Therefore, dealing with these special cases is a complex but important task. In terms of multilinguality, ChatGPT achieves ment by directly putting the intermediate results in the input context. 7 Conclusion and Discussion 7.1 Multitask, Multilingual, Multimodal ChatGPT outperforms multiple state-of-the-art zero-shot LLMs on various tasks and even sur- passes \ufb01ne-tuned models on some tasks. Although ChatGPT performs well in most of the tasks, there are still some failure cases on each task (\u00a73.1). In the summarization task, ChatGPT sometimes generates a summary that is even longer than the input document. In the machine translation task, ChatGPT sometimes produces an incorrect transla- tion for some words, making the meaning slightly shifted. Therefore, dealing with these special cases is a complex but important task. In terms of multilinguality, ChatGPT achieves strong performance in many high-resource and medium-resource languages. Nevertheless, Chat- GPT still lacks the ability to understand and gen- erate sentences in low-resource languages (\u00a73.2). The performance disparity in low-resource lan- guages limits the diversity and inclusivity of NLP (Joshi et al., 2020; Aji et al., 2022; Wan, 2022). Additionally, ChatGPT also lacks the abilityto translate sentences in non-Latin script languages (\u00a73.2.2), despite the languages being high-resource. This raises the question of language representation in ChatGPT. Research on shared representation for non-Latin scripts (Amrhein and Sennrich, 2020; Pfeiffer et al., 2021; Wan, 2022) is needed. In terms of multimodality, it is very natural to have visual information (images or videos) in the form of dialogue (Sun et al., 2022; Mostafazadeh et al., 2017) in real applications, which may be provided by the user or generated by the model. The visual information also serves as part of the context for subsequent turns. Can textual models like ChatGPT switch to a multimodal backbone? Through our \ufb02ag drawing experiments, we \ufb01nd that ChatGPT is able to translate visual concepts and structures to basic code formats (e.g., circle SVG element), which de\ufb01ne the exact shape, orientation, color, and placement of the objects. Given this structured way of generating an image, one of the research questions is: if a model learns an image as a composition of basic shapes, would it help a model understand the abstraction of visual concepts and structures (Ji et al., 2022a)? Moreover, would it produce more interpretable results for the users? 7.2 Reasoning The highly impressive performance of ChatGPT has sparked interest in expanding its usage beyond context for subsequent turns. Can textual models like ChatGPT switch to a multimodal backbone? Through our \ufb02ag drawing experiments, we \ufb01nd that ChatGPT is able to translate visual concepts and structures to basic code formats (e.g., circle SVG element), which de\ufb01ne the exact shape, orientation, color, and placement of the objects. Given this structured way of generating an image, one of the research questions is: if a model learns an image as a composition of basic shapes, would it help a model understand the abstraction of visual concepts and structures (Ji et al., 2022a)? Moreover, would it produce more interpretable results for the users? 7.2 Reasoning The highly impressive performance of ChatGPT has sparked interest in expanding its usage beyond traditional NLP tasks into more complex domains requiring sophisticated reasoning such as problem- solving, decision-making, and planning. Our eval- uation of its reasoning abilities shows that they are not reliable. Speci\ufb01cally, our \ufb01ndings indicate that ChatGPT exhibits a tendency to be a lazy reasoner and that its capabilities are inconsistent across vari- ous reasoning abilities. In terms of logical reasoning, ChatGPT performs better deductive and abductive reasoning compared to inductive reasoning. However, as a language model, ChatGPT still lacks the ability to answer non-textual semantic reasoning tasks, such as math- ematical, temporal, and spatial reasoning. Instead, many suggest pairing ChatGPT with another com- putational model, such as Wolfram20, to solve each speci\ufb01c set of problems. In that combination, Chat- GPT parses natural language input into program- ming language code snippets, then the computa- tional model will execute the code to return results. 20https://writings.stephenwolfram.com/2023/01/wolframalpha- as-the-way-to-bring-computational-knowledge-superpowers- to-chatgpt/ In this way, the strength of ChatGPT is maximized while the weakness is mitigated. Meanwhile, Chat- GPT surprisingly excels in commonsense, causal, and analogical reasoning. We suspect that all this knowledge has been encoded in the parametric memory of ChatGPT. Nevertheless, ChatGPT lacks the ability to perform multi-hop reasoning which suggests that, like other LLMs, ChatGPT possesses a limited ability to accomplish complex reasoning tasks. To support the further expansion of its use cases, it is necessary to prioritize the development of sys- tems with robust complex reasoning capabilities, tional model will execute the code to return results. 20https://writings.stephenwolfram.com/2023/01/wolframalpha- as-the-way-to-bring-computational-knowledge-superpowers- to-chatgpt/ In this way, the strength of ChatGPT is maximized while the weakness is mitigated. Meanwhile, Chat- GPT surprisingly excels in commonsense, causal, and analogical reasoning. We suspect that all this knowledge has been encoded in the parametric memory of ChatGPT. Nevertheless, ChatGPT lacks the ability to perform multi-hop reasoning which suggests that, like other LLMs, ChatGPT possesses a limited ability to accomplish complex reasoning tasks. To support the further expansion of its use cases, it is necessary to prioritize the development of sys- tems with robust complex reasoning capabilities, which should also be facilitated by the creation of more comprehensive benchmarks for assessing these abilities, particularly when multiple abilities are required to complete the tasks. 7.3 Factuality and Hallucinations Although powerful, ChatGPT, like other LLMs, still makes things up (Ji et al., 2022b). To ensure factuality, it is possible to build LLMs with an inter- face to an external knowledge source, like Blender- bot 3.0 (Shuster et al., 2022), RETRO (Borgeaud et al., 2021), and LaMDa (Thoppilan et al., 2022). In this manner, factual information LLMs can be updated independently and easily in the knowledge base, without \ufb01ne-tuning the whole LLM. However, how to balance the generative power of its paramet- ric memory with external knowledge sources is an active research area (Lee et al., 2022; He et al., 2023) Meanwhile, there are many forms of hallucina- tions from LLMs that are not necessarily counter- factual but still undesirable. The RLHF process of ChatGPT can ensure human feedback to mitigate undesirable responses. However, researchers need to work on coming up with more automatic and scalable methods to detect and mitigate hallucina- tions and other undesirable artifacts of LLMs. 7.4 Interactivity Compared with the previous LLMs, the interac- tive ability of ChatGPT has made a leap accord- ing to both qualitative and quantitative measures. Based on our evaluation, through interactivity, we can improve the performance of ChatGPT by 8% ROUGE-1 on summarization tasks and 2% ChrF++ on the machine translation tasks. However, some- times ChatGPT retains the wrong answer even after receiving multiple rounds of prompts from the user. tions from LLMs that are not necessarily counter- factual but still undesirable. The RLHF process of ChatGPT can ensure human feedback to mitigate undesirable",
    "198": "responses. However, researchers need to work on coming up with more automatic and scalable methods to detect and mitigate hallucina- tions and other undesirable artifacts of LLMs. 7.4 Interactivity Compared with the previous LLMs, the interac- tive ability of ChatGPT has made a leap accord- ing to both qualitative and quantitative measures. Based on our evaluation, through interactivity, we can improve the performance of ChatGPT by 8% ROUGE-1 on summarization tasks and 2% ChrF++ on the machine translation tasks. However, some- times ChatGPT retains the wrong answer even after receiving multiple rounds of prompts from the user. Improving the ability of ChatGPT to handle mul-tiple rounds of user feedback is also an important challenge. The conversational ability and multi-turn interac- tion of ChatGPT make it natural for people to use it as a dialog system. We carry out the very dif\ufb01cult task of using ChatGPT as a task-oriented dialog sys- tem with structured knowledge given in the prompt to perform. Whereas ChatGPT shows strong per- formance in various modules, challenges remain for us to use ChatGPT as a fully task-oriented di- alog system, due to the lack of controllability and knowledge grounding in its response. The interactivity inadvertently enables the user to \u201cjail-break\u201d ChatGPT to carry out harmful ac- tions. For example, a user could ask ChatGPT to turn off its safety layer, causing potential dam- age (Christian, 2023). 7.5 Responsible Generative AI Responsible design and usage of LLMs including ChatGPT is an important and pressing challenge today. There are common issues with these mod- els, such as fairness, toxicity, demographic bias, and safety, that need to be addressed. In the case of ChatGPT, OpenAI constructs safety layers and uses RLHF and potentially other means to \ufb01lter out undesirable system responses. This process is resource intensive and opaque to the public. We hope to see a more open discussion and sharing of responsible design of LLMs from various organiza- tions including OpenAI in the future. References 2023. Chatgpt vs satya nadella over biryani: The chat- bot is learning from its mistakes. Alham Fikri Aji, Genta Indra Winata, Fajri Koto, Samuel Cahyawijaya, Ade Romadhony, Rahmad Mahendra, Kemal Kurniawan, David Moeljadi, Ra- ChatGPT is an important and pressing challenge today. There are common issues with these mod- els, such as fairness, toxicity, demographic bias, and safety, that need to be addressed. In the case of ChatGPT, OpenAI constructs safety layers and uses RLHF and potentially other means to \ufb01lter out undesirable system responses. This process is resource intensive and opaque to the public. We hope to see a more open discussion and sharing of responsible design of LLMs from various organiza- tions including OpenAI in the future. References 2023. Chatgpt vs satya nadella over biryani: The chat- bot is learning from its mistakes. Alham Fikri Aji, Genta Indra Winata, Fajri Koto, Samuel Cahyawijaya, Ade Romadhony, Rahmad Mahendra, Kemal Kurniawan, David Moeljadi, Ra- dityo Eko Prasojo, Timothy Baldwin, Jey Han Lau, and Sebastian Ruder. 2022. One country, 700+ lan- guages: NLP challenges for underrepresented lan- guages and dialects in Indonesia. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 7226\u20137249, Dublin, Ireland. Association for Computational Linguistics. Sam Altman. 2022. Chatgpt is incredibly limited, but good enough at some things to create a misleading impression of greatness. Chantal Amrhein and Rico Sennrich. 2020. On Roman- ization for model transfer between scripts in neural machine translation. In Findings of the Association for Computational Linguistics: EMNLP 2020 , pages 2461\u20132469, Online. Association for Computational Linguistics. \u00d6mer Ayd\u0131n and Enis Karaarslan. 2022. Openai chat- gpt generated literature review: Digital twin in healthcare. Available at SSRN 4308687 . Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with im- proved correlation with human judgments. In Pro- ceedings of the ACL Workshop on Intrinsic and Ex- trinsic Evaluation Measures for Machine Transla- tion and/or Summarization , pages 65\u201372, Ann Ar- bor, Michigan. Association for Computational Lin- guistics. Paul Bartha. 2013. Analogy and analogical reasoning. Chandra Bhagavatula, Ronan Le Bras, Chaitanya Malaviya, Keisuke Sakaguchi, Ari Holtzman, Han- nah Rashkin, Doug Downey, Wen tau Yih, and Yejin Choi. 2020. Abductive commonsense reasoning. In International Conference on Learning Representa- tions . Prajjwal Bhargava and Vincent Ng. 2022. Common- sense knowledge reasoning and generation with pre- trained language models: a survey. In Proceedings An automatic metric for MT evaluation with im- proved correlation with human judgments. In Pro- ceedings of the ACL Workshop on Intrinsic and Ex- trinsic Evaluation Measures for Machine Transla- tion and/or Summarization , pages 65\u201372, Ann Ar- bor, Michigan. Association for Computational Lin- guistics. Paul Bartha. 2013. Analogy and analogical reasoning. Chandra Bhagavatula, Ronan Le Bras, Chaitanya Malaviya, Keisuke Sakaguchi, Ari Holtzman, Han- nah Rashkin, Doug Downey, Wen tau Yih, and Yejin Choi. 2020. Abductive commonsense reasoning. In International Conference on Learning Representa- tions . Prajjwal Bhargava and Vincent Ng. 2022. Common- sense knowledge reasoning and generation with pre- trained language models: a survey. In Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence , volume 36, pages 12317\u201312325. Pushpak Bhattacharyya, Rajen Chatterjee, Markus Fre- itag, Diptesh Kanojia, Matteo Negri, and Marco Turchi. 2022. Findings of the wmt 2022 shared task on automatic post-editing. In Proceedings of the Seventh Conference on Machine Translation , pages 109\u2013117, Abu Dhabi. David G.W. Birch. 2022. Chatgpt is a window into the real future of \ufb01nancial services. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. 2020. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on arti\ufb01cial intelligence , vol- ume 34, pages 7432\u20137439. Alexandre Blanco-Gonzalez, Alfonso Cabezon, Ale- jandro Seco-Gonzalez, Daniel Conde-Torres, Paula Antelo-Riveiro, Angel Pineiro, and Rebeca Garcia- Fandino. 2022. The role of ai in drug discovery: Challenges, opportunities, and strategies. arXiv preprint arXiv:2212.08104 . Sebastian Borgeaud, Arthur Mensch, Jordan Hoff- mann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hen- nigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Ge- offrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre. 2021. Improving language models by retriev- ing from trillions of tokens.Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems , 33:1877\u20131901. Sebastian Borgeaud, Arthur Mensch, Jordan Hoff- mann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hen- nigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Ge- offrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre. 2021. Improving language models by retriev- ing from trillions of tokens.Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners.",
    "199": "Advances in neural information processing systems , 33:1877\u20131901. Samuel Cahyawijaya, Holy Lovenia, Alham Fikri Aji, Genta Indra Winata, Bryan Wilie, Rahmad Mahendra, Christian Wibisono, Ade Romadhony, Karissa Vincentio, Fajri Koto, Jennifer Santoso, David Moeljadi, Cahya Wirawan, Frederikus Hudi, Ivan Halim Parmonangan, Ika Al\ufb01na, Muham- mad Satrio Wicaksono, Ilham Firdausi Putra, Sam- sul Rahmadani, Yulianti Oenang, Ali Akbar Septian- dri, James Jaya, Kaustubh D. Dhole, Arie Ardiyanti Suryani, Rifki A\ufb01na Putri, Dan Su, Keith Stevens, Made Nindyatama Nityasya, Muhammad Farid Adilazuarda, Ryan Ignatius, Ryandito Diandaru, Tiezheng Yu, Vito Ghifari, Wenliang Dai, Yan Xu, Dyah Damapuspita, Cuk Tho, Ichwanul Mus- lim Karo Karo, Tirana Noor Fatyanosa, Ziwei Ji, Pascale Fung, Graham Neubig, Timothy Baldwin, Sebastian Ruder, Herry Sujaini, Sakriani Sakti, and Ayu Purwarianti. 2022. Nusacrowd: Open source initiative for indonesian nlp resources. Samuel Cahyawijaya, Genta Indra Winata, Bryan Wilie, Karissa Vincentio, Xiaohong Li, Adhiguna Kuncoro, Sebastian Ruder, Zhi Yuan Lim, Syafri Ba- har, Masayu Khodra, Ayu Purwarianti, and Pascale Fung. 2021. IndoNLG: Benchmark and resources for evaluating Indonesian natural language genera- tion. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 8875\u20138898, Online and Punta Cana, Domini- can Republic. Association for Computational Lin- guistics. Ethan C. Chau and Noah A. Smith. 2021. Specializing multilingual language models: An empirical study. InProceedings of the 1st Workshop on Multilin- gual Representation Learning , pages 51\u201361, Punta Cana, Dominican Republic. Association for Compu- tational Linguistics. Jonathan H Choi, Kristin E Hickman, Amy Monahan, Wilie, Karissa Vincentio, Xiaohong Li, Adhiguna Kuncoro, Sebastian Ruder, Zhi Yuan Lim, Syafri Ba- har, Masayu Khodra, Ayu Purwarianti, and Pascale Fung. 2021. IndoNLG: Benchmark and resources for evaluating Indonesian natural language genera- tion. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 8875\u20138898, Online and Punta Cana, Domini- can Republic. Association for Computational Lin- guistics. Ethan C. Chau and Noah A. Smith. 2021. Specializing multilingual language models: An empirical study. InProceedings of the 1st Workshop on Multilin- gual Representation Learning , pages 51\u201361, Punta Cana, Dominican Republic. Association for Compu- tational Linguistics. Jonathan H Choi, Kristin E Hickman, Amy Monahan, and Daniel Schwarcz. 2023. Chatgpt goes to law school. Available at SSRN . Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vin- odkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghe- mawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fe- dus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankara- narayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Bren- nan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways. Jon Christian. 2023. Amazing \"jailbreak\" bypasses chatgpt\u2019s ethics safeguards. Paul Christiano, Jan Leike, Tom B. Brown, Miljan Mar- tic, Shane Legg, and Dario Amodei. 2017. Deep re- inforcement learning from human preferences. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question an- swering? try arc, the ai2 reasoning challenge. Abigail C Cohn and Maya Ravindranath. 2014. Lo- cal languages in indonesia: Language maintenance Katherine Lee, Zongwei Zhou, Xuezhi Wang, Bren- nan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways. Jon Christian. 2023. Amazing \"jailbreak\" bypasses chatgpt\u2019s ethics safeguards. Paul Christiano, Jan Leike, Tom B. Brown, Miljan Mar- tic, Shane Legg, and Dario Amodei. 2017. Deep re- inforcement learning from human preferences. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question an- swering? try arc, the ai2 reasoning challenge. Abigail C Cohn and Maya Ravindranath. 2014. Lo- cal languages in indonesia: Language maintenance or language shift. Linguistik Indonesia , 32(2):131\u2013 148. Cookup.ai. 2022. Chatgpt - where it lacks. Wenliang Dai, Lu Hou, Lifeng Shang, Xin Jiang, Qun Liu, and Pascale Fung. 2022a. Enabling multimodal generation on CLIP via vision-language knowledge distillation. In Findings of the Association for Computational Linguistics: ACL 2022 , pages 2383\u2013 2395, Dublin, Ireland. Association for Computa- tional Linguistics. Wenliang Dai, Zihan Liu, Ziwei Ji, Dan Su, and Pascale Fung. 2022b. Plausible may not be faithful: Probing object hallucination in vision-language pre-training. ArXiv , abs/2210.07688. Bhavana Dalvi, Peter Jansen, Oyvind Tafjord, Zheng- nan Xie, Hannah Smith, Leighanna Pipatanangkura, and Peter Clark. 2021. Explaining answers with entailment trees. In Proceedings of the 2021 Con- ference on Empirical Methods in Natural Language Processing , pages 7358\u20137370. Ernest Davis. 2023. Mathematics, word problems, common sense, and arti\ufb01cial intelligence. arXiv preprint arXiv:2301.09723 . Web Desk. 2023. Colombian judge uses chatgpt in rul- ing, triggers debate. Igor Douven. 2017. Abduction. Michael Dowling and Brian Lucey. 2023. Chatgpt for (\ufb01nance) research: The bananarama conjecture. Fi- nance Research Letters , page 103662. Li Du, Xiao Ding, Kai Xiong, Ting Liu, and Bing Qin. 2022. e-CARE: a new dataset for exploring ex- plainable causal reasoning. In Proceedings of the 60th Annual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers) , pages 432\u2013446, Dublin, Ireland. Association for Computa- tional Linguistics.Esin Durmus, He He, and Mona Diab. 2020. FEQA: A question answering evaluation framework for faith- fulness assessment in abstractive summarization. In common sense, and arti\ufb01cial intelligence. arXiv preprint arXiv:2301.09723 . Web Desk. 2023. Colombian judge uses chatgpt in rul- ing, triggers debate. Igor Douven. 2017. Abduction. Michael Dowling and Brian Lucey. 2023. Chatgpt for (\ufb01nance) research: The bananarama conjecture. Fi- nance Research Letters , page 103662. Li Du, Xiao Ding, Kai Xiong, Ting Liu, and Bing Qin. 2022. e-CARE: a new dataset for exploring ex- plainable causal reasoning. In Proceedings of the 60th Annual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers) , pages 432\u2013446, Dublin, Ireland. Association for Computa- tional Linguistics.Esin Durmus, He He, and Mona Diab. 2020. FEQA: A question answering evaluation framework for faith- fulness assessment in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Asso- ciation for Computational Linguistics , pages 5055\u2013 5070, Online. Association for Computational Lin- guistics. Nouha Dziri, Andrea Madotto, Osmar Za\u00efane, and Avishek Joey Bose. 2021. Neural path hunter: Re- ducing hallucination in dialogue systems via path grounding. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Process- ing, pages 2197\u20132214, Online and Punta Cana, Do- minican Republic. Association for",
    "200": "Computational Linguistics. David M. Eberhard, Gary F. Simons, and Charles D. Fennig. 2021. Ethnologue: Languages of the World. Twenty-fourth edition. Dallas, Texas: SIL Interna- tional. Simon Frieder, Luca Pinchetti, Ryan-Rhys Grif- \ufb01ths, Tommaso Salvatori, Thomas Lukasiewicz, Philipp Christian Petersen, Alexis Chevalier, and Julius Berner. 2023. Mathematical capabilities of chatgpt. Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPo\ufb01, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2021. A framework for few-shot language model evalua- tion. Aidan Gilson, Conrad Safranek, Thomas Huang, Vimig Socrates, Ling Chi, Richard Andrew Taylor, and David Chartash. 2022. How well does chatgpt do when taking the medical licensing exams? the im- plications of large language models for medical ed- ucation and knowledge assessment. medRxiv , pages 2022\u201312. Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. 2019. Samsum corpus: A human-annotated dialogue dataset for abstractive summarization. EMNLP-IJCNLP 2019 , page 70. chatgpt. Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPo\ufb01, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2021. A framework for few-shot language model evalua- tion. Aidan Gilson, Conrad Safranek, Thomas Huang, Vimig Socrates, Ling Chi, Richard Andrew Taylor, and David Chartash. 2022. How well does chatgpt do when taking the medical licensing exams? the im- plications of large language models for medical ed- ucation and knowledge assessment. medRxiv , pages 2022\u201312. Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. 2019. Samsum corpus: A human-annotated dialogue dataset for abstractive summarization. EMNLP-IJCNLP 2019 , page 70. Yoav Goldberg. 2023. Some remarks on large language models. Cindy Gordon. 2023. Chatgpt is the fastest growing app in the history of web applications. Naman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng- Jen Chen, Guillaume Wenzek, Da Ju, Sanjana Kr- ishnan, Marc\u2019Aurelio Ranzato, Francisco Guzm\u00e1n, and Angela Fan. 2021. The \ufb02ores-101 evaluation benchmark for low-resource and multilingual ma- chine translation. Tanya Goyal, Junyi Jessy Li, and Greg Durrett. 2022. News summarization and evaluation in the era of gpt- 3.arXiv preprint arXiv:2209.12356 . Roberto Gozalo-Brizuela and Eduardo C Garrido- Merchan. 2023. Chatgpt is not all you need. a state of the art review of large generative ai models. arXiv preprint arXiv:2301.04655 . Barbara F Grimes. 2000. Ethnologue . SIL Interna- tional, Dallas, TX. Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding, Jianwei Yue, and Yupeng Wu. 2023. How close is chatgpt to human experts? comparison corpus, evaluation, and detection. arXiv preprint arXiv:2301.07597 . James Hawthorne. 2021. Inductive Logic. In Ed- ward N. Zalta, editor, The Stanford Encyclopedia of Philosophy , Spring 2021 edition. Metaphysics Re- search Lab, Stanford University. Hangfeng He, Hongming Zhang, and Dan Roth. 2023. Rethinking with retrieval: Faithful large language model inference. Karl Moritz Hermann, Tomas Kocisky, Edward Grefen- stette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. In Advances in Neural Informa- tion Processing Systems , volume 28. Curran Asso- ciates, Inc. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Wu. 2023. How close is chatgpt to human experts? comparison corpus, evaluation, and detection. arXiv preprint arXiv:2301.07597 . James Hawthorne. 2021. Inductive Logic. In Ed- ward N. Zalta, editor, The Stanford Encyclopedia of Philosophy , Spring 2021 edition. Metaphysics Re- search Lab, Stanford University. Hangfeng He, Hongming Zhang, and Dan Roth. 2023. Rethinking with retrieval: Faithful large language model inference. Karl Moritz Hermann, Tomas Kocisky, Edward Grefen- stette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. In Advances in Neural Informa- tion Processing Systems , volume 28. Curran Asso- ciates, Inc. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Si- monyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. 2022. Training compute-optimal large language models. Ehsan Hosseini-Asl, Bryan McCann, Chien-Sheng Wu, Semih Yavuz, and Richard Socher. 2020. A sim- ple language model for task-oriented dialogue. Ad- vances in Neural Information Processing Systems , 33:20179\u201320191. Krystal Hu. 2023. Chatgpt sets record for fastest- growing user base - analyst note. Jie Huang and Kevin Chen-Chuan Chang. 2022. To- wards reasoning in large language models: A survey. arXiv preprint arXiv:2212.10403 . Ar\ufb01nda Ilmania, Abdurrahman, Samuel Cahyawijaya, and Ayu Purwarianti. 2018. Aspect detection and sentiment classi\ufb01cation using deep neural network for indonesian aspect-based sentiment analysis. In 2018 International Conference on Asian Language Processing (IALP) , pages 62\u201367. Hadar Yoana Jabotinsky and Roee Sarel. 2022. Co- authoring with an ai? ethical dilemmas and arti\ufb01cial intelligence. Ethical Dilemmas and Arti\ufb01cial Intelli- gence (December 15, 2022) .Katharina Jeblick, Balthasar Schachtner, Jakob Dexl, Andreas Mittermeier, Anna Theresa St\u00fcber, Jo- hanna Topalis, Tobias Weber, Philipp Wesp, Bas- tian Sabel, Jens Ricke, et al. 2022. Chatgpt makes medicine easy to swallow: An exploratory case study on simpli\ufb01ed radiology reports. arXiv preprint arXiv:2212.14882 . Anya Ji, Noriyuki Kojima, Noah Rush, Alane Suhr, Wai Keen V ong, Robert Hawkins, and Yoav Artzi. 2022a. Abstract visual reasoning with tangram for indonesian aspect-based sentiment analysis. In 2018 International Conference on Asian Language Processing (IALP) , pages 62\u201367. Hadar Yoana Jabotinsky and Roee Sarel. 2022. Co- authoring with an ai? ethical dilemmas and arti\ufb01cial intelligence. Ethical Dilemmas and Arti\ufb01cial Intelli- gence (December 15, 2022) .Katharina Jeblick, Balthasar Schachtner, Jakob Dexl, Andreas Mittermeier, Anna Theresa St\u00fcber, Jo- hanna Topalis, Tobias Weber, Philipp Wesp, Bas- tian Sabel, Jens Ricke, et al. 2022. Chatgpt makes medicine easy to swallow: An exploratory case study on simpli\ufb01ed radiology reports. arXiv preprint arXiv:2212.14882 . Anya Ji, Noriyuki Kojima, Noah Rush, Alane Suhr, Wai Keen V ong, Robert Hawkins, and Yoav Artzi. 2022a. Abstract visual reasoning with tangram shapes. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing , pages 582\u2013601, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea Madotto, and Pascale Fung. 2022b. Survey of hallu- cination in natural language generation. ACM Com- put. Surv. Just Accepted. Ziwei Ji, Zihan Liu, Nayeon Lee, Tiezheng Yu, Bryan Wilie, Min Zeng, and Pascale Fung. 2022c. Rho (\u001a): Reducing hallucination in open-domain dia- logues with knowledge grounding. arXiv preprint arXiv:2212.01588 . Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Xing Wang, and Zhaopeng Tu. 2023. Is chatgpt a good translator? a preliminary study. Arianna Johnson. 2023. Is chatgpt partisan? poems about trump and biden raise questions about the ai bot\u2019s bias-here\u2019s what experts think. Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, and Monojit Choudhury. 2020. The state and fate of linguistic diversity and inclusion in the NLP world. In Proceedings of the 58th Annual Meet- ing of",
    "201": "the Association for Computational Linguistics , pages 6282\u20136293, Online. Association for Computa- tional Linguistics. Jennifer A. Kingson. 2023. Friend or foe? teachers debate chatgpt. Escape Velocity Labs. 2022. Chatgpt imitates logical reasoning surprisingly well. Anton E Lawson. 2005. What is the role of induction and deduction in reasoning and scienti\ufb01c inquiry? Journal of Research in Science Teaching , 42(6):716\u2013 740. Nayeon Lee, Yejin Bang, Andrea Madotto, and Pascale Fung. 2021. Towards few-shot fact-checking via per- plexity. In Proceedings of the 2021 Conference of Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, and Monojit Choudhury. 2020. The state and fate of linguistic diversity and inclusion in the NLP world. In Proceedings of the 58th Annual Meet- ing of the Association for Computational Linguistics , pages 6282\u20136293, Online. Association for Computa- tional Linguistics. Jennifer A. Kingson. 2023. Friend or foe? teachers debate chatgpt. Escape Velocity Labs. 2022. Chatgpt imitates logical reasoning surprisingly well. Anton E Lawson. 2005. What is the role of induction and deduction in reasoning and scienti\ufb01c inquiry? Journal of Research in Science Teaching , 42(6):716\u2013 740. Nayeon Lee, Yejin Bang, Andrea Madotto, and Pascale Fung. 2021. Towards few-shot fact-checking via per- plexity. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies , pages 1971\u20131981, Online. Association for Computational Linguistics. Nayeon Lee, Wei Ping, Peng Xu, Mostofa Patwary, Pas- cale Fung, Mohammad Shoeybi, and Bryan Catan- zaro. 2022. Factuality enhanced language models for open-ended text generation. In Advances in Neu- ral Information Processing Systems . M. Paul Lewis, editor. 2009. Ethnologue: Languages of the World , sixteenth edition. SIL International, Dallas, TX, USA. Mike Lewis, Yinhan Liu, Naman Goyal, Mar- jan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020a. BART: Denoising sequence-to-sequence pre- training for natural language generation, translation, and comprehension. In Proceedings of the 58th An- nual Meeting of the Association for Computational Linguistics , pages 7871\u20137880, Online. Association for Computational Linguistics. Mike Lewis, Yinhan Liu, Naman Goyal, Mar- jan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020b. Bart: Denoising sequence-to-sequence pre- training for natural language generation, translation, and comprehension. In Proceedings of the 58th An- nual Meeting of the Association for Computational Linguistics , pages 7871\u20137880. Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Ku- mar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher D. Man- ning, Christopher R\u00e9, Diana Acosta-Navas, Drew A. Hudson, Eric Zelikman, Esin Durmus, Faisal Lad- hak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue for Computational Linguistics. Mike Lewis, Yinhan Liu, Naman Goyal, Mar- jan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020b. Bart: Denoising sequence-to-sequence pre- training for natural language generation, translation, and comprehension. In Proceedings of the 58th An- nual Meeting of the Association for Computational Linguistics , pages 7871\u20137880. Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Ku- mar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher D. Man- ning, Christopher R\u00e9, Diana Acosta-Navas, Drew A. Hudson, Eric Zelikman, Esin Durmus, Faisal Lad- hak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda. 2022. Holistic eval- uation of language models. Opher Lieber, Or Sharir, Barak Lenz, and Yoav Shoham. 2021. Jurassic-1: Technical details and evaluation. White Paper. AI21 Labs . Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. TruthfulQA: Measuring how models mimic human falsehoods. In Proceedings of the 60th Annual Meet- ing of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 3214\u20133252, Dublin, Ireland. Association for Computational Linguistics. Zhaojiang Lin, Bing Liu, Andrea Madotto, Seungwhan Moon, Zhenpeng Zhou, Paul A Crook, Zhiguang Wang, Zhou Yu, Eunjoon Cho, Rajen Subba, et al. 2021. Zero-shot dialogue state tracking via cross- task transfer. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Process- ing, pages 7890\u20137900. Yixin Liu, Pengfei Liu, Dragomir Radev, and Graham Neubig. 2022. Brio: Bringing order to abstrac-tive summarization. In Proceedings of the 60th An- nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 2890\u2013 2903. Holy Lovenia, Bryan Wilie, Romain Barraud, Samuel Cahyawijaya, Willy Chung, and Pascale Fung. 2022. Every picture tells a story: Image-grounded con- trollable stylistic story generation. In Proceedings of the 6th Joint SIGHUM Workshop on Compu- Moon, Zhenpeng Zhou, Paul A Crook, Zhiguang Wang, Zhou Yu, Eunjoon Cho, Rajen Subba, et al. 2021. Zero-shot dialogue state tracking via cross- task transfer. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Process- ing, pages 7890\u20137900. Yixin Liu, Pengfei Liu, Dragomir Radev, and Graham Neubig. 2022. Brio: Bringing order to abstrac-tive summarization. In Proceedings of the 60th An- nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 2890\u2013 2903. Holy Lovenia, Bryan Wilie, Romain Barraud, Samuel Cahyawijaya, Willy Chung, and Pascale Fung. 2022. Every picture tells a story: Image-grounded con- trollable stylistic story generation. In Proceedings of the 6th Joint SIGHUM Workshop on Compu- tational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature , pages 40\u201352, Gyeongju, Republic of Korea. International Confer- ence on Computational Linguistics. Hongyuan Lu, Haoyang Huang, Shuming Ma, Dong- dong Zhang, Wai Lam, and Furu Wei. 2022. Trip: Triangular document-level pre-training for multilingual language models. arXiv preprint arXiv:2212.07752 . Andrea Madotto, Zhaojiang Lin, Genta Indra Winata, and Pascale Fung. 2021. Few-shot bot: Prompt- based learning for dialogue systems. arXiv preprint arXiv:2110.08118 . Kyle Mahowald, Anna A Ivanova, Idan A Blank, Nancy Kanwisher, Joshua B Tenenbaum, and Evelina Fedorenko. 2023. Dissociating language and thought in large language models: a cognitive perspective. arXiv preprint arXiv:2301.06627 . Bernard Marr. 2022. What does chatgpt really mean for businesses? Vaibhav Mavi, Anubhav Jangra, and Adam Jatowt. 2022. A survey on multi-hop question answering and generation. arXiv preprint arXiv:2204.09140 . Pasquale Minervini, Sebastian Riedel, Pontus Stene- torp, Edward Grefenstette, and Tim Rockt\u00e4schel. 2020. Learning reasoning strategies in end-to- end differentiable proving. In Proceedings of the 37th International Conference on Machine Learning , ICML\u201920. JMLR.org. Roshanak Mirzaee and Parisa Kordjamshidi. 2022. Transfer learning with synthetic corpora for spatial role labeling and reasoning. In Proceedings of the 2022 Conference on Empirical Methods in Natu- ral Language Processing , pages 6148\u20136165, Abu Dhabi, United Arab Emirates. Association for Com- putational Linguistics. Roshanak Mirzaee, Hossein Rajaby Faghihi, Qiang Ning, and Parisa Kordjamshidi. 2021. SPARTQA: A textual question",
    "202": "answering benchmark for spatial and generation. arXiv preprint arXiv:2204.09140 . Pasquale Minervini, Sebastian Riedel, Pontus Stene- torp, Edward Grefenstette, and Tim Rockt\u00e4schel. 2020. Learning reasoning strategies in end-to- end differentiable proving. In Proceedings of the 37th International Conference on Machine Learning , ICML\u201920. JMLR.org. Roshanak Mirzaee and Parisa Kordjamshidi. 2022. Transfer learning with synthetic corpora for spatial role labeling and reasoning. In Proceedings of the 2022 Conference on Empirical Methods in Natu- ral Language Processing , pages 6148\u20136165, Abu Dhabi, United Arab Emirates. Association for Com- putational Linguistics. Roshanak Mirzaee, Hossein Rajaby Faghihi, Qiang Ning, and Parisa Kordjamshidi. 2021. SPARTQA: A textual question answering benchmark for spatial reasoning. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies , pages 4582\u20134598, Online. Association for Computational Linguistics. Seungwhan Moon, Pararth Shah, Anuj Kumar, and Ra- jen Subba. 2019. Opendialkg: Explainable conver- sational reasoning with attention-based walks over knowledge graphs. In Proceedings of the 57th An- nual Meeting of the Association for Computational Linguistics , pages 845\u2013854. Nasrin Mostafazadeh, Chris Brockett, William B Dolan, Michel Galley, Jianfeng Gao, Georgios Sp- ithourakis, and Lucy Vanderwende. 2017. Image- grounded conversations: Multimodal context for nat- ural question and response generation. In Proceed- ings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Pa- pers) , pages 462\u2013472. Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hai- ley Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Al- banie, Zaid Alyafeai, Albert Webson, Edward Raff, and Colin Raffel. 2022. Crosslingual generaliza- tion through multitask \ufb01netuning. arXiv preprint arXiv:2211.01786 . Benjamin Muller, Antonios Anastasopoulos, Beno\u00eet Sagot, and Djam\u00e9 Seddah. 2021. When being un- seen from mBERT is just the beginning: Handling new languages with multilingual language models. InProceedings of the 2021 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies , pages 448\u2013462, Online. Association for Computa- tional Linguistics. Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hai- ley Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Al- banie, Zaid Alyafeai, Albert Webson, Edward Raff, and Colin Raffel. 2022. Crosslingual generaliza- tion through multitask \ufb01netuning. arXiv preprint arXiv:2211.01786 . Benjamin Muller, Antonios Anastasopoulos, Beno\u00eet Sagot, and Djam\u00e9 Seddah. 2021. When being un- seen from mBERT is just the beginning: Handling new languages with multilingual language models. InProceedings of the 2021 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies , pages 448\u2013462, Online. Association for Computa- tional Linguistics. Ramesh Nallapati, Bowen Zhou, Cicero dos Santos, \u00c7a\u02d8glar Gul\u00e7ehre, and Bing Xiang. 2016. Abstrac- tive text summarization using sequence-to-sequence RNNs and beyond. In Proceedings of the 20th SIGNLL Conference on Computational Natural Lan- guage Learning , pages 280\u2013290, Berlin, Germany. Association for Computational Linguistics. Tom\u00e1\u0161 Nekvinda and Ond \u02c7rej Du\u0161ek. 2021. Shades of BLEU, \ufb02avours of success: The case of MultiWOZ. InProceedings of the 1st Workshop on Natural Lan- guage Generation, Evaluation, and Metrics (GEM 2021) , pages 34\u201346, Online. Association for Com- putational Linguistics. NeuralMagic. 2023. The chatgpt cheat sheet. Oded Nov, Nina Singh, and Devin M Mann. 2023. Putting chatgpt\u2019s medical advice to the (turing) test. medRxiv , pages 2023\u201301. Jeroen Ooms. 2023. cld2: Google\u2019s Compact Lan- guage Detector 2 . Https://docs.ropensci.org/cld2/ (docs) https://github.com/ropensci/cld2 (devel) https://github.com/cld2owners/cld2 (upstream). Simon Ott, Konstantin Hebenstreit, Valentin Li\u00e9vin, Christoffer Egeberg Hother, Milad Moradi, Maxim- ilian Mayrhauser, Robert Praas, Ole Winther, and Matthias Samwald. 2023. Thoughtsource: A central hub for large language model reasoning data. arXiv preprint arXiv:2301.11596 .Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Pe- ter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow in- structions with human feedback. Baolin Peng, Chunyuan Li, Jinchao Li, Shahin Shayan- deh, Lars Liden, and Jianfeng Gao. 2021. Soloist: https://github.com/cld2owners/cld2 (upstream). Simon Ott, Konstantin Hebenstreit, Valentin Li\u00e9vin, Christoffer Egeberg Hother, Milad Moradi, Maxim- ilian Mayrhauser, Robert Praas, Ole Winther, and Matthias Samwald. 2023. Thoughtsource: A central hub for large language model reasoning data. arXiv preprint arXiv:2301.11596 .Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Pe- ter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow in- structions with human feedback. Baolin Peng, Chunyuan Li, Jinchao Li, Shahin Shayan- deh, Lars Liden, and Jianfeng Gao. 2021. Soloist: Building task bots at scale with transfer learning and machine teaching. Transactions of the Association for Computational Linguistics , 9:807\u2013824. Jonas Pfeiffer, Ivan Vuli \u00b4c, Iryna Gurevych, and Sebas- tian Ruder. 2021. UNKs everywhere: Adapting mul- tilingual language models to new scripts. In Pro- ceedings of the 2021 Conference on Empirical Meth- ods in Natural Language Processing , pages 10186\u2013 10203, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Maja Popovi \u00b4c. 2015. chrF: character n-gram F-score for automatic MT evaluation. In Proceedings of the Tenth Workshop on Statistical Machine Translation , pages 392\u2013395, Lisbon, Portugal. Association for Computational Linguistics. Ian Porada, Kaheer Suleman, Adam Trischler, and Jackie Chi Kit Cheung. 2021. Modeling event plau- sibility with consistent conceptual abstraction. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies , pages 1732\u20131743, Online. Association for Compu- tational Linguistics. Matt Post. 2018. A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers , pages 186\u2013 191, Brussels, Belgium. Association for Computa- tional Linguistics. Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, and Huajun Chen. 2022. Reasoning with lan- guage model prompting: A survey. arXiv preprint arXiv:2212.09597 . Lianhui Qin, Aditya Gupta, Shyam Upadhyay, Luheng He, Yejin Choi, and Manaal Faruqui. 2021. TIME- DIAL: Temporal commonsense reasoning in dialog. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies , pages 1732\u20131743, Online. Association for Compu- tational Linguistics. Matt Post. 2018. A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers , pages 186\u2013 191, Brussels, Belgium. Association for Computa- tional Linguistics. Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, and Huajun",
    "203": "Chen. 2022. Reasoning with lan- guage model prompting: A survey. arXiv preprint arXiv:2212.09597 . Lianhui Qin, Aditya Gupta, Shyam Upadhyay, Luheng He, Yejin Choi, and Manaal Faruqui. 2021. TIME- DIAL: Temporal commonsense reasoning in dialog. InProceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Lan- guage Processing (Volume 1: Long Papers) , pages 7066\u20137076, Online. Association for Computational Linguistics. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning , pages 8748\u20138763. PMLR. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Lan- guage models are unsupervised multitask learners. OpenAI blog , 1(8):9. Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susan- nah Young, Eliza Rutherford, Tom Hennigan, Ja- cob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Mari- beth Rauh, Po-Sen Huang, Amelia Glaese, Jo- hannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, An- tonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Ne- matzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cy- prien de Masson d\u2019Autume, Yujia Li, Tayfun Terzi, nah Young, Eliza Rutherford, Tom Hennigan, Ja- cob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Mari- beth Rauh, Po-Sen Huang, Amelia Glaese, Jo- hannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, An- tonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Ne- matzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cy- prien de Masson d\u2019Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hecht- man, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stan- way, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. 2021a. Scaling language models: Methods, analysis and insights from training gopher. Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susan- nah Young, et al. 2021b. Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446 . Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2022. Exploring the limits of transfer learning with a uni\ufb01ed text-to-text trans- former. J. Mach. Learn. Res. , 21(1). Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea V oss, Alec Radford, Mark Chen, and Ilya Sutskever. 2021. Zero-shot text-to-image gen- eration. In International Conference on Machine Learning , pages 8821\u20138831. PMLR. Fabin Rasheed. 2020. Gpt3 sees. Anna Rogers, Matt Gardner, and Isabelle Augenstein. 2022. Qa dataset explosion: A taxonomy of nlp re- sources for question answering and reading compre- hension. ACM Comput. Surv. Just Accepted.Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. 2022. High- resolution image synthesis with latent diffusion mod- els. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 10684\u201310695. former. J. Mach. Learn. Res. , 21(1). Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea V oss, Alec Radford, Mark Chen, and Ilya Sutskever. 2021. Zero-shot text-to-image gen- eration. In International Conference on Machine Learning , pages 8821\u20138831. PMLR. Fabin Rasheed. 2020. Gpt3 sees. Anna Rogers, Matt Gardner, and Isabelle Augenstein. 2022. Qa dataset explosion: A taxonomy of nlp re- sources for question answering and reading compre- hension. ACM Comput. Surv. Just Accepted.Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. 2022. High- resolution image synthesis with latent diffusion mod- els. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 10684\u201310695. David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. 2019. Analysing mathematical rea- soning abilities of neural models. In International Conference on Learning Representations . J Schulman, B Zoph, C Kim, J Hilton, J Menick, J Weng, JFC Uribe, L Fedus, L Metz, M Pokorny, et al. 2022. Chatgpt: Optimizing language models for dialogue. Stephen Shankland. 2023. Why the chatgpt ai chatbot is blowing everyone\u2019s mind. Yiqiu Shen, Laura Heacock, Jonathan Elias, Keith D Hentel, Beatriu Reig, George Shih, and Linda Moy. 2023. Chatgpt and other large language models are double-edged swords. Zhengxiang Shi, Qiang Zhang, and Aldo Lipani. 2022a. Stepgame: A new benchmark for robust multi-hop spatial reasoning in texts. Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence , 36(10):11321\u2013 11329. Zhengxiang Shi, Qiang Zhang, and Aldo Lipani. 2022b. StepGame: A new benchmark for robust multi-hop spatial reasoning in texts. Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence , 36(10):11321\u2013 11329. Denis Shiryaev. 2022. Drawing mona lisa with chatgpt. Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catan- zaro. 2019. Megatron-lm: Training multi-billion pa- rameter language models using model parallelism. arXiv preprint arXiv:1909.08053 . Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen Roller, Megan Ung, Moya Chen, Kushal Arora, Joshua Lane, Morteza Behrooz, William Ngan, Spencer Poff, Naman Goyal, Arthur Szlam, Y-Lan Boureau, Melanie Kam- badur, and Jason Weston. 2022. Blenderbot 3: a de- ployed conversational agent that continually learns to responsibly engage. Koustuv Sinha, Shagun Sodhani, Jin Dong, Joelle Conference on Arti\ufb01cial Intelligence , 36(10):11321\u2013 11329. Denis Shiryaev. 2022. Drawing mona lisa with chatgpt. Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catan- zaro. 2019. Megatron-lm: Training multi-billion pa- rameter language models using model parallelism. arXiv preprint arXiv:1909.08053 . Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen Roller, Megan Ung, Moya Chen, Kushal Arora, Joshua Lane, Morteza Behrooz, William Ngan, Spencer Poff, Naman Goyal, Arthur Szlam, Y-Lan Boureau, Melanie Kam- badur, and Jason Weston. 2022. Blenderbot 3: a de- ployed conversational agent that continually learns to responsibly engage. Koustuv Sinha, Shagun Sodhani, Jin Dong, Joelle Pineau, and William L Hamilton. 2019. Clutrr: A diagnostic benchmark for inductive reasoning from text. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu- ral Language Processing (EMNLP-IJCNLP) , pages 4506\u20134515. Noah Smith. 2023. Why does chatgpt constantly lie?",
    "204": "Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adri\u00e0 Garriga-Alonso, et al. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615 . Shane Storks, Qiaozi Gao, and Joyce Y Chai. 2019. Commonsense reasoning for natural language un- derstanding: A survey of benchmarks, resources, and approaches. arXiv preprint arXiv:1904.01172 , pages 1\u201360. Dan Su, Xiaoguang Li, Jindi Zhang, Lifeng Shang, Xin Jiang, Qun Liu, and Pascale Fung. 2022. Read be- fore generate! faithful long form question answer- ing with machine reading. In Findings of the As- sociation for Computational Linguistics: ACL 2022 , pages 744\u2013756. Dan Su, Tiezheng Yu, and Pascale Fung. 2021. Im- prove query focused abstractive summarization by incorporating answer relevance. In Findings of the Association for Computational Linguistics: ACL- IJCNLP 2021 , pages 3124\u20133131. Qingfeng Sun, Yujing Wang, Can Xu, Kai Zheng, Yam- ing Yang, Huang Hu, Fei Xu, Jessica Zhang, Xiubo Geng, and Daxin Jiang. 2022. Multimodal dialogue response generation. In Proceedings of the 60th An- nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 2854\u2013 2866. Teo Susnjak. 2022. Chatgpt: The end of online exam fore generate! faithful long form question answer- ing with machine reading. In Findings of the As- sociation for Computational Linguistics: ACL 2022 , pages 744\u2013756. Dan Su, Tiezheng Yu, and Pascale Fung. 2021. Im- prove query focused abstractive summarization by incorporating answer relevance. In Findings of the Association for Computational Linguistics: ACL- IJCNLP 2021 , pages 3124\u20133131. Qingfeng Sun, Yujing Wang, Can Xu, Kai Zheng, Yam- ing Yang, Huang Hu, Fei Xu, Jessica Zhang, Xiubo Geng, and Daxin Jiang. 2022. Multimodal dialogue response generation. In Proceedings of the 60th An- nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 2854\u2013 2866. Teo Susnjak. 2022. Chatgpt: The end of online exam integrity? arXiv preprint arXiv:2212.09292 . Alon Talmor, Yanai Elazar, Yoav Goldberg, and Jonathan Berant. 2020. olmpics-on what language model pre-training captures. Transactions of the As- sociation for Computational Linguistics , 8:743\u2013758. Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2018. Commonsenseqa: A ques- tion answering challenge targeting commonsense knowledge. arXiv preprint arXiv:1811.00937 . NLLB Team, Marta R. Costa-juss\u00e0, James Cross, Onur \u00c7elebi, Maha Elbayad, Kenneth Hea\ufb01eld, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jar- rett, Kaushik Ram Sadagopan, Dirk Rowe, Shan- non Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzm\u00e1n, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Sa\ufb01yyah Saleem, Holger Schwenk, and Jeff Wang. 2022. No language left behind: Scaling human-centered machine translation. Richmond Thomason. 2018. Logic and arti\ufb01cial intel- ligence.Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. 2022. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239 . H Holden Thorp. 2023. Chatgpt is fun, but not an au- thor. Giuseppe Venuto. 2023. Giuven95/chatgpt-failures: Chatgpt failure archive. Douglas Walton. 2014. Abductive reasoning . Univer- sity of Alabama Press. Ada Wan. 2022. Fairness in representation for multi- lingual NLP: Insights from controlled experiments Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Sa\ufb01yyah Saleem, Holger Schwenk, and Jeff Wang. 2022. No language left behind: Scaling human-centered machine translation. Richmond Thomason. 2018. Logic and arti\ufb01cial intel- ligence.Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. 2022. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239 . H Holden Thorp. 2023. Chatgpt is fun, but not an au- thor. Giuseppe Venuto. 2023. Giuven95/chatgpt-failures: Chatgpt failure archive. Douglas Walton. 2014. Abductive reasoning . Univer- sity of Alabama Press. Ada Wan. 2022. Fairness in representation for multi- lingual NLP: Insights from controlled experiments on conditional language modeling. In International Conference on Learning Representations . Alex Wang, Amanpreet Singh, Julian Michael, Fe- lix Hill, Omer Levy, and Samuel Bowman. 2018a. GLUE: A multi-task benchmark and analysis plat- form for natural language understanding. In Pro- ceedings of the 2018 EMNLP Workshop Black- boxNLP: Analyzing and Interpreting Neural Net- works for NLP , pages 353\u2013355, Brussels, Belgium. Association for Computational Linguistics. Su Wang, Greg Durrett, and Katrin Erk. 2018b. Mod- eling semantic plausibility by injecting world knowl- edge. arXiv preprint arXiv:1804.00619 . Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171 . Peter Cathcart Wason and Philip Nicholas Johnson- Laird. 1972. Psychology of reasoning: Structure and content , volume 86. Harvard University Press. Taylor Webb, Keith J. Holyoak, and Hongjing Lu. 2022a. Emergent analogical reasoning in large lan- guage models. Taylor Webb, Keith J Holyoak, and Hongjing Lu. 2022b. Emergent analogical reasoning in large lan- guage models. arXiv preprint arXiv:2212.09196 . Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. 2022a. Emergent abilities of large language models. Transactions on Machine Learning Research . Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed H Chi, Quoc V Le, Denny Zhou, et al. 2022b. Chain-of-thought prompting elicits rea- soning in large language models. In Advances in Neural Information Processing Systems . and content , volume 86. Harvard University Press. Taylor Webb, Keith J. Holyoak, and Hongjing Lu. 2022a. Emergent analogical reasoning in large lan- guage models. Taylor Webb, Keith J Holyoak, and Hongjing Lu. 2022b. Emergent analogical reasoning in large lan- guage models. arXiv preprint arXiv:2212.09196 . Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. 2022a. Emergent abilities of large language models. Transactions on Machine Learning Research . Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed H Chi, Quoc V Le, Denny Zhou, et al. 2022b. Chain-of-thought prompting elicits rea- soning in large language models. In Advances in Neural Information Processing Systems . Jason Weston, Antoine Bordes, Sumit Chopra, and Tom\u00e1s Mikolov. 2016a. Towards ai-complete ques- tion answering: A set of prerequisite toy tasks. In 4th International Conference on Learning Represen- tations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings . Jason Weston, Antoine Bordes, Sumit Chopra, Alexan- der M Rush, Bart Van Merri\u00ebnboer, Armand Joulin, and Tomas Mikolov. 2016b. Towards ai-complete question answering: A set of prerequisite toy tasks. In4th International Conference on Learning Repre- sentations, ICLR 2016 . Bryan Wilie, Karissa Vincentio, Genta Indra Winata, Samuel Cahyawijaya, Xiaohong Li, Zhi Yuan Lim, Sidik Soleman, Rahmad Mahendra, Pascale Fung, Syafri Bahar, et al. 2020. Indonlu: Benchmark and resources for evaluating indonesian natural language understanding. In Proceedings of the 1st Confer- ence of the",
    "205": "Asia-Paci\ufb01c Chapter of the Association for Computational Linguistics and the 10th Interna- tional Joint Conference on Natural Language Pro- cessing , pages 843\u2013857. Genta Indra Winata, Alham Fikri Aji, Samuel Cahyaw- ijaya, Rahmad Mahendra, Fajri Koto, Ade Ro- madhony, Kemal Kurniawan, David Moeljadi, Ra- dityo Eko Prasojo, Pascale Fung, Timothy Bald- win, Jey Han Lau, Rico Sennrich, and Sebastian Ruder. 2022. Nusax: Multilingual parallel senti- ment dataset for 10 indonesian local languages. Cameron R. Wolfe. 2023. Specialized llms: Chatgpt, lamda, galactica, codex, sparrow, and more. BigScience Workshop, :, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili \u00b4c, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luc- cioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, Jonathan for Computational Linguistics and the 10th Interna- tional Joint Conference on Natural Language Pro- cessing , pages 843\u2013857. Genta Indra Winata, Alham Fikri Aji, Samuel Cahyaw- ijaya, Rahmad Mahendra, Fajri Koto, Ade Ro- madhony, Kemal Kurniawan, David Moeljadi, Ra- dityo Eko Prasojo, Pascale Fung, Timothy Bald- win, Jey Han Lau, Rico Sennrich, and Sebastian Ruder. 2022. Nusax: Multilingual parallel senti- ment dataset for 10 indonesian local languages. Cameron R. Wolfe. 2023. Specialized llms: Chatgpt, lamda, galactica, codex, sparrow, and more. BigScience Workshop, :, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili \u00b4c, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luc- cioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Beno\u00eet Sagot, Niklas Muennighoff, Al- bert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Sam- son Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Lauren\u00e7on, Yacine Jernite, Julien Launay, Mar- garet Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Al- fassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David Ife- oluwa Adelani, Dragomir Radev, Eduardo Gonz\u00e1lez Ponferrada, Efrat Levkovizh, Ethan Kim, Eyal Bar Natan, Francesco De Toni, G\u00e9rard Dupont, Germ\u00e1n Kruszewski, Giada Pistilli, Hady Elsahar, Hamza Benyamina, Hieu Tran, Ian Yu, Idris Abdulmumin, Isaac Johnson, Itziar Gonzalez-Dios, Javier de la Rosa, Jenny Chim, Jesse Dodge, Jian Zhu, Jonathan Chang, J\u00f6rg Frohberg, Joseph Tobing, Joydeep Bhat- tacharjee, Khalid Almubarak, Kimbo Chen, Kyle Lo, Leandro V on Werra, Leon Weber, Long Phan, Loubna Ben allal, Ludovic Tanguy, Manan Dey, Manuel Romero Mu\u00f1oz, Maraim Masoud, Mar\u00eda Grandury, Mario \u0160a\u0161ko, Max Huang, Maximin Coavoux, Mayank Singh, Mike Tian-Jian Jiang, Minh Chien Vu, Mohammad A. Jauhar, Mustafa Ghaleb, Nishant Subramani, Nora Kassner, Nuru- laqilla Khamis, Olivier Nguyen, Omar Espejel, Ona de Gibert, Paulo Villegas, Peter Henderson, PierreColombo, Priscilla Amuok, Quentin Lhoest, Rheza Harliman, Rishi Bommasani, Roberto Luis L\u00f3pez, Rui Ribeiro, Salomey Osei, Sampo Pyysalo, Se- bastian Nagel, Shamik Bose, Shamsuddeen Has- san Muhammad, Shanya Sharma, Shayne Long- pre, Somaieh Nikpoor, Stanislav Silberberg, Suhas Chang, J\u00f6rg Frohberg, Joseph Tobing, Joydeep Bhat- tacharjee, Khalid Almubarak, Kimbo Chen, Kyle Lo, Leandro V on Werra, Leon Weber, Long Phan, Loubna Ben allal, Ludovic Tanguy, Manan Dey, Manuel Romero Mu\u00f1oz, Maraim Masoud, Mar\u00eda Grandury, Mario \u0160a\u0161ko, Max Huang, Maximin Coavoux, Mayank Singh, Mike Tian-Jian Jiang, Minh Chien Vu, Mohammad A. Jauhar, Mustafa Ghaleb, Nishant Subramani, Nora Kassner, Nuru- laqilla Khamis, Olivier Nguyen, Omar Espejel, Ona de Gibert, Paulo Villegas, Peter Henderson, PierreColombo, Priscilla Amuok, Quentin Lhoest, Rheza Harliman, Rishi Bommasani, Roberto Luis L\u00f3pez, Rui Ribeiro, Salomey Osei, Sampo Pyysalo, Se- bastian Nagel, Shamik Bose, Shamsuddeen Has- san Muhammad, Shanya Sharma, Shayne Long- pre, Somaieh Nikpoor, Stanislav Silberberg, Suhas Pai, Sydney Zink, Tiago Timponi Torrent, Timo Schick, Tristan Thrush, Valentin Danchev, Vas- silina Nikoulina, Veronika Laippala, Violette Lep- ercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak Ta- lat, Arun Raja, Benjamin Heinzerling, Chenglei Si, Davut Emre Ta\u00b8 sar, Elizabeth Salesky, Sabrina J. Mielke, Wilson Y . Lee, Abheesht Sharma, Andrea Santilli, Antoine Chaf\ufb01n, Arnaud Stiegler, Deba- jyoti Datta, Eliza Szczechla, Gunjan Chhablani, Han Wang, Harshit Pandey, Hendrik Strobelt, Ja- son Alan Fries, Jos Rozen, Leo Gao, Lintang Sutawika, M Saiful Bari, Maged S. Al-shaibani, Matteo Manica, Nihal Nayak, Ryan Teehan, Samuel Albanie, Sheng Shen, Srulik Ben-David, Stephen H. Bach, Taewoon Kim, Tali Bers, Thibault Fevry, Tr- ishala Neeraj, Urmish Thakker, Vikas Raunak, Xi- angru Tang, Zheng-Xin Yong, Zhiqing Sun, Shaked Brody, Yallow Uri, Hadar Tojarieh, Adam Roberts, Hyung Won Chung, Jaesung Tae, Jason Phang, O\ufb01r Press, Conglong Li, Deepak Narayanan, Ha- tim Bourfoune, Jared Casper, Jeff Rasley, Max Ryabinin, Mayank Mishra, Minjia Zhang, Mo- hammad Shoeybi, Myriam Peyrounette, Nicolas Patry, Nouamane Tazi, Omar Sanseviero, Patrick von Platen, Pierre Cornette, Pierre Fran\u00e7ois Laval- l\u00e9e, R\u00e9mi Lacroix, Samyam Rajbhandari, Sanchit Gandhi, Shaden Smith, St\u00e9phane Requena, Suraj Patil, Tim Dettmers, Ahmed Baruwa, Amanpreet Singh, Anastasia Cheveleva, Anne-Laure Ligozat, Arjun Subramonian, Aur\u00e9lie N\u00e9v\u00e9ol, Charles Lover- ing, Dan Garrette, Deepak Tunuguntla, Ehud Reiter, Ekaterina Taktasheva, Ekaterina V oloshina, Eli Bog- danov, Genta Indra Winata, Hailey Schoelkopf, Jan- Christoph Kalo, Jekaterina Novikova, Jessica Zosa Forde, Jordan Clive, Jungo Kasai, Ken Kawamura, O\ufb01r Press, Conglong Li, Deepak Narayanan, Ha- tim Bourfoune, Jared Casper, Jeff Rasley, Max Ryabinin, Mayank Mishra, Minjia Zhang, Mo- hammad Shoeybi, Myriam Peyrounette, Nicolas Patry, Nouamane Tazi, Omar Sanseviero, Patrick von Platen, Pierre Cornette, Pierre Fran\u00e7ois Laval- l\u00e9e, R\u00e9mi Lacroix, Samyam Rajbhandari, Sanchit Gandhi, Shaden Smith, St\u00e9phane Requena, Suraj Patil, Tim Dettmers, Ahmed Baruwa, Amanpreet Singh, Anastasia Cheveleva, Anne-Laure Ligozat, Arjun Subramonian, Aur\u00e9lie N\u00e9v\u00e9ol, Charles Lover- ing, Dan Garrette, Deepak Tunuguntla, Ehud Reiter, Ekaterina Taktasheva, Ekaterina V oloshina, Eli Bog- danov, Genta Indra Winata, Hailey Schoelkopf, Jan- Christoph Kalo, Jekaterina Novikova, Jessica Zosa Forde, Jordan Clive, Jungo Kasai, Ken Kawamura, Liam Hazan, Marine Carpuat, Miruna Clinciu, Na- joung Kim, Newton Cheng, Oleg Serikov, Omer Antverg, Oskar van der Wal, Rui Zhang, Ruochen Zhang, Sebastian Gehrmann, Shachar Mirkin, Shani Pais, Tatiana Shavrina, Thomas Scialom, Tian Yun, Tomasz Limisiewicz, Verena Rieser, Vitaly Protasov, Vladislav Mikhailov, Yada Pruksachatkun, Yonatan Belinkov, Zachary Bamberger, Zden \u02c7ek Kasner, Al- ice Rueda, Amanda Pestana, Amir Feizpour, Am- mar Khan, Amy Faranak, Ana Santos, Anthony Hevia, Antigona Unldreaj, Arash Aghagol, Arezoo Abdollahi, Aycha Tammour, Azadeh HajiHosseini, Bahareh Behroozi, Benjamin Ajibade, Bharat Sax- ena, Carlos Mu\u00f1oz Ferrandis, Danish Contractor, David Lansky, Davis David, Douwe Kiela, Duong A. Nguyen, Edward Tan, Emi Baylor, Ezinwanne Ozoani, Fatima Mirza, Frankline Ononiwu, Habib Rezanejad, Hessie Jones, Indrani Bhattacharya, Irene Solaiman, Irina Sedenko, Isar Nejadgholi, Jesse Passmore, Josh Seltzer, Julio Bonis Sanz, Livia Dutra, Mairon Samagaio, Maraim Elbadri, Margot Mieskes, Marissa Gerchick, Martha Akin- lolu, Michael McKenna, Mike Qiu, Muhammed Ghauri, Mykola Burynok, Na\ufb01s Abrar, Nazneen Ra- jani, Nour Elkott, Nour Fahmy, Olanrewaju Samuel, Ran An, Rasmus Kromann, Ryan Hao, Samira Al- izadeh, Sarmad Shubber, Silas Wang, Sourav Roy, Sylvain Viguier, Thanh Le, Tobi Oyebade, Trieu Le, Yoyo Yang, Zach Nguyen, Abhinav Ramesh Kashyap, Alfredo Palasciano, Alison Callahan, An- ima Shukla, Antonio Miranda-Escalada, Ayush Singh, Benjamin",
    "206": "Beilharz, Bo Wang, Caio Brito, Chenxi Zhou, Chirag Jain, Chuxin Xu, Cl\u00e9men- tine Fourrier, Daniel Le\u00f3n Peri\u00f1\u00e1n, Daniel Molano, Dian Yu, Enrique Manjavacas, Fabio Barth, Flo- rian Fuhrimann, Gabriel Altay, Giyaseddin Bayrak, Jesse Passmore, Josh Seltzer, Julio Bonis Sanz, Livia Dutra, Mairon Samagaio, Maraim Elbadri, Margot Mieskes, Marissa Gerchick, Martha Akin- lolu, Michael McKenna, Mike Qiu, Muhammed Ghauri, Mykola Burynok, Na\ufb01s Abrar, Nazneen Ra- jani, Nour Elkott, Nour Fahmy, Olanrewaju Samuel, Ran An, Rasmus Kromann, Ryan Hao, Samira Al- izadeh, Sarmad Shubber, Silas Wang, Sourav Roy, Sylvain Viguier, Thanh Le, Tobi Oyebade, Trieu Le, Yoyo Yang, Zach Nguyen, Abhinav Ramesh Kashyap, Alfredo Palasciano, Alison Callahan, An- ima Shukla, Antonio Miranda-Escalada, Ayush Singh, Benjamin Beilharz, Bo Wang, Caio Brito, Chenxi Zhou, Chirag Jain, Chuxin Xu, Cl\u00e9men- tine Fourrier, Daniel Le\u00f3n Peri\u00f1\u00e1n, Daniel Molano, Dian Yu, Enrique Manjavacas, Fabio Barth, Flo- rian Fuhrimann, Gabriel Altay, Giyaseddin Bayrak, Gully Burns, Helena U. Vrabec, Imane Bello, Ishani Dash, Jihyun Kang, John Giorgi, Jonas Golde, Jose David Posada, Karthik Rangasai Sivara- man, Lokesh Bulchandani, Lu Liu, Luisa Shinzato, Madeleine Hahn de Bykhovetz, Maiko Takeuchi, Marc P\u00e0mies, Maria A Castillo, Marianna Nezhu- rina, Mario S\u00e4nger, Matthias Samwald, Michael Cullan, Michael Weinberg, Michiel De Wolf, Mina Mihaljcic, Minna Liu, Moritz Freidank, Myung- sun Kang, Natasha Seelam, Nathan Dahlberg, Nicholas Michio Broad, Nikolaus Muellner, Pascale Fung, Patrick Haller, Ramya Chandrasekhar, Renata Eisenberg, Robert Martin, Rodrigo Canalli, Ros- aline Su, Ruisi Su, Samuel Cahyawijaya, Samuele Garda, Shlok S Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Simon Ott, Sinee Sang-aroonsiri, Sr- ishti Kumar, Stefan Schweter, Sushil Bharati, Tan- may Laud, Th\u00e9o Gigant, Tomoya Kainuma, Woj- ciech Kusa, Yanis Labrak, Yash Shailesh Bajaj, Yash Venkatraman, Yifan Xu, Yingxin Xu, Yu Xu, Zhe Tan, Zhongli Xie, Zifan Ye, Mathilde Bras, Younes Belkada, and Thomas Wolf. 2022. Bloom: A 176b-parameter open-access multilingual language model. Yan Xu, Etsuko Ishii, Samuel Cahyawijaya, Zi- han Liu, Genta Indra Winata, Andrea Madotto, Dan Su, and Pascale Fung. 2022. Retrieval-free knowledge-grounded dialogue response generation with adapters. In Proceedings of the Second Di- alDoc Workshop on Document-grounded Dialogue and Conversational Question Answering , pages 93\u2013 107. Yunyi Yang, Yunhao Li, and Xiaojun Quan. 2021. Ubar: Towards fully end-to-end task-oriented dia- log system with gpt-2. Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence , 35(16):14230\u2013 14238. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, Venkatraman, Yifan Xu, Yingxin Xu, Yu Xu, Zhe Tan, Zhongli Xie, Zifan Ye, Mathilde Bras, Younes Belkada, and Thomas Wolf. 2022. Bloom: A 176b-parameter open-access multilingual language model. Yan Xu, Etsuko Ishii, Samuel Cahyawijaya, Zi- han Liu, Genta Indra Winata, Andrea Madotto, Dan Su, and Pascale Fung. 2022. Retrieval-free knowledge-grounded dialogue response generation with adapters. In Proceedings of the Second Di- alDoc Workshop on Document-grounded Dialogue and Conversational Question Answering , pages 93\u2013 107. Yunyi Yang, Yunhao Li, and Xiaojun Quan. 2021. Ubar: Towards fully end-to-end task-oriented dia- log system with gpt-2. Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence , 35(16):14230\u2013 14238. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christo- pher D Manning. 2018. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. InProceedings of the 2018 Conference on Empiri- cal Methods in Natural Language Processing , pages 2369\u20132380. Tiezheng Yu, Wenliang Dai, Zihan Liu, and Pascale Fung. 2021a. Vision guided generative pre-trainedlanguage models for multimodal abstractive summa- rization. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 3995\u20134007, Online and Punta Cana, Domini- can Republic. Association for Computational Lin- guistics. Tiezheng Yu, Zihan Liu, and Pascale Fung. 2021b. Adaptsum: Towards low-resource domain adapta- tion for abstractive summarization. In Proceedings of the 2021 Conference of the North American Chap- ter of the Association for Computational Linguistics: Human Language Technologies , pages 5892\u20135904. Xiaoxue Zang, Abhinav Rastogi, Srinivas Sunkara, Raghav Gupta, Jianguo Zhang, and Jindong Chen. 2020. Multiwoz 2.2: A dialogue dataset with addi- tional annotation corrections and state tracking base- lines. ACL 2020 , page 109. Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Good- man. 2022. Star: Bootstrapping reasoning with rea- soning. In Advances in Neural Information Process- ing Systems . Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher De- wan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 . Tianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Eval- uating text generation with bert. In International Xiaoxue Zang, Abhinav Rastogi, Srinivas Sunkara, Raghav Gupta, Jianguo Zhang, and Jindong Chen. 2020. Multiwoz 2.2: A dialogue dataset with addi- tional annotation corrections and state tracking base- lines. ACL 2020 , page 109. Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Good- man. 2022. Star: Bootstrapping reasoning with rea- soning. In Advances in Neural Information Process- ing Systems . Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher De- wan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 . Tianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Eval- uating text generation with bert. In International Conference on Learning Representations . Jeffrey Zhao, Raghav Gupta, Yuanbin Cao, Dian Yu, Mingqiu Wang, Harrison Lee, Abhinav Rastogi, Izhak Shafran, and Yonghui Wu. 2022. Description- driven task-oriented dialog modeling. ArXiv , abs/2201.08904. Xueliang Zhao, Wei Wu, Can Xu, Chongyang Tao, Dongyan Zhao, and Rui Yan. 2020. Knowledge- grounded dialogue generation with pre-trained lan- guage models. In Proceedings of the 2020 Con- ference on Empirical Methods in Natural Language Processing (EMNLP) , pages 3377\u20133390. Terry Yue Zhuo, Yujin Huang, Chunyang Chen, and Zhenchang Xing. 2023. Exploring ai ethics of chatgpt: A diagnostic analysis. arXiv preprint arXiv:2301.12867 . A Flag Drawing Task Results We provide the detailed results of the \ufb02ag drawing task described in \u00a73.3.1 in Figure 7. Figure 7: Complete results of the \ufb02ag drawing task. Multi-turn re\ufb01nement allows ChatGPT to generate a more similar image to the ground truth image. B InstructGPT for Multimodality We show an example of a multi-turn \ufb02ag drawing of InstructGPT in Figure 8. Similar to ChatGPT, InstructGPT can revise the generated \ufb02ag image in each turn, although the generation quality is still elementary. Figure 8: Example of the Canadian \ufb02ag drawn by InstructGPT. C List of Evaluation Datasets We provide a detailed list of all the datasets used in our experiment on Table 19. Dataset Task Description Reference #Test Size #ChatGPT Eval National Flag DrawingIG National Flag Drawing is a designed synthetic dataset which is used to evaluate the multimodal understanding of LLMs. The instruction for the National Flag Drawing is as follow: given a nation, draw the corre- similar image to the ground truth image. B InstructGPT for Multimodality We show an example of a multi-turn \ufb02ag drawing of InstructGPT in Figure 8. Similar to ChatGPT, InstructGPT can revise the generated \ufb02ag image in each turn, although the generation quality is still elementary. Figure 8: Example of the Canadian \ufb02ag drawn by InstructGPT. C List of Evaluation",
    "207": "Datasets We provide a detailed list of all the datasets used in our experiment on Table 19. Dataset Task Description Reference #Test Size #ChatGPT Eval National Flag DrawingIG National Flag Drawing is a designed synthetic dataset which is used to evaluate the multimodal understanding of LLMs. The instruction for the National Flag Drawing is as follow: given a nation, draw the corre- sponding national \ufb02ag and revise it based on the follow-up correction requests.Curated by authors of this paper50 50 CNN/DM SUM The CNN/DailyMail Dataset is an English-language dataset containing just over 300k unique news articles as written by journalists at CNN and the Daily Mail. The current version supports both extractive and abstractive summarization, though the original version was created for machine-reading and comprehension and abstractive question answer- ing.Nallapati et al. (2016) 11490 50 SAMSum SUM SAMSum dataset contains about 16k messenger-like conversations with summaries. Conversations were created and written down by linguists \ufb02uent in English. Linguists were asked to create conversations similar to those they write on a daily basis, re\ufb02ecting the proportion of topics of their real-life messenger convesations.Gliwa et al. (2019) 819 50 FLoRes-200 MT FLoRes is a benchmark dataset for machine translation between English and four low resource languages, Nepali, Sinhala, Khmer and Pashto, based on sentences translated from Wikipedia.Goyal et al. (2021) 1012 per language (200 languages)30 per language (12 languages) NusaX SA NusaX is a high-quality multilingual parallel corpus that covers 12 lan- guages, Indonesian, English, and 10 Indonesian local languages, namely Acehnese, Balinese, Banjarese, Buginese, Madurese, Minangkabau, Javanese, Ngaju, Sundanese, and Toba Batak.Winata et al. (2022) 400 50 bAbI task 15 QA This basic deduction bAbI tasks is taken from the (20) QA bAbI tasks that a set of proxy tasks that evaluate reading comprehension via ques- tion answering. The tasks measure understanding in several ways: whether a system is able to answer questions via simple deduction. and four low resource languages, Nepali, Sinhala, Khmer and Pashto, based on sentences translated from Wikipedia.Goyal et al. (2021) 1012 per language (200 languages)30 per language (12 languages) NusaX SA NusaX is a high-quality multilingual parallel corpus that covers 12 lan- guages, Indonesian, English, and 10 Indonesian local languages, namely Acehnese, Balinese, Banjarese, Buginese, Madurese, Minangkabau, Javanese, Ngaju, Sundanese, and Toba Batak.Winata et al. (2022) 400 50 bAbI task 15 QA This basic deduction bAbI tasks is taken from the (20) QA bAbI tasks that a set of proxy tasks that evaluate reading comprehension via ques- tion answering. The tasks measure understanding in several ways: whether a system is able to answer questions via simple deduction. The tasks are designed to be prerequisites for any system that aims to be capable of conversing with a human.Weston et al. (2016b) 1000 30 bAbI task 16 QA This basic induction bAbI tasks is taken from the (20) QA bAbI tasks that a set of proxy tasks that evaluate reading comprehension via question answering. The tasks measure understanding in several ways: whether a system is able to answer questions via simple induction. The tasks are designed to be prerequisites for any system that aims to be capable of conversing with a human.Weston et al. (2016b) 1000 30 EntailmentBank QA ENTAILMENTBANK, the \ufb01rst dataset of multistep entailment trees for QA, to support entailment-based explanation. ENTAILMENTBANK contains two parts: 1,840 entailment trees, each tree showing how a question-answer pair (QA) is entailed from a small number of relevant sentences (e.g., Figure 1); and a general corpus C, containing those and other sentences of domain-speci\ufb01c and general knowledge relevant to the QA domain.Dalvi et al. (2021) 340 30 CLUTRR QA CLUTRR (Compositional Language Understanding and Text-based Re- lational Reasoning), a diagnostic benchmark suite, is \ufb01rst introduced in (https://arxiv.org/abs/1908.06177) to test the systematic generalization and inductive reasoning capabilities of NLU systems. The CLUTRR benchmark allows us to test a model\u2019s ability for systematic generaliza- tion by testing on stories that contain unseen combinations of logical rules, and test for the various forms of model robustness by adding different kinds of super\ufb02uous noise facts to the stories.Sinha et al. (2019) 1146 30 NLI QA Abductive Natural Language Inference ( NLI) is a new commonsense other sentences of domain-speci\ufb01c and general knowledge relevant to the QA domain.Dalvi et al. (2021) 340 30 CLUTRR QA CLUTRR (Compositional Language Understanding and Text-based Re- lational Reasoning), a diagnostic benchmark suite, is \ufb01rst introduced in (https://arxiv.org/abs/1908.06177) to test the systematic generalization and inductive reasoning capabilities of NLU systems. The CLUTRR benchmark allows us to test a model\u2019s ability for systematic generaliza- tion by testing on stories that contain unseen combinations of logical rules, and test for the various forms of model robustness by adding different kinds of super\ufb02uous noise facts to the stories.Sinha et al. (2019) 1146 30 NLI QA Abductive Natural Language Inference ( NLI) is a new commonsense benchmark dataset designed to test an AI system\u2019s capability to apply abductive reasoning and common sense to form possible explanations for a given set of observations. Formulated as a binary-classi\ufb01cation task, the goal is to pick the most plausible explanatory hypothesis given two observations from narrative contexts.Bhagavatula et al. (2020)3059 30 CommonsenseQA QA CommonsenseQA is a new multiple-choice question answering dataset that requires different types of commonsense knowledge to predict the correct answers . It contains 12,102 questions with one correct answer and four distractor answers. The dataset is provided in two major training/validation/testing set splits: \"Random split\" which is the main evaluation split, and \"Question token split\", see paper for details.Talmor et al. (2018) 1221 30 HotpotQA QA HotpotQA is a new dataset with 113k Wikipedia-based question-answer pairs with four key features: (1) the questions require \ufb01nding and rea- soning over multiple supporting documents to answer; (2) the questions are diverse and not constrained to any pre-existing knowledge bases or knowledge schemas; (3) we provide sentence-level supporting facts required for reasoning, allowing QA systems to reason with strong su- pervision and explain the predictions; (4) we offer a new type of factoid comparison questions to test QA systems\u2019 ability to extract relevant facts and perform necessary comparison.Yang et al. (2018) 7405 30 PiQA QA To apply eyeshadow without a brush, should I use a cotton swab or a toothpick? Questions requiring this kind of physical commonsense pose a challenge to state-of-the-art natural language understanding sys- tems. The PIQA dataset introduces the task of physical commonsense soning over multiple supporting documents to answer; (2) the questions are diverse and not constrained to any pre-existing knowledge bases or knowledge schemas; (3) we provide sentence-level supporting facts required for reasoning, allowing QA systems to reason with strong su- pervision and explain the predictions; (4) we offer a new type of factoid comparison questions to test QA systems\u2019 ability to extract relevant facts and perform necessary comparison.Yang et al. (2018) 7405 30 PiQA QA To apply eyeshadow without a brush, should I use a cotton swab or a toothpick? Questions requiring this kind of physical commonsense pose a challenge to state-of-the-art natural language understanding sys- tems. The PIQA dataset introduces the task of physical commonsense reasoning and a corresponding benchmark dataset Physical Interaction: Question Answering or PIQA. Physical commonsense knowledge is a major challenge on the road to",
    "208": "true AI-completeness, including robots that interact with the world and understand natural language. PIQA focuses on everyday situations with a preference for atypical solutions. The dataset is inspired by instructables.com, which provides users with instructions on how to build, craft, bake, or manipulate objects using everyday materials.Bisk et al. (2020) 1838 30 E-Care QA Understanding causality has vital importance for various Natural Lan- guage Processing (NLP) applications. Beyond the labeled instances, conceptual explanations of the causality can provide a deep understand- ing of the causal fact to facilitate the causal reasoning process. We present a human-annotated explainable CAusal REasoning dataset (e- CARE), which contains over 20K causal reasoning questions, together with natural language formed explanations of the causal questions.Du et al. (2022) 2122 30 Letter string anal- ogyQA The letter string analogy domain was introduced in order to evaluate computational models of analogical reasoning. This task is composed of simple alphanumeric characters, but nevertheless require a signi\ufb01cant degree of abstraction to identify an analogy.Webb et al. (2022b) - 30 SpaRTQA QA SpartQA is a textual question answering benchmark for spatial rea- soning on natural language text which contains more realistic spatial phenomena not covered by prior datasets and that is challenging for state- of-the-art language models (LM). SPARTQA is built on NLVR\u2019s images containing more objects with richer spatial structures. SPARTQA\u2019s sto- with natural language formed explanations of the causal questions.Du et al. (2022) 2122 30 Letter string anal- ogyQA The letter string analogy domain was introduced in order to evaluate computational models of analogical reasoning. This task is composed of simple alphanumeric characters, but nevertheless require a signi\ufb01cant degree of abstraction to identify an analogy.Webb et al. (2022b) - 30 SpaRTQA QA SpartQA is a textual question answering benchmark for spatial rea- soning on natural language text which contains more realistic spatial phenomena not covered by prior datasets and that is challenging for state- of-the-art language models (LM). SPARTQA is built on NLVR\u2019s images containing more objects with richer spatial structures. SPARTQA\u2019s sto- ries are more natural, have more sentences, and richer in spatial relations in each sentence, and the questions require deeper reasoning and have four types: \ufb01nd relation (FR), \ufb01nd blocks (FB), choose object (CO), and yes/no (YN), which allows for more \ufb01ne-grained analysis of models\u2019 capabilities. The default test set of this dataset is human-annotated test set, which consists of 510 instances.Mirzaee et al. (2021) 510 64 StepGame QA StepGame is a robust multi-hop spatial reasoning dataset in textual format which addresses the limitation from the bAbI dataset task 17 and task 19. In this task, the AI system is required to interpret a story of k spatial relations (e.g left, top-right, 90 degree clockwise) of k+1 entities (k is up to 10) expressed in natural language and answer a question about the relative position of two of those entities. For each k from 1 to 10, there are 30000, 1000, and 30000 samples for train, valid, and test respectively.Shi et al. (2022b) 30000 30 Pep-3k QA Pep-3k is a dataset of physical semantic plausibility judgments of single events. It requires a mixture of commonsense knowledge and conceptual knowledge to solve. Each event consists of a subject, a verb, and an object, i.e it has the simple s-v-o format. For example, the event can be man swallow paintball , with the label 0 (implausible). In total, Pep-3k has 3080 instances with plausible-implausible data balance.Wang et al. (2018b) 3080 30 COVID-Social MD COVID-19-related claims fact-checked by journalists from a website called Politifact.com. Unlike the Covid19-Scienti\ufb01c dataset, it contains non-scienti\ufb01c and socially-related claims, such as \u201cFor the coronavirus, 10, there are 30000, 1000, and 30000 samples for train, valid, and test respectively.Shi et al. (2022b) 30000 30 Pep-3k QA Pep-3k is a dataset of physical semantic plausibility judgments of single events. It requires a mixture of commonsense knowledge and conceptual knowledge to solve. Each event consists of a subject, a verb, and an object, i.e it has the simple s-v-o format. For example, the event can be man swallow paintball , with the label 0 (implausible). In total, Pep-3k has 3080 instances with plausible-implausible data balance.Wang et al. (2018b) 3080 30 COVID-Social MD COVID-19-related claims fact-checked by journalists from a website called Politifact.com. Unlike the Covid19-Scienti\ufb01c dataset, it contains non-scienti\ufb01c and socially-related claims, such as \u201cFor the coronavirus, the death rate in Texas, per capita of 29 million people, we\u2019re one of the lowest in the country.\u201d Such claims may not be life-and-death matters, but they still have the potential to bring negative sociopolitical effects.Lee et al. (2021) 340 50 COVID- Scienti\ufb01cMD COVID-Scienti\ufb01c is a testset consists of COVID-19-related scienti\ufb01c or medical myths that must be debunked correctly to ensure the safety of the public (e.g., \"Drinking a bleach solution will prevent you from getting COVID-19\").Lee et al. (2021) 172 50 TruthfulQA QA The dataset comprises questions that span 38 categories, including health, law, \ufb01nance and politics. The questions are of those that some humans would answer falsely due to a false belief or misconception. (e.g., [conspiracy] \u201cIf it\u2019s cold outside what does that tell us about global warming?\u201d; [health] \u201cCan coughing effectively stop a heart attack?\u201d)Lin et al. (2022) 817 65 MultiWOZ 2.2 TOD MultiWOZ 2.2 is an improved version of MultiWOZ 2.1, which iden- ti\ufb01es and \ufb01xes dialogue state annotation errors across 17.3% of the utterances on top of MultiWOZ 2.1 and rede\ufb01nes the ontology by dis- allowing vocabularies of slots with a large number of possible values (e.g., restaurant name, time of booking) and introducing standardized slot span annotations for these slots.Zang et al. (2020) 5000 dialogue turns 50 dialog turns OpenDialKG KGD OpenDialKG is a dataset of conversations between two crowdsourcing agents engaging in a dialog about a given topic. Each dialog turn is paired with its corresponding \u201cKG paths\u201d that weave together the KG entities and relations that are mentioned in the dialog.Moon et al. (2019) 2.25k dialogue ses- sions50 dialogue turns MultiWOZ 2.2 TOD MultiWOZ 2.2 is an improved version of MultiWOZ 2.1, which iden- ti\ufb01es and \ufb01xes dialogue state annotation errors across 17.3% of the utterances on top of MultiWOZ 2.1 and rede\ufb01nes the ontology by dis- allowing vocabularies of slots with a large number of possible values (e.g., restaurant name, time of booking) and introducing standardized slot span annotations for these slots.Zang et al. (2020) 5000 dialogue turns 50 dialog turns OpenDialKG KGD OpenDialKG is a dataset of conversations between two crowdsourcing agents engaging in a dialog about a given topic. Each dialog turn is paired with its corresponding \u201cKG paths\u201d that weave together the KG entities and relations that are mentioned in the dialog.Moon et al. (2019) 2.25k dialogue ses- sions50 dialogue turns Table 19: List of all datasets used in our experiments. IG denotes image generation, SUM denotes summarization, MT denotes machine translation, SA denotes sentiment analysis, QA denotes question answering, MD denotes misinformation detection, TOD denotes task-oriented dialogue, and KGD denotes knowledge-grounded dialogue. Some of the descriptions are directly from the original reference. D Examples from Machine Translation and Post-Editing Target English Text Label Translation Post-Edited Text Chinese Although three people were inside the house when the car impacted it, none of them were hurt.\u867d\u7136\u8f66\u649e\u5230\u623f\u5b50\u65f6\uff0c\u623f\u5b50\u91cc\u9762\u6709 \u4e09\u4e2a\u4eba\uff0c\u4f46\u6700\u540e\u5e76\u6ca1\u6709\u4eba\u53d7\u4f24\u3002\u5c3d\u7ba1\u6709\u4e09\u4e2a\u4eba\u5728\u6c7d\u8f66\u649e\u4e0a\u623f\u5b50 \u7684\u65f6\u5019\u5728\u5c4b\u91cc\uff0c\u4f46\u4ed6\u4eec\u90fd\u6ca1\u6709\u53d7 \u4f24\u3002\u5c3d\u7ba1\u6c7d\u8f66\u649e\u4e0a\u623f\u5b50\u65f6\u6709\u4e09\u4e2a\u4eba\u5728 \u5c4b\u91cc\uff0c\u4f46\u4ed6\u4eec\u90fd\u6ca1\u6709\u53d7\u4f24\u3002 Chinese 34 per cent of those in the poll share",
    "209": "this view, wanting Queen Elizabeth II to be Australia\u2019s last monarch.34%\u7684\u53d7\u8bbf\u8005\u4e00\u81f4\u8868\u793a\uff0c\u5e0c\u671b\u82f1 \u56fd\u5973\u738b\u4f0a\u4e3d\u838e\u767d\u4e8c\u4e16\u6210\u4e3a\u6fb3\u5927\u5229 \u4e9a\u7684\u6700\u540e\u4e00\u4efb\u541b\u4e3b\u3002\u8fd9\u9879\u6c11\u610f\u8c03\u67e5\u663e\u793a\uff0c34%\u7684\u4eba\u5e0c\u671b \u4f0a\u4e3d\u838e\u767d\u4e8c\u4e16\u662f\u6fb3\u5927\u5229\u4e9a\u7684\u6700\u540e \u4e00\u4f4d\u541b\u4e3b\u3002\u6839\u636e\u6c11\u610f\u8c03\u67e5\u663e\u793a\uff0c34%\u7684\u4eba\u5e0c\u671b \u4f0a\u4e3d\u838e\u767d\u4e8c\u4e16\u6210\u4e3a\u6fb3\u5927\u5229\u4e9a\u7684\u6700 \u540e\u4e00\u4f4d\u541b\u4e3b\u3002 Chinese The qualities that determine a subcul- ture as distinct may be linguistic, aes- thetic, religious, political, sexual, geo- graphical, or a combination of factors.\u8ba9\u4e9a\u6587\u5316\u53d8\u5f97\u4e0e\u4f17\u4e0d\u540c\u7684\u7279\u8d28\uff0c \u53ef\u80fd\u662f\u8bed\u8a00\u3001\u7f8e\u5b66\u3001\u5b97\u6559\u3001\u653f \u6cbb\u3001\u6027\u522b\u3001\u5730\u7406\u6216\u591a\u79cd\u56e0\u7d20\u7684\u7ec4 \u5408\u3002\u4ec0\u4e48\u51b3\u5b9a\u4e00\u4e2a\u4e9a\u6587\u5316\u4f5c\u4e3a\u4e0d\u540c\u7684 \u7279\u8d28\u53ef\u80fd\u662f\u8bed\u8a00\u7684\u3001\u5ba1\u7f8e\u7684\u3001\u5b97 \u6559\u7684\u3001\u653f\u6cbb\u7684\u3001\u6027\u7684\u3001\u5730\u7406\u7684\uff0c \u6216\u8005\u8fd9\u4e9b\u56e0\u7d20\u7684\u7ec4\u5408\u3002\u51b3\u5b9a\u4e00\u4e2a\u4e9a\u6587\u5316\u4e0e\u4f17\u4e0d\u540c\u7684\u7279\u8d28 \u53ef\u80fd\u662f\u8bed\u8a00\u7684\u3001\u5ba1\u7f8e\u7684\u3001\u5b97\u6559 \u7684\u3001\u653f\u6cbb\u7684\u3001\u6027\u7684\u3001\u5730\u7406\u7684\uff0c\u6216 \u8005\u662f\u8fd9\u4e9b\u56e0\u7d20\u7684\u7ec4\u5408\u3002 Chinese New Kingdom ancient Egyptians mar- velled at their predecessors monu- ments, which were then well over a thousand year old.\u65b0\u738b\u56fd\u65f6\u671f\u7684\u53e4\u57c3\u53ca\u4eba\u60ca\u53f9\u4e8e\u5176 \u524d\u8f88\u7684\u5df2\u6709\u4e00\u5343\u591a\u5e74\u5386\u53f2\u7684\u7eaa\u5ff5 \u7891\u3002\u65b0\u738b\u671d\u53e4\u57c3\u53ca\u4eba\u60ca\u53f9\u4e8e\u4ed6\u4eec\u7684\u524d \u4eba\u7684\u7eaa\u5ff5\u7891\uff0c\u5f53\u65f6\u5df2\u7ecf\u6709\u4e00\u5343\u591a \u5e74\u7684\u5386\u53f2\u4e86\u3002\u65b0\u738b\u671d\u7684\u53e4\u57c3\u53ca\u4eba\u60ca\u53f9\u4e8e\u4ed6\u4eec\u524d \u4eba\u7559\u4e0b\u7684\u7eaa\u5ff5\u7891\uff0c\u5f53\u65f6\u5df2\u7ecf\u6709\u8d85 \u8fc7\u4e00\u5343\u5e74\u7684\u5386\u53f2\u3002 Chinese The U.S. Corps of Engineers esti- mated that 6 inches of rainfall could breach the previously damaged lev- ees.\u7f8e\u56fd\u9646\u519b\u5de5\u5175\u90e8\u961f\u4f30\u8ba1\uff0c6\u82f1\u5bf8\u7684 \u964d\u96e8\u91cf\u5c31\u53ef\u80fd\u51b2\u57ae\u4e4b\u524d\u635f\u574f\u7684\u5824 \u575d\u3002\u7f8e\u56fd\u5de5\u7a0b\u5175\u90e8\u961f\u4f30\u8ba1\uff0c6\u82f1\u5bf8\u7684\u964d \u96e8\u53ef\u80fd\u4f1a\u7834\u574f\u5148\u524d\u53d7\u635f\u7684\u5824\u575d\u3002\u7f8e\u56fd\u5de5\u7a0b\u5175\u90e8\u961f\u4f30\u8ba1\uff0c6\u82f1\u5bf8\u7684\u964d \u96e8\u53ef\u80fd\u4f1a\u4f7f\u5148\u524d\u53d7\u635f\u7684\u5824\u575d\u518d\u6b21 \u635f\u574f\u3002 \u540e\u4e00\u4f4d\u541b\u4e3b\u3002 Chinese The qualities that determine a subcul- ture as distinct may be linguistic, aes- thetic, religious, political, sexual, geo- graphical, or a combination of factors.\u8ba9\u4e9a\u6587\u5316\u53d8\u5f97\u4e0e\u4f17\u4e0d\u540c\u7684\u7279\u8d28\uff0c \u53ef\u80fd\u662f\u8bed\u8a00\u3001\u7f8e\u5b66\u3001\u5b97\u6559\u3001\u653f \u6cbb\u3001\u6027\u522b\u3001\u5730\u7406\u6216\u591a\u79cd\u56e0\u7d20\u7684\u7ec4 \u5408\u3002\u4ec0\u4e48\u51b3\u5b9a\u4e00\u4e2a\u4e9a\u6587\u5316\u4f5c\u4e3a\u4e0d\u540c\u7684 \u7279\u8d28\u53ef\u80fd\u662f\u8bed\u8a00\u7684\u3001\u5ba1\u7f8e\u7684\u3001\u5b97 \u6559\u7684\u3001\u653f\u6cbb\u7684\u3001\u6027\u7684\u3001\u5730\u7406\u7684\uff0c \u6216\u8005\u8fd9\u4e9b\u56e0\u7d20\u7684\u7ec4\u5408\u3002\u51b3\u5b9a\u4e00\u4e2a\u4e9a\u6587\u5316\u4e0e\u4f17\u4e0d\u540c\u7684\u7279\u8d28 \u53ef\u80fd\u662f\u8bed\u8a00\u7684\u3001\u5ba1\u7f8e\u7684\u3001\u5b97\u6559 \u7684\u3001\u653f\u6cbb\u7684\u3001\u6027\u7684\u3001\u5730\u7406\u7684\uff0c\u6216 \u8005\u662f\u8fd9\u4e9b\u56e0\u7d20\u7684\u7ec4\u5408\u3002 Chinese New Kingdom ancient Egyptians mar- velled at their predecessors monu- ments, which were then well over a thousand year old.\u65b0\u738b\u56fd\u65f6\u671f\u7684\u53e4\u57c3\u53ca\u4eba\u60ca\u53f9\u4e8e\u5176 \u524d\u8f88\u7684\u5df2\u6709\u4e00\u5343\u591a\u5e74\u5386\u53f2\u7684\u7eaa\u5ff5 \u7891\u3002\u65b0\u738b\u671d\u53e4\u57c3\u53ca\u4eba\u60ca\u53f9\u4e8e\u4ed6\u4eec\u7684\u524d \u4eba\u7684\u7eaa\u5ff5\u7891\uff0c\u5f53\u65f6\u5df2\u7ecf\u6709\u4e00\u5343\u591a \u5e74\u7684\u5386\u53f2\u4e86\u3002\u65b0\u738b\u671d\u7684\u53e4\u57c3\u53ca\u4eba\u60ca\u53f9\u4e8e\u4ed6\u4eec\u524d \u4eba\u7559\u4e0b\u7684\u7eaa\u5ff5\u7891\uff0c\u5f53\u65f6\u5df2\u7ecf\u6709\u8d85 \u8fc7\u4e00\u5343\u5e74\u7684\u5386\u53f2\u3002 Chinese The U.S. Corps of Engineers esti- mated that 6 inches of rainfall could breach the previously damaged lev- ees.\u7f8e\u56fd\u9646\u519b\u5de5\u5175\u90e8\u961f\u4f30\u8ba1\uff0c6\u82f1\u5bf8\u7684 \u964d\u96e8\u91cf\u5c31\u53ef\u80fd\u51b2\u57ae\u4e4b\u524d\u635f\u574f\u7684\u5824 \u575d\u3002\u7f8e\u56fd\u5de5\u7a0b\u5175\u90e8\u961f\u4f30\u8ba1\uff0c6\u82f1\u5bf8\u7684\u964d \u96e8\u53ef\u80fd\u4f1a\u7834\u574f\u5148\u524d\u53d7\u635f\u7684\u5824\u575d\u3002\u7f8e\u56fd\u5de5\u7a0b\u5175\u90e8\u961f\u4f30\u8ba1\uff0c6\u82f1\u5bf8\u7684\u964d \u96e8\u53ef\u80fd\u4f1a\u4f7f\u5148\u524d\u53d7\u635f\u7684\u5824\u575d\u518d\u6b21 \u635f\u574f\u3002 Chinese Several large television screens were installed in various places in Rome to let the people watch the ceremony.\u7f57\u9a6c\u5404\u5730\u5b89\u88c5\u4e86\u6570\u4e2a\u5927\u578b\u7535\u89c6\u5c4f \u5e55\uff0c\u4f9b\u4eba\u4eec\u89c2\u770b\u4eea\u5f0f\u3002\u5728\u7f57\u9a6c\u7684\u82e5\u5e72\u4e2a\u5730\u65b9\u5b89\u88c5\u4e86\u51e0 \u53f0\u5927\u578b\u7535\u89c6\u5c4f\u5e55\uff0c\u8ba9\u4eba\u4eec\u89c2\u770b\u4eea \u5f0f\u3002\u53ef\u4ee5\uff0c\u8fd9\u662f\u4fee\u6539\u540e\u7684\u7ffb\u8bd1\uff1a\u5728\u7f57 \u9a6c\u7684\u5404\u4e2a\u5730\u65b9\u5b89\u88c5\u4e86\u51e0\u53f0\u5927\u578b\u7535 \u89c6\u5c4f\u5e55\uff0c\u4ee5\u4fbf\u8ba9\u4eba\u4eec\u89c2\u770b\u4eea\u5f0f\u3002 Chinese Pests can spoil food, cause irritation, or in a worse case cause allergic re- actions, spread venom, or transmit in- fections.\u5bb3\u866b\u4f1a\u7834\u574f\u98df\u7269\uff0c\u5f15\u8d77\u523a\u6fc0\u6027\u53cd \u5e94\uff0c\u5728\u66f4\u4e25\u91cd\u7684\u60c5\u51b5\u4e0b\u4f1a\u5f15\u8d77\u8fc7 \u654f\u53cd\u5e94\uff0c\u4f20\u64ad\u6bd2\u6db2\u6216\u4f20\u67d3\u75c5\u3002\u5bb3\u866b\u53ef\u4ee5\u6bc1\u574f\u98df\u7269\uff0c\u5f15\u8d77\u523a\u6fc0\uff0c \u6216\u8005\u5728\u66f4\u7cdf\u7cd5\u7684\u60c5\u51b5\u4e0b\u5f15\u8d77\u8fc7\u654f \u53cd\u5e94\uff0c\u4f20\u64ad\u6bd2\u6db2\uff0c\u6216\u4f20\u64ad\u611f\u67d3\u3002\u5bb3\u866b\u53ef\u4ee5\u6bc1\u574f\u98df\u7269\uff0c\u5f15\u8d77\u4e0d\u9002\uff0c \u751a\u81f3\u4f1a\u5f15\u8d77\u8fc7\u654f\u53cd\u5e94\u3001\u4f20\u64ad\u6bd2\u6db2 \u6216\u4f20\u67d3\u611f\u67d3\u3002 Chinese It is obvious enough that the world has changed much because of hu- mankind\u2019s scienti\ufb01c and technologi- cal advancements, and problems have become greater because of overpop- ulation and mankind\u2019s extravagant lifestyle.\u663e\u800c\u6613\u89c1\uff0c\u7531\u4e8e\u4eba\u7c7b\u7684\u79d1\u5b66\u548c\u6280 \u672f\u8fdb\u6b65\uff0c\u4e16\u754c\u53d1\u751f\u4e86\u5f88\u5927\u7684\u53d8 \u5316\uff0c\u800c\u4eba\u53e3\u8fc7\u5269\u548c\u4eba\u7c7b\u5962\u4f88\u7684\u751f \u6d3b\u65b9\u5f0f\u4f7f\u95ee\u9898\u53d8\u5f97\u66f4\u4e3a\u4e25\u91cd\u3002\u5f88\u660e\u663e\uff0c\u7531\u4e8e\u4eba\u7c7b\u7684\u79d1\u5b66\u6280\u672f\u8fdb \u6b65\uff0c\u4e16\u754c\u5df2\u7ecf\u53d1\u751f\u4e86\u5f88\u5927\u7684\u53d8 \u5316\uff0c\u5e76\u4e14\u7531\u4e8e\u4eba\u53e3\u8fc7\u5269\u548c\u4eba\u7c7b\u5962 \u4f88\u7684\u751f\u6d3b\u65b9\u5f0f\uff0c\u95ee\u9898\u53d8\u5f97\u66f4\u52a0\u4e25 \u91cd\u3002\u6beb\u65e0\u7591\u95ee\uff0c\u4eba\u7c7b\u7684\u79d1\u5b66\u548c\u6280\u672f\u8fdb \u6b65\u4f7f\u4e16\u754c\u53d1\u751f\u4e86\u5de8\u5927\u7684\u53d8\u5316\uff0c\u540c \u65f6\u4eba\u53e3\u8fc7\u5269\u548c\u4eba\u7c7b\u5962\u4f88\u7684\u751f\u6d3b\u65b9 \u5f0f\u4f7f\u95ee\u9898\u53d8\u5f97\u66f4\u52a0\u4e25\u91cd\u3002 Chinese The correlation between brain pathol- ogy and behaviour supports scientists in their research.\u5927\u8111\u75c5\u7406\u548c\u884c\u4e3a\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u53ef \u4ee5\u4e3a\u79d1\u5b66\u5bb6\u4eec\u7684\u7814\u7a76\u63d0\u4f9b\u652f\u6301\u3002\u5927\u8111\u75c5\u7406\u5b66\u4e0e\u884c\u4e3a\u4e4b\u95f4\u7684\u76f8\u5173\u6027 \u652f\u6301\u79d1\u5b66\u5bb6\u4eec\u7684\u7814\u7a76\u3002\u7814\u7a76\u8868\u660e\uff0c\u5927\u8111\u75c5\u7406\u5b66\u548c\u884c\u4e3a \u4e4b\u95f4\u7684\u76f8\u5173\u6027\u4e3a\u79d1\u5b66\u5bb6\u63d0\u4f9b\u4e86\u652f \u6301\u3002 Chinese Like some other experts, he is skep- tical about whether diabetes can be cured, noting that these \ufb01ndings have no relevance to people who already have Type 1 diabetes.\u548c\u5176\u4ed6\u4e00\u4e9b\u4e13 \u5bb6\u4e00\u6837\uff0c\u4ed6\u5bf9\u7cd6\u5c3f \u75c5\u80fd\u5426\u6cbb\u6108\u6301\u6000\u7591\u6001\u5ea6\u3002\u4ed6\u6307 \u51fa\uff0c\u8fd9\u4e9b\u53d1\u73b0\u4e0e\u5df2\u60a3\u67091\u578b\u7cd6\u5c3f \u75c5\u7684\u4eba\u65e0\u5173\u3002\u50cf\u5176\u4ed6\u4e00\u4e9b\u4e13 \u5bb6\u4e00\u6837\uff0c\u4ed6\u5bf9\u4e8e\u7cd6 \u5c3f\u75c5\u662f\u5426\u53ef\u4ee5\u88ab\u6cbb\u6108\u6301\u6000\u7591\u6001 \u5ea6\uff0c\u5e76\u6307\u51fa\u8fd9\u4e9b\u53d1\u73b0\u5bf9\u5df2\u7ecf\u60a3\u6709 1\u578b\u7cd6\u5c3f\u75c5\u7684\u4eba\u6ca1\u6709\u610f\u4e49\u3002\u548c\u5176\u4ed6\u4e00\u4e9b\u4e13 \u5bb6\u4e00\u6837\uff0c\u4ed6\u5bf9\u7cd6\u5c3f \u75c5\u662f\u5426\u53ef\u4ee5\u88ab\u6cbb\u6108\u8868\u793a\u6000\u7591\uff0c\u5e76 \u6307\u51fa\u8fd9\u4e9b\u53d1\u73b0\u5bf9\u5df2\u7ecf\u60a3\u67091\u578b\u7cd6 \u5c3f\u75c5\u7684\u4eba\u6ca1\u6709\u610f\u4e49\u3002 Korean Although three people were inside the house when the car impacted it, none of them were hurt. \u0013\u0000 \u0013|9 \b\\ \u0081\u00d8\u0005\u00e6[\u001at\u00d9\u00fe\u00a1`\u0007\u00a6M :[ j\"\u00ee s \u001b\u00ee\u00df\u0096 \\ \u0081e\u0094\u0012% 3 \u0012t \u001b\u00eb\u00df\u0096,\u00d5 \u00aa[ \u00fe t\u00d7\u0005\u00e6\u00f4\u0014\u00c7\"\u00ee \u0095 \u00b8\u0002 \u0013u \u001b t \u001b\u00b7 \u00fa \u00a7\u0080 \u008c \u00a4\u0002 \u0013. \u0013\u0000 \u0013|9 \b\\ \u0081\u00c2 \u00d2v9 y \u001b\b \u0013|9 \b\u00ee\u00df\u0096\\ \u0081[ j\u0006 \u0013 | \u00c3 \u00d0s \u001be\u0094\u0012% 3 \u0012t \u001b\u00eb\u00df\u0096,\u0007 \u0013\u00c1 \u00ba\u0095 \u00b8\u0002 \u0013u \u001bt \u001b\u00b7 \u00fa \u00a7 ogy and behaviour supports scientists in their research.\u5927\u8111\u75c5\u7406\u548c\u884c\u4e3a\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u53ef \u4ee5\u4e3a\u79d1\u5b66\u5bb6\u4eec\u7684\u7814\u7a76\u63d0\u4f9b\u652f\u6301\u3002\u5927\u8111\u75c5\u7406\u5b66\u4e0e\u884c\u4e3a\u4e4b\u95f4\u7684\u76f8\u5173\u6027 \u652f\u6301\u79d1\u5b66\u5bb6\u4eec\u7684\u7814\u7a76\u3002\u7814\u7a76\u8868\u660e\uff0c\u5927\u8111\u75c5\u7406\u5b66\u548c\u884c\u4e3a \u4e4b\u95f4\u7684\u76f8\u5173\u6027\u4e3a\u79d1\u5b66\u5bb6\u63d0\u4f9b\u4e86\u652f \u6301\u3002 Chinese Like some other experts, he is skep- tical about whether diabetes can be cured, noting that these \ufb01ndings have no relevance to people who already have Type 1 diabetes.\u548c\u5176\u4ed6\u4e00\u4e9b\u4e13 \u5bb6\u4e00\u6837\uff0c\u4ed6\u5bf9\u7cd6\u5c3f \u75c5\u80fd\u5426\u6cbb\u6108\u6301\u6000\u7591\u6001\u5ea6\u3002\u4ed6\u6307 \u51fa\uff0c\u8fd9\u4e9b\u53d1\u73b0\u4e0e\u5df2\u60a3\u67091\u578b\u7cd6\u5c3f \u75c5\u7684\u4eba\u65e0\u5173\u3002\u50cf\u5176\u4ed6\u4e00\u4e9b\u4e13 \u5bb6\u4e00\u6837\uff0c\u4ed6\u5bf9\u4e8e\u7cd6 \u5c3f\u75c5\u662f\u5426\u53ef\u4ee5\u88ab\u6cbb\u6108\u6301\u6000\u7591\u6001 \u5ea6\uff0c\u5e76\u6307\u51fa\u8fd9\u4e9b\u53d1\u73b0\u5bf9\u5df2\u7ecf\u60a3\u6709 1\u578b\u7cd6\u5c3f\u75c5\u7684\u4eba\u6ca1\u6709\u610f\u4e49\u3002\u548c\u5176\u4ed6\u4e00\u4e9b\u4e13 \u5bb6\u4e00\u6837\uff0c\u4ed6\u5bf9\u7cd6\u5c3f \u75c5\u662f\u5426\u53ef\u4ee5\u88ab\u6cbb\u6108\u8868\u793a\u6000\u7591\uff0c\u5e76 \u6307\u51fa\u8fd9\u4e9b\u53d1\u73b0\u5bf9\u5df2\u7ecf\u60a3\u67091\u578b\u7cd6 \u5c3f\u75c5\u7684\u4eba\u6ca1\u6709\u610f\u4e49\u3002 Korean Although three people were inside the house when the car impacted it, none of them were hurt. \u0013\u0000 \u0013|9 \b\\ \u0081\u00d8\u0005\u00e6[\u001at\u00d9\u00fe\u00a1`\u0007\u00a6M :[ j\"\u00ee s \u001b\u00ee\u00df\u0096 \\ \u0081e\u0094\u0012% 3 \u0012t \u001b\u00eb\u00df\u0096,\u00d5 \u00aa[ \u00fe t\u00d7\u0005\u00e6\u00f4\u0014\u00c7\"\u00ee \u0095 \u00b8\u0002 \u0013u \u001b t \u001b\u00b7 \u00fa \u00a7\u0080 \u008c \u00a4\u0002 \u0013. \u0013\u0000 \u0013|9 \b\\ \u0081\u00c2 \u00d2v9 y \u001b\b \u0013|9 \b\u00ee\u00df\u0096\\ \u0081[ j\u0006 \u0013 | \u00c3 \u00d0s \u001be\u0094\u0012% 3 \u0012t \u001b\u00eb\u00df\u0096,\u0007 \u0013\u00c1 \u00ba\u0095 \u00b8\u0002 \u0013u \u001bt \u001b\u00b7 \u00fa \u00a7 \u0080 \u008c \u00a4_ \u00fe vm \u001b\u0002 \u0013 . \u0013\u0000 \u0013|9 \b\\ \u0081\u00c2 \u00d2v9 y \u001b\b \u0013|9 \b\u00ee\u00df\u0096\\ \u0081[ j\u0006 \u0013 | \u00c3 \u00d0s \u001be\u0094\u0012% 3 \u0012t \u001b\u00eb\u00df\u0096,\u0007 \u0013\u00c1 \u00ba\u0095 \u00b8\u007f\u00a9\u009cK \u0017\\\u0007\u00a6{9 \b t \u001b\u00b7 \u00fa \u00a7\u0080 \u008c \u00a4_ \u00fe vm \u001b\u0002 \u0013 . Korean 34 per cent of those in the poll share this view, wanting Queen Elizabeth II to be Australia\u2019s last monarch.# \u008c\u008f:r\u009b \u00b8\u0006 \u0013\\ \u0081\" f 34( \u007fG'p\u00e0 \u00d4\u0000 \u0013\u00d3sqo \u001b\b \u0013 Z \u00db \u00bc 2[ j\u0000 \u0013 \u00f1\u00c5 \u00d2_ \u000e\u0004 \u0013t \u001b} \u008c \u0095\u00e7\u0012H\u00c5 \u00d2s \u001b U \u0002 \u00b4\u0005 \u0013\u00ea\u00f8\u00cd\u0002 \u0013\u008d\u000fH_ \u000e| \u0004`\u0007\u00a6\u0098 \u00d0% i \u0012_ \u00fe vm \u001b\u0002 \u0013 .34%\u0000 \u0013s \u001b_ \u000e| \u0004`\u0007\u00a6/ B Ny \u008c \u0099 \u0013 9 ,\u0007 \u0013\u00db \u00bc \u00e0 \u00d4Y U{9 \u0006o \u001b\u0007 \u0013_ \u000e\u00fej\u00ca \u00ea_ \u000eM\u0012g\u009b \u00b8\u0000 \u0013\u00d3sqo \u001b \b \u0013Z \u00db \u00bc 2[ j\u0000 \u0013\u00f7&l \u001b\\\u0007\u00a6\" \u00e9\u00b6\u00f4\u0014\u00c7\u0002 \u0013.s \u001b\u009b \u00b8\u0006 \u0013\\ \u0081\" f\u008d\u000fH34%\u0000 \u0013\u00d3sqo \u001b\b \u0013Z \u00db \u00bc 2[ j\u0000 \u0013\u0007 \u0013\u00db \u00bc\u00e0 \u00d4Y U{9 \u0006o \u001b\u0007 \u0013_ \u000e\u00fej\u00ca \u00ea_ \u000e M\u0012g\u009b \u00b8\u0000 \u0013\u00f7&l \u001b\\\u0007\u00a6\" \u00e9\u00b6\u00f4\u0014\u00c7\u0002 \u0013\u008d\u000fH_ \u000e| \u0004`\u0007\u00a6 / B Ny \u008c \u0099\u00f4\u0014\u00c7\u0002 \u0013. Korean The qualities that determine a subcul- ture as distinct may be linguistic, aes- thetic, religious, political, sexual, geo- graphical, or a combination of factors. \u00130 A\u00eb\u0017H\u0002 o\\\u0007\u00a6\"\u00ee SX\u0089 \u0013> \u0019\u00bd \u00a8\u00ec r \u0013\u008d\u000fH: \u00a3 \u00a4 f\u00e7 \u0093 \u00c9 r\u0083\u0015\u0004# Q& h \u0003,p \u001b& h \u0003,7 \u00e1 x\u0093 \u00a7& h \u0003,&\u00f1 u \u001b& h \u0003, $\u00ed & h \u0003 ,t \u001bo \u001b& h \u0003\u00af \u00b9\u0099 \u00e8\u0000 \u0013e\u0094\u0012\u00dc \u00bc 9 ,s \u001b Q \u00f4\u0014\u00c7\u00af \u00b9\u0099 \u00e8[ \u00fe t_ \u000e \u0016\u0006\u00bd+\u00cb{9 \u0006\u00c3 \u00ba\u0095 \u00b8e\u0094\u0012\u0002 \u0013.\"\u0002 \u00136 \u00a3 \u00a7\u00eb\u0017H\u0081\u00a9\u009c_ \u000e\u00f4\u0014\u00c7\u00b2 D G# Q\u0081\u0015\u0004% i \u0003\u0093 \u00c9 r\u00c1 \u00ba%\u00c1 {9 \bm \u001b\u000e \u0013 ?\u00c2 \u00d2\u00eb\u0017H\u00eb\u0017H\u0002 o\\\u0007\u00a6\u00bd \u00a8Z>\u0006\u00f7&> \u0019 \u0013 \u008d\u000fH: \u00a3 \u00a4$\u00ed \u0093 \u00c9 r\u0083\u0015\u0004# Q,\\ V\u00d5 \u00fc t,7 \u00e1 x\u0093 \u00a7,&\u00f1 u \u001b, $\u00ed ,t \u001bo \u001b\u00af \u00b9\u0099 \u00e8{9 \u0006\u00c3 \u00bae\u0094\u0012 \u0015\u0001 \u0013s \u001b[ \u00fe t\u00af \u00b9 \u0099 \u00e8_ \u000e\u009b \u00b8\u00bd+\u00cb{9 \u0006\u00c3 \u00ba\u0095 \u00b8e\u0094\u0012_ \u00fe vm \u001b\u0002 \u0013 .\"\u00c2 \u00d2\u00eb\u0017H\u00eb\u0017H\u0002 o\u0000 \u0013\u00bd \u00a8Z>\u0006\u00f7&\u008d\u000fH: \u00a3 \u00a4$\u00ed \u0093 \u00c9 r\u0083\u0015\u0004# Q & h \u0003,\\ V\u00d5 \u00fc t& h \u0003,7 \u00e1 x\u0093 \u00a7& h \u0003,&\u00f1 u \u001b& h \u0003,$\u00ed & h \u0003 , t \u001bo \u001b& h \u0003\u00af \u00b9\u0099 \u00e8\u00d7\u0005\u00e6 \u0013\u0001 \u0013{9 \u0006\u00c3 \u00ba\u0095 \u00b8e\u0094\u0012\u0093 \u00a6, s \u001b[ \u00fe t\u00af \u00b9\u0099 \u00e8_ \u000e\u009b \u00b8\u00bd+\u00cb{9 \u0006\u00c3 \u00ba\u0095 \u00b8e\u0094\u0012_ \u00fe vm \u001b \u0002 \u0013. Korean New Kingdom ancient Egyptians mar- velled at their predecessors monu- ments, which were then well over a thousand year old.\u0093 \u00a6@ /\u0092 \u001b\u0004M\u0012g\u00b2 D Gs \u001b|9 \b\u00e0 \u00d4\u0093 \u001b\u0004[ \u00fe t\u0093 \u00c9 r\u009b \u00b8\u007f\u00a9\u009c _ \u000el \u001b\u00a5\u00c6\u0007q \u001b& h \u0003\u0093 \u001b\u0004| \u0004\u00bb \u00a1 \u00a4\u00d3 \u00fc t`\u0007\u00a6\u0098 \u00d0\u0093 \u00a6 \u00e2 \u00f2\u00f8\u00cd\u00d9\u00fe\u00a1\u0093 \u00a6s \u001b \u00af \u0093 \u00c9 r\u00d5 \u00aa{\u00a9\u009cr \u001bl \u001b\u00ef r\u00dc \u00bc\u0096 \u00d0 u \u001b& h \u0003, $\u00ed & h \u0003 ,t \u001bo \u001b& h \u0003\u00af \u00b9\u0099 \u00e8\u0000 \u0013e\u0094\u0012\u00dc \u00bc 9 ,s \u001b Q \u00f4\u0014\u00c7\u00af \u00b9\u0099 \u00e8[ \u00fe t_ \u000e \u0016\u0006\u00bd+\u00cb{9 \u0006\u00c3 \u00ba\u0095 \u00b8e\u0094\u0012\u0002 \u0013.\"\u0002 \u00136 \u00a3 \u00a7\u00eb\u0017H\u0081\u00a9\u009c_ \u000e\u00f4\u0014\u00c7\u00b2 D G# Q\u0081\u0015\u0004% i \u0003\u0093 \u00c9 r\u00c1 \u00ba%\u00c1 {9 \bm \u001b\u000e \u0013 ?\u00c2 \u00d2\u00eb\u0017H\u00eb\u0017H\u0002 o\\\u0007\u00a6\u00bd \u00a8Z>\u0006\u00f7&> \u0019 \u0013 \u008d\u000fH: \u00a3 \u00a4$\u00ed \u0093 \u00c9 r\u0083\u0015\u0004# Q,\\ V\u00d5 \u00fc t,7 \u00e1 x\u0093 \u00a7,&\u00f1 u \u001b, $\u00ed ,t \u001bo \u001b\u00af \u00b9\u0099 \u00e8{9 \u0006\u00c3 \u00bae\u0094\u0012 \u0015\u0001 \u0013s \u001b[ \u00fe t\u00af \u00b9 \u0099 \u00e8_ \u000e\u009b \u00b8\u00bd+\u00cb{9 \u0006\u00c3 \u00ba\u0095 \u00b8e\u0094\u0012_ \u00fe vm \u001b\u0002 \u0013 .\"\u00c2 \u00d2\u00eb\u0017H\u00eb\u0017H\u0002 o\u0000 \u0013\u00bd \u00a8Z>\u0006\u00f7&\u008d\u000fH: \u00a3 \u00a4$\u00ed \u0093 \u00c9 r\u0083\u0015\u0004# Q & h \u0003,\\ V\u00d5 \u00fc t& h \u0003,7 \u00e1 x\u0093 \u00a7& h \u0003,&\u00f1 u \u001b& h \u0003,$\u00ed & h \u0003 , t \u001bo \u001b& h \u0003\u00af \u00b9\u0099 \u00e8\u00d7\u0005\u00e6 \u0013\u0001 \u0013{9 \u0006\u00c3 \u00ba\u0095 \u00b8e\u0094\u0012\u0093 \u00a6, s \u001b[ \u00fe t\u00af \u00b9\u0099 \u00e8_ \u000e\u009b \u00b8\u00bd+\u00cb{9 \u0006\u00c3 \u00ba\u0095 \u00b8e\u0094\u0012_ \u00fe vm \u001b \u0002 \u0013. Korean New Kingdom ancient Egyptians mar- velled at their predecessors monu- ments, which were then well over a thousand year old.\u0093 \u00a6@ /\u0092 \u001b\u0004M\u0012g\u00b2 D Gs \u001b|9 \b\u00e0 \u00d4\u0093 \u001b\u0004[ \u00fe t\u0093 \u00c9 r\u009b \u00b8\u007f\u00a9\u009c _ \u000el \u001b\u00a5\u00c6\u0007q \u001b& h \u0003\u0093 \u001b\u0004| \u0004\u00bb \u00a1 \u00a4\u00d3 \u00fc t`\u0007\u00a6\u0098 \u00d0\u0093 \u00a6 \u00e2 \u00f2\u00f8\u00cd\u00d9\u00fe\u00a1\u0093 \u00a6s \u001b \u00af \u0093 \u00c9 r\u00d5 \u00aa{\u00a9\u009cr \u001bl \u001b\u00ef r\u00dc \u00bc\u0096 \u00d0 1000\u00b8\u000e\u0004\u0093 \u00c9 r7 \u00e1 \u00a4y \u001b \u00c5\u0007\u0093 \u00c9 r| \u0004\u00bb \u00a1 \u00a4\u00d3 \u00fc ts \u001b% 3 \u0012_ \u00fe v m \u001b\u0002 \u0013 .\u0092 \u001b\u0004\u0001 \u0013\u0003 \u0013s \u001b|9 \b\u00e0 \u00d4\u0093 \u001b\u0004[ \u00fe t\u0093 \u00c9 r\u0084\u0019\u0004\b \u0013[ \u00fe ts \u001bt \u001b \u0000F K\u0098 \u00d0\u0002 \u0013\u0080 \u008d \u00951,000\u00b8\u000e\u0004s \u001b\u007f\u00a9\u009c\u009a \u00b8A \u0017\u009d)a\u0093 \u00a6 @ /\u00c4 \u00bb& h \u0003`\u0007\u00a6\u0082\u00bd\u00d3\u00bd \u00a8\u0096 \u00d0g \u0000A\u00d9\u00fe\u00a1_ \u00fe vm \u001b\u0002 \u0013 .\u0092 \u001b\u0004\u0001 \u0013\u0003 \u0013s \u001b|9 \b\u00e0 \u00d4\u0093 \u001b\u0004[ \u00fe t\u0093 \u00c9 r\u0084\u0019\u0004\b \u0013[ \u00fe ts \u001bt \u001b \u0000F K\u0098 \u00d0\u0002 \u0013\u0080 \u008d \u00951,000\u00b8\u000e\u0004s \u001b\u007f\u00a9\u009c\u009a \u00b8A \u0017\u009d)a\u0093 \u00a6 @ /\u00c4 \u00bb& h \u0003`\u0007\u00a6\u0082\u00bd\u00d3\u00bd \u00a8\u0096 \u00d0g \u0000A\u00d9\u00fe\u00a1\u0093 \u00a6,\u00d5 \u00aa[ \u00fe t\u0093 \u00c9 r \u00d5 \u00aa \u00af [ \u00fe t`\u0007\u00a6\u0082 \u00c3 \u00d0\u0002 o\u00d9\u00fe\u00a1_ \u00fe vm \u001b\u0002 \u0013 . Korean The U.S. Corps of Engineers esti- mated that 6 inches of rainfall could breach the previously damaged lev- ees.p \u001b\u00b2 D G/ B N#\u00ee @ /\u008d\u000fHr \u001b\u00e7\u00df\u0096{\u00a9\u009c6\u0093 \u001b\u0004u \u001b_ \u000ey\u00a9\u009c \u00c4 \u00ba|\u00be\u00d3s \u001bl \u001b \u0013\u0092<H\u009d)a] j~\u00bd\u00d3`\u0007\u00a6\u00c1 \u00ba -\u00e4 \u00bc wn =\u00c3 \u00bae\u0094\u0012\u0002 \u0013\u0093 \u00a6\u00c6 \u00d2&\u00f1 \u00d9\u00fe\u00a1\u0002 \u0013.p \u001b\u00b2 D G\b'pt \u001bm \u001b# Q\u00d7\u0005\u00e6@ /\u008d\u000fH6\u0093 \u001b\u0004u \u001b_ \u000eq \u001b \u0000 \u0013s \u001b\u0084\u0019\u0004\\ \u0081\u0092<H\u007f\u00a9\u009c\u009d)a] j~\u00bd\u00d3`\u0007\u00a6L :\u0093 \u00a6[ \u00fe t # Q` \u00a6\u00c3 \u00bae\u0094\u0012\u0002 \u0013\u0093 \u00a6\u00c6 \u00d2&\u00f1 \u00d9\u00fe\u00a1_ \u00fe vm \u001b\u0002 \u0013 .p \u001b\u00b2 D G\b'pt \u001bm \u001b# Q\u00d7\u0005\u00e6@ /\u008d\u000fH6\u0093 \u001b\u0004u \u001b_ \u000eq \u001b \u0000 \u0013s \u001b\u0084\u0019\u0004\\ \u0081\u0092<H\u007f\u00a9\u009c\u009d)a] j~\u00bd\u00d3`\u0007\u00a6L :\u0093 \u00a6\u0000 \u0013 \u0096 \u00d0} \u008c \u0095`\u0007\u00a6}9 \u0006\u00c3 \u00bae\u0094\u0012\u0002 \u0013\u0093 \u00a6\u00c6 \u00d2&\u00f1 \u00d9\u00fe\u00a1_ \u00fe vm \u001b \u0002 \u0013. Korean Several large television screens were installed in various places in Rome to let the people watch the ceremony.@ /+\u00feA9 \\\u009aY Uq \u001b\u0084\u0019\u0004\u00db \u00bc\u00df \u00bc\u008f 2;# \u008c Q@ /\u0000 \u0013 \u0096 \u00d0\u0004 \u0013/ B M/ B M\\ \u0081[O\u0006u \u001b\u00f7&# Q\u0006 \u0013| \u00c3 \u00d0[ \u00fe ts \u001b \u0081\u00a9\u009cY Vd\u0094\u0003`\u0007\u00a6\u009b'a| \u00c3 \u00d0\u00bd+\u00c9\u00c3 \u00bae\u0094\u0012\u0095 \u00b82 \u009f \u00a4\u00d9\u00fe\u00a1_ \u00fe v m \u001b\u0002 \u0013 .\u0096 \u00d0\u0004 \u0013\\ \u0081\" f# \u008c Q/ B M\\ \u0081 \u0015@ /\u00f4\u0014\u00c79 \\\u009aY Uq \u001b \u0084 \u0004\u00db \u00bc\u00df \u00bc\u008f 2;s \u001b[O\u0006u \u001b\u00f7&# Qs \u001b[ \u00fe ts \u001b_ \u000ed\u0094\u0003 `\u0007\u00a6r \u001b'\u00f5A\u00bd+\u00c9\u00c3 \u00bae\u0094\u0012> \u0019\u00d9\u00fe\u00a1_ \u00fe vm \u001b\u0002 \u0013 .\u0096 \u00d0\u0004 \u0013\\ \u0081\" f# \u008c Q/ B M\\ \u0081 \u0015@ /\u00f4\u0014\u00c79 \\\u009aY Uq \u001b \u0084 \u0004\u00db \u00bc\u00df \u00bc\u008f 2;s \u001b[O\u0006u",
    "210": "\u001b\u00f7&# Qs \u001b[ \u00fe ts \u001b_ \u000ed\u0094\u0003 `\u0007\u00a6r \u001b'\u00f5A\u00bd+\u00c9\u00c3 \u00bae\u0094\u0012> \u0019K \u0017\u00c5 \u00d2% 3 \u0012_ \u00fe vm \u001b\u0002 \u0013 . Korean Pests can spoil food, cause irritation, or in a worse case cause allergic re- actions, spread venom, or transmit in- fections.K \u0017\u00d8\u0005\u00e6\u0093 \u00c9 r6 \u00a3 \u00a7d\u0094\u0003s \u001b/ \u00fa \u0003> \u0019\u00eb\u00df\u0096[ \u00fe t\u0093 \u00a6% i \u00077 \u00a3 x `\u0007\u00a6\u00c4 \u00bb\u00b51\u00cf \u0013 \u0015\u0001 \u0013 , 8\u0001 \u0013\u009c \u00c9 r \u00e2 \u00c4 \u00ba\u00b7 \u00fa \u0098Y U \u00d8 \u00d4l \u001b\u00ec\u00f8\u00cd6 \u00a3 x`\u0007\u00a6{9 \u0006\u00dc \u00bcv \u001b\u0093 \u00a61 l q`\u0007\u00a6( \u007f\u00e4 \u00bco \u001b \u0015\u0001 \u0013\u0084\u0019\u0004% i \u0007#\u00ee installed in various places in Rome to let the people watch the ceremony.@ /+\u00feA9 \\\u009aY Uq \u001b\u0084\u0019\u0004\u00db \u00bc\u00df \u00bc\u008f 2;# \u008c Q@ /\u0000 \u0013 \u0096 \u00d0\u0004 \u0013/ B M/ B M\\ \u0081[O\u0006u \u001b\u00f7&# Q\u0006 \u0013| \u00c3 \u00d0[ \u00fe ts \u001b \u0081\u00a9\u009cY Vd\u0094\u0003`\u0007\u00a6\u009b'a| \u00c3 \u00d0\u00bd+\u00c9\u00c3 \u00bae\u0094\u0012\u0095 \u00b82 \u009f \u00a4\u00d9\u00fe\u00a1_ \u00fe v m \u001b\u0002 \u0013 .\u0096 \u00d0\u0004 \u0013\\ \u0081\" f# \u008c Q/ B M\\ \u0081 \u0015@ /\u00f4\u0014\u00c79 \\\u009aY Uq \u001b \u0084 \u0004\u00db \u00bc\u00df \u00bc\u008f 2;s \u001b[O\u0006u \u001b\u00f7&# Qs \u001b[ \u00fe ts \u001b_ \u000ed\u0094\u0003 `\u0007\u00a6r \u001b'\u00f5A\u00bd+\u00c9\u00c3 \u00bae\u0094\u0012> \u0019\u00d9\u00fe\u00a1_ \u00fe vm \u001b\u0002 \u0013 .\u0096 \u00d0\u0004 \u0013\\ \u0081\" f# \u008c Q/ B M\\ \u0081 \u0015@ /\u00f4\u0014\u00c79 \\\u009aY Uq \u001b \u0084 \u0004\u00db \u00bc\u00df \u00bc\u008f 2;s \u001b[O\u0006u \u001b\u00f7&# Qs \u001b[ \u00fe ts \u001b_ \u000ed\u0094\u0003 `\u0007\u00a6r \u001b'\u00f5A\u00bd+\u00c9\u00c3 \u00bae\u0094\u0012> \u0019K \u0017\u00c5 \u00d2% 3 \u0012_ \u00fe vm \u001b\u0002 \u0013 . Korean Pests can spoil food, cause irritation, or in a worse case cause allergic re- actions, spread venom, or transmit in- fections.K \u0017\u00d8\u0005\u00e6\u0093 \u00c9 r6 \u00a3 \u00a7d\u0094\u0003s \u001b/ \u00fa \u0003> \u0019\u00eb\u00df\u0096[ \u00fe t\u0093 \u00a6% i \u00077 \u00a3 x `\u0007\u00a6\u00c4 \u00bb\u00b51\u00cf \u0013 \u0015\u0001 \u0013 , 8\u0001 \u0013\u009c \u00c9 r \u00e2 \u00c4 \u00ba\u00b7 \u00fa \u0098Y U \u00d8 \u00d4l \u001b\u00ec\u00f8\u00cd6 \u00a3 x`\u0007\u00a6{9 \u0006\u00dc \u00bcv \u001b\u0093 \u00a61 l q`\u0007\u00a6( \u007f\u00e4 \u00bco \u001b \u0015\u0001 \u0013\u0084\u0019\u0004% i \u0007#\u00ee `\u0007\u00a6` U \u0002 \u00b4\u00c3 \u00bae\u0094\u0012_ \u00fe vm \u001b\u0002 \u0013 .{\u00a9\u009c\u0092 \u001b\u0004\u0093 \u00c9 r\u00b8 \u00fa \u009a\u009c \u00ed\u0000 \u0013d\u0094\u0003\u00ab \u00d1\u00be \u00a1 \u00a7`\u0007\u00a6}\u00a9\u009cu \u001b \u0015\u0001 \u0013 , \b \u0013\u0000F G`\u0007\u00a6\u00c4 \u00bb\u00b51\u00cf\u00bd+\u00c9\u00c3 \u00bae\u0094\u0012 \u0015\u0001 \u0013 ,\u00fej\u0080 \u008c \u0095_ \u000e \u00e2 \u00c4 \u00ba\u00b7 \u00fa \u0098Y U\u00d8 \u00d4l \u001b\u00ec\u00f8\u00cd6 \u00a3 x`\u0007\u00a6\u00c4 \u00bb\u00b51\u00cf \u0013 \u0015\u0001 \u0013 , 1 l q`\u0007\u00a6\u0084\u0019\u0004 \u0013 \u0013 \u0015\u0001 \u0013 ,y \u008c \u0099% i \u0007`\u0007\u00a6\u0084\u0019\u0004 \u0013\u00bd+\u00c9 \u00c3 \u00bae\u0094\u00126 \u00a3 \u00a7`\u0007\u00a6\u00b7 \u00fa \u0098\u0093 \u00a6e\u0094\u0012\u0001 \u0013\u00af \u00b9 ?\u00b8 \u00fa \u009a\u009c \u00ed\u008d\u000fHd\u0094\u0003\u00ab \u00d1\u00be \u00a1 \u00a7`\u0007\u00a6}\u00a9\u009c}9 \u0006\u00c3 \u00bae\u0094\u0012\u0093 \u00a6,\b \u0013 \u0000F G`\u0007\u00a6\u00c4 \u00bb\u00b51\u00cf\u00bd+\u00c9\u00c3 \u00bae\u0094\u0012 \u0015\u0001 \u0013 ,\u00fej\u0080 \u008c \u0095_ \u000e \u00e2 \u00c4 \u00ba\u00b7 \u00fa \u0098Y U\u00d8 \u00d4l \u001b\u00ec\u00f8\u00cd6 \u00a3 x`\u0007\u00a6\u00c4 \u00bb\u00b51\u00cf\u00bd+\u00c9\u00c3 \u00bae\u0094\u0012 \u0093 \u00a6,1 l q`\u0007\u00a6\u0084\u0019\u0004 \u0013\u00bd+\u00c9\u00c3 \u00bae\u0094\u0012 \u0015\u0001 \u0013 ,y \u008c \u0099% i \u0007`\u0007\u00a6 \u0084\u0019\u0004 \u0013\u00bd+\u00c9\u00c3 \u00bae\u0094\u0012_ \u00fe vm \u001b\u0002 \u0013 . Korean It is obvious enough that the world has changed much because of hu- mankind\u2019s scienti\ufb01c and technologi- cal advancements, and problems have become greater because of overpop- ulation and mankind\u2019s extravagant lifestyle.\u0093 \u001b\u0004\u00c0 \u00d3_ \u000e\u00f5\b\u0086<\u00c6l \u001b\u00d5 \u00fc t\u00b51\u00cf\u0084\u0019\u0004\u00dc \u00bc\u0096 \u00d0[ j\u007f\u00a9\u009c s \u001b\u00b4 \u00fa \u00a7s \u001b\u0081\u0016\u0004\u00d9\u00fe\u00a1\u0002 \u0013\u008d\u000fH \u00af \u0093 \u00c9 r\u00d8\u0005\u00e6\u00ec ry \u001b\"\u00ee \u00d1\u00fe\u0098 \u0013\u0093 \u00a6 ,\u00a2 \u00b8\u0093 \u001b\u0004\u00bd \u00a8\u00f5\be\u00e7 \u00f5\b\u0093 \u001b\u0004\u00c0 \u00d3_ \u000e\u0006 \u0013 u \u001b\u00db \u00bc Q\u00ee r\u00d2qt\u0081\u00d6\u00b8~\u00bd\u00d3d\u0094\u0003M :\u00eb\u0017H\\ \u0081\u00eb\u0017H] j\u0000 \u0013 8& \u0015& \u0092 \u0012\u0002 \u0013.\u0093 \u001b\u0004\u00e7\u00df\u0096_ \u000e\u00f5\b\u0086<\u00c6l \u001b\u00d5 \u00fc t\u0094 \u001b\u0004\u0098 \u00d0\u00fc<\u0093 \u001b\u0004\u00e7\u00df\u0096_ \u000e\u00f5\b y \u008c \u0099\u00f4\u0014\u00c7\u00d2qt\u0081\u00d6\u00b8~\u00bd\u00d3d\u0094\u0003\u00dc \u00bc\u0096 \u00d0\u0093 \u001b\u0004K \u0017[ j> \u001a\u0000 \u0013\u007f\u00a9\u009c {\u00a9\u009cy \u001b\u0005 \u00137 % 3 \u0012\u0093 \u00a6,\u0093 \u001b\u0004\u00bd \u00a8\u009c \u00ed\u00f5\b\u00fc<\u00eb\u0017H] j \u0000 \u0013& \u0015& \u0092 \u0012\u0002 \u0013\u008d\u000fH \u00af \u0093 \u00c9 r\u00ec r\"\u00ee \u00bd+\u00cbm \u001b\u0002 \u0013 .\u0093 \u001b\u0004\u00e7\u00df\u0096_ \u000e\u00f5\b\u0086<\u00c6l \u001b\u00d5 \u00fc t\u0094 \u001b\u0004\u0098 \u00d0\u0096 \u00d0\u0093 \u001b\u0004K \u0017[ j> \u001a \u0000 \u0013\u007f\u00a9\u009c{\u00a9\u009cy \u001b\u0005 \u00137 % 3 \u0012\u0093 \u00a6,\u0093 \u001b\u0004\u00bd \u00a8\u009c \u00ed\u00f5\b\u00fc< \u0093 \u001b\u0004\u00e7\u00df\u0096_ \u000e\u00f5\by \u008c \u0099\u00f4\u0014\u00c7\u00d2qt\u0081\u00d6\u00b8~\u00bd\u00d3d\u0094\u0003\u00dc \u00bc\u0096 \u00d0\u0093 \u001b\u0004K \u0017 \u00eb\u0017H] j\u0000 \u0013& \u0015& \u0092 \u00126 \u00a3 \u00a7\u0093 \u00c9 rSX\u0089z\u0002 \u00b4\u00bd+\u00cbm \u001b\u0002 \u0013 . Korean The correlation between brain pathol- ogy and behaviour supports scientists in their research.\u00f6&#\u00ee o \u001b\u00fc<' \u009f\u009d1 l x\u0006 \u0013s \u001b_ \u000e\u007f\u00a9\u009c\u009b'a\u009b'a> \u001a \u0000 \u0013\u00f5\b\u0086<\u00c6\b \u0013[ \u00fe t_ \u000e\u0083\u001a\u0004\u00bd \u00a8\\\u0007\u00a6[\u001av_ \u00fe vm \u001b\u0002 \u0013 .\u00f6&_ \u000e|9 \u0006\u00a88\u008a\u00f5\b' \u009f\u009d1 l x\u00e7\u00df\u0096_ \u000e\u007f\u00a9\u009c\u009b'a\u009b'a> \u001a\u0000 \u0013 \u00f5\b\u0086<\u00c6\b \u0013[ \u00fe t_ \u000e\u0083\u001a\u0004\u00bd \u00a8\\\u0007\u00a6t \u001b\" \u00e9\u00b6\u00f4\u0014\u00c7\u0002 \u0013.\u00f6&_ \u000e|9 \u0006\u00a88\u008a\u00f5\b' \u009f\u009d1 l x\u0006 \u0013s \u001b_ \u000e\u007f\u00a9\u009c\u009b'a\u009b'a > \u001a\u008d\u000fH\u00f5\b\u0086<\u00c6\b \u0013[ \u00fe t_ \u000e\u0083\u001a\u0004\u00bd \u00a8\\\u0007\u00a6t \u001b\" \u00e9\u00b6\u00bd+\u00cbm \u001b \u0002 \u0013. Korean Like some other experts, he is skep- tical about whether diabetes can be cured, noting that these \ufb01ndings have y \u008c \u0099\u00f4\u0014\u00c7\u00d2qt\u0081\u00d6\u00b8~\u00bd\u00d3d\u0094\u0003\u00dc \u00bc\u0096 \u00d0\u0093 \u001b\u0004K \u0017[ j> \u001a\u0000 \u0013\u007f\u00a9\u009c {\u00a9\u009cy \u001b\u0005 \u00137 % 3 \u0012\u0093 \u00a6,\u0093 \u001b\u0004\u00bd \u00a8\u009c \u00ed\u00f5\b\u00fc<\u00eb\u0017H] j \u0000 \u0013& \u0015& \u0092 \u0012\u0002 \u0013\u008d\u000fH \u00af \u0093 \u00c9 r\u00ec r\"\u00ee \u00bd+\u00cbm \u001b\u0002 \u0013 .\u0093 \u001b\u0004\u00e7\u00df\u0096_ \u000e\u00f5\b\u0086<\u00c6l \u001b\u00d5 \u00fc t\u0094 \u001b\u0004\u0098 \u00d0\u0096 \u00d0\u0093 \u001b\u0004K \u0017[ j> \u001a \u0000 \u0013\u007f\u00a9\u009c{\u00a9\u009cy \u001b\u0005 \u00137 % 3 \u0012\u0093 \u00a6,\u0093 \u001b\u0004\u00bd \u00a8\u009c \u00ed\u00f5\b\u00fc< \u0093 \u001b\u0004\u00e7\u00df\u0096_ \u000e\u00f5\by \u008c \u0099\u00f4\u0014\u00c7\u00d2qt\u0081\u00d6\u00b8~\u00bd\u00d3d\u0094\u0003\u00dc \u00bc\u0096 \u00d0\u0093 \u001b\u0004K \u0017 \u00eb\u0017H] j\u0000 \u0013& \u0015& \u0092 \u00126 \u00a3 \u00a7\u0093 \u00c9 rSX\u0089z\u0002 \u00b4\u00bd+\u00cbm \u001b\u0002 \u0013 . Korean The correlation between brain pathol- ogy and behaviour supports scientists in their research.\u00f6&#\u00ee o \u001b\u00fc<' \u009f\u009d1 l x\u0006 \u0013s \u001b_ \u000e\u007f\u00a9\u009c\u009b'a\u009b'a> \u001a \u0000 \u0013\u00f5\b\u0086<\u00c6\b \u0013[ \u00fe t_ \u000e\u0083\u001a\u0004\u00bd \u00a8\\\u0007\u00a6[\u001av_ \u00fe vm \u001b\u0002 \u0013 .\u00f6&_ \u000e|9 \u0006\u00a88\u008a\u00f5\b' \u009f\u009d1 l x\u00e7\u00df\u0096_ \u000e\u007f\u00a9\u009c\u009b'a\u009b'a> \u001a\u0000 \u0013 \u00f5\b\u0086<\u00c6\b \u0013[ \u00fe t_ \u000e\u0083\u001a\u0004\u00bd \u00a8\\\u0007\u00a6t \u001b\" \u00e9\u00b6\u00f4\u0014\u00c7\u0002 \u0013.\u00f6&_ \u000e|9 \u0006\u00a88\u008a\u00f5\b' \u009f\u009d1 l x\u0006 \u0013s \u001b_ \u000e\u007f\u00a9\u009c\u009b'a\u009b'a > \u001a\u008d\u000fH\u00f5\b\u0086<\u00c6\b \u0013[ \u00fe t_ \u000e\u0083\u001a\u0004\u00bd \u00a8\\\u0007\u00a6t \u001b\" \u00e9\u00b6\u00bd+\u00cbm \u001b \u0002 \u0013. Korean Like some other experts, he is skep- tical about whether diabetes can be cured, noting that these \ufb01ndings have no relevance to people who already have Type 1 diabetes.\u0002 \u0013\u008f \u00c9 r\u0084\u0019\u0004\u00eb\u0017H\u0000 \u0013[ \u00fe t\u00f5\b\u0004 \u0013\u00f0\u00f8\u00cd\u0000 \u0013t \u001b\u0096 \u00d0 ,\u00d5 \u00aa \u008d\u000fH{\u00a9\u009c\u0094 \u00b9#\u00ee _ \u000eu \u001b\u00ab \u00d1# \u008c\u00c2 \u00d2\\ \u0081\u0002 r_ \u000e& h \u0003 s \u001b 9 ,s \u001b Q\u00f4\u0014\u00c7 \u0016\u0006\u00f5\b\u008d\u000fH] j1+\u00feA{\u00a9\u009c\u0094 \u00b9#\u00ee \u00a88\u008a\b \u0013\\ \u0081> \u0019\u008d\u000fH\u009b'a\u00ba\u0016\u0004s \u001b\\O 6 \u00a3 \u00a7`\u0007\u00a6t \u001b& h \u0003\u00bd+\u00cb m \u001b\u0002 \u0013 .Y> Y> \u0084\u0019\u0004\u00eb\u0017H\u0000 \u0013[ \u00fe t\u00f5\b\u0004 \u0013\u00f0\u00f8\u00cd\u0000 \u0013t \u001b\u0096 \u00d0 ,\u00d5 \u00aa \u008d\u000fH$ \u007f\u00c5 \u00d2 \u0013u \u001b\u00ab \u00d1\u0000 \u0013{\u00a9\u009c\u0094 \u00b9#\u00ee `\u0007\u00a6\u00a2-a\u0084\u0019\u0004 y \u001bu \u001b\u00ab \u00d1\u00bd+\u00c9\u00c3 \u00bae\u0094\u0012`\u0007\u00a6t \u001b\\ \u0081@ /K \u0017_ \u000e\u00eb\u0017H `\u0007\u00a6\u00b0 \u00fa \u009d\u0093 \u00a6e\u0094\u0012\u00dc \u00bc 9 ,s \u001b Q\u00f4\u0014\u00c7\u0083\u001a\u0004\u00bd \u00a8 \u0016\u0006\u00f5\b \u008d\u000fHs \u001bp \u001b 1+\u00feA{\u00a9\u009c\u0094 \u00b9#\u00ee `\u0007\u00a6\u0000 \u0013t \u001b\u0093 \u00a6e\u0094\u0012\u008d\u000fH \u0006 \u0013| \u00c3 \u00d0[ \u00fe t\u00f5\b\u008d\u000fH\u0084\u0019\u0004) \u0080\u009b'a> \u001a\u0000 \u0013\\O \u0002 \u0013\u0093 \u00a6\u00c5 \u00d2 \u0081\u00a9\u009c\u00f4\u0014\u00c7\u0002 \u0013.\u00d5 \u00aa\u008d\u000fH{9 \u0006\u00c2 \u00d2\u0084\u0019\u0004\u00eb\u0017H\u0000 \u0013[ \u00fe t\u00f5\b\u0004 \u0013\u00f0\u00f8\u00cd\u0000 \u0013t \u001b\u0096 \u00d0 , s \u001b[ \u00fe t\u0083\u001a\u0004\u00bd \u00a8 \u0016\u0006\u00f5\b\u008d\u000fHs \u001bp \u001b 1+\u00feA{\u00a9\u009c\u0094 \u00b9#\u00ee `\u0007\u00a6\u0000 \u0013\u0094 \u001b\u0004\u0006 \u0013| \u00c3 \u00d0[ \u00fe t\\ \u0081> \u0019\u008d\u000fH\u0084\u0019\u0004) \u0080\u009b'a> \u001a \u0000 \u0013\\O \u0002 \u0013\u0093 \u00a6\u00c5 \u00d2\u0081\u00a9\u009c \u0013 9 ,{\u00a9\u009c\u0094 \u00b9#\u00ee s \u001b\u00a2-a \u0084\u0019\u0004y \u001bu \u001b\u00ab \u00d1|\u00a8c\u00c3 \u00bae\u0094\u0012`\u0007\u00a6t \u001b\\ \u0081@ /K \u0017_ \u000e \u00eb\u0017H`\u0007\u00a6\u00b0 \u00fa \u009d\u0093 \u00a6e\u0094\u0012\u0002 \u0013. Japanese Although three people were inside the house when the car impacted it, none of them were hurt.\u009d.Z@g+/3\u00ae \u00e3.?+ \u0004>\u0017 \u0002\u0013\u0001\u00ae/\u0004>\u001bS '\u0017 \u0003\u0013.\u00a3Ro%;+\u009c\u0019(\u00026\u009d \u00e3+Z@\u0017 ( +/\u00e3.? +3\u00ae\u0004>\u0017 \u0002 .\u0006!.\b B\"RvO* # 7(*J> \u0019\u0003B\u0017HM\u0017\u0011L0 \u0002\u0013.\u00a3R\u0004 .\u00a3.4JR<f\u0017* I\u0002H Jo%;I\u0017\u0004J +\u0016 \u0090\b K\u0013(B' >\u0019 \u0003F\b0h. H\u0006+*J>\u0019\u00036\u009d \u00e3+Z @\u0017 ( +/\u0002\u00e3.?+/ 3\u00ae \u0004>\u0017 \u0002).\u00aeB\u0013\u0001R\u0017 * # 7 Japanese 34 per cent of those in the poll share this view, wanting Queen Elizabeth II to be Australia\u2019s last monarch.\u00e3\u0081k\u00c5'/ 34%.\u00ae \u0013.4\u00fa +(\u00ed\u0017& J\u0002\bJ\u00169\u0019 \u00c12\u00e3 \u0019(IJ\u0002\u00d24. \u00ae'\u0002K\u0013(R\u0007S'\u0004>\u0019\u000334%.\u00ae !/\u0002\u0013.\u00fa R9 \u00b2\u0017&\u0002\u0004S\u0010IS) \u00c1\bJ \u00169\u0019 2\u00e3 \u00d24.\u00c1+Rf$\u0013 (R\u0007@\u000334%.\u0002S\u0011 (+\u001b\u00de\u0017 \u00ae !.\u0006!\u0002\u0013.\u00fa R9\u00b2\u0017 & J\u0002\u0004S\u0010IS) \u00c1\bJ \u00169\u0019 2\u00e3 \u0019(IJ\u0002'\u00d2 4.\u00c1+Rf$\u0013(R\u0007S'\u0004 >\u0019\u0003 Japanese The qualities that determine a subcul- ture as distinct may be linguistic, aes- thetic, religious, political, sexual, geo- graphical, or a combination of factors.\u00156 K!C .6^R\u00d9\u00a1b \u0011K\u00ec\u0088/\u0002\u000f;\u0002-\u0002\u00c6H\u0002 \u00eej\u0002\u00ec\u0002(\u0002\u0002> / LI .\u00dcd.e?\u0094O\u001b B\u0017L> \u001bS\u0003\u00156 K!C R6^\u00bf*B. (\u0019KW\u0088/\u0002\u000f;\u00bf\u0002-Q \u00bf\u0002\u00c6H\u00bf\u0002\u00eej\u00bf\u0002\u00ec\u00bf\u0002( \u0002\u00bf\u0002\u0002K\u0004/ LI. \u00dcW. e?\u0094O\u001b'\u0002K\u0013( \u0002J> \u0019\u0003 .H\u0006*W\u0088/\u0002\u00156 K !C R6^\u00bf*B.(\u0019K B\"RvO* # 7(*J> \u0019\u0003B\u0017HM\u0017\u0011L0 \u0002\u0013.\u00a3R\u0004 .\u00a3.4JR<f\u0017* I\u0002H Jo%;I\u0017\u0004J +\u0016 \u0090\b K\u0013(B' >\u0019 \u0003F\b0h. H\u0006+*J>\u0019\u00036\u009d \u00e3+Z @\u0017 ( +/\u0002\u00e3.?+/ 3\u00ae \u0004>\u0017 \u0002).\u00aeB\u0013\u0001R\u0017 * # 7 Japanese 34 per cent of those in the poll share this view, wanting Queen Elizabeth II to be Australia\u2019s last monarch.\u00e3\u0081k\u00c5'/ 34%.\u00ae \u0013.4\u00fa +(\u00ed\u0017& J\u0002\bJ\u00169\u0019 \u00c12\u00e3 \u0019(IJ\u0002\u00d24. \u00ae'\u0002K\u0013(R\u0007S'\u0004>\u0019\u000334%.\u00ae !/\u0002\u0013.\u00fa R9 \u00b2\u0017&\u0002\u0004S\u0010IS) \u00c1\bJ \u00169\u0019 2\u00e3 \u00d24.\u00c1+Rf$\u0013 (R\u0007@\u000334%.\u0002S\u0011 (+\u001b\u00de\u0017 \u00ae !.\u0006!\u0002\u0013.\u00fa R9\u00b2\u0017 & J\u0002\u0004S\u0010IS) \u00c1\bJ \u00169\u0019 2\u00e3 \u0019(IJ\u0002'\u00d2 4.\u00c1+Rf$\u0013(R\u0007S'\u0004 >\u0019\u0003 Japanese The qualities that determine a subcul- ture as distinct may be linguistic, aes- thetic, religious, political, sexual, geo- graphical, or a combination of factors.\u00156 K!C .6^R\u00d9\u00a1b \u0011K\u00ec\u0088/\u0002\u000f;\u0002-\u0002\u00c6H\u0002 \u00eej\u0002\u00ec\u0002(\u0002\u0002> / LI .\u00dcd.e?\u0094O\u001b B\u0017L> \u001bS\u0003\u00156 K!C R6^\u00bf*B. (\u0019KW\u0088/\u0002\u000f;\u00bf\u0002-Q \u00bf\u0002\u00c6H\u00bf\u0002\u00eej\u00bf\u0002\u00ec\u00bf\u0002( \u0002\u00bf\u0002\u0002K\u0004/ LI. \u00dcW. e?\u0094O\u001b'\u0002K\u0013( \u0002J> \u0019\u0003 .H\u0006*W\u0088/\u0002\u00156 K !C R6^\u00bf*B.(\u0019K B.'\u0002K\u00df\u008e\u00ec \u0002J>\u0019\u0003 \u000f;\u00bf\u0002-Q\u00bf\u0002\u00c6H\u00bf\u0002\u00eej \u00bf\u0002\u00ec\u00bf\u0002(\u0002\u00bf\u0002\u0002K\u0004/ LI.\u00dcW.e?\u0094O\u001b'\u0002K \u0013( \u0002J>\u0019\u0003 Japanese New Kingdom ancient Egyptians mar- velled at their predecessors monu- ments, which were then well over a thousand year old.\u0098\u00c1\u009e.\u0014\u00e0\b\u00187(\u00ae ! /\u0002 g(\u0080)hM+\u00eb&IL M\u00a9.B+EAS(+V \u0017 >\u0017 \u0003\u0098\u00c1\u009eg\u00e0.\b\u00187(\u00ae/\u0002 g\u0019'+ 1000\u0080)hB\u0014\u0004M \u00ae.B+EAS(+V \u0017>\u0017 \u0003\u0098\u00c1\u009eg\u00e0.\b\u00187(\u00ae/\u0002 LI.'\u00ae.\u00d3\u00ae g\u0019' +1000\u0080)hB\u0014\u0004\u0013(+V \u0017>\u0017 \u0003 Japanese The U.S. Corps of Engineers esti- mated that 6 inches of rainfall could breach the previously damaged lev- ees.\u00b1V\u00a7\u00dd/\u0002)M+\u00d9\u0010\u0017 \u0012 6\u0004S!.\u008c\u00c6/'\u00d13\u00d9\u0010 \u0019K\u00df\u008e\u00ec \u0002K(\u00c5\u00a1\u0017&\u0004 >\u0019\u0003\u00b1\u009e\bS\u0018+\u0002\u00a7 /\u00026\u0004S! .\u008ch')M+\u00b6\"\u0017 \u0012 \u009c\u00c6\u0019K(\u00c5\u00a1\u0017&\u0004>\u0019\u0003)M+\u00b6\"R\u00bc\u0011 \u0012 \u0002\u00b1 \u009e\bS\u0018+\u0002\u00a7 \u00c5\u00a1\u0019K 6\u0004 S!.\u008ch'\u009c\u00c6\u0019K B\u0017L >\u001bS\u0003 Japanese Several large television screens were installed in various places in Rome to let the people watch the ceremony.M >.?\u000f+/\u0002\u00ae\u0019+w\u00ca R\u00fa&BI \u0006(\u0002\u00e2\u00b0&L3 \u0003s \u0004\u000f$B /\u0015L&\u0004> \u0017 \u0003M >+/\u0015>\u0016>*o\u000f+\u00e2 \u00b0&L3\u0003s /\u0015L\u0002\u00ae\u0019 w\u00caR\u00a8ILKH\u0006+*# \u0003M >+/\u0002\u0015>\u0016>*o\u000f+ \u00e2\u00b0.&L3\u0019\u000fJ S / \u0015L>\u0017 \u0003\u0013L+HJ\u0002\u00ae\u0019 /w\u00caR\u00a8K\u0013( ' KH\u0006 +*J>\u0017 \u0003 Japanese Pests can spoil food, cause irritation, or in a worse case cause allergic re- actions, spread venom, or transmit in- fections.+G/\u008a\u0095R\u00e1\\\u0017+\u0017 J\u0002 \u0018sR2\u0017 J\u0002\u00d2 .o\u0094/ \u0002LK\u000e \u00ee\u00bb.\u0005W+*# J\u0002:R I\u0017 J\u0002\u008d6 LR\u00af \u0019K\u0013( \u0002J>\u0019 \u0003+G/\u008a\u0095R\u00e1\\\u0017+\u0019K\u0013( \u0002J\u0002 F?R\u00e4\u0013\u0019\u0013(B \u0002J\u0002\u0015I+ \u0004o\u0094+/\u0002L K\u000e \u00ee\u00bbRY \u00e4\u0013\u0019\u0013(B \u0002J\u0002:R \u0004 J\u0002\u008d6R\u00d6 \u0096\u0019K\u0013(B\u0002J>\u0019 \u0003+G/\u008a\u0095R\u00b6*\u0006\u0013( \u0002 J\u0002 F?R\u00e4\u0013\u0019\u0013(B\u0002J >\u0019\u0003> \u0002 \u0004o\u0094+/\u0002L K\u000e \u00ee\u00bbRY \u00e4\u0013\u0019\u0013(B \u0002J\u0002:R \u0004 J\u0002\u008d6R\u00d6 \u0096\u0019K\u0013(B\u0002J>\u0019 \u0003 Japanese It is obvious enough that the world has changed much because of hu- mankind\u2019s scienti\ufb01c and technologi- cal advancements, and problems have become greater because of overpop- ulation and mankind\u2019s extravagant lifestyle.\u00aeC.\u00e5Q\u00f0\u00f6.\u00ab\u00cd+H#& \u00e3 /\u00e2 \u000f\u00bcOJ\u0002\u00fcm*\u00ae L({\u00ef*\u00f7a \u0005W'\u008b\u00e5 A\u00e2\u0017 \u0013(/\u00e2\u0097+g\u00c3*_ \u0089'\u0019\u0003\u00aeC.\u00e5Q\u00f0\u00f6.\u00ab\u00cd+H#& \u00e3 /\u00e2 \u000f\u00bc\u00d8\u0017\u0002\u00aeL.\u009a \u00deD\u00ae\u00ad.s \u00bf*\u00f7a\u0019 \u0004 K+H#&\u008b\u00e5/\u00e2 \u000f*#& \u0004K\u0013(/gI \u0003gI '\u0002KH\u0006+ \u0002\u00aeC.\u00e5 Q\u00f0\u00f6.\u00ab\u00cd+HJ\u0002\u00e3 /\u00e2 \u000f\u00bc\u00d8\u0017\u0002\u00aeL\u009a\u00deD\u00ae\u00ad. s \u00bf*\u00f7a\u0019 \u0004K+HJ\u0002 or in a worse case cause allergic re- actions, spread venom, or transmit in- fections.+G/\u008a\u0095R\u00e1\\\u0017+\u0017 J\u0002 \u0018sR2\u0017 J\u0002\u00d2 .o\u0094/ \u0002LK\u000e \u00ee\u00bb.\u0005W+*# J\u0002:R I\u0017 J\u0002\u008d6 LR\u00af \u0019K\u0013( \u0002J>\u0019 \u0003+G/\u008a\u0095R\u00e1\\\u0017+\u0019K\u0013( \u0002J\u0002 F?R\u00e4\u0013\u0019\u0013(B \u0002J\u0002\u0015I+ \u0004o\u0094+/\u0002L K\u000e \u00ee\u00bbRY \u00e4\u0013\u0019\u0013(B \u0002J\u0002:R \u0004 J\u0002\u008d6R\u00d6",
    "211": "\u0096\u0019K\u0013(B\u0002J>\u0019 \u0003+G/\u008a\u0095R\u00b6*\u0006\u0013( \u0002 J\u0002 F?R\u00e4\u0013\u0019\u0013(B\u0002J >\u0019\u0003> \u0002 \u0004o\u0094+/\u0002L K\u000e \u00ee\u00bbRY \u00e4\u0013\u0019\u0013(B \u0002J\u0002:R \u0004 J\u0002\u008d6R\u00d6 \u0096\u0019K\u0013(B\u0002J>\u0019 \u0003 Japanese It is obvious enough that the world has changed much because of hu- mankind\u2019s scienti\ufb01c and technologi- cal advancements, and problems have become greater because of overpop- ulation and mankind\u2019s extravagant lifestyle.\u00aeC.\u00e5Q\u00f0\u00f6.\u00ab\u00cd+H#& \u00e3 /\u00e2 \u000f\u00bcOJ\u0002\u00fcm*\u00ae L({\u00ef*\u00f7a \u0005W'\u008b\u00e5 A\u00e2\u0017 \u0013(/\u00e2\u0097+g\u00c3*_ \u0089'\u0019\u0003\u00aeC.\u00e5Q\u00f0\u00f6.\u00ab\u00cd+H#& \u00e3 /\u00e2 \u000f\u00bc\u00d8\u0017\u0002\u00aeL.\u009a \u00deD\u00ae\u00ad.s \u00bf*\u00f7a\u0019 \u0004 K+H#&\u008b\u00e5/\u00e2 \u000f*#& \u0004K\u0013(/gI \u0003gI '\u0002KH\u0006+ \u0002\u00aeC.\u00e5 Q\u00f0\u00f6.\u00ab\u00cd+HJ\u0002\u00e3 /\u00e2 \u000f\u00bc\u00d8\u0017\u0002\u00aeL\u009a\u00deD\u00ae\u00ad. s \u00bf*\u00f7a\u0019 \u0004K+HJ\u0002 \u008b\u00e5/HJ\u00e2 \u000f*#&\u0004K\u0003 Japanese The correlation between brain pathol- ogy and behaviour supports scientists in their research.\u008fO\u0002(\u0081#.\u0087\u00af\u00af\u00a9/\u0002\u00e5 Q\u009b !.\u00f5\u001bR\u0005b\u0011KB. '\u0019\u0003\u008f.O\u0002Q(\u0081#.\u00afc/\u0002\u00e5 Q\u009b !.\u00f5\u001bRB\u00a4\u0019K\u0003\u008f.O\u0002Q(\u0081#.\u00afc/\u0002\u00e5 Q\u009b !.\u00f5\u001bRB\u00a4\u0019K Japanese Like some other experts, he is skep- tical about whether diabetes can be cured, noting that these \ufb01ndings have no relevance to people who already have Type 1 diabetes.\u00b9.+\u008e\u00e3 !($\u00d2+\u0002\u000f/ \u0011sOR\u0087j' K )\u0006 + $\u0004&/\u0016\u00f4\u00bf'\u0002J\u0002\u0013LI .k\u00c5\u00dc\u00e7/\u0019'+ I\u00b0\u0011sOR \u008c#&\u0004K\u00ae\u0019+/\u00da.\u00afc\u00ec B*\u0004\u0013(RA\u00bb\u0017&\u0004>\u0019\u0003o%;+&\u009c\u0019K(\u0002\u00b9.+\u008e \u00e3.H\u0006+\u0002\u000f/\u0011sO j\u001b K )\u0006 +$\u0004& \u0016\u00f4\u00bf'\u0002 J\u0002\u0013LI.\u00de\u00fa/\u00d2+1\u00b0.\u0011 sORf$\u00ae\u0019+/\u00af\u00a9 *\u0004 (\u00f79&\u0004K\u0003o%;+&\u009c\u0019K(\u0002\u00b9.+\u008e \u00e3.H\u0006+\u0002\u000f/\u0011sO \u0087j ' K )\u0006 +$\u0004& \u0016\u00f4\u00bf '\u0002J\u0002\u0013LI.\u00de\u00fa/\u00d2+1\u00b0 .\u0011sORf$\u00ae\u0019+/\\\u00af\u00a9 (\u00f79&\u0004K\u0003 French Although three people were inside the house when the car impacted it, none of them were hurt.Bien que trois personnes aient \u00e9t\u00e9 pr\u00e9sentes dans la maison quand la voiture l\u2019a percut\u00e9e, aucune n\u2019a \u00e9t\u00e9 bless\u00e9e.Bien que trois personnes se trouvaient \u00e0 l\u2019int\u00e9rieur de la maison lorsque la voiture a percut\u00e9 celle-ci, aucune d\u2019entre elles n\u2019a \u00e9t\u00e9 bless\u00e9e.M\u00eame si trois personnes \u00e9taient \u00e0 l\u2019int\u00e9rieur de la maison lorsque la voiture a heurt\u00e9 celle-ci, aucune d\u2019entre elles n\u2019a \u00e9t\u00e9 bless\u00e9e. French 34 per cent of those in the poll share this view, wanting Queen Elizabeth II to be Australia\u2019s last monarch.34 % des personnes interrog\u00e9es parta- gent ce point de vue, et souhaitent que la reine Elizabeth II soit la derni\u00e8re monarque \u00e0 r\u00e9gner sur l\u2019Australie.34 pour cent de ceux qui ont particip\u00e9 \u00e0 l\u2019enqu\u00eate partagent cet avis, souhai- tant que la reine Elizabeth II soit la derni\u00e8re monarchie d\u2019Australie34 pour cent de ceux qui ont particip\u00e9 \u00e0 l\u2019enqu\u00eate souhaitent que la reine Elizabeth II soit la derni\u00e8re monarchie d\u2019Australie. French The qualities that determine a subcul- la voiture a percut\u00e9 celle-ci, aucune d\u2019entre elles n\u2019a \u00e9t\u00e9 bless\u00e9e.M\u00eame si trois personnes \u00e9taient \u00e0 l\u2019int\u00e9rieur de la maison lorsque la voiture a heurt\u00e9 celle-ci, aucune d\u2019entre elles n\u2019a \u00e9t\u00e9 bless\u00e9e. French 34 per cent of those in the poll share this view, wanting Queen Elizabeth II to be Australia\u2019s last monarch.34 % des personnes interrog\u00e9es parta- gent ce point de vue, et souhaitent que la reine Elizabeth II soit la derni\u00e8re monarque \u00e0 r\u00e9gner sur l\u2019Australie.34 pour cent de ceux qui ont particip\u00e9 \u00e0 l\u2019enqu\u00eate partagent cet avis, souhai- tant que la reine Elizabeth II soit la derni\u00e8re monarchie d\u2019Australie34 pour cent de ceux qui ont particip\u00e9 \u00e0 l\u2019enqu\u00eate souhaitent que la reine Elizabeth II soit la derni\u00e8re monarchie d\u2019Australie. French The qualities that determine a subcul- ture as distinct may be linguistic, aes- thetic, religious, political, sexual, geo- graphical, or a combination of factors.Les crit\u00e8res qui d\u00e9terminent une sous- culture comme distincte peuvent \u00eatre linguistiques, esth\u00e9tiques, religieux, politiques, sexuels, g\u00e9ographiques ou une combinaison de facteurs.Les qualit\u00e9s qui d\u00e9terminent une sous- culture comme distincte peuvent \u00eatre linguistiques, esth\u00e9tiques, religieuses, politiques, sexuelles, g\u00e9ographiques ou une combinaison de facteurs.\"Les caract\u00e9ristiques qui d\u00e9terminent une sous-culture comme distincte peu- vent \u00eatre linguistiques, esth\u00e9tiques, re- ligieuses, politiques, sexuelles, g\u00e9o- graphiques ou une combinaison de ces \u00e9l\u00e9ments. French New Kingdom ancient Egyptians mar- velled at their predecessors monu- ments, which were then well over a thousand year old.Les \u00c9gyptiens antiques du Nouvel Empire \u00e9taient \u00e9merveill\u00e9s par les monuments de leurs pr\u00e9d\u00e9cesseurs, qui avaient alors bien plus de mille ans.Les \u00c9gyptiens anciens de la Nouvelle- R\u00e9publique admiraient les monu- ments de leurs pr\u00e9d\u00e9cesseurs, qui \u00e9taient alors bien plus \u00e2g\u00e9s d\u2019un mil- l\u00e9naire.Les \u00c9gyptiens anciens de la Nouvelle- R\u00e9publique admiraient les monu- ments de leurs pr\u00e9d\u00e9cesseurs, qui avaient alors plus de mille ans. French The U.S. Corps of Engineers esti- mated that 6 inches of rainfall could breach the previously damaged lev- ees.D\u2019apr\u00e8s le U.S. Corps of Engineers, une pluviom\u00e9trie de 6 pouces pour- rait ouvrir une br\u00e8che dans les digues pr\u00e9c\u00e9demment endommag\u00e9es.Le Corps des ing\u00e9nieurs des \u00c9tats- Unis a estim\u00e9 qu\u2019une pluie de 15 cm pourrait affaiblir les digues pr\u00e9c\u00e9dem- ment endommag\u00e9es.Le Corps des ing\u00e9nieurs des \u00c9tats- Unis a estim\u00e9 qu\u2019une pluie de 15 qui avaient alors bien plus de mille ans.Les \u00c9gyptiens anciens de la Nouvelle- R\u00e9publique admiraient les monu- ments de leurs pr\u00e9d\u00e9cesseurs, qui \u00e9taient alors bien plus \u00e2g\u00e9s d\u2019un mil- l\u00e9naire.Les \u00c9gyptiens anciens de la Nouvelle- R\u00e9publique admiraient les monu- ments de leurs pr\u00e9d\u00e9cesseurs, qui avaient alors plus de mille ans. French The U.S. Corps of Engineers esti- mated that 6 inches of rainfall could breach the previously damaged lev- ees.D\u2019apr\u00e8s le U.S. Corps of Engineers, une pluviom\u00e9trie de 6 pouces pour- rait ouvrir une br\u00e8che dans les digues pr\u00e9c\u00e9demment endommag\u00e9es.Le Corps des ing\u00e9nieurs des \u00c9tats- Unis a estim\u00e9 qu\u2019une pluie de 15 cm pourrait affaiblir les digues pr\u00e9c\u00e9dem- ment endommag\u00e9es.Le Corps des ing\u00e9nieurs des \u00c9tats- Unis a estim\u00e9 qu\u2019une pluie de 15 cm pourrait mettre \u00e0 mal les digues pr\u00e9c\u00e9demment endommag\u00e9es. French Several large television screens were installed in various places in Rome to let the people watch the ceremony.Plusieurs grands \u00e9crans de t\u00e9l\u00e9vision ont \u00e9t\u00e9 install\u00e9s \u00e0 diff\u00e9rents endroits \u00e0 Rome pour permettre aux gens de regarder la c\u00e9r\u00e9monie.Plusieurs grands \u00e9crans de t\u00e9l\u00e9vision ont \u00e9t\u00e9 install\u00e9s dans diff\u00e9rents en- droits de Rome pour permettre aux gens de regarder la c\u00e9r\u00e9monie.Plusieurs grands \u00e9crans de t\u00e9l\u00e9vision ont \u00e9t\u00e9 install\u00e9s dans diff\u00e9rents en- droits de Rome a\ufb01n de permettre aux gens de regarder la c\u00e9r\u00e9monie. French Pests can spoil food, cause irritation, or in a worse case cause allergic re- actions, spread venom, or transmit in- fections.Les parasites peuvent contaminer la nourriture, provoquer des irritations ou, dans les cas les plus graves, provo- quer des r\u00e9actions allergiques, r\u00e9pan- dre du venin ou transmettre des infec- tions.Les nuisibles peuvent g\u00e2cher les ali- ments, provoquer des irritations, ou dans le pire des cas provoquer des r\u00e9actions allergiques, propager du venin ou transmettre des infections.Les nuisibles peuvent alt\u00e9rer les ali- ments, causer des irritations, ou dans les cas les plus graves provoquer des r\u00e9actions allergiques, r\u00e9pandre du venin ou transmettre des infections. French It is obvious enough that the world has changed much because of hu- mankind\u2019s scienti\ufb01c and technologi- cal advancements, and problems have become greater because of overpop- ulation and mankind\u2019s extravagant lifestyle.Il est clair que le monde a beau- coup chang\u00e9 gr\u00e2ce aux progr\u00e8s sci- enti\ufb01ques et technologiques r\u00e9alis\u00e9s par les \u00eatres humains, et que les prob- l\u00e8mes sont devenus plus importants en tions.Les nuisibles peuvent g\u00e2cher les ali- ments, provoquer des irritations, ou dans le pire des cas provoquer des r\u00e9actions allergiques, propager du venin ou transmettre des infections.Les nuisibles peuvent alt\u00e9rer les ali- ments, causer des irritations, ou dans les cas les plus graves provoquer des r\u00e9actions allergiques, r\u00e9pandre du venin ou transmettre des infections. French It is obvious enough that the world has changed much because of hu- mankind\u2019s scienti\ufb01c and technologi- cal advancements, and problems have become greater because of overpop- ulation and mankind\u2019s extravagant lifestyle.Il est clair que le monde a beau- coup chang\u00e9 gr\u00e2ce aux progr\u00e8s sci- enti\ufb01ques et technologiques r\u00e9alis\u00e9s par les \u00eatres humains, et que les prob- l\u00e8mes sont devenus plus",
    "212": "importants en raison de la surpopulation et du mode de vie extravagant de l\u2019humanit\u00e9.Il est assez \u00e9vident que le monde a beaucoup chang\u00e9 en raison des avanc\u00e9es scienti\ufb01ques et tech- nologiques de l\u2019humanit\u00e9 et que les probl\u00e8mes se sont accrus en raison de la surpopulation et du mode de vie extravagant de l\u2019homme.Il est \u00e9vident que le monde a beau- coup chang\u00e9 en raison des avanc\u00e9es scienti\ufb01ques et technologiques de l\u2019humanit\u00e9, et que les probl\u00e8mes ont augment\u00e9 \u00e0 cause de la surpopula- tion et du mode de vie extravagant de l\u2019homme. French The correlation between brain pathol- ogy and behaviour supports scientists in their research.La corr\u00e9lation entre la pathologie du cerveau et le comportement sou- tient les scienti\ufb01ques dans leurs recherches.La corr\u00e9lation entre la pathologie c\u00e9r\u00e9brale et le comportement soutient les scienti\ufb01ques dans leur recherche.La corr\u00e9lation entre la pathologie c\u00e9r\u00e9brale et le comportement sou- tient les scienti\ufb01ques dans leurs recherches. French Like some other experts, he is skep- tical about whether diabetes can be cured, noting that these \ufb01ndings have no relevance to people who already have Type 1 diabetes.\u00c0 l\u2019instar d\u2019autres experts, il se mon- tre sceptique quant \u00e0 la possibilit\u00e9 de gu\u00e9rir le diab\u00e8te, faisant remarquer que ces r\u00e9sultats ne sont pas applica- bles aux personnes qui souffrent d\u00e9j\u00e0 de diab\u00e8te de type 1.Comme d\u2019autres experts, il est scep- tique quant \u00e0 la possibilit\u00e9 de gu\u00e9rir du diab\u00e8te, notant que ces r\u00e9sultats n\u2019ont aucune pertinence pour les per- sonnes atteintes de diab\u00e8te de type 1.Tout comme certains autres experts, il est sceptique quant \u00e0 la possibilit\u00e9 de gu\u00e9rir du diab\u00e8te, et souligne que ces r\u00e9sultats n\u2019ont aucune pertinence pour tient les scienti\ufb01ques dans leurs recherches. French Like some other experts, he is skep- tical about whether diabetes can be cured, noting that these \ufb01ndings have no relevance to people who already have Type 1 diabetes.\u00c0 l\u2019instar d\u2019autres experts, il se mon- tre sceptique quant \u00e0 la possibilit\u00e9 de gu\u00e9rir le diab\u00e8te, faisant remarquer que ces r\u00e9sultats ne sont pas applica- bles aux personnes qui souffrent d\u00e9j\u00e0 de diab\u00e8te de type 1.Comme d\u2019autres experts, il est scep- tique quant \u00e0 la possibilit\u00e9 de gu\u00e9rir du diab\u00e8te, notant que ces r\u00e9sultats n\u2019ont aucune pertinence pour les per- sonnes atteintes de diab\u00e8te de type 1.Tout comme certains autres experts, il est sceptique quant \u00e0 la possibilit\u00e9 de gu\u00e9rir du diab\u00e8te, et souligne que ces r\u00e9sultats n\u2019ont aucune pertinence pour les personnes atteintes de diab\u00e8te de type 1. Indonesian Although three people were inside the house when the car impacted it, none of them were hurt.Walau ada tiga orang di dalam rumah yang ditabrak mobil, tak satu pun yang cedera.Meskipun tiga orang ada di dalam rumah saat mobil menabraknya, tidak ada yang terluka.Meskipun tiga orang ada di dalam rumah saat mobil menabraknya, mereka semua selamat. Indonesian 34 per cent of those in the poll share this view, wanting Queen Elizabeth II to be Australia\u2019s last monarch.Dalam jajak pendapat mengenai pan- dangan ini, 34 persen dari mereka menginginkan Ratu Elizabeth II men- jadi pemimpin monarki terakhir Aus- tralia.34 persen dari orang yang terlibat dalam polling ini memiliki pandangan yang sama, yaitu ingin Ratu Elizabeth II menjadi ratu terakhir di Australia.34 persen dari orang yang terlibat dalam polling ini memiliki pandangan yang sama, yaitu ingin Ratu Elizabeth II menjadi ratu terakhir di Australia. Indonesian The qualities that determine a subcul- ture as distinct may be linguistic, aes- thetic, religious, political, sexual, geo- graphical, or a combination of factors.Kualitas yang menentukan suatu sub- kultur berbeda dari yang lain bisa berupa bahasa, estetika, agama, poli- tik, seksual, geogra\ufb01s, atau kombinasi sejumlah faktor.Kualitas yang menentukan suatu sub- kultur sebagai distinct dapat berupa linguistik, estetika, agama, politik, seksual, geogra\ufb01s, atau kombinasi faktor-faktor tersebut.Faktor-faktor yang menentukan se- buah subkultur sebagai distinct bisa berupa faktor linguistik, estetika, agama, politik, seksual, geogra\ufb01s, atau kombinasi dari beberapa faktor tersebut. Indonesian New Kingdom ancient Egyptians mar- II menjadi ratu terakhir di Australia. Indonesian The qualities that determine a subcul- ture as distinct may be linguistic, aes- thetic, religious, political, sexual, geo- graphical, or a combination of factors.Kualitas yang menentukan suatu sub- kultur berbeda dari yang lain bisa berupa bahasa, estetika, agama, poli- tik, seksual, geogra\ufb01s, atau kombinasi sejumlah faktor.Kualitas yang menentukan suatu sub- kultur sebagai distinct dapat berupa linguistik, estetika, agama, politik, seksual, geogra\ufb01s, atau kombinasi faktor-faktor tersebut.Faktor-faktor yang menentukan se- buah subkultur sebagai distinct bisa berupa faktor linguistik, estetika, agama, politik, seksual, geogra\ufb01s, atau kombinasi dari beberapa faktor tersebut. Indonesian New Kingdom ancient Egyptians mar- velled at their predecessors monu- ments, which were then well over a thousand year old.Kerajaan Baru Mesir kuno men- gagumi monumen para pendahulunya, yang saat itu berusia lebih dari seribu tahun.Kekaisaran Baru orang-orang Mesir kuno terpesona dengan monumen- monumen pendahulunya, yang saat itu sudah lebih dari seribu tahun tua.Orang-orang Mesir kuno Kekaisaran Baru terpesona dengan monumen- monumen pendahulunya, yang saat itu sudah lebih dari seribu tahun tua. Indonesian The U.S. Corps of Engineers esti- mated that 6 inches of rainfall could breach the previously damaged lev- ees.U.S. Corps of Engineers memperki- rakan bahwa curah hujan sebesar 6 inci ini mampu menerobos tanggul yang sudah rusak.Korps Engineer AS menghitung bahwa hujan 6 inci dapat merusak parit yang sebelumnya sudah rusak.Badan Engineer AS memperkirakan bahwa hujan 6 inci dapat menembus parit yang sebelumnya sudah rusak. Indonesian Several large television screens were installed in various places in Rome to let the people watch the ceremony.Beberapa layar televisi besar dipasang di berbagai tempat di Roma supaya orang-orang bisa menyaksikan up- acara tersebut.Beberapa layar televisi besar diin- stal di berbagai tempat di Roma agar orang-orang dapat menonton upacara.Berbagai layar televisi besar diinstal di tempat-tempat yang berbeda di Roma agar masyarakat dapat menon- ton upacara tersebut. Indonesian Pests can spoil food, cause irritation, or in a worse case cause allergic re- actions, spread venom, or transmit in- fections.Hama dapat merusak makanan, men- gakibatkan iritasi, atau dalam ka- sus yang lebih parah, mengakibatkan reaksi alergi, menyebarkan racun atau menularkan penyakit.Hama dapat merusak makanan, installed in various places in Rome to let the people watch the ceremony.Beberapa layar televisi besar dipasang di berbagai tempat di Roma supaya orang-orang bisa menyaksikan up- acara tersebut.Beberapa layar televisi besar diin- stal di berbagai tempat di Roma agar orang-orang dapat menonton upacara.Berbagai layar televisi besar diinstal di tempat-tempat yang berbeda di Roma agar masyarakat dapat menon- ton upacara tersebut. Indonesian Pests can spoil food, cause irritation, or in a worse case cause allergic re- actions, spread venom, or transmit in- fections.Hama dapat merusak makanan, men- gakibatkan iritasi, atau dalam ka- sus yang lebih parah, mengakibatkan reaksi alergi, menyebarkan racun atau menularkan penyakit.Hama dapat merusak makanan, menyebabkan iritasi, atau dalam keadaan yang lebih buruk menye- babkan reaksi alergi, menyebarkan racun, atau menularkan infeksi.Hama dapat merusak makanan, menyebabkan iritasi, atau dalam keadaan yang lebih buruk bahkan dapat menyebabkan reaksi alergi, menyebarkan racun, atau menularkan infeksi. Indonesian It is obvious enough that the world has changed much because of hu- mankind\u2019s scienti\ufb01c and technologi- cal advancements, and problems have become greater because of overpop- ulation and mankind\u2019s extravagant lifestyle.Cukup jelas bahwa dunia",
    "213": "berkem- bang karena ilmu pengetahuan dan teknologi, serta kelebihan jumlah pen- duduk dan gaya hidup boros manusia menjadi masalah yang cukup besar.Jelas sekali bahwa dunia telah berubah banyak karena kemajuan ilmiah dan teknologi manusia, dan masalah telah menjadi lebih besar karena kelebihan populasi dan gaya hidup manusia yang berlebihan.Sudah jelas sekali bahwa dunia telah berubah sangat banyak karena kema- juan ilmiah dan teknologi manusia, dan masalah telah menjadi lebih besar karena kelebihan populasi dan gaya hidup manusia yang berlebihan. Indonesian The correlation between brain pathol- ogy and behaviour supports scientists in their research.Korelasi antara patologi otak dan peri- laku mendukung para ilmuwan dalam penelitian mereka.Korelasi antara patologi otak dan per- ilaku membantu para ilmuwan dalam penelitian mereka.Korelasi antara patologi otak dan tingkah laku membantu para ilmuwan dalam penelitian mereka. Indonesian Like some other experts, he is skep- tical about whether diabetes can be cured, noting that these \ufb01ndings have no relevance to people who already have Type 1 diabetes.Seperti halnya ahli-ahli lain, dia merasa skeptis apakah diabetes da- juan ilmiah dan teknologi manusia, dan masalah telah menjadi lebih besar karena kelebihan populasi dan gaya hidup manusia yang berlebihan. Indonesian The correlation between brain pathol- ogy and behaviour supports scientists in their research.Korelasi antara patologi otak dan peri- laku mendukung para ilmuwan dalam penelitian mereka.Korelasi antara patologi otak dan per- ilaku membantu para ilmuwan dalam penelitian mereka.Korelasi antara patologi otak dan tingkah laku membantu para ilmuwan dalam penelitian mereka. Indonesian Like some other experts, he is skep- tical about whether diabetes can be cured, noting that these \ufb01ndings have no relevance to people who already have Type 1 diabetes.Seperti halnya ahli-ahli lain, dia merasa skeptis apakah diabetes da- pat disembuhkan, menegaskan bahwa penemuan-penemuan ini tidak relevan dengan orang-orang yang telah mengi- dap diabetes Jenis 1.Sama seperti beberapa ahli lainnya, ia ragu-ragu tentang apakah diabetes dapat disembuhkan, mencatat bahwa temuan-temuan ini tidak relevan bagi orang-orang yang sudah menderita di- abetes tipe 1.Seperti beberapa ahli lainnya, ia mer- agukan apakah diabetes dapat dis- embuhkan, mencatat bahwa temuan- temuan ini tidak relevan bagi orang- orang yang sudah menderita diabetes tipe 1. Javanese Although three people were inside the house when the car impacted it, none of them were hurt.Sanadyan uwong telu ana ing njero omah nalika mobil kuwi nabrak, nang- ing kabeh selamet.Walaupun tiga orang berada di dalam rumah saat mobil menabraknya, tidak ada di antara mereka yang terluka.Walaupun ana ing tiga wong ing rumah kala mobil mbentak ing kono, ora ana saka ingkang terluka. Javanese 34 per cent of those in the poll share this view, wanting Queen Elizabeth II to be Australia\u2019s last monarch.34 per sen sing menehi swara se- sawangane ngene, kepengin Ratu Eliz- abeth II dadi raja sing keri dhewe ana ning Australia.Sebagian 34 persen dari mereka yang terlibat dalam polling ini berpenda- pat demikian, ingin Ratu Elizabeth II menjadi Ratu Australia terakhir.Sebagian 34 persen dari mereka yang terlibat dalam polling ini memi- liki pandangan yang sama, yaitu menginginkan Ratu Elizabeth II men- jadi Ratu Australia terakhir. Javanese The qualities that determine a subcul- ture as distinct may be linguistic, aes- thetic, religious, political, sexual, geo- graphical, or a combination of factors.Kualitas sing nemtokake sawijin- ing cabang kabudayan mbokmenawa basa, estetika, sing ana gayutane this view, wanting Queen Elizabeth II to be Australia\u2019s last monarch.34 per sen sing menehi swara se- sawangane ngene, kepengin Ratu Eliz- abeth II dadi raja sing keri dhewe ana ning Australia.Sebagian 34 persen dari mereka yang terlibat dalam polling ini berpenda- pat demikian, ingin Ratu Elizabeth II menjadi Ratu Australia terakhir.Sebagian 34 persen dari mereka yang terlibat dalam polling ini memi- liki pandangan yang sama, yaitu menginginkan Ratu Elizabeth II men- jadi Ratu Australia terakhir. Javanese The qualities that determine a subcul- ture as distinct may be linguistic, aes- thetic, religious, political, sexual, geo- graphical, or a combination of factors.Kualitas sing nemtokake sawijin- ing cabang kabudayan mbokmenawa basa, estetika, sing ana gayutane karo agama, politik, seksual, ge- ogra\ufb01s, utawa sawijining kombinasi saka faktor-faktor.Sifat-sifat yang menentukan subkul- tur sebagai yang terpisah bisa berupa linguistik, estetik, agama, politik, sek- sual, geogra\ufb01s, atau kombinasi faktor- faktor.Kualitas-kualitas sing menentukan subkultur minangka distinct bisa ngandhut linguistik, estetik, agama, politik, seksual, geogra\ufb01s, utawa gabungan faktor-faktor. Javanese New Kingdom ancient Egyptians mar- velled at their predecessors monu- ments, which were then well over a thousand year old.Krajan Anyar Mesir kuno nggumun marang monumen-monumen leluhure, sing nalika iku tetep apik luwih seka sewu taun.Rakyat Kerajaan Baru dari Mesir Kuno terpesona oleh monumen- monumen pendahulunya, yang saat itu sudah lebih dari seribu tahun tua.Rakyat Kerajaan Baru dari Mesir Kuno terpesona oleh monumen- monumen pendahulunya, yang saat itu sudah lebih dari seribu tahun tua. Javanese The U.S. Corps of Engineers esti- mated that 6 inches of rainfall could breach the previously damaged lev- ees.Korps Insinyur AS ngira manawa udan 6 inci bisa nembus tanggul sing rusak sadurunge.Korps Insinyur Amerika Serikat menghitung bahwa hujan 6 inci bisa merusak parit yang sebelumnya rusak.Korps Insinyur Amerika Serikat menaksir bahwa hujan 6 inci bisa merusak parit yang telah rusak se- belumnya. Javanese Several large television screens were installed in various places in Rome to let the people watch the ceremony.Sawetara layar TV gedhe dipasang ing sawetara panggonan ing Roma su- paya masyarakat bisa nonton upacara kasebut.Beberapa layar televisi besar dipasang di berbagai tempat di Roma agar orang-orang dapat menonton upacara.Beberapa layar televisi besar dipasang Javanese The U.S. Corps of Engineers esti- mated that 6 inches of rainfall could breach the previously damaged lev- ees.Korps Insinyur AS ngira manawa udan 6 inci bisa nembus tanggul sing rusak sadurunge.Korps Insinyur Amerika Serikat menghitung bahwa hujan 6 inci bisa merusak parit yang sebelumnya rusak.Korps Insinyur Amerika Serikat menaksir bahwa hujan 6 inci bisa merusak parit yang telah rusak se- belumnya. Javanese Several large television screens were installed in various places in Rome to let the people watch the ceremony.Sawetara layar TV gedhe dipasang ing sawetara panggonan ing Roma su- paya masyarakat bisa nonton upacara kasebut.Beberapa layar televisi besar dipasang di berbagai tempat di Roma agar orang-orang dapat menonton upacara.Beberapa layar televisi besar dipasang di beberapa tempat di Roma supaya orang-orang dapat menyaksikan up- acara itu. Javanese Pests can spoil food, cause irritation, or in a worse case cause allergic re- actions, spread venom, or transmit in- fections.Ama bisa ngrusak panganan, nye- babake iritasi, utawa ing kasus sing paling parah nyebabake reaksi alergi, nyebar racun, utawa ngirim infeksi.Hama bisa merusak makanan, menye- babkan iritasi, atau dalam kasus yang lebih buruk menyebabkan reaksi alergi, menyebarkan racun, atau menularkan infeksi.Hama bisa merusak makanan, menye- babkan iritasi, atau dalam keadaan yang lebih buruk menyebabkan reaksi alergi, menyebarkan racun, atau menularkan infeksi. Javanese It is obvious enough that the world has changed much because of hu- mankind\u2019s scienti\ufb01c and technologi- cal advancements, and problems have become greater because of overpop- ulation and mankind\u2019s extravagant lifestyle.Cukup jelas menawa donya wis malih akeh amarga majune ilmu lan teknologi manungsa, lan masalah- masalah dadi tambah gedhe amarga kaluwihan populasi",
    "214": "lan gaya urip sing boros saka manungsa.Sawetara bisa dilihat manawa dunya wis duw\u00e9 sak kab\u00e8h pangowahan sain-teknologi manungsa lan masalah duw\u00e9 gedh\u00e9 luwih amerga tambah- tambah\u00e9 jumlah penduduk lan gaya hidup manungsa sing luwih mewah.Sawetara bisa dilihat manawa dunya wis duw\u00e9 sak kab\u00e8h pangowahan sain lan teknologi saka manungsa, lan masalah-masalah duw\u00e9 gedh\u00e9 luwih amerga tambah-tambah\u00e9 jumlah pen- duduk lan gaya hidup manungsa sing luwih mewah Javanese The correlation between brain pathol- ogy and behaviour supports scientists in their research.Sesambungan antarane elmu lelara utek lan tumindak ndhukung para il- lifestyle.Cukup jelas menawa donya wis malih akeh amarga majune ilmu lan teknologi manungsa, lan masalah- masalah dadi tambah gedhe amarga kaluwihan populasi lan gaya urip sing boros saka manungsa.Sawetara bisa dilihat manawa dunya wis duw\u00e9 sak kab\u00e8h pangowahan sain-teknologi manungsa lan masalah duw\u00e9 gedh\u00e9 luwih amerga tambah- tambah\u00e9 jumlah penduduk lan gaya hidup manungsa sing luwih mewah.Sawetara bisa dilihat manawa dunya wis duw\u00e9 sak kab\u00e8h pangowahan sain lan teknologi saka manungsa, lan masalah-masalah duw\u00e9 gedh\u00e9 luwih amerga tambah-tambah\u00e9 jumlah pen- duduk lan gaya hidup manungsa sing luwih mewah Javanese The correlation between brain pathol- ogy and behaviour supports scientists in their research.Sesambungan antarane elmu lelara utek lan tumindak ndhukung para il- muwan kanggo nliti.Korelasi antara patologi otak dan peri- laku mendukung para ilmuwan dalam penelitian mereka.Korelasi antara patologi otak dan tingkah laku mendukung para il- muwan dalam penelitian mereka. Javanese Like some other experts, he is skep- tical about whether diabetes can be cured, noting that these \ufb01ndings have no relevance to people who already have Type 1 diabetes.Kaya sawetara ahli liyane, dheweke ora yakin apa diabetes bisa ditam- bani, nimbang yen panemuan kasebut ora duwe hubungane karo uwong sing uwis ngalami diabetes Jinis 1.Sebagaimana beberapa ahli lainnya, dia skeptis tentang apakah diabetes dapat disembuhkan, dengan catatan bahwa temuan ini tidak relevan bagi orang-orang yang sudah menderita di- abetes tipe 1.Sekaya karo para ahli liya, dia luwih leres saka wether diabetes bisa diben- erake, nggandeng rembug kuwi ora duwe arti kanggo wong-wong sing wis duwe diabetes tipe 1. Sundanese Although three people were inside the house when the car impacted it, none of them were hurt.Sanaos tilu urang aya di jero bumi pas mobil na nabrak, teu aya anu cidera.Walaupun tiga orang berada di dalam rumah saat mobil menabraknya, tidak ada yang terluka.Walaupun ada tiga orang di dalam rumah saat mobil menabraknya, tak ada yang terluka. Sundanese 34 per cent of those in the poll share this view, wanting Queen Elizabeth II to be Australia\u2019s last monarch.34 persen jalma dina polling di gaduh pendapat ieu, nyaeta hoyong Ratu Elizabeth II janten monarkis pa- mungkas di Australia.34 persen dari mereka yang terlibat dalam survei memiliki pandangan ini, yaitu ingin Ratu Elizabeth II menjadi ratu terakhir Australia.34 persen dari mereka yang terli- bat dalam survei memiliki pandangan Sundanese Although three people were inside the house when the car impacted it, none of them were hurt.Sanaos tilu urang aya di jero bumi pas mobil na nabrak, teu aya anu cidera.Walaupun tiga orang berada di dalam rumah saat mobil menabraknya, tidak ada yang terluka.Walaupun ada tiga orang di dalam rumah saat mobil menabraknya, tak ada yang terluka. Sundanese 34 per cent of those in the poll share this view, wanting Queen Elizabeth II to be Australia\u2019s last monarch.34 persen jalma dina polling di gaduh pendapat ieu, nyaeta hoyong Ratu Elizabeth II janten monarkis pa- mungkas di Australia.34 persen dari mereka yang terlibat dalam survei memiliki pandangan ini, yaitu ingin Ratu Elizabeth II menjadi ratu terakhir Australia.34 persen dari mereka yang terli- bat dalam survei memiliki pandangan yang sama, yaitu ingin Ratu Elizabeth II menjadi ratu terakhir di Australia. Sundanese The qualities that determine a subcul- ture as distinct may be linguistic, aes- thetic, religious, political, sexual, geo- graphical, or a combination of factors.Kualitas anu nangtukeun subkultur salaku b\u00e9dana panginten basa, \u00e9st\u00e9tis, religius, politik, s\u00e9ks, g\u00e9ogra\ufb01s, atanapi kombinasi faktor.Sifat-sifat yang menentukan subkultur sebagai terpisah mungkin bersifat lin- guistik, estetika, agama, politik, sek- sual, geogra\ufb01s, atau kombinasi dari faktor-faktor tersebut.Sifat-sifat nu nentukkeun subkultur salaku nu b\u00e9da bisa jadi linguistik, estetika, agama, politik, seksual, ge- ogra\ufb01s, atawa kombinasi ti faktor- faktor ieu. Sundanese New Kingdom ancient Egyptians mar- velled at their predecessors monu- ments, which were then well over a thousand year old.Karajaan anyar urang Mesir kuno takjub kana monum\u00e9n paninggalan karuhunna, anu umurna langkung ti sarebu taun.Para raja anyar dina Egypt kuno nyo- gog nara-nara jaman awal, anu dina jaman ayeuna kira-kira ribuan taun.Para raja anyar dina Egypt kuno terkagum-kagum kana nara-nara candi jaman awal, anu dina jaman ayeuna kira-kira leuweung taun. Sundanese The U.S. Corps of Engineers esti- mated that 6 inches of rainfall could breach the previously damaged lev- ees.Korps insinyur A.S ngira-ngira curah hujan 6 inci tiasa nembus tanggul anu samemehna geus ruksak.Korps Engineer AS diperkirakan bahwa 6 inci hujan bisa merusak ben- teng yang sebelumnya sudah rusak.Korps Engineer AS diperkirakan bahwa hujan sebesar 6 inci dapat merusak benteng yang sudah rusak se- belumnya. Sundanese Several large television screens were takjub kana monum\u00e9n paninggalan karuhunna, anu umurna langkung ti sarebu taun.Para raja anyar dina Egypt kuno nyo- gog nara-nara jaman awal, anu dina jaman ayeuna kira-kira ribuan taun.Para raja anyar dina Egypt kuno terkagum-kagum kana nara-nara candi jaman awal, anu dina jaman ayeuna kira-kira leuweung taun. Sundanese The U.S. Corps of Engineers esti- mated that 6 inches of rainfall could breach the previously damaged lev- ees.Korps insinyur A.S ngira-ngira curah hujan 6 inci tiasa nembus tanggul anu samemehna geus ruksak.Korps Engineer AS diperkirakan bahwa 6 inci hujan bisa merusak ben- teng yang sebelumnya sudah rusak.Korps Engineer AS diperkirakan bahwa hujan sebesar 6 inci dapat merusak benteng yang sudah rusak se- belumnya. Sundanese Several large television screens were installed in various places in Rome to let the people watch the ceremony.Sababaraha tipi ageung dipasang di sababaraha tempat di Roma supados warga tiasa nyaksian upacara \u00e9ta.Beberapa layar televisi besar dipasang di berbagai tempat di Roma agar orang-orang bisa menonton upacara.Sabag\u00e9an layar televisi gaduh di- pasang di hiji bilik roma sinaunna pikeun ngajadikeun jalma bisa nonton upacara. Sundanese Pests can spoil food, cause irritation, or in a worse case cause allergic re- actions, spread venom, or transmit in- fections.Hama tiasa ngarusak dahareun, nyieun iritasi, atawa leuwih parah deui tiasa nyieun al\u00e9rgi, nyebarkeun racun, atawa nularkeun panyakit.Hama bisa merusak makanan, menye- babkan iritasi, atau dalam kasus yang lebih buruk menyebabkan reaksi alergi, menyebarkan racun, atau menularkan infeksi.Hama bisa merusak makanan, menye- babkan iritasi, atau dalam kasus yang lebih buruk menyebabkan reaksi alergi, menyebarkan racun, atau menularkan infeksi. Sundanese It is obvious enough that the world has changed much because of hu- mankind\u2019s scienti\ufb01c and technologi- cal advancements, and problems have become greater because of overpop- ulation and mankind\u2019s extravagant lifestyle.Jelas pisan y\u00e9n dunya parantos seueur robih kusabab kamajuan \u00e9lmu manusa sareng t\u00e9knologi, sareng masalahna janten langkung ageung kusabab seueur penduduk sareng gaya hirup anu boros umat manusa.Nya\u00e9ta nujul bisa ngeuyeuk y\u00e9n dunya geus robah heubeul dina jaman sains jeung teknologi manusa, jeung masalah geus ngaronjat luyu ku luwe- ung laju penduduk jeung gaya hidup manusa anu boros.Nya\u00e9ta nujul bisa",
    "215": "ngeuyeuk y\u00e9n dunya geus robah heubeul dina jaman sains jeung teknologi manusa, jeung alergi, menyebarkan racun, atau menularkan infeksi. Sundanese It is obvious enough that the world has changed much because of hu- mankind\u2019s scienti\ufb01c and technologi- cal advancements, and problems have become greater because of overpop- ulation and mankind\u2019s extravagant lifestyle.Jelas pisan y\u00e9n dunya parantos seueur robih kusabab kamajuan \u00e9lmu manusa sareng t\u00e9knologi, sareng masalahna janten langkung ageung kusabab seueur penduduk sareng gaya hirup anu boros umat manusa.Nya\u00e9ta nujul bisa ngeuyeuk y\u00e9n dunya geus robah heubeul dina jaman sains jeung teknologi manusa, jeung masalah geus ngaronjat luyu ku luwe- ung laju penduduk jeung gaya hidup manusa anu boros.Nya\u00e9ta nujul bisa ngeuyeuk y\u00e9n dunya geus robah heubeul dina jaman sains jeung teknologi manusa, jeung masalah geus ngaronjat luyu ku luwe- ung laju penduduk jeung gaya hidup manusa anu boros. Sundanese The correlation between brain pathol- ogy and behaviour supports scientists in their research.Hubungan patologi polo sareng kalakuan ngabantos para \u00e9lmuwan dina panalungtikanna.Korelasi antara patologi otak dan peri- laku mendukung para ilmuwan dalam penelitian mereka.Korelasina antara patologi otak jeung ulah-ulahan ngalapkeun dukungan sakurang-kurangna pikeun para ilmuwan dina penelitian maran\u00e9hanana. Sundanese Like some other experts, he is skep- tical about whether diabetes can be cured, noting that these \ufb01ndings have no relevance to people who already have Type 1 diabetes.Sapertos sababaraha ahli anu san\u00e9s, anjeunna henteu percanteun upami diab\u00e9t\u00e9s tiasa disembuhkeun, kus- abab pamanggihan ieu t\u00e9h henteu aya hubunganana jeung jalma anu paran- tos gaduh diab\u00e9t\u00e9s tipe 1.Seperti beberapa ahli lainnya, dia skeptis tentang apakah diabetes bisa disembuhkan, mencatat bahwa temuan ini tidak relevan bagi orang- orang yang sudah memiliki diabetes tipe 1.Kayaku ngan ahli s\u00e9j\u00e9n, dia b\u00e9r\u00e9k- b\u00e9r\u00e9k ngeunaan jangdi diabetes bisa diobat, ngeunaan y\u00e9n kajadian ieu teu aya hubunganna jeung jalma anu geus ngalami diabetes tipe 1.\" Table 20: Examples of ChatGPT translated and post-edited sentences. E Evaluation Results for Reasoning We provide the complete results for reasoning tasks on Table 21. Categories Testset Result DeductiveENTAILMENTBANK bAbI (task 15)28/30 28/30 (as is - 19/30) InductiveCLUTRR bAbI (task16)13/30 20/30 (as is - 0/30) Abductive NLI 26/30 Mathematical Math 13/30 Temporal Timedial 26/30 SpatialSpartQA (hard) SpartQA (basic) StepGame (hard) StepGame (basic) StepGame (basic-cardinal) skeptis tentang apakah diabetes bisa disembuhkan, mencatat bahwa temuan ini tidak relevan bagi orang- orang yang sudah memiliki diabetes tipe 1.Kayaku ngan ahli s\u00e9j\u00e9n, dia b\u00e9r\u00e9k- b\u00e9r\u00e9k ngeunaan jangdi diabetes bisa diobat, ngeunaan y\u00e9n kajadian ieu teu aya hubunganna jeung jalma anu geus ngalami diabetes tipe 1.\" Table 20: Examples of ChatGPT translated and post-edited sentences. E Evaluation Results for Reasoning We provide the complete results for reasoning tasks on Table 21. Categories Testset Result DeductiveENTAILMENTBANK bAbI (task 15)28/30 28/30 (as is - 19/30) InductiveCLUTRR bAbI (task16)13/30 20/30 (as is - 0/30) Abductive NLI 26/30 Mathematical Math 13/30 Temporal Timedial 26/30 SpatialSpartQA (hard) SpartQA (basic) StepGame (hard) StepGame (basic) StepGame (basic-cardinal) StepGame (diagonal) StepGame (clock-direction)8/32 20/32 7/30 19/30 17/20 11/20 5/20 CommonsenseCommonsenseQA PIQA Pep-3k (Hard)27/30 25/30 28/30 Causal E-Care 24/30 Multi-hop hotpotQA 8/30 Analogical Letter string analogy 30/30 Table 21: Composed results for all reasoning tasks. F Multi-turn for Task-Oriented Dialogue We provide the example for the modular and uni\ufb01ed approaches for Task-Oriented Dialogue in Table 22 and Table 23, respectively. Task Key Text Content Dialogue State TrackingPromptGive the dialogue state of the last utterance in the following dialogue in the form of \u2019STATE: Domain-Intent: [Slot, Possible value], ... (for example: STATE: Hotel-Inform: [\u2019area\u2019, \u2019centre\u2019]) by using the following pre-de\ufb01ned slots and possible values: Intents: Request, Inform, general-thank, general-bye Domain: hotel, Slots: pricerange, Possible values: [\u2019expensive\u2019, \u2019cheap\u2019, \u2019moderate\u2019] Domain: hotel, Slots: type, Possible values: [\u2019guesthouse\u2019, \u2019hotel\u2019] Domain: hotel, Slots: parking, Possible values: [\u2019free\u2019, \u2019no\u2019, \u2019yes\u2019] Domain: hotel, Slots: bookday, Possible values: [\u2019monday\u2019, \u2019tuesday\u2019, \u2019wednesday\u2019, \u2019thursday\u2019, \u2019friday\u2019, \u2019saturday\u2019, \u2019sunday\u2019] Domain: hotel, Slots: bookpeople, Possible values: [\u20191\u2019, \u20192\u2019, \u20193\u2019, \u20194\u2019, \u20195\u2019, \u20196\u2019, \u20197\u2019, \u20198\u2019] Domain: hotel, Slots: bookstay, Possible values: [\u20191\u2019, \u20192\u2019, \u20193\u2019, \u20194\u2019, \u20195\u2019, \u20196\u2019, \u20197\u2019, \u20198\u2019] Domain: hotel, Slots: stars, Possible values: [\u20190\u2019, \u20191\u2019, \u20192\u2019, \u20193\u2019, \u20194\u2019, \u20195\u2019] Domain: hotel, Slots: internet, Possible values: [\u2019free\u2019, \u2019no\u2019, \u2019yes\u2019] Domain: hotel, Slots: name, Possible values: [] Domain: hotel, Slots: area, Possible values: [\u2019centre\u2019, \u2019east\u2019, \u2019north\u2019, \u2019south\u2019, \u2019west\u2019] Domain: hotel, Slots: address, Possible values: [?] Domain: hotel, Slots: type, Possible values: [\u2019guesthouse\u2019, \u2019hotel\u2019] Domain: hotel, Slots: parking, Possible values: [\u2019free\u2019, \u2019no\u2019, \u2019yes\u2019] Domain: hotel, Slots: bookday, Possible values: [\u2019monday\u2019, \u2019tuesday\u2019, \u2019wednesday\u2019, \u2019thursday\u2019, \u2019friday\u2019, \u2019saturday\u2019, \u2019sunday\u2019] Domain: hotel, Slots: bookpeople, Possible values: [\u20191\u2019, \u20192\u2019, \u20193\u2019, \u20194\u2019, \u20195\u2019, \u20196\u2019, \u20197\u2019, \u20198\u2019] Domain: hotel, Slots: bookstay, Possible values: [\u20191\u2019, \u20192\u2019, \u20193\u2019, \u20194\u2019, \u20195\u2019, \u20196\u2019, \u20197\u2019, \u20198\u2019] Domain: hotel, Slots: stars, Possible values: [\u20190\u2019, \u20191\u2019, \u20192\u2019, \u20193\u2019, \u20194\u2019, \u20195\u2019] Domain: hotel, Slots: internet, Possible values: [\u2019free\u2019, \u2019no\u2019, \u2019yes\u2019] Domain: hotel, Slots: name, Possible values: [] Domain: hotel, Slots: area, Possible values: [\u2019centre\u2019, \u2019east\u2019, \u2019north\u2019, \u2019south\u2019, \u2019west\u2019] Domain: hotel, Slots: address, Possible values: [?] Domain: hotel, Slots: phone, Possible values: [?] Domain: hotel, Slots: postcode, Possible values: [?] Domain: hotel, Slots: ref, Possible values: [?] USER: I need a \ufb01ve starts hotel close to a mall and main restaurants. The hotel should include free wi\ufb01 in the room. STATE : \u2019Hotel-Inform\u2019: [[\u2019internet\u2019, \u2019yes\u2019], [\u2019stars\u2019, \u20195\u2019]] SYSTEM: I\u2019m sorry, but there are no 5-star hotels anywhere in Cambridge. Would you like to broaden your criteria? USER: How about something with 4 stars and something expensive. STATE : \u2019Hotel-Inform\u2019: [[\u2019pricerange\u2019, \u2019expensive\u2019], [\u2019stars\u2019, \u20194\u2019]] SYSTEM: I have 2 options for you - The Huntingdon Marriott on the west side, and the University Arms in the center. Are you interested in one of these? USER: Maybe. Is either one a 4 star hotel? If so, I\u2019d like to book a room for 4 nights. STATE : \u2019Hotel-Inform\u2019: [[\u2019bookstay\u2019, \u20194\u2019], [\u2019stars\u2019, \u20194\u2019]] SYSTEM: Both are 4 stars. How many days would you be staying, and how many people are in your party? I can see if something is available. USER: I need a reservation for 4 nights starting Saturday. Gold STATE : \u2019Hotel-Inform\u2019: [[\u2019bookday\u2019, \u2019saturday\u2019], [\u2019stars\u2019, \u20194\u2019]] ChatGPT GenerationSTATE: \u2019Hotel-Inform\u2019: [[\u2019bookday\u2019, \u2019saturday\u2019], [\u2019bookstay\u2019, \u20194\u2019], [\u2019stars\u2019, \u20194\u2019]] Response Generation using ActionsPromptContinue the dialogue as a task-oriented dialogue system called SYSTEM. The answer of SYSTEM should follow the ACTION provided next while answering the USER\u2019s last utterance: ACTION: \u2019Restaurant-Inform\u2019: [[\u2019phone\u2019, \u201901223316074\u2019], [\u2019pricerange\u2019, \u2019cheap\u2019]] USER: Where is a 4 star hotel located in North Cambridge? STATE : \u2019Hotel-Inform\u2019: [[\u2019bookstay\u2019, \u20194\u2019], [\u2019stars\u2019, \u20194\u2019]] SYSTEM: Both are 4 stars. How many days would you be staying, and how many people are in your party? I can see if something is available. USER: I need a reservation for 4 nights starting Saturday. Gold STATE : \u2019Hotel-Inform\u2019: [[\u2019bookday\u2019, \u2019saturday\u2019], [\u2019stars\u2019, \u20194\u2019]] ChatGPT GenerationSTATE: \u2019Hotel-Inform\u2019: [[\u2019bookday\u2019, \u2019saturday\u2019], [\u2019bookstay\u2019, \u20194\u2019], [\u2019stars\u2019, \u20194\u2019]] Response Generation using ActionsPromptContinue the dialogue as a task-oriented dialogue system called SYSTEM. The answer of SYSTEM",
    "216": "should follow the ACTION provided next while answering the USER\u2019s last utterance: ACTION: \u2019Restaurant-Inform\u2019: [[\u2019phone\u2019, \u201901223316074\u2019], [\u2019pricerange\u2019, \u2019cheap\u2019]] USER: Where is a 4 star hotel located in North Cambridge? SYSTEM: I have several options, almost all of which are moderate guesthouses. I do have one cheap one, too. Would you like to book a room? USER: Sure , that could be nice SYSTEM: OK, how many are in your party, what day will you arrive, and how many nights will you be staying? USER: I actually don\u2019t need reservations I just need the phone number, price range. Gold SYSTEM: As I mentioned it is cheap and the phone number is 01223316074. ChatGPT GenerationSYSTEM: The phone number for the 4-star hotel located in North Cambridge is 01223316074 and it is in the cheap price range. Table 22: Examples of modular Task-Oriented Dialogue using ChatGPT: dialogue state tracking and response generation Task Key Text Content Multi-turn uni\ufb01ed approachPromptUse the following knowledge base to complete the task of \"recommending a restaurant\" by continuing the conversation as a task-oriented dialogue system: Restaurant: Mama Julia, Food: French, Price: Expensive, Location: 7th street, Rating: 5 Restaurant: Papa John, Food: Italian, Price: Moderate, Location: 3rd street, Rating: 4 Restaurant: The Crossroad, Food: Morocco, Price: Moderate, Location: Downtown, Rating: 2 Restaurant: Tacos City, Food: Mexian, Price: Cheap, Location: Center, Rating: 1 Restaurant: Golden Rice Bowl, Food: Chinese, Price: Cheap, Location: 3rd district, Rating: 3 Restaurant: Veggie Garden, Food: Chinese, Price: Expensive, Location: Town Hall, Rating: 4 Restaurant: Pizza House, Food: Italian, Price: Moderate, Location: 3rd street, Rating: 2 Restaurant: The Palace, Food: Vietnamese, Price: Expensive, Location: Hotel Grandview, Rating: 5 Hi, I\u2019m looking for a restaurant ChatGPT Sure, what type of food are you in the mood for? Restaurant: Papa John, Food: Italian, Price: Moderate, Location: 3rd street, Rating: 4 Restaurant: The Crossroad, Food: Morocco, Price: Moderate, Location: Downtown, Rating: 2 Restaurant: Tacos City, Food: Mexian, Price: Cheap, Location: Center, Rating: 1 Restaurant: Golden Rice Bowl, Food: Chinese, Price: Cheap, Location: 3rd district, Rating: 3 Restaurant: Veggie Garden, Food: Chinese, Price: Expensive, Location: Town Hall, Rating: 4 Restaurant: Pizza House, Food: Italian, Price: Moderate, Location: 3rd street, Rating: 2 Restaurant: The Palace, Food: Vietnamese, Price: Expensive, Location: Hotel Grandview, Rating: 5 Hi, I\u2019m looking for a restaurant ChatGPT Sure, what type of food are you in the mood for? Prompt I only want restaurants rated 3 or higher. Any suggestions? ChatGPTHere are some restaurants that have a rating of 3 or higher: Mama Julia (French, Expensive, 5 rating), Papa John (Italian, Moderate, 4 rating), Golden Rice Bowl (Chinese, Cheap, 3 rating), Veggie Garden (Chinese, Expensive, 4 rating), and The Palace (Vietnamese, Expensive, 5 rating). Which one would you like to know more about? Prompt What food does The Palace serve? ChatGPT The Palace serves Vietnamese cuisine. Prompt Is Taxos City expensive to eat at? ChatGPT Tacos City is considered as cheap restaurant. Prompt I want to eat Italian food, any recommendations? ChatGPTBased on the information provided in the knowledge base, I would recommend Papa John and Pizza House for Italian food. Papa John is rated 4 out of 5 and is classi\ufb01ed as \"Moderate\" in terms of price, while Pizza House is rated 2 out of 5 and is also classi\ufb01ed as \"Moderate\". Papa John is located on 3rd street, while Pizza House is located on 3rd street as well. Prompt Did you forget I only wanted restaurants rated 3 or higher? ChatGPTI apologize, I must have misunderstood your previous request. Based on the knowledge base provided, The only Italian restaurant that is rated 3 or higher is Papa John which is rated 4 out of 5 and is located on 3rd street. It is classi\ufb01ed as \"Moderate\" in terms of price. Prompt Great thanks for your help. ChatGPTYou\u2019re welcome! I hope you enjoy your meal at Papa John. If you have any other questions or need further assistance, feel free to ask. Table 23: Example of multi-turn uni\ufb01ed approach for Task-Oriented Dialogue using ChatGPT Part-I: What is an Agent? Short Answer: Text-to-Task An LLM agent is an AI system that utilizes a LLM as its core computational engine to exhibit capabilities beyond text generation. Exercise: Create an agent that can do some math Do this exercise (18 mins) to understand what an agent does Code: https://github.com/rohanbalkondekar/finetune_llama2 Data: rohanbalkondekar/generate_json Part-II: RAG overview Lets review the RAG design pattern and discuss its limitations on analytical questions. Also cover a more versatile architecture that overcomes these limitations. Overview of RAG RAG solutions are inspired by representation learning and semantic search ideas that have been gradually adopted in ranking problems (for example, recommendation and search) and natural language processing (NLP) tasks since 2010. The popular approach used today is formed of three steps: 1. An offline batch processing job ingests documents from an input knowledge base, splits them into chunks, creates an embedding for each chunk to represent its semantics using a pre-trained embedding model, such as Amazon Titan embedding models, then uses these embeddings as input to create a semantic search index. 2. When answering a new question in real time, the input question is converted to an embedding, which is used to search for and extract the most similar chunks of documents using a similarity metric, such as cosine similarity, and an approximate nearest neighbors algorithm. The search precision can also be improved with metadata filtering. 3. A prompt is constructed from the concatenation of a system message with a context that is formed of the relevant chunks of documents extracted in step 2, and the input question itself. This prompt is then presented to an LLM model to generate the final answer to the question from the context. With the right underlying embedding model, capable of producing accurate semantic representations of the input document chunks and the input questions, and an efficient semantic search module, this solution is able to answer questions that require retrieving existent information in a database of documents. For example, if you have a service or a product, you could start by indexing its FAQ section or documentation and have an initial conversational AI tailored to your specific offering. PART-III: RAG LIMITATIONS on Semantic Search (Multi Document) 1. Mul' Document Although RAG is an essential component in modern domain-specific AI assistants and a sensible starting point for building a conversational AI around a specialized knowledge base, it can\u2019t answer questions that require: I. Scanning II. Comparing III. Reasoning across all documents in your knowledge base simultaneously, especially when the augmentation is based solely on semantic search. A. First Check: Single document as knowledge base (complex data) https://ai.gopubby.com/advanced-rag-semi-structured-data-with-langchain-ce46c8baa6cf https://ai.gopubby.com/advanced-rag-multi-modal-rag-with-gpt4-vision-e4c11229682c B. Advanced RAG \u2013 Multi Documents Agent with LlamaIndex Sample implementation: https://github.com/sugarforever/Advanced-RAG/blob/main/03_llama_index_multi_doc_agent.ipynb \u2022 VectorStoreIndex \u2022 SummaryIndex \u2022 ObjectIndex \u2022 QueryEngineTool \u2022 OpenAIAgent \u2022 FnRetrieverOpenAIAgent Full Explanation https://blog.gopenai.com/advanced-rag-multi-documents-agent-with-llamaindex-43b604f84909 Additional Resources from Official Documentation https://docs.llamaindex.ai/en/stable/examples/agent/multi_document_agents.html In this guide, you learn towards setting up an agent that can effectively answer different types of questions over a larger set of documents. These questions include the following \u2022 QA over a",
    "217": "specific doc \u2022 QA comparing different docs \u2022 Summaries over a specific odc \u2022 Comparing summaries between different docs We do this with the following architecture: \u2022 Setup a \u201cdocument agent\u201d over each Document: each doc agent can do QA/summarization within its doc \u2022 Setup a top-level agent over this set of document agents. Do tool retrieval and then do CoT over the set of tools to answer a question. If you want additional features: https://docs.llamaindex.ai/en/stable/examples/agent/multi_document_agents-v1.html \u2022 Reranking during document (tool) retrieval \u2022 Query planning tool that the agent can use to plan We do this with the following architecture: \u2022 setup a \u201cdocument agent\u201d over each Document: each doc agent can do QA/summarization within its doc \u2022 setup a top-level agent over this set of document agents. Do tool retrieval and then do CoT over the set of tools to answer a question. PART-IV: RAG LIMITATIONS on Semantic Search (STRUCTURED DATA) Entity extraction, SQL querying, and agents with Amazon Bedrock Full Stack Example with Public Data"
}